issue_id,author,issue_type,timestamp,title,body,community
11,apaszke,pr,2016-09-07T17:20:21Z,"add multiprocessing unit tests, working file-descriptor based solution and OSX","**Don't merge, this PR is only for test purposes**
",pytorch
12,apaszke,pr,2016-09-07T17:20:55Z,Initial utils implementation + bug fixes,"**Don't merge, this PR is only for test purposes**
",pytorch
24,apaszke,pr,2016-09-14T02:20:02Z,More modules for nn + improvements in CUDA tests,"**Do not merge. This PR is just for review and test purposes.**
",pytorch
33,apaszke,pr,2016-09-15T20:04:11Z,Add more functions to autograd,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
34,apaszke,pr,2016-09-15T21:12:26Z,Various improvements and fixes,"**Do not merge. This PR is only for review and test purposes.**
- Type conversions now use auto gpu (#25)
- Printing tensors and storages with `inf`s and `nan`s no longer raises errors (#27)
- Error messages of tensor methods are now much better (#26)
- Storages can be now converted to different types
- CUDA storages now have a `getDevice()` method
",pytorch
37,apaszke,pr,2016-09-16T21:28:40Z,Add more functions to autograd,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
38,apaszke,pr,2016-09-16T22:25:09Z,Fix multiprocessing on OS X,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
40,apaszke,pr,2016-09-19T16:19:56Z,Codemod to remove camel case method naming,"Also, changed the default tensor type to float.

**Do not merge. This PR is only for review and test purposes.**
",pytorch
45,apaszke,pr,2016-09-21T15:38:03Z,Refactor _C extension to export some utilities,,pytorch
47,apaszke,pr,2016-09-22T03:44:47Z,"Improve storage, tensor and module C error messages + fix for dl flags in nightly python","**Do not merge. This PR is only for review and test purposes.**
",pytorch
50,apaszke,pr,2016-09-24T01:07:29Z,Improvements in torch.nn,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
51,apaszke,pr,2016-09-25T19:27:11Z,Various improvements,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
58,apaszke,pr,2016-09-27T00:04:06Z,Add data parallel functions to nn,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
62,apaszke,pr,2016-09-28T17:42:33Z,Add ffi utils for user C extensions,"**Do not merge. This PR is only for review and test purposes.**

This change depends on the upcoming changes to `torch.cuda` (right now tests and ffi utils always import and use this module).

Example:

``` python
# build_ext.py
import glob
from torch.utils.ffi import compile_extension
compile_extension('ext.my_lib', 'src/exported_api.h', glob('src/*.c'))
```

``` python
# main.py
import torch
from ext import my_lib
my_lib.my_fn(torch.randn(5, 5))
```
",pytorch
69,apaszke,pr,2016-09-29T16:39:47Z,PRNG related stuff + RReLU implementation,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
70,apaszke,pr,2016-09-29T19:04:29Z,Error message improvements + small test fix,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
74,apaszke,pr,2016-09-30T18:34:24Z,Add more functions to autograd,,pytorch
75,apaszke,pr,2016-09-30T20:10:32Z,Fixes for TH error handlers,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
93,apaszke,pr,2016-10-03T04:36:29Z,Bug fixes,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
96,apaszke,pr,2016-10-03T14:35:12Z,Fix Variable indexing bugs,,pytorch
98,apaszke,pr,2016-10-04T02:14:12Z,Fix memory leak when constructing a tensor from numpy,,pytorch
99,apaszke,pr,2016-10-04T18:56:18Z,Serialization improvements,"- save all data in little endian order
- allow to remap storage locations at load time
- add `parameter_dict()` method that returns all model parameters in a dict (using dot notation as keys)

**Do not merge. This PR is only for review and test purposes.**
",pytorch
100,apaszke,pr,2016-10-05T01:25:20Z,Allow specifying per-parameter optimization parameters,"If one wants to specify different optimization parameters on per-variable basis they can do it like this now:

``` python
optimizer = optim.SGD([
    {'params': model.base.parameters(), 'lr': 1e-1},
    {'params': model.classifier.parameters(), 'momentum': 0.9}
], lr=1e-3) # this lr will be the default for all groups that don't specify it
```

**Do not merge. This PR is only for review and test purposes.**
",pytorch
103,apaszke,pr,2016-10-05T15:27:23Z,Autograd fixes,"**WARNING: This PR changes the default of `requires_grad` flag in Variables.** It can possibly break a bunch of code, but is a more reasonable default, since most Variables created explicitly by users don't really require grad.

**Do not merge. This PR is only for review and test purposes.**
",pytorch
105,apaszke,pr,2016-10-05T17:47:43Z,Add versioning and shared storage handling to autograd,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
112,apaszke,pr,2016-10-09T01:09:29Z,Add LAPACK functions + small fixes in error messages,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
113,apaszke,pr,2016-10-09T22:58:10Z,Improvements in torch and torch.nn,"- FullConv -> ConvTranspose
- ConvTranspose and MaxUnpooling support an additional `output_size` argument to their call operators

``` python
unpool(input, indices, output_size=(100, 100))
# or
unpool(input, indices, output_size=original_input.size())
```
- torch.cat was simplified. New signature is just `torch.cat(sequence tensors[, int dimension])`. Also, I've removed the method version, which was incorrect (concatenated all given tensors in `self`).
- Cuda OOM errors used to trigger an invalid argument error message in the tensor constructor, now they are correctly propagated.

**Do not merge. This PR is only for review and test purposes.**
",pytorch
115,apaszke,pr,2016-10-11T18:51:20Z,Major improvements to autograd + a couple of CUDA improvements,"This is a huge modification to autograd. All core functions and types are moved to C (however they can still be used in python using the same interface - see `BasicEngine`). This improves RNN training speed on Penn Treebank more than 2x (and can actually saturate a GPU with a small RNN during backward).
",pytorch
117,apaszke,pr,2016-10-12T15:37:37Z,Add keyword argument support to most tensor functions,"**Do not merge. This PR is only for review and test purposes.**
",pytorch
161,apaszke,pr,2016-10-24T08:45:25Z,Lots of small fixes,"- python2.7 compatibility and cffi version checks in `torch.utils.ffi` (#156, #157)
- platform checks in `torch.cuda` (failed on macOS)
- don't build nccl on macOS
- bugs in `torch.legacy.nn`
- `.train()` and `.evaluate()` for modules and containers
- 0-based random (#123)
- stateless functions no longer print ""type type doesn't implement stateless methods"" when a method resolution fails
- added `torch.from_numpy` (#159)
- fixed clang warnings
- tests are now deterministic (#125)
- added tensor values buffering when constructing CUDA tensors from sequences (#158)
- show exponent when printing vectors (#147)
",pytorch
170,apaszke,pr,2016-10-26T13:31:50Z,Add Parameter class to nn,"After this change assigning Variables as attributes doesn't add them as parameters. To do so, one needs to create them as `torch.nn.Parameter` instances.
",pytorch
172,apaszke,pr,2016-10-26T20:11:29Z,Add support for indexing with ellipsis,,pytorch
176,apaszke,pr,2016-10-28T17:42:42Z,Issue fixes,"- Added support for indexing with numpy scalar types (#165)
- Renamed training and evaluation Module methods (#168)
- CUDA tensors now include GPU id in their `__repr__` (#92)
- Fixed bugs in variable's `__setitem__` (for 1D Variables) and `__getitem__` will never be a CUDA sync point now (#140)
",pytorch
177,apaszke,pr,2016-10-28T18:49:02Z,Allow to not use all function outputs in autograd,"Also, implement `variable.no_grad()` that returns a new variable with the same data, but with `requires_grad == False` (#166).
",pytorch
192,apaszke,pr,2016-11-01T12:25:27Z,Fixed refcounting bugs + minor improvements,"* Fix segfaults when deserializing variables (#188)
* Improve Parameter's `__repr__` (#182)
* Accept file paths in `torch.save` and `torch.load` (#190)
* Fix a bug in criterion's backward (#184).",pytorch
195,apaszke,pr,2016-11-02T08:25:29Z,Look for libcudart in default CUDA installation paths,Fixes #153.,pytorch
196,apaszke,pr,2016-11-02T15:19:00Z,Add more optimizers,The tests check convergence on the Rosenbrock function and compare steps with legacy implementations. (#175),pytorch
204,apaszke,pr,2016-11-05T21:38:48Z,Fix bugs in legacy.nn and expose blas operations as tensor methods,,pytorch
207,apaszke,pr,2016-11-08T12:39:32Z,Bug fixes,"Fixes CPU tests in Python 3.3 and 3.4, and resolves #144.",pytorch
209,apaszke,pr,2016-11-08T18:43:56Z,0mp implement buffer protocol for storage,@0mp ,pytorch
210,apaszke,pr,2016-11-08T20:05:08Z,Implement functional interface for Variables (torch.*),fixes https://github.com/pytorch/pytorch/issues/206,pytorch
224,apaszke,pr,2016-11-16T22:47:52Z,Fixes for functions that support no bias mode,Fixes #218.,pytorch
226,apaszke,pr,2016-11-17T19:21:19Z,Add cuDNN bindings for 2D transposed convolution,Fixes #219,pytorch
227,apaszke,pr,2016-11-17T21:19:03Z,Add half copy/conversions,Fixes #222 ,pytorch
239,apaszke,pr,2016-11-19T22:03:23Z,Fix reference cycles in autograd and relax semantics of save_for_backward and backward,"1. Adds `torch.multinomial` for CUDA tensors (#236)
2. Fixes reference cycles in autograd (if outputs are saved for backward) (#235)
3. `save_for_backward` now accepts `None` arguments, and `backward` can now return too many `grad_input`s, as long as the excessive values are `None` (#221)",pytorch
240,apaszke,pr,2016-11-19T23:00:07Z,Update THNN,,pytorch
243,apaszke,pr,2016-11-21T22:34:05Z,Fixes for data_parallel and numpy bridge,"* Fix errors when data_parallel was used with a single GPU (#234)
* Fix errors when data_parallel was used with multiple outputs (or inputs) (#242)
* Improve data_parallel tests
* Forbid constructing tensors from numpy arrays with negative strides. This will be fixed in the future, but it will require pushing some changes to TH. For now a readable error message is printed. (#230)",pytorch
248,apaszke,pr,2016-11-23T19:47:07Z,Add .t7 file reader,"`torch.utils.serialization.load_lua` can now open files saved with Lua Torch. It loads all modules available in `torch.legacy.nn` (except for SpatialContrastiveNormalization, that has some weird bug and gives different results).",pytorch
250,apaszke,pr,2016-11-24T12:21:27Z,Fixes for autograd bugs,"* `torch.max(var, 0)` did a max over the last dimension instead of the first (reported by Alex Conneau)
* `var.expand` now accepts `torch.Size` as argument (#249)
* Implemented in-place operators for variables 
* Fixed bug when the Variable constructor wasn't setting the error flags properly.
* Return accreal as correct python type (previously it was always float). (#251)
* **Modified the signature of `.max()`, `.median()` and other selection functions to _always_ return a tuple of values and indices. This is a breaking change.**",pytorch
282,apaszke,pr,2016-12-01T19:28:26Z,Batched fixes,"* Remove `adam`, `sgd`, etc. as attributes of `torch.optim` package
* Add `torch.cuda.set_device` (#260)
* Implement `__len__` for tensors (#270)
* Implement `.type()` for `torch.nn` modules (#272)
* Make `torch.randperm` always return a `torch.LongTensor` (#267)
* Improve cuDNN detection at build time (#280)
* Raise `IndexError` in tensors' `__getitem__`. (needed in #277)
* Allow saving dynamically defined modules (#278)
* Import most common packages (cuda, autograd, nn, optim) by default (#283) ",pytorch
290,apaszke,pr,2016-12-02T15:23:25Z,Allow returning changed gradients from the hooks,"* Allow returning changed gradients from the hooks (#262)
* Fix bmm for variables (#299)
* Adds dimension checks for `t()` and `t_()` (#301)
* Fixes #306.
* Fixes an issue with `narrow` docs (#291)
* Adds support for loading `tds` objects (#300)
* `train()` and `eval()` no longer change `requires_grad` flags of parameters.",pytorch
294,apaszke,pr,2016-12-08T21:40:39Z,Add support for stochastic functions in autograd,See #279 for the API design.,pytorch
295,apaszke,pr,2016-12-09T16:36:38Z,Add docstring support to cwrap,,pytorch
325,apaszke,pr,2016-12-17T10:59:48Z,Fix multinomial bug and decrease precision of normal test,"Fixes #323 and #320. Also, fixed one more bug in `expand_as`",pytorch
332,apaszke,pr,2016-12-19T22:18:39Z,Expose gather and equals for CUDA tensors,,pytorch
341,apaszke,pr,2016-12-21T20:59:05Z,Add autograd.backward,"```python
autograd.backward([var1, var2], [grad_var1, grad_var2])
```
does a single backward pass through a full graph preceding var1 and var2. Implements #335.",pytorch
342,apaszke,pr,2016-12-21T22:59:42Z,Implement comparison and logical operations for tensors,I tried to have them behave like numpy arrays. Fixes #296.,pytorch
360,apaszke,pr,2016-12-27T22:21:48Z,Small fixes,"* Variables are now picklable using protocols < 2 (#336)
* TypeError is raised instead of ValueError for invalid arguments
* Added support for negative dimensions in a few functions (we should support this everywhere at some point, but this will require some changes in cwrap) (#349, #355)
* `__len__`, `half` for Variables (#348, #356)
* Type checking in cwrap is now more strict - booleans will no longer be accepted as integer arguments (#330)",pytorch
364,apaszke,pr,2016-12-28T16:51:19Z,Accept outputs in out argument,"```python
torch.add(output, input1, input2)
torch.max(values, indices, tensor, 1)
```
becomes
```python
torch.add(input1, input2, out=output)
torch.max(tensor, 1, out=(values, indices))
a.add(b, out=c) # this was not possible before
```

`out` is a keyword only argument.",pytorch
368,apaszke,pr,2016-12-29T17:10:51Z,Docs improvements,"* Fixed all warnings from nn docs
* Fixed all warnings from optim docs
* Added autograd docs",pytorch
380,apaszke,pr,2016-12-30T18:22:17Z,Rename functions to _functions in all modules,"Also two small fixes:
* Normalize weights to probabilities in gradient formula of multinomial Function
* Moved `PixelShuffle` implementation to functional package",pytorch
387,apaszke,pr,2016-12-31T11:52:00Z,Add custom docs stylesheet,"Desktop:

<img width=""1371"" alt=""Main page"" src=""https://cloud.githubusercontent.com/assets/4583066/21577333/99c993fe-cf57-11e6-84a3-87da167f1020.png"">

<img width=""1372"" alt=""torch.optim example"" src=""https://cloud.githubusercontent.com/assets/4583066/21577334/a4b83c48-cf57-11e6-9f85-8dbf0020759f.png"">

<img width=""1370"" alt=""torch example"" src=""https://cloud.githubusercontent.com/assets/4583066/21577340/dc5d2154-cf57-11e6-8ede-b21172410587.png"">

Mobile:

<img width=""50%"" alt=""Main"" src=""https://cloud.githubusercontent.com/assets/4583066/21577462/930b8eec-cf5b-11e6-978d-a4ecb3798a86.PNG"">

<img width=""50%"" alt=""Menu"" src=""https://cloud.githubusercontent.com/assets/4583066/21577464/9504a22e-cf5b-11e6-9ec3-7ac636e6772f.PNG"">


",pytorch
388,apaszke,pr,2016-12-31T16:12:48Z,Multiprocessing improvements,"* Fixed broken tests on macOS.
* Variable's grad will never be shared across processes now.
* Added an additional check for Variable flags in `__init__`.
* Added `is_shared()` that returns `True` iff the tensor/storage data is in shared memory.
* Implemented missing cases of `__matmul__` (#384)
* Fixed issues with `chunk` (#369)",pytorch
389,apaszke,pr,2016-12-31T16:57:15Z,Doc css fixes for mobile and large screens,,pytorch
391,apaszke,pr,2017-01-02T00:27:07Z,Fix handling of leaf Variables in autograd,TODO: add docs about in-place ops on variables and their consequences,pytorch
397,apaszke,pr,2017-01-03T17:21:26Z,Fix invalidArguments to take kwargs and out into account,"`invalidArguments` now understands [type-annotation](https://docs.python.org/3/library/typing.html) like syntax, and checks the type of tuple elements (can be different) and sequence elements (have to be uniform).

I'm only wondering if we should show the `out` with its type in all options. Any other suggestions welcome.

Fixes #365.

Demos:

* invalid keyword arg
<img width=""911"" alt=""screen shot 2017-01-03 at 18 08 47"" src=""https://cloud.githubusercontent.com/assets/4583066/21616130/a48b4086-d1e0-11e6-9d2d-c225ce7685a9.png"">

* valid keyword arg, invalid type
<img width=""1085"" alt=""screen shot 2017-01-03 at 18 09 25"" src=""https://cloud.githubusercontent.com/assets/4583066/21616154/b72a9002-d1e0-11e6-9477-3c3f9a079910.png"">

* invalid `out` type
<img width=""1118"" alt=""screen shot 2017-01-03 at 18 16 55"" src=""https://cloud.githubusercontent.com/assets/4583066/21616191/db15d6fc-d1e0-11e6-98b9-33143f9d3ce1.png"">

* valid `out` type, invalid arguments
<img width=""1118"" alt=""screen shot 2017-01-03 at 18 09 55"" src=""https://cloud.githubusercontent.com/assets/4583066/21616203/e881eb50-d1e0-11e6-89ac-63d0a07df733.png"">

* invalid out tuple (wrong order)
<img width=""938"" alt=""screen shot 2017-01-03 at 18 10 20"" src=""https://cloud.githubusercontent.com/assets/4583066/21616226/fa14f038-d1e0-11e6-863f-190d67b06282.png"">

* valid out tuple, invalid arguments
<img width=""921"" alt=""screen shot 2017-01-03 at 18 10 34"" src=""https://cloud.githubusercontent.com/assets/4583066/21616244/06591c98-d1e1-11e6-9d13-dbae8938d10f.png"">

* invalid `cat` sequence
<img width=""681"" alt=""screen shot 2017-01-03 at 18 10 51"" src=""https://cloud.githubusercontent.com/assets/4583066/21616259/10e5bd60-d1e1-11e6-9faf-24d918b26a40.png"">

* valid `cat` sequence, invalid `dim`
<img width=""713"" alt=""screen shot 2017-01-03 at 18 14 35"" src=""https://cloud.githubusercontent.com/assets/4583066/21616274/216969c0-d1e1-11e6-8214-b42c3b1a6d50.png"">",pytorch
400,apaszke,pr,2017-01-03T22:01:47Z,Multiprocessing + autograd docs,,pytorch
405,ngimel,pr,2017-01-05T00:07:42Z,Fix rnn sphynx docs,,pytorch
445,ngimel,pr,2017-01-13T17:18:51Z,fix cudnn rnn batch_first with tests,"Fix for #441 with added batch_first tests, @adamlerer ",pytorch
448,ngimel,pr,2017-01-13T19:31:36Z,add cudnn deb package installation paths to cudnn discovery,,pytorch
453,ngimel,pr,2017-01-14T02:20:39Z,"if nccl is available, do not compile it and load system version",,pytorch
454,apaszke,pr,2017-01-15T17:51:20Z,Fixes and improvements,"* Improved multiprocessing docs
* Wrote optim docs
* Optimizers now check that `params` is not a Variable nor a tensor (#433)
* Dropout checks `p` argument (#403)
* cwrap no longer filters out valid `torch.topk` options (#432)
* `torch.randn(2, 3, 4, asdf=False)` now raises an error (earlier invalid kwargs were ignored)
* `nn.Container`s are now copyable (#415)",pytorch
455,apaszke,pr,2017-01-16T20:06:43Z,ðŸŽ‰ Breaking changes ðŸŽ‰,"* Removed `cmin`, `cmax` and `cinv` (functionality of `cmin`, `cmax` split between `max`/`min` and `clamp`; `cinv` renamed to `reciprocal`)
* Merged `nn.Container` code into `nn.Module` (`nn.Container` will now start showing deprecation warnings)",pytorch
457,apaszke,pr,2017-01-16T22:39:41Z,"Notes, docs, fixes",,pytorch
460,apaszke,pr,2017-01-17T19:08:19Z,Add torch.utils.data docs and improve notes,,pytorch
479,apaszke,pr,2017-01-18T15:53:44Z,Improve ffi utils,,pytorch
489,apaszke,pr,2017-01-18T23:27:18Z,Fix for non-contiguous from_numpy,Fixes #484.,pytorch
492,ngimel,pr,2017-01-19T05:56:17Z,Fix unpooling docs,,pytorch
512,apaszke,pr,2017-01-19T22:12:09Z,Docs improvements,,pytorch
515,ngimel,pr,2017-01-20T01:00:34Z,adding cudnn V6 support,"cudnn V6 rc is now available. This PR adds dilated convolution support via cudnn, and updates rnn calls for the new version. I understand it should not go on master branch, so let me know how best to handle it. ",pytorch
533,apaszke,pr,2017-01-21T11:57:46Z,Fix segfault when a None gradient was given to a hook,Fixes #531.,pytorch
539,apaszke,pr,2017-01-21T20:58:17Z,Port LBFGS from Lua optim,fixes #483 ,pytorch
546,apaszke,pr,2017-01-22T23:09:32Z,Fixes v3,Fixes #523 and #543.,pytorch
548,apaszke,pr,2017-01-23T00:03:42Z,Add more contiguity checks,Add checks where cuDNN assumes that the tensors are contiguous. All other unsupported configurations should give `CUDNN_STATUS_BAD_PARAM`.,pytorch
577,apaszke,pr,2017-01-24T20:43:21Z,Fixes and additions,"* Fixes segfault when `torch.Size` is constructed with invalid arguments (#555).
* Improve optimizer serialization and add `load_state_dict` (#549).
* Make `Variable`s non-comparable (#538).
* Add upsampling modules and functions to nn (#550)",pytorch
583,ngimel,pr,2017-01-25T01:42:37Z,add dockerfiles,"Add dockerfiles for building with cudnn v5 and v6, update README. ",pytorch
593,apaszke,pr,2017-01-25T23:01:33Z,Fixes and improvements,"* Fixed bug in ELU backward when `inplace=True` (#582)
* Tests can now be run with `--seed` flag that overrides the default seed.
* Added test for `BatchNorm` in evaluation mode (#590)
* Minor fix in autograd docs
* Support cc-like flags in cuDNN build tools (#573)
* Fixed `index_select` backward (failed when indices contained duplicate entries).",pytorch
614,apaszke,pr,2017-01-27T20:46:59Z,Fix travis builds,"Let the travis finish the build before merging, so we can be sure that it works ok now.",pytorch
620,apaszke,pr,2017-01-28T12:42:24Z,Fix non-contiguous grad handling in cuDNN RNN,"Hotfix for #619. I'll go over cuDNN RNN code once more, but just wanted to have this fixed.",pytorch
628,apaszke,pr,2017-01-28T22:35:58Z,Add more bernoulli options in cwrap,"Fixes #596, #623, #607 ",pytorch
636,apaszke,pr,2017-01-29T23:23:57Z,"Implement more autograd functions and fix sort, topk signature","**WARNING: this is a breaking change.**

I went over examples and tutorials and didn't find any occurences of `sort` and `topk` used on `Variable`.

Fixes #626 and implements all high priority functions from #440.",pytorch
647,apaszke,pr,2017-01-30T22:13:34Z,[EXPERIMENTAL] Initial version of distributed package,"It's not ready for usage yet, but rebasing that massive branch is a pain, so let's merge it for now. It prints a big warning when imported:

```
================================================================================
                                    WARNING
================================================================================
torch.distributed is a highly experimental package. The API will change without
notice and we're can't guarantee full correctness and expected performance yet.
We'll announce it once it will be ready.
```",pytorch
659,apaszke,pr,2017-01-31T19:13:34Z,Fix bug with inplace TH(CU)NN,,pytorch
661,apaszke,pr,2017-01-31T21:05:29Z,Some fixes and improvements,"* Added support for indexing tensors with LongTensors (#363)
* Added `torch.unbind` - splits tensor into a tuple, removing a given dimension (#427)
* Renamed `set_index` to `_set_index` (#469)
* Improved CUDA detection in THPP (it's not possible to force no cuda) (#638)",pytorch
664,thuyen,pr,2017-02-01T02:20:12Z,ByteTensor should be unsigned,Just little fix to the docs,pytorch
668,ngimel,pr,2017-02-01T20:44:11Z,use pseudo-fp16 for convolutions,should fix #520,pytorch
669,apaszke,pr,2017-02-01T21:37:39Z,Add ModuleList and ParameterList to nn,,pytorch
708,apaszke,pr,2017-02-09T19:55:20Z,Fix for non-contiguous grad_output in cuDNN conv,I'll improve the tests to include non-contiguous inputs and grad outputs soon.,pytorch
731,kashif,pr,2017-02-13T13:28:04Z,test on osx,,pytorch
732,kashif,pr,2017-02-13T13:50:20Z,std::min() requires same type,does not compile on osx,pytorch
734,apaszke,pr,2017-02-13T18:50:20Z,Bug fixes and test improvements,"* Add same device asserts for cuDNN (#720)
* Fix bug in `Module.zero_grad` (#719)
* Add CUDA double and half THNN backends (#716, #48)
* Add output shape checks in conv (#704)
* cuDNN RNN can be now backproped through multiple times (#703)
* Fix auto-gpu in `torch.cat` (#689)
* Add tests for non-contiguous inputs and gradients in nn
* Fixed some flaky tests
* Added some more asserts to cuDNN RNNs",pytorch
737,kashif,pr,2017-02-13T23:03:30Z,compile with cudart,"This makes it work on osx with cuda and cudnn. Else it give:

```
...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/torch/__init__.py"", line 45, in <module>
    from torch._C import *
ImportError: dlopen(/usr/local/lib/python2.7/site-packages/torch/_C.so, 10): Symbol not found: _cudaDeviceSynchronize
  Referenced from: /usr/local/lib/python2.7/site-packages/torch/_C.so
  Expected in: flat namespace
 in /usr/local/lib/python2.7/site-packages/torch/_C.so
```",pytorch
752,apaszke,pr,2017-02-15T20:23:06Z,Bug fixes,"* `torch.Size` methods convert return type from `tuple` back to `torch.Size` (#730)
* Fixed bug in `Engine::compute_dependencies` (#726)
* `detach()` now actually removes the creator (#685)
* Added `torch.__version__` (#742)",pytorch
753,ngimel,pr,2017-02-15T22:56:04Z,allow DataParallel to have tuple inputs on a single GPU,"...for consistency with multi-GPU case, useful for rnns.  ",pytorch
762,apaszke,pr,2017-02-16T17:51:58Z,"Test PR, do not merge",# DO NOT MERGE,pytorch
769,apaszke,pr,2017-02-17T12:43:16Z,Fix Engine::compute_dependencies,,pytorch
794,apaszke,pr,2017-02-20T12:19:25Z,Fixes and improvements,"* Added better error message for CUDA -> numpy conversions (#779)
* Added checks for type and size of rewards in StochasticFunction (#781)
* cuDNN RNN Function now returns a correct number of gradients (#776)
* input will be made contiguous only once in backward of cuDNN RNN (#621)
* DataParallel now supports multiple inputs (#649)
* Added `torch.nn.utils.clip_grad_norm` (#309)
* Added support for indexing with `None` and slices with positive steps (partially #229)
* Added support for numpy arrays in `default_collate` (#782)
* Fixed misspelling and made NLLLoss2d accept weights (#784)

NLLLoss2d test will be extremely flaky until torch/cunn#445 is merged.",pytorch
844,apaszke,pr,2017-02-25T00:02:01Z,Distributed updates,"I'm merging this, because we've already had conflicts in THPP.",pytorch
852,apaszke,pr,2017-02-25T23:08:27Z,Bug fixes,"* Expose stateless methods for `torch.cuda.HalfTensor` (#827)
* Prevent creation of reference cycles in autograd (#839)
* Fix for Byte and Long tensor indexing of Variables (#828)
* Reshape grad_output in backward of basic ops (#818)
* Add docs for `ModuleList` and `ParameterList` (#814)
* Allow using expand to broadcast/unsqueeze tensors
* Make indexing 100% compatible with numpy again. Unfortunately this rolls back part of the support we had (LongTensors can now be be used only as the sole argument to indexing functions).
* Make `Optimizer.load_state_dict` use its `__setstate__` so we can implement backward-compatible extensions for optimizers (#766, #810).",pytorch
855,apaszke,pr,2017-02-26T12:51:26Z,Expose torch.HalfTensor,"Fixes #838.

Test plans/ideas welcome.",pytorch
859,apaszke,pr,2017-02-26T21:45:38Z,Improve autograd memory usage,References to tensors given to `save_for_backward()` and `mark_non_differentiable()` weren't freed if the function didn't require gradient. This lead to an increased memory usage when large parts of graph didn't require grad (e.g. ResNet-152 finetuning with batch_size 16 used to consume the same amount of memory as batch 256 after this commit).,pytorch
868,apaszke,pr,2017-02-27T21:01:45Z,Fix one more reference cycle and ensure correct flag propagation,Fixes #863.,pytorch
873,apaszke,pr,2017-02-28T00:55:16Z,Add support for variable length sequences in RNNs,"The PR is still lacking the docs and pep8 fixes, so it's not ready for merge yet, but I wanted to get it out today, so it can be reviewed. I'll fix any comments tomorrow.

Fixes #789.

cc @jekbradbury ",pytorch
886,apaszke,pr,2017-03-01T15:55:17Z,Small bug fixes,"* Duplicate backward roots are now handled properly
* Added support for backprop through cuDNN RNN in evaluation mode
* cuDNN dropout descriptors will be updated when RNN module's `.dropout` attribute is changed. Additionally, they are now lazily initialized, so the descriptors won't even be seeded if `dropout == 0` (cuDNN is fine with that, so I assume it's ok).

@ngimel can you please take a look at the changes in the backend?",pytorch
887,apaszke,pr,2017-03-01T16:32:36Z,Add Nesterov Momentum,"Adds the Nesterov variant of momentum as an option for SGD.

By @ajbrock. I rebased the changes and added `__setstate__`.",pytorch
905,apaszke,pr,2017-03-02T23:18:34Z,Fix slicing with step,Slicing with steps that don't divide the length now work properly.,pytorch
912,apaszke,pr,2017-03-03T18:40:44Z,Fix issues with lazy grad initialization,,pytorch
982,apaszke,pr,2017-03-11T16:16:24Z,Bug fixes,"* Embedding with `max_norm` could have modified indices (#943)
* Removed `AutoGPU` from some functions (partially fixes #917)
* Fixed a memory leak in `torch.from_numpy` (allocator object hasn't been freed properly)
* Submodules and parameters can now shadow attributes in `__dict__` on assignment (#977).",pytorch
996,ngimel,pr,2017-03-13T23:56:44Z,"Don't use cudnn for negative padding or output_padding, check torch.backends.cudnn.enabled","This fixes #964 (if padding or output_padding is negative, cudnn is not used), however, the test that I've added for negative output_padding fails even when I'm running it on cpu only: 
```
FAIL: test_ConvTranspose2d_neg_output_padding (__main__.TestNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_nn.py"", line 1988, in <lambda>
    setattr(TestNN, test_name, lambda self, test=test: test(self))
  File ""/opt/pytorch/test/common_nn.py"", line 543, in __call__
    self._do_test(test_case, module, input)
  File ""test_nn.py"", line 69, in _do_test
    test_case.check_jacobian(module, input, self.jacobian_input)
  File ""/opt/pytorch/test/common_nn.py"", line 431, in check_jacobian
    PRECISION
AssertionError: 8.35154853160465 not less than or equal to 1e-05
```
Not quite sure why. Does pytorch actually support negative output_padding?",pytorch
999,ngimel,pr,2017-03-14T20:03:24Z,fix conv1d backward segfault,"Fix for #967 - fix segfault in conv1d when input does not require_grad. @apaszke, @colesbury, apparently in tests input always requires grad - so how should test be modified to catch things like this?",pytorch
1016,apaszke,pr,2017-03-16T16:31:37Z,Autograd refactor,"Progress:
- [x] Change names in the backend to better match the new graph representation
- [x] Remove Function superclass from Variable. Replace them with special nodes in the graph
- [x] Implement the new function definition.
  - [x] Make sure it's compatible with legacy definitions.
- [x] Make sure Variables are never unpacked in the Engine (partially done in the first commit)
- [x] Implement some of the built in Functions using the new format (to see if it's actually usable)
- [x] Add a function that returns a list of higher order grads

---

The tasks below are left for future PRs. The first two can be easily paralellized and can be done by others too.

- Adapt all definitions of built in Functions (using `@differentiable_once` for now).
- Implement all `backward` functions using Variables (so they can be differentiated multiple times)
- Add deprecation warnings for old Function format
- Add a switch for saving .creator graph traces

## Summary of changes

### New function definition format

**Note that the old declarations are still supported - most of the core implementations are still not converted.**

New format allows to implement jacobian vector products (jvp, L-op) of functions depending on jvp's of other functions (aka. grad of grad, hessian-vector products).

The new declarations look like this:
```python
class MultiplyAdd(Function):
                                                            # 1.
    @staticmethod
    def forward(ctx, input1, scalar, input2):               # 2.
        ctx.scalar = scalar                                 # 3.
        return input1 + scalar * input2

    @staticmethod
    def backward(ctx, grad_output):                         # 4.
        return grad_output, None, ctx.scalar * grad_output  # 5.
```

#### Adnotations:
1. Functions no longer can have an `__init__` method. Think of them as pairs of pure functions that are formulas specifying how to compute the function and its jvp (`Dfn * grad_output`).
2. Beacuse of 1., `forward` can now accept arguments of arbitrary types (used to only accept Variables). Any Variables appearing in `args` will be unpacked into Tensors. **Arguments are not recursively searched. For example, a list of Variables won't be unpacked into a list of Tensors, and they won't be registered as inputs in the graph**. Keyword arguments are not supported (need arg ordering to construct the graph).
3. `forward` gets a `ctx` as a first argument - this is an object (of unspecified type - not an instance of this class) with an interface identical to `self` in old style definitions (`save_for_backward`, `mark_non_differentiable`, etc.) and is used to pass information to the `backward` call. For example, this function needs to save a `scalar` argument. Note that you shouldn't assign input or output tensors to it, however intermediate buffers are ok.
4. `grad_output` is now a Variable, and the whole backward method needs to be implemented in terms of Variables (they shouldn't be unpacked into tensors, or the derivative graph will be malformed, see notes on `@once_differentiable` below). `ctx` will be the same object that was passed to `forward`.
5. `backward ` should return gradients for all arguments given to `forward` (even non-Variable arguments, but it should be `None` in such case). Unnecessary trailing `None`s are still accepted (useful when `forward` has optional arguments).

For comparison, here's how a legacy definition of `MultiplyAdd` would look like:

```python
class MultiplyAdd(Function):
    def __init__(self, scalar):
        super().__init__()
        self.scalar = scalar

    def forward(self, input1, scalar, input2):
        return input1 + self.scalar * input2

    def backward(self, grad_output):
        return grad_output, self.scalar * grad_output
```

### `@once_differentiable`

The fact that `backward` now takes Variables might unnecessarily complicate implementations of custom function that e.g. call into other libs. For that reason, this PR also introduces a `@once_differentiable` decorator, that can be used to wrap `backward`. After adding it, `backward` functions will get a tensor `grad_output` and will be expected to return a grad input tensor for each tensor argument given in `forward` (and `None` for all other args).

```python
class SciPyFunction(Function):
    @staticmethod
    def forward(ctx, input1, input2):
        return scipy.my_function(input1.numpy(), input2.numpy())

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return scipy.my_function2(grad_output), scipy.my_function3(grad_output)
```

### `torch.autograd.backward`

Added `create_graph`. If True, the graph for vjp will be created (defaults to False), allowing to differentiate the grad computation. Defaults to True if `grad_variables` contains at least one non-volatile Variable, and False otherwise.
Renamed `retain_variables` to `retain_graph`. The old argument will remain supported until v0.3, but will print deprecation warnings. If unspecified, defaults to the value of `create_graph`.

If `grad_variables` contains tensors, they are automatically promoted to Variables (volatile unless `create_graph` is True). Also, None entries in `grad_variables` are now accepted if their corresponding `variables` entries are scalar Variables (grad_output filled with 1 is allocated for them). Additionally, if all `grad_variables` could be `None` the argument is now optional.

### `torch.autograd.grad`

While Chainer-style API is great for first order grads, it doesn't work nearly as well when computing higher order derivatives. See this example:

```python
x = Variable(torch.randn(2, 2), requires_grad=True)
x.mul(2).sum().backward(create_graph=True)
y = x.grad.mul(2).sum()
# This accumulates grad of grad into x.grad, adding together results of both backward() calls
y.backward() 
```

For that reason, this PR also implements `grad` - a functional-style function that computes the vjp, and instead of accumulating it into `.grad` of all leaves, it returns a list of grads w.r.t. given function inputs (parameters are considered inputs too).

Example:
```python
from torch.autograd import grad

x = Variable(torch.randn(2, 2), requires_grad=True)
x.mul(2).sum().backward(create_graph=True)
y = x.grad.mul(2).sum()
# The line below **doesn't change x.grad**
x_hv = grad(y, x) # grad of y w.r.t. x. This argument would be the grad_output, but y is scalar
```

Arguments `outputs`, `inputs`, `grad_outputs` arguments can be both sequences of Variables (or Tensors and Nones in case of `grad_outputs`), or single Variables.

If one doesn't request the grad w.r.t. all leaf Variables, unneeded gradients are not computed, and won't be accumulated into them (by default `grad` has no side effects). If `only_inputs` argument is set to False, the whole graph will be differentiated, grads w.r.t. `inputs` will be returned in a list and not accumulated into `.grad`, grads w.r.t. all other leaves will be accumulated into their `.grad`.

### `.grad` semantics

By default the semantics are the same as right now. When not using any of the options implemented in this PR, `.grad` Variables will be volatile, and incoming grads will be accumulated in-place (both Variable and its `.data` will be the same objects - while we don't guarantee that, some people depend on that in their scripts, so it's best to support it unless there's no other way).
However, when using derivative graphs, these Variables will need to have their `.grad_fn` set correctly, and shouldn't be modified in-place (they might have been used in some functions!). For that reason in such cases the `.grad` attribute will point to a new Variable, with new `.data`, after each accumulation.

To sum up:

| `.grad`  | New grad | Action                                                                                                          |
|----------|----------|-----------------------------------------------------------------------------------------------------------------|
| volatile | volatile | Accumulated in-place into `.grad`                                                                               |
| volatile | -        | Accumulated in-place into `.grad` which remains volatile                                                        |
| -        | volatile | New grad is converted to a Variable that doesn't require grad and added out-of-place. Result overwrites `.grad` |
| -        | -        | Added out-of-place. Result overwrites `.grad`                                                                   |

### Implementation details

* Variables no longer subclass Function and therefore can no longer appear in the graph. After this PR graphs contain AccumulateGrad nodes instead of Variables.
* Engine now supports per-function callbacks that are called before evaluating the function, and if they return false the `apply` function won't be called (all gradients will default to null, which is an equivalent of 0), and its `next_functions` won't be added to the ready queue (unless they are already waiting for execution and this was their last dependency).",pytorch
1026,apaszke,pr,2017-03-17T19:44:51Z,Fixes for Prod and Expand functions,"Fixes #773, #995, #998.

cc: @ChangYong-Oh  @Teaonly",pytorch
1047,jihunchoi,pr,2017-03-20T13:19:46Z,Correct typo in batchnorm documentation,"When applying batchnorm, the second dimension always indicates the number of hidden units.
The dimension is expressed as `C` in the documentation, however there was a typo in that of `BatchNorm1d`.",pytorch
1062,apaszke,pr,2017-03-22T16:04:46Z,Bug fixes,"* Fix size mismatch in `CosineEmbeddingLoss` (#1058)
* **Make `random_` range exclusive** (it used to be exclusive when only the upper bound was specified, and inclusive when both were given). Also, all **`generator` args are now keyword only in random functions**. This should help with the confusing doc format. (#1046)
* Fixed two bugs in `torch.cat`:
  * It shouldn't accept `reverse` (it's not a `PySequence`) (#1015)
  * It now **disallows catting along inexistent dimensions** (to make it consistent with numpy and Variable cat) (#1014)
* Return the total norm of parameters from `clip_grad_norm` (#1032)
* Fix a bug in L-BFGS that caused it to use uninitialized locals (#1039)

Breaking changes are in bold.",pytorch
1066,apaszke,pr,2017-03-22T21:45:26Z,test,"**DO NOT MERGE**

This PR is only for testing some webhook stuff.",pytorch
1067,ngimel,pr,2017-03-22T21:59:59Z,fix formatting in upsampling docs,,pytorch
1075,ngimel,pr,2017-03-23T17:28:37Z,make inplace tests compare input grads,,pytorch
1076,apaszke,pr,2017-03-23T17:32:26Z,Fix formula for stddevs grad in Normal function,,pytorch
1079,ngimel,pr,2017-03-23T22:26:25Z,"don't use inplace backward, remove unnecessary zero for grad_input","Don't use inplace backward to avoid cloning, remove .zero_() for out-of-place. ",pytorch
1089,apaszke,pr,2017-03-24T19:06:13Z,Pass NULL rinfo_ to btrifact by default,Needs a subtree update first.,pytorch
1090,ngimel,pr,2017-03-24T19:21:57Z,Add back zero fill for ger,"Ger does not have beta argument, so has to be zero-filled.",pytorch
1093,apaszke,pr,2017-03-24T20:10:43Z,"Revert ""Add back zero fill for ger""","Reverts pytorch/pytorch#1090. See torch/cutorch#734. Needs subtree update.

cc: @ngimel",pytorch
1094,ngimel,pr,2017-03-24T20:22:28Z,Don't do extra resize in linear bias,,pytorch
1135,ngimel,pr,2017-03-29T00:00:00Z,Change dockerfile for cudnn v6,,pytorch
1140,apaszke,pr,2017-03-29T17:49:51Z,Fix deadlock in autograd,"Apparently that `notify_all()` was sometimes executed when the main thread wasn't sleeping, but doing some other stuff while holding the mutex. Once it returned to sleep it was never woken up again.

Fixes #1136",pytorch
1141,ngimel,pr,2017-03-29T23:37:41Z,add support for persistent rnns,"Couple minor issues
1) it warns in tests, when testing variable length input with persistent (that's not a supported configuration). I can just omit test for this configuration. 
2) I was hoping to pass info that persistent is not supported from cudnn backend back to module, so that cudnn backend does not try to create persistent descriptor again and again, but that did not work. ",pytorch
1174,apaszke,pr,2017-04-02T21:03:59Z,Bug fixes,"* Reshape grad in the `Dot` function (inputs don't have to be 1D vectors...) (#1155)
* Raise `AttributeError` in `Module.__getattr__` instead of delegating the call to an inexistent method (#1126)
* Added `Variable.type_as` (#1163)
* Unify argument names of `norm` and `renorm` (#1145)
* Add `torch.arange` and deprecate `torch.rangeÂ§ (#733)",pytorch
1226,apaszke,pr,2017-04-10T22:08:43Z,Bug fixes,"* Handle all errors if Module's sources can't be loaded (#1212)
* Improve serialization error messages (in case `read` returns 0)
* Retain the type of numpy scalars in `collate_fn` (#1113)
* Import TripletMarginLoss (#1224) 
* Fix `is_tensor` and `is_storage` to support old-style classes (#1003)
* Add support for keyword arguments in `torch.cat` (#1028)
* Fix coalesced CUDA collectives for nonhomogeneous lists (#1179)",pytorch
1235,ngimel,pr,2017-04-11T22:36:16Z,update Dockerfile not to use requirements.txt,,pytorch
1290,apaszke,pr,2017-04-18T22:56:21Z,Change the default algo for cuDNN conv forward to PRECOMP_GEMM,"Workspace size should be still fairly small, but it is often faster than IMPLICIT_GEMM.",pytorch
1304,ngimel,pr,2017-04-19T20:47:02Z,Don't install conda package in docker image,,pytorch
1340,apaszke,pr,2017-04-23T15:14:59Z,"Revert ""add keyword `out` for autograd function Concat to match torch.cat""","Reverts pytorch/pytorch#1336.

Variables should never have `out` arguments.",pytorch
1364,apaszke,pr,2017-04-26T19:13:05Z,Parallelize TensorMethods.cpp builds,This reduces build time from 163s to 53s. Quite useful when playing with `TensorMethods.cwrap`. Depends on torch/torch7#1014.,pytorch
1388,ngimel,pr,2017-04-28T00:05:28Z,temp fix for transposed dilated convolution,Temporary fix for a failing test with transposed dilated convolution. Have to figure out what is actually going on. ,pytorch
1394,ngimel,pr,2017-04-28T17:34:11Z,re-enable dilated convolutions on Kepler,,pytorch
1396,apaszke,pr,2017-04-28T21:04:08Z,THD updates,,pytorch
1399,apaszke,pr,2017-04-28T23:33:30Z,Use THCUNN backward kernels for Tanh and Sigmoid in Autograd,It's better to use fused kernels than not ðŸ™‚ ,pytorch
1400,apaszke,pr,2017-04-28T23:37:20Z,Parallelize TensorMethods.cpp builds,Now works with Python 2 too. See #1364 for original PR. ,pytorch
1408,peterjc123,pr,2017-04-30T06:44:26Z,added nadam optimizer,The PR added the new optimizer of Nadam. I wrote this code according to the [Nadam code](https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L495) in Keras and the [Adam code](http://pytorch.org/docs/_modules/torch/optim/adam.html#Adam) in the PyTorch repo. I've tested the code and it shows better performance in MNIST than original Adam.,pytorch
1451,cclauss,pr,2017-05-03T03:59:02Z,Try flake8 on Python 3,,pytorch
1454,apaszke,pr,2017-05-03T11:58:03Z,Use at most one shared_ptr block at a time to manage THPFunctions,"Fixes #1450. Also, a minor fix for `build_all.sh`.",pytorch
1464,apaszke,pr,2017-05-03T22:04:23Z,Fix memory leak introduced by 72e8190,Fixes #1455.,pytorch
1502,apaszke,pr,2017-05-07T12:43:35Z,Add F.cosine_similarity,Supersedes #860.,pytorch
1506,apaszke,pr,2017-05-07T19:53:15Z,Autograd bugfixes and improvements,"* Turns out we can't really take a short path for volatile inputs. The flags aren't propagated correctly in case a non-volatile input is modified in an in-place op with a volatile Variable -- e.g. `x[2] = y`, where `x` is not volatile, and `y` is. Also, volatile ops didn't mark inputs as dirty.
* Replace deprecated `retain_variables` with `retain_graph` 
* Add new flags to `Variable.backward` (and delegate to `torch.autograd.backward` to simplify the code)
* Minor fix for `Prod` backward (`grad_input` wasn't volatile, even if `grad_output` was)
* If a Variable has non-volatile gradient (i.e. when higher order grads are used), `model.zero_grad()` now replaces the grad with a new zero-filled Variable. This is useful if one wants to zero grad but used the old Variable to compute sth. Additionally assignments to `.grad` are now allowed (new grad is checked for its device, type and size).
* Exposed `.variable` attribute from `AccumulateGrad` nodes (cc @szagoruyko)",pytorch
1573,apaszke,pr,2017-05-16T14:58:54Z,Bug fixes,"* Fixed a number of clang warnings
* Fixed checks that picked ConvNd algos based on params (#1565)
* Indexing Variables with LongTensors now handles duplicate indices correctly (#1528)
* `Variable.type_as` now supports both tensor and variable arguments (#1163)",pytorch
1632,apaszke,pr,2017-05-23T19:43:51Z,Add scatterAdd and fix scatter grad formula,Fixes #1631.,pytorch
1635,apaszke,pr,2017-05-23T20:58:08Z,Improve handling of graph roots in autograd engine,"After this change the engine will create a no-op node that points to all roots. This allows us to take advantage of the code that evaluates functions to resolve the dependencies correctly, instead of having separate methods for roots. Fixes #1605.",pytorch
1690,apaszke,pr,2017-05-31T19:07:49Z,THD,,pytorch
1704,taion,pr,2017-06-02T18:53:54Z,Improve README copyediting,"I apologize in advance for how dumb this is. I got bothered by the ""monolothic"" typo, but then ended up getting sidetracked.",pytorch
1715,apaszke,pr,2017-06-04T11:02:54Z,Add DistributedDataParallel,"```python
import torch
import torch.distributed as dist
import torchvision

dist.init_process_group(backend='gloo')

model = torchvision.models.resnet50().cuda()
model = torch.nn.DistributedDataParallel(model) # prepend Distributed

dataset = ...
# so each process sees only a subset of the whole dataset
sampler = torch.utils.data.DistributedSampler(dataset)
data_loader = torch.utils.data.DataLoader(
        ..., sampler=sampler)

for batch, target in data_loader:
    # optimize model, log, etc.
```",pytorch
1727,apaszke,pr,2017-06-05T16:51:33Z,Bug fixes,"* Fix Prod backward formula (and a few errors in autograd tests too) (#1709)
* Fix grad type of compare functions (#1677)
* Fix a bug when type checks didn't accept `None` values returned from grad hooks (#1269)
* Renamed `masked_copy_` to `masked_scatter_`. The old name is still available but is deprecated (#1662)",pytorch
1729,ngimel,pr,2017-06-05T18:02:33Z,reduce the size of Docker image,Partially addresses #1619,pytorch
1732,ngimel,pr,2017-06-05T21:24:08Z,Runtime dockerfile ,for #1619,pytorch
1765,rdipietro,pr,2017-06-09T19:57:15Z,Fix typo in ParameterList documentation,,pytorch
1786,peterjc123,pr,2017-06-13T04:58:16Z,Improve Windows Compatibility,"Improves Windows Compatibility by modifying the code.
The changes mainly fall into four categories: 
1. data type conversions like long to int64_t
2. Windows-specific scripts like build_all.bat
3. Windows-specific C++ codes or libs like libshm-windows 
4. Windows-specific python codes helps finding DLLs",pytorch
1892,ngimel,pr,2017-06-23T20:50:43Z,fix arguments for cudnnFindEx for transposed wgrad,For issue reported on slack. ,pytorch
2030,Kongsea,pr,2017-07-10T02:23:59Z,"Fix typos in the docstrings of Conv3d, AvgPool3d and MaxPool3d",,pytorch
2034,Kongsea,pr,2017-07-10T10:06:15Z,Fix typos in docstrings,,pytorch
2053,rdipietro,pr,2017-07-11T15:38:18Z,Add reverse_padded_sequence function,Add a function for reversing padded sequences according to their lengths. More information here: https://github.com/pytorch/pytorch/issues/1794,pytorch
2067,apaszke,pr,2017-07-12T16:36:43Z,Distributed fixes and docs,,pytorch
2089,ngimel,pr,2017-07-13T22:22:36Z,add launch_bounds to greedy kernels,Fix #2049 (and couple more kernels that are prone to failure),pytorch
2090,ngimel,pr,2017-07-13T22:24:12Z,"install vision in devel dockerfile, minor fixes to dockerfile",Fix #2009,pytorch
2109,ngimel,pr,2017-07-15T01:04:03Z,fix baddbmm for expanded tensors,Fix for #2022 ,pytorch
2114,brettkoonce,pr,2017-07-15T19:32:46Z,spelling tweaks for documentation,,pytorch
2121,apaszke,pr,2017-07-16T18:13:11Z,Bug fixes,"* Handle unsqueezed dimensions in Repeat backward (#2095)
* `parallel_apply` can now accept arbitrary inputs (#1992)
* Keyword args are now allows in options that also use `long_args` (#1616)
* `Variable.expand` now accepts all inputs that `Tensor.expand` accepts (#1910)
* Unified argument names for Variable and Tensor methods (#1924, #1839)
* Changed assertions with side effects to `if` statements (#1848)",pytorch
2168,apaszke,pr,2017-07-20T18:23:59Z,Fixes for DistributedDataParallel,,pytorch
2171,ngimel,pr,2017-07-20T23:04:10Z,.creator -> .grad_fn in the code example,,pytorch
2179,apaszke,pr,2017-07-21T14:13:37Z,Improve memory usage of cuDNN RNN modules,"This should solve #914 in most cases. Weights of RNN modules are now allocated in the default cuDNN format (wasting some memory in no-bias case), and type/device casts preserve it. In-place modifications of data are fine, and if someone resets the parameter tensors to point to different memory, a warning will be raised, and the code will fall back to old behaviour.",pytorch
2183,apaszke,pr,2017-07-21T18:12:41Z,Add a support matrix for distributed backends,,pytorch
2185,ngimel,pr,2017-07-22T00:44:12Z,Increase flaky test tolerance,,pytorch
2193,apaszke,pr,2017-07-24T14:37:22Z,Fix minor bug in parallel_apply,,pytorch
2199,apaszke,pr,2017-07-25T13:50:10Z,Add Variable.retain_grad,Improved version of #2078,pytorch
2200,apaszke,pr,2017-07-25T14:52:41Z,DataParallel device_ids slicing fixes,Fixes #2184. I missed that place last time,pytorch
2266,apaszke,pr,2017-08-01T15:05:46Z,Ensure all RNNs have _data_ptrs,Fixes facebookresearch/InferSent#12,pytorch
2295,apaszke,pr,2017-08-04T16:13:39Z,Update autograd notes,,pytorch
2402,peterjc123,pr,2017-08-13T18:16:15Z,Improve Windows Compatibility(for 0.2.0),The code changes that helps build on Windows.,pytorch
2415,ngimel,pr,2017-08-14T20:41:01Z,accumulate in accType for reductions over dimensions,fix for #2391,pytorch
2438,peterjc123,pr,2017-08-16T02:49:24Z,Improve Windows Compatibility(for lib/ATen),Win64 support for lib/ATen,pytorch
2439,peterjc123,pr,2017-08-16T02:50:38Z,Improve Windows Compatibility(for lib/TH),Win64 support for lib/TH,pytorch
2440,peterjc123,pr,2017-08-16T02:51:35Z,Improve Windows Compatibility(for lib/THC),Win64 support for lib/THC,pytorch
2442,peterjc123,pr,2017-08-16T02:53:06Z,Improve Windows Compatibility(for lib/THCS),Win64 support for lib/THCS,pytorch
2443,peterjc123,pr,2017-08-16T02:53:53Z,Improve Windows Compatibility(for lib/THCUNN),Win64 support for lib/THCUNN,pytorch
2444,peterjc123,pr,2017-08-16T02:54:51Z,Improve Windows Compatibility(for lib/THD),Win64 support for lib/THD,pytorch
2446,peterjc123,pr,2017-08-16T02:55:42Z,Improve Windows Compatibility(for lib/THNN),Win64 support for lib/THNN,pytorch
2447,peterjc123,pr,2017-08-16T02:56:20Z,Improve Windows Compatibility(for lib/THPP),Win64 support for lib/THPP,pytorch
2449,peterjc123,pr,2017-08-16T02:57:23Z,Improve Windows Compatibility(for lib/THS),Win64 support for lib/THS,pytorch
2451,peterjc123,pr,2017-08-16T02:58:37Z,Improve Windows Compatibility(for csrc),Win64 support for csrc,pytorch
2452,peterjc123,pr,2017-08-16T02:59:26Z,Improve Windows Compatibility(for lib/nccl),Win64 support for lib/nccl,pytorch
2453,peterjc123,pr,2017-08-16T03:00:18Z,Improve Windows Compatibility(for Python scripts),"Win64 support for Python scripts


",pytorch
2454,peterjc123,pr,2017-08-16T03:01:15Z,Improve Windows Compatibility(for Windows scripts),"Win64 README, .gitignore, build and test scripts",pytorch
2455,peterjc123,pr,2017-08-16T03:02:14Z,Improve Windows Compatibility(for libshm),Win64 support for libshm,pytorch
2472,taehoonlee,pr,2017-08-17T12:05:21Z,Fix typos,This PR fixes some typos.,pytorch
2537,apaszke,pr,2017-08-25T14:06:24Z,Fix bugs caused by flatten_parameters(),Fixes #2460 and #2383 ,pytorch
2581,apaszke,pr,2017-08-30T19:51:20Z,Ensure GIL is held in ObjectPtrAllocators,"Also, make Storages that share data with numpy arrays non-resizable.

Fixes #2348.

",pytorch
2632,ngimel,pr,2017-09-05T18:31:38Z,fix indices for data_parallel and add parameter gradient tests,"Hopefully, should fix #2612 and #2573 ",pytorch
2694,ngimel,pr,2017-09-11T20:54:51Z,fix alignment warning,Fix for #2692,pytorch
2728,ssnl,pr,2017-09-13T22:06:53Z,Adaptive average pooling 3d,"Adding AdaptiveAvgPool3d as per request in #1988 .

Test plan:
./run_test.sh",pytorch
2738,brettkoonce,pr,2017-09-14T18:15:05Z,"minor spelling, intialize->initialize",,pytorch
2743,apaszke,pr,2017-09-14T21:16:59Z,Save output_nr in SavedVariable,Fixes #2736.,pytorch
2760,apaszke,pr,2017-09-17T05:49:12Z,Fix non-CUDA builds after Windows PRs,,pytorch
2766,peterjc123,pr,2017-09-18T02:27:51Z,Fixing build errors in Windows for lib/ATen,Fixing build errors in Windows for lib/ATen and MSVC,pytorch
2782,ssnl,pr,2017-09-19T07:11:13Z,Adaptive max pool 3d,"Adding AdaptiveMaxPool3d as per request in #1988 .

Commit structure:
1. renamed spatial version variable names for same reason as in #2728 
2. reordered spatial version variables to be of order B(batch) D(feature) H(height) W(width)
3. refactored spatial version code by using START_IND and END_IND macros, removed some unnecessary computation.
4. changed spatial version CUDA kernel to have input strides of type `int64_t`, removed an unused input in spatial CUDA kernels, and fixed a typo in `AdaptiveMaxPool1d` doc.
5. fixed `TestNN._test_maxpool_indices` for adaptive max pooling layers.
6. volumetric adaptive max pooling, including CPU, CUDA, Python, tests.

Test plan:
./run_test.sh",pytorch
2792,apaszke,pr,2017-09-19T21:19:10Z,Use OMP for pointwise ops,"A rebase + minor fixes on top of #2137.

cc @ruotianluo ",pytorch
2800,peterjc123,pr,2017-09-20T11:28:05Z,Improve Windows Compatibility(for scripts),"Python scripts, .gitignore, build and test scripts ",pytorch
2801,peterjc123,pr,2017-09-20T11:30:04Z,Improve Windows Compatibility(for csrc),Win64 support for csrc,pytorch
2804,apaszke,pr,2017-09-20T15:33:44Z,Add tools for autograd profiling,Rebase of ezyang/pytorch#236,pytorch
2855,apaszke,pr,2017-09-25T15:49:51Z,Make distributed recv return sender rank,,pytorch
2859,apaszke,pr,2017-09-25T16:11:37Z,Allow specifying unused inputs to torch.autograd.grad,"I don't want to allow that by default, because it might help catch bugs where certain inputs don't need to be specified, but a flag is helpful when one wants to experiment with architectures that have unreachable parameters in second backward.",pytorch
2872,ssnl,pr,2017-09-26T20:55:58Z,Change Variable.cuda to be consistent with Tensor.cuda,Fix for issue #2685 .,pytorch
2873,peterjc123,pr,2017-09-27T04:03:28Z,"Fix ""undefined identifier M_PI"" for MSVC",,pytorch
2887,MicaelCarvalho,pr,2017-09-28T15:10:57Z,Cleanup for 'prob_dist' in multinomial function (fixes #1584),"The multinomial function is not correctly cleaning up the resize applied on prob_dist. This problem occurs in three different situations, and can be reproduced with the following code:
```
import torch
weights = torch.Tensor([0, 10, 3, 0])
print(weights) #Â [torch.FloatTensor of size 4]
torch.multinomial(weights, 5)
print(weights) # [torch.FloatTensor of size 1x4]

weights = torch.Tensor([0, 10, 3, 0])
print(weights) # [torch.FloatTensor of size 4]
torch.multinomial(weights, 0)
print(weights) # [torch.FloatTensor of size 1x4]

weights = torch.Tensor([0, 0, 0, 0])
print(weights) # [torch.FloatTensor of size 4]
torch.multinomial(weights, 2)
print(weights) # [torch.FloatTensor of size 1x4]
```

This PR fixes the problem for all three cases.",pytorch
2896,MicaelCarvalho,pr,2017-09-29T15:26:39Z,Fix multinomial sampling with total/partial probabilities = 0,"_This PR probably needs to be discussed. I'm going to present the problem, what changes with the PR and the possible concerns with respect to these changes._

---

## The problem

First known report on Slack's channel `beginner` on 27 sept, by myself.

The function `multinomial` assumes non-negativity and non-zero sum for the input, however, it does not treat elements with value = zero, and returns random-like indices instead. Example:

```
import torch
weights = torch.Tensor([0, 10, 3, 0])
torch.multinomial(weights, 4, replacement=False)
# > 1, 2, 0, 0
weights = torch.Tensor([1, 10, 3, 0])
torch.multinomial(weights, 4, replacement=False)
# > 1, 0, 2, 1
```

Basically, in the first example it runs out of 'positive' probabilities and starts outputting 0 (always). But on the second example it outputs 1 when it doesn't have any probs left. The biggest problem is that there is no reason for it to output 1, and it is not always 1 (sometimes 0, never 2).

Taking a closer look on the code, I realized the function was designed to work with **non-zero values**, and not non-zero sum, it presents erratic behavior when there are zeros inside the matrix. This seems to be an undesirable behavior, since the caller would have to manually check every probability, and it is not that uncommon to find situations in which we have a weight matrix with zeros inside and we still want to sample from it.

## What changed

Two constraints were removed from the `multinomial` function:
  1. Number of samples must be inferior or equal to the number of categories
  2. Sum of values must be positive

Values are sampled in the same was as before, except when there is no values left to be sampled. In the latter case, the sampled position is `-1`, and the examples given before output the following now:

```
import torch
weights = torch.Tensor([0, 10, 3, 0])
torch.multinomial(weights, 4, replacement=False)
# > 1, 2, -1, -1
weights = torch.Tensor([1, 10, 3, 0])
torch.multinomial(weights, 4, replacement=False)
# > 1, 2, 0, -1 # or ""1, 0, 2, -1"", etc. Still a weighted random on the probs
```

With the difference that zero-sum probabilities are accepted, and the number of samples can be bigger than the number of categories:

```
import torch
weights = torch.Tensor([0, 0, 0, 0])
torch.multinomial(weights, 4, replacement=False) # zero sum
# > -1, -1, -1, -1
weights = torch.Tensor([1, 10, 3, 0])
torch.multinomial(weights, 6, replacement=False) # 6 samples
# > 2, 1, 0, -1, -1, -1
```

## What did not change

Values are still supposed to be non-negative. Passing negative values for the probabilities will affect the behavior of the function:

```
import torch
weights = torch.Tensor([0, 0, 6, -5])
torch.multinomial(weights, 4, replacement=False) # negative value inside
# > 2, -1, -1, -1 # positive value outweighs negative value, but things are weird in the probability vector inside the function
weights = torch.Tensor([1, 4, 1, -5])
torch.multinomial(weights, 4, replacement=False) # negative value inside
# > 0, -1, -1, -1 # positive value outweighs negative value and allow for only 1 sample to be drawn, weird things in prob vector as well
weights = torch.Tensor([0, 0, 1, -5])
torch.multinomial(weights, 4, replacement=False) # negative value inside
# > -1, -1, -1, -1 # Sum is negative, it will skip sampling
```

## What should be debated about these changes

The behavior of the function changed: it used to output ""random"" (not really, but well...) indexes when the vector reached probability 0 because of some zero-valued item inside of it ; now it outputs -1 for every sample not drawn. The previous behavior allowed for anyone to use the output of the `multinomial` function to index a matrix, later erasing invalid elements -- _the problem is that these invalid elements were not easily identifiable_.

With the proposed changes, the user would have to replace the -1 indexes with a valid index before using the output of `multinomial` to index a vector/matrix. The benefit of this approach is that it would easy to identify any invalid elements (i.e. prob = 0) to remove them from the samples.",pytorch
2897,peterjc123,pr,2017-09-29T15:48:16Z,Fix the memory leak using multiple workers in Windows,Fix the memory leak using multiple workers in Windows,pytorch
2899,apaszke,pr,2017-09-29T15:58:33Z,Softmax,"- Cleaned up THNN and THCUNN code and kernels
- Improved THCUNN kernel performance 5x, making it match cuDNN performance
- Added support for computing softmax over arbitrary dims.
- Deprecated implicit dim selection in softmax/softmin/log_softmax
- Both functions now accept inputs with arbitrarily many dimensions
- Autograd functions no longer save the input (it's unnecessary)
- Added cuDNN bindings for softmax, but they are unused as THCUNN
  matches or even exceeds cuDNN performance

cc: @karpathy",pytorch
2900,apaszke,pr,2017-09-29T18:05:04Z,Simplify getApplyGrid in THC,I've seen integer overflows in this place when running some benchmarks (can't remember how to trigger this now). This is simpler and doesn't have all these weird type casts,pytorch
2901,ngimel,pr,2017-09-29T18:40:34Z,"use nccl deb in Dockerfile, easier to change python version",,pytorch
2902,ngimel,pr,2017-09-29T18:41:10Z,add error checking to grid sampler,,pytorch
2911,peterjc123,pr,2017-09-30T08:26:30Z,Fix build on Windows for lib/ATen,Fix build on Windows for lib/ATen,pytorch
2921,taehoonlee,pr,2017-10-01T05:50:53Z,Fix typos,"This PR fixes some typos: `Tranposed`, `wheter`, `funcitonal`, `be be`, `is is`, and `containig`.",pytorch
2927,taehoonlee,pr,2017-10-02T07:34:47Z,Fix typos,"This PR fixes some typos: `seperated`, `outout`, `Outout`, `Ouput`, and `mistmatched`.",pytorch
2937,ssnl,pr,2017-10-02T21:34:42Z,Fix typo in document of nn.AdaptiveMaxPool1d,"Indices returned by nn.AdaptiveMaxPool1d can be only consumed by the nn.MaxUnpool1d, not 2d.

Removed some extra spaces.",pytorch
2938,ssnl,pr,2017-10-02T22:06:43Z,fix nn.HingeEmbeddingLoss doc,"The `nn.HingeEmbeddingLoss` doc is quite confusing. It says 2d `x` & 1d `y`, and then continues to say they can be of arbitrary shapes with same number of elements.

This PR solves #2756 .",pytorch
2941,peterjc123,pr,2017-10-03T02:09:29Z,Improve Windows Compatibility(for csrc/scripts),Win64 support for csrc and scripts,pytorch
2942,apaszke,pr,2017-10-03T07:09:01Z,Limit number of demangler invocations in autograd profiler,"The script below takes 2s without this patch, 0.34s with this patch, and 0.32s with profiler off.

```python
import torch
from torch.autograd import Variable
from torch.autograd.profiler import profile

x = Variable(torch.randn(5, 5))
with profile() as p:
    for i in range(1000):
        x = x + 2
```",pytorch
2944,apaszke,pr,2017-10-03T08:40:31Z,Add inplace checks in JIT,"We use the ONNX pass to de-inplace operators, and in case there are any left we raise an error.

Note that all CppOps we have right now are out-of-place so it's not necessary, but will be once we expose autogenerated autograd functions. Currently they won't even get traced, which is a problem on its own.",pytorch
2947,apaszke,pr,2017-10-03T13:21:00Z,Minor fixes,"* Fix a THPP compile error (#2785)
* Add cuDNN include dir before CUDA dirs in setup.py in case someone wanted to override one that's installed there as well (#2510)
* Fix THC exponential distribution implementation to never sample infinity - cuRAND returns numbers in (0, 1] (#2561)",pytorch
2951,ssnl,pr,2017-10-03T15:23:17Z,Generates random tensor of non-equal values for adaptive max pool test,Fixes #2939 . The only drawback is that the test size can't be too large due to `randperm` limitation. But it should be fine if `numel() < (1 << 20)` (don't know the exact limit since I didn't test higher values). ,pytorch
2958,ssnl,pr,2017-10-03T19:37:05Z,Set seed at top level for tests that random before run_tests call,"Fixes some intermittent test failures due to initialization not respecting SEED option.

cc: @houseroad ",pytorch
2960,ssnl,pr,2017-10-03T20:34:04Z,Fix scatter size check,"Fixes #1653 

This ensures the following condition on `scatter` for both cpu and cuda:

1. index.size[d] <= output.size[d] for d != dim 
2. index.size[d] <= src.size[d] for all d

The drawback is introducing a weird macro argument. Let me know if I should think of other ways,",pytorch
2961,ssnl,pr,2017-10-03T21:06:10Z,Raise error when each channel only has 1 value in batch norm,Fixes #1381 by raising error when a channel observes only 1 value for a batch in batch norm.,pytorch
2977,ssnl,pr,2017-10-04T17:31:42Z,Fix BN size check in eval mode,"Follow up on PR #2961 . Fixes the case in eval mode.

cc: @colesbury ",pytorch
2994,ssnl,pr,2017-10-05T21:49:46Z,Fix NN tests not truly respecting SEED option,"Previously, many of the NN tests has random inputs/args/targets set at **top** level, before seeding is applied at beginning of each test at https://github.com/pytorch/pytorch/blob/master/test/common.py#L142-L145. This makes some tests to be flaky and even sensitive to order of definition in the files. In this PR, for `input`, `constructor_args` and `target` (criterion tests), we introduce three ways of definition:

1. directly specifying values, e.g. `input=torch.arange(1, 9).view(2, 4)`. This should ***not*** contain nondeterministic  functions.
2. specifying a `lambda` function to generate it, e.g. `constructor_args_fn=lambda: torch.rand(2,4)`
3. specifying sizes, which will be used to generate N(0, 1) at test time, e.g. `target_size=(4,5)`, `target_size=[(2, 4), (3,)]`.

The values are only generated when the containing `TestCase` first accesses them, and are cached afterwards.

After doing this, we can safely remove the top-level seeding ""hack"" used before.

Commit structure:

1. Implement the 3-way specification.
2. Remove top-level seeding options in `common.py` and `test_autograd.py`
3. Update NN tests to new format
4. Fix two legacy NN modules that modifies `input` in `clearState`, causing `input` to be invalid at `_cuda` tests.

Test time comparison:
Before this PR: `test_nn.py`: 810s; `test_legacy_nn.py`: 93s
After this PR: `test_nn.py`: 806s; `test_legacy_nn.py`: 94s
",pytorch
3003,ssnl,pr,2017-10-06T15:10:04Z,Allow variable input in Tensor.type_as,"Fixes #2582 

Local import in method is needed to avoid circular import.",pytorch
3008,ssnl,pr,2017-10-06T17:52:16Z,Fix segfault when reflection padding out of range,"Fixes #2563 .
",pytorch
3010,ssnl,pr,2017-10-06T18:26:23Z,Fix isContiguous,"Fixes #2996 .

Currently the isContiguous doesn't perform size check at dim 0 if `size[0] == 1`, causing tensors non-contiguous at dim 1 to incorrectly pass the check.",pytorch
3011,ssnl,pr,2017-10-06T19:10:22Z,Fix isContiguousDim,"Fixes #2996 . Strides for size 1 dims should not matter.

Test: successfully runs script in #2996 .",pytorch
3032,ssnl,pr,2017-10-09T15:07:53Z,Allow default dim for topk if only kwargs are specified.,Fixes #2131 .,pytorch
3037,vishwakftw,pr,2017-10-09T18:24:51Z,Sequential with slicing capabilities,"This closes https://github.com/pytorch/pytorch/issues/2174

Please let me know what you think about this.",pytorch
3042,ssnl,pr,2017-10-09T21:18:29Z,"Add random_ for cuda, fix random_ for cpu ","Implements #712 .

CPU random_() has the bug of having max value 2^32. This is a problem for DoubleTensor and LongTensor. Fixed in this PR as well.",pytorch
3044,apaszke,pr,2017-10-09T22:35:53Z,Add a hack for RNN export to ONNX,"The export itself is not implemented yet (it's that `symbolic` that raises an error at the moment), but the whole framework is in place.",pytorch
3052,ssnl,pr,2017-10-10T20:03:06Z,More shape checking for ConvNd,Provides better error handle and message. Fixes #3039 .,pytorch
3056,ssnl,pr,2017-10-10T21:26:16Z,Add document on how Module.cuda() and optims should work together,"In #2021 , many users are facing an error when calling `Module.cuda()` after constructing an optimizer. The error is expected, because the optimizer is initialized with different parameters with what it sees in `.step()`. It is difficult to make a better error message for it as well, as it can potentially happen with in any place of an optim that saves parameter-specific tensors in its state dict. As pointed out by @matt-gardner in #2021 , better documentation should be helpful. ",pytorch
3069,ssnl,pr,2017-10-11T14:22:08Z,Fix a typo in optim.rst,Blame #3056 ,pytorch
3071,apaszke,pr,2017-10-11T15:41:09Z,Fix macOS build (with CUDA),Fixes #3051,pytorch
3091,ssnl,pr,2017-10-12T15:44:04Z,isCountiguous problems,"1. Fixes #3064 . Cause: cuDNN doesn't like tensor to have bizarre stride at size-1 dimensions. Fixed by letting THCTensor's `newContiguous` fix these strides. 

2. Previous fix to `isContiguousDim` in #3011 is only partially correct. That allows some dimension before size-1 dimension to incorrectly fail check, e.g. size `[4,1,3]`, stride `[3,50,1]`. It is impossible to do point checks without considering all dimensions of interest. This PR changes `isContiguousDim` to `isContiguousRange`.

3. Fix a mis-scoped `undef` in #3042 .

",pytorch
3110,ssnl,pr,2017-10-13T17:37:16Z,Update conv documentations,"`nn.Conv` and `nn.functional.conv` docs are not very clear about padding argument. This PR updates the doc to be more consistent, and also improves wording in various places. Notice that I changed the order in `functional.py` so that `conv1d` is before `conv2d`.

Issue: #3092 
cc @ozancaglayan ",pytorch
3113,ssnl,pr,2017-10-13T18:18:11Z,Limit print scale by sys.float_info,"Fixes #3035 . `float_info.min * float_info.epsilon` gives smallest unnormalized float representable in the system. 

Test:
```
>>> torch.DoubleTensor([1e-323])

9.88131e-324 *
  1.0000
[torch.DoubleTensor of size 1]
```",pytorch
3120,apaszke,pr,2017-10-14T17:48:33Z,Fix nvprof mode in autograd profiler,"I realized that the `use_nvprof` mode of autograd profiler can't be implemented to return summary in the profiled process, because there's no way to force nvprof to flush data to disk. Instead, the trace has to be loaded offline using `torch.autograd.profiler.load_nvprof(path)`.

---

Tested by running this script:
```python
import torch
from torch.autograd import Variable
from torchvision import models

x = Variable(torch.randn(1, 3, 224, 224).cuda())
model = models.resnet18()
model.cuda()

with torch.cuda.profiler.profile():
    model(x) # Run once, to pre-allocate memory, and initialize CUDA profiler
    with torch.autograd.profiler.emit_nvtx() as p:
        model(x)
```
with this command:
```
nvprof --profile-from-start off -f -o out.prof python3 tmp.py
```

and checked that
```
ipython3 -c ""torch.autograd.profiler.open_nvprof('out.prof')"" | head
```
prints
```
[<FunctionEvent id=2 cpu_time=108.569us cuda_time=269.342us name=ConvForward>,                       
 <FunctionEvent id=3 cpu_time=43.092us cuda_time=331.935us name=N5torch8autograd16BatchNormForwardE>,
 <FunctionEvent id=4 cpu_time=77.851us cuda_time=47.744us name=Threshold>,                           
 <FunctionEvent id=5 cpu_time=125.062us cuda_time=124.159us name=MaxPool2d>,                         
 <FunctionEvent id=6 cpu_time=52.249us cuda_time=147.103us name=ConvForward>,                        
 <FunctionEvent id=7 cpu_time=35.582us cuda_time=98.656us name=N5torch8autograd16BatchNormForwardE>, 
 <FunctionEvent id=8 cpu_time=46.830us cuda_time=14.752us name=Threshold>,                           
 <FunctionEvent id=9 cpu_time=44.941us cuda_time=138.559us name=ConvForward>,                        
 <FunctionEvent id=10 cpu_time=33.857us cuda_time=98.112us name=N5torch8autograd16BatchNormForwardE>,
```",pytorch
3133,ssnl,pr,2017-10-16T15:31:49Z,Change device_id to device in python land,"Fixes #3131 . 

However, `device_id`s are unchanged in `torch/cuda/random.py` (https://github.com/pytorch/pytorch/blob/master/torch/cuda/random.py) because there is an existing imported function called `device`. To change those to being consistent, should we do an `import as` for it?",pytorch
3137,ssnl,pr,2017-10-16T19:29:02Z,Sparse Adam optimizer for sparse gradients,"1. Implements Lazy Adam optimizer. See details in #1285 .

2. Adds various checks to other optimizers that do not support sparse gradients.

3. Adds test for sparse gradients with SGD

cc: @ezyang ",pytorch
3139,ssnl,pr,2017-10-16T20:22:06Z,Memory issue for SGD and sparse grad,"Fixes #3129 .

Previously we use buffer with same type as grad. For sparse grad, the buffer grows continuously as we never call coalesce. Since the buffer will likely become dense anyways, this PR changes it so that buffer are init'd as same type of param.

Performance on script in #3219 
Before change: GPU memory usage fluctuates greatly, with max at 10800MB. Decaying speed from 110it/s to 44it/s in 9 epochs.
Adding coalesce in each call: Constant GPU memory usage 1231MB. Constant speed at 150it/s.
After change: Constant GPU memory usage 1197MB. Constant speed at 210it/s.
",pytorch
3144,rasbt,pr,2017-10-17T03:38:16Z,Minor autograd reference change in readme,"Hi,

this is just a minor change in the Readme, changing ""autograd, autograd, Chainer"" to ""autograd-torch, autograd, Chainer"" as having autograd 2 times in a row looked a bit weird to me and people may mistake it as a typo.
",pytorch
3148,ssnl,pr,2017-10-17T14:46:53Z,isContiguous problems,"Reopened version of #3091 .

cc: @apaszke ",pytorch
3151,apaszke,pr,2017-10-17T17:58:00Z,Prevent torch.autograd.grad from freeing buffers in the entire graph,"Right now `torch.autograd.grad` stops executing functions at some point, but will still return zero grads so that dependencies for `next_functions` are freed. This is ok, because we don't execute any unnecessary function (they're aborted by an engine callback), but is wasteful since we have to iterate over the whole graph again. However, the worst part is that this iteration will actually free the buffers in the part that is not executed, causing errors later. See the new test for an example.

cc: @srush",pytorch
3175,apaszke,pr,2017-10-19T10:43:12Z,ATen update,,pytorch
3178,ssnl,pr,2017-10-19T14:53:20Z,Make sparse tensors work with reduce_add_coalesced and broadcast_coalesced,"As a result, sparse tensors now work with data_parallel. Fixes #1456 .

Commit structure:
1. Add support for sparse tensors in various places for `add_coalesced` and `broadcast_coalesced`.
2. Fix `_type` not working properly on sparse. Previously it didn't convert indices tensor when converted between cuda and cpu.
3. Add test in `test_cuda.py`.
4. Fix `pynew` `AutoGPU` arguments.

",pytorch
3211,ssnl,pr,2017-10-20T21:47:11Z,"Fix sparse bugs; add reduce_add, broadcase, data_parallel for sparse","This PR adds various fixes for sparse tensors. In particular, it contains the following:

Fixes:
1. Fixes `.type()` not converting indices tensor.
2. Fixes ATen not considering sparse cuda tensors as cuda and missing `get_device` bindings, causing `AutoGPU` checks to fail in backward engine.
3. Fixes `AutoGPU` missing args in `pynew`.
4. Fixes sparse tensor coalesce. The bug is caused by using `thrust::unique_by_key`, which modifies the key tensor. In case where key is 1D, original implementation is operating on the same tensor indices data storage, and thus modifies it. This is fixed by adding a flag to `THCSTensor_(newFlattenedIndices)`, controlling whether the returned value is forced to be a clone.

New functionalities:
Added support for sparse tensors in `broadcast_coalesced` and `reduce_add_coalesced`, and thus enabling `data_parallel` (fixing #1456 ). In particular, the `_take_tensors` function is modified:

1.  ~~~In case of `reduce_add_coalesced`, it returns each sparse tensor in a separate chunk because reduce_add on combined tensors only makes sense in dense case. In addition, the yielded tensors are now only maintaining order within sparse/dense class, as specified in code comments. Hence, an additional method `_reorder_tensors_as` is provided to get back the original order.~~~
2. ~~~In case `broadcast_coalesced`, sparse tensors of same type are returned together in chunk. Furthermore, `_flatten_tensors` and `_unflatten_tensors` are updated to support combining sparse tensors, allowing faster broadcasting.~~~

See comment below for details.

Various tests are added for the fixes and new functionalities above.

cc: @ezyang @colesbury
",pytorch
3217,peterjc123,pr,2017-10-21T06:31:01Z,Fixing lib/THNN build for Windows,The index variable for MSVC (OpenMp) should be signed. Fixed this using macros.,pytorch
3218,peterjc123,pr,2017-10-21T06:32:55Z,Fix missing <functional> and export decorations in lib/ATen,,pytorch
3228,vfdev-5,pr,2017-10-22T20:57:59Z,Change size by kernel_size in __repr__,"Probably, __repr__ should return `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))` -> `MaxPool2d (kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))`",pytorch
3229,vfdev-5,pr,2017-10-22T20:58:25Z,Space is missing in __repr___ of conv,"`Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))` -> `Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))` as for other layers, e.g
`MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))`",pytorch
3245,apaszke,pr,2017-10-23T20:23:55Z,Softmax/LogSoftMax refactor (wrapped up),"These commits wrap up the previous Softmax refactor. All the important changes are on the CUDA side. Once I unified the code I also added a special instantiation that made the kernels faster in certain cases (small inner dim, large softmax dim - might be useful in NLP for short sequences?)

**tl;dr** CUDA Softmax now supports a `dim` argument, and is usually 4x-256x faster than the previous implementation (it didn't have a spatial implementation before). Now, it also shares kernels with LogSoftmax, and certain optimizations benefited the log case giving up to 64x speedup in certain cases as well.

---

Here are the plots that show old / new timing ratios for different sizes of `dim` and size of the innermost dimensions (on the left of dim). Paralellizing over batch is easy, so it is fixed as 64 in all plots. Red dots are better, blue are regressions. Note that the plot is log in all axis (so z of 8 means 2^8x faster)

#### Softmax

Benefits from this diff all over the place. The old kernel was written in a quite archaic way.

![softmax](https://user-images.githubusercontent.com/4583066/31911090-d728ffe2-b83f-11e7-92a4-3a4c2cc6516c.png)

#### LogSoftmax

Benefits from adding a custom kernel for the cases when `inner_size` is no longer 1, so we can't use the super fast kernel, but the `dim_size` is large, so using a single thread to reduce values is slow. It is only enabled for a subset of the space where it provided speedups.

![log](https://user-images.githubusercontent.com/4583066/31911085-d259995e-b83f-11e7-98b4-1eae37691065.png)

#### Overall times

These are the log plots (in all axes) of the time (no more ratios) for the new algorithm. Softmax on the left, LogSoftmax on the right:

![times](https://user-images.githubusercontent.com/4583066/31911403-ce52591c-b840-11e7-8a29-ef575beb4126.png)",pytorch
3260,vishwakftw,pr,2017-10-24T13:29:54Z,Implementation of activation Swish,"This PR focuses on the implementation of the ""Swish"" activation function given by this paper: [Swish](https://arxiv.org/abs/1710.05941).

Let me know what you think. My implementation uses in the pre-existing implementations of `Sigmoid`, and hence I thought it would not be required to add the activation from scratch.",pytorch
3262,ssnl,pr,2017-10-24T15:59:01Z,Update ATen,"First time doing this. Let me know if I do things wrongly.

For #3211 .",pytorch
3265,ngimel,pr,2017-10-24T16:46:55Z,perf improvements for depthwise convolutions,"batch size | Input channels | Height, Width | kH, kW | stride | current time | #3057 time | Speed-up
-- | -- | -- | -- | -- | -- | -- | --
1 | 32 | 112 | 3 | 1 | 0.0003635788 | 0.000360384 | 0.9912128843
1 | 64 | 112 | 3 | 2 | 0.000231576 | 0.0003566647 | 1.5401626686
1 | 128 | 56 | 3 | 1 | 0.0002005291 | 0.000383029 | 1.9100917868
1 | 128 | 56 | 3 | 2 | 0.000126195 | 0.0002591753 | 2.053769129
1 | 256 | 28 | 3 | 1 | 0.0001298428 | 0.0003213358 | 2.4748071979
1 | 256 | 28 | 3 | 2 | 0.0001278973 | 0.000162158 | 1.2678771158
1 | 512 | 14 | 3 | 1 | 0.0001259661 | 0.0001772976 | 1.4075027444
1 | 512 | 14 | 3 | 2 | 0.0001271725 | 0.0001322794 | 1.0401574803
1 | 1024 | 7 | 3 | 1 | 0.0001308775 | 0.0001342106 | 1.0254672642
64 | 32 | 112 | 3 | 1 | 0.0076541996 | 0.0177096987 | 2.3137231327
64 | 64 | 112 | 3 | 2 | 0.0076271296 | 0.0168428278 | 2.2082787077
64 | 128 | 56 | 3 | 1 | 0.0070373774 | 0.0168711519 | 2.3973635443
64 | 128 | 56 | 3 | 2 | 0.003865943 | 0.0091258287 | 2.3605699435
64 | 256 | 28 | 3 | 1 | 0.0035696888 | 0.0085421801 | 2.392976124
64 | 256 | 28 | 3 | 2 | 0.0021048355 | 0.005251503 | 2.4949707306
64 | 512 | 14 | 3 | 1 | 0.0019623423 | 0.0044877005 | 2.2869101627
64 | 512 | 14 | 3 | 2 | 0.0011884212 | 0.0028495121 | 2.3977290053
64 | 1024 | 7 | 3 | 1 | 0.0012278891 | 0.002658639 | 2.1652110428
128 | 32 | 112 | 3 | 1 | 0.0144340229 | 0.0354276562 | 2.4544547567
128 | 64 | 112 | 3 | 2 | 0.0154968691 | 0.0339261246 | 2.1892244415
128 | 128 | 56 | 3 | 1 | 0.014062891 | 0.0335231686 | 2.3838034831
128 | 128 | 56 | 3 | 2 | 0.0080575609 | 0.018141408 | 2.2514763643
128 | 256 | 28 | 3 | 1 | 0.0070493364 | 0.0169120741 | 2.3991015678
128 | 256 | 28 | 3 | 2 | 0.0041349077 | 0.0103336859 | 2.4991333709
128 | 512 | 14 | 3 | 1 | 0.003833847 | 0.0086662102 | 2.2604475533
128 | 512 | 14 | 3 | 2 | 0.0022731161 | 0.0053928566 | 2.3724510024
128 | 1024 | 7 | 3 | 1 | 0.0023054218 | 0.0047499275 | 2.0603290298

The biggest performance improvements are due to templating kernels. The benchmarks comparing to #3057 performance are above, I've taken sizes from https://github.com/marvis/pytorch-mobilenet/blob/master/benchmark.py#L19-L46 and some are slightly different from what was listed in #3057. Benchmarks are for 50 iterations, time is given per iteration. 
",pytorch
3267,ssnl,pr,2017-10-24T17:35:57Z,Update legacy VolumetricMaxPooling tests to respect new seed options,"Blame #2994 .

cc @ezyang ",pytorch
3269,ssnl,pr,2017-10-24T18:23:21Z,Update ATen,Needed for #3178 . :),pytorch
3271,ssnl,pr,2017-10-24T19:38:42Z,"Add zero, zeros_like, _dimI and _dimV for sparse tensors","This will enable better code for #3137 , #3211 , and future sparse optimizers.",pytorch
3289,ssnl,pr,2017-10-25T18:30:16Z,Dynamically find min log scale,"Fixes #3195 . Blame #3113 .

Hardcoding -323 won't work since FTZ/DAZ will make `pow(10, -323)` zero, causing division by zero error again.

cc @colesbury ",pytorch
3291,apaszke,pr,2017-10-25T19:02:41Z,Fixes for JIT,"Add the script to auto-generate functions calling ATen methods corresponding to JIT nodes.

Also, re-enable JIT tests (they're all working now).

Generated `constructors` look like this:
```cpp
  {""btrisolve-3"", [](Node *node) {
  
    return TensorOp([=](const variable_list& vars) -> variable_list {
      return pack_list(at::btrisolve(vars[0], vars[1], vars[2]));
    }, ""btrisolve"", 3);
  }},
  {""cat-1-dim"", [](Node *node) {
    int64_t dim = (node->i(stringToSymbol(""dim"")));
    return TensorOp([=](const variable_list& vars) -> variable_list {
      return pack_list(at::cat(as_tensor_list(vars), dim));
    }, ""cat"", 1);
  }},
```",pytorch
3302,apaszke,pr,2017-10-26T07:47:10Z,Fix pack_padded_sequence to accept inputs of arbitrary sizes,,pytorch
3327,ssnl,pr,2017-10-27T18:34:51Z,Use raw string for documentation in nn,"Sphinx documentation should be in raw string to avoid issues like: https://stackoverflow.com/questions/16468397/mathjax-expression-in-sphinx-python-not-rendering-correclty
and https://github.com/pytorch/pytorch/issues/3186.

This PR converts top level doc strings under `torch.nn` to raw string.

Fixing #3186 ",pytorch
3329,ssnl,pr,2017-10-27T20:57:27Z,Add sparseTensor.new wrapper bindings,"Fixes #3312 . Relevant to #3137 .

However, I'd appreciate if someone can explain to me how all these `.cwrap` bindings work. From poking around, my understanding is:

1. Both ATen cwrap and `csrc/generic` crwap are needed for the method to exist.
2. Somehow, if a python `new` is not supplied in a cwrap binding, calling `.new` will automatically invoke the `tp_new` on the c `PyObject`. I'm also curious where this happens.

",pytorch
3331,darrengarvey,pr,2017-10-27T21:09:08Z,Fix compilation without numpy.,"I accidentally tried compiling without numpy and stumbled into this. The docs say to install numpy but building without it looks like it's intended to be supported, hence the fix.

For reference, the error:

    Tensor.cpp:309:47: error: â€˜PyArray_Checkâ€™ was not declared in this scope",pytorch
3333,darrengarvey,pr,2017-10-27T22:29:31Z,Add .dockerignore.,"`.gitignore` should have uninteresting files listed, so acts as a good
`.dockerignore`. Reduces the build context sent to the docker daemon from to
2.927GB (after building locally) to 66.66MB (:O).",pytorch
3336,gokceneraslan,pr,2017-10-28T10:43:18Z,Prevent numerical issues with poisson_nll_loss when log_input=False,"Evaluation of the logarithm of the input variable in poisson negative log likelihood leads to NaN loss if variable being evaluated is zero. Small epsilon is added to prevent this. See equivalent Keras epsilon here: https://github.com/fchollet/keras/blob/master/keras/losses.py#L68

Here is the code to reproduce the issue:

```python
import numpy as np
import torch
from torch.autograd import Variable

torch.manual_seed(42)
np.random.seed(42)

poisson_mean = 4
poisson_numsample = 200

samples = np.random.poisson(poisson_mean, poisson_numsample)

param = Variable(torch.zeros(1), requires_grad=True)
torch_samples = Variable(torch.from_numpy(samples).float(), requires_grad=False)

optimizer = torch.optim.RMSprop([param], lr=0.1)
loss = torch.nn.PoissonNLLLoss(log_input=False, full=True)

for i in range(5000):
    optimizer.zero_grad()
    output = loss(param, torch_samples)
    output.backward()

    if i % 1000 == 0:
        print('Loss:', output.data[0])

    optimizer.step()

print('Sample mean: ', torch_samples.mean().data.numpy())
print('Mean is : ', param.data.numpy())

```",pytorch
3370,ssnl,pr,2017-10-30T17:23:08Z,"Follow up #3211 (sparse broadcast_coalesced, reduce_add_coalesced)","Follow up on comments in #3211 

1. Add comments for helper functions. @ezyang 
2. Add check for case that gpus return differently typed (dense/sparse) tensors. @apaszke ",pytorch
3381,ssnl,pr,2017-10-30T20:25:14Z,Make sparse (new) functions conform that storage is not NULL,Fixes #3367 .,pytorch
3399,gokceneraslan,pr,2017-10-31T18:31:33Z,Typo fix in torch.median,,pytorch
3400,ngimel,pr,2017-10-31T18:53:53Z,make weight grads contiguous for linear with no bias,"Otherwise, weight grad comes out of mm discontiguous, and gradient update kernel takes twice as long as it should. ",pytorch
3407,ssnl,pr,2017-10-31T20:55:11Z,Fix Upsample1d incorrect number of output element,Fixes https://github.com/pytorch/pytorch/issues/3309 .,pytorch
3419,ssnl,pr,2017-11-01T16:23:16Z,Enable debugging flag for nvcc,"Currently we don't pass `-g -G` flags to nvcc when `DEBUG=1`.  This makes using `cuda-gdb` to debug device code particularly difficult. This PR enables such flags when `CUDA_DEBUG=1`.

Tested with script in https://github.com/pytorch/pytorch/issues/3309. Verified that `cuda-gdb` can break in and display device code successfully.",pytorch
3422,apaszke,pr,2017-11-01T17:12:02Z,Register VariableType calls in autograd profiler,Also improve NYI error message to give a hint that it's an error from VariableType and not any other place in the codebase (hit when running some of the C++ hacky benchmarks).,pytorch
3429,ssnl,pr,2017-11-01T20:00:40Z,Allow empty index tensor for index_select,"Fixes #3416 .

Test:
Verified the snippet in #3416 runs on both cpu and cuda. Verified that `TestTorch.test_advancedindex` and `TestCuda.test_advancedindex` passes with the new inserted assertion.",pytorch
3445,ngimel,pr,2017-11-02T17:24:17Z,add dockerfile with cuda9 volta support,"Magma is commented out for now, also, to improve compile time and decrease image size I've removed 3.5 architecture (not much point compiling with cuda 9 for Keplers). ",pytorch
3447,peterjc123,pr,2017-11-02T18:13:34Z,Fix failed tests using tempfile in Windows,"According to the posts [1](https://blogs.msdn.microsoft.com/oldnewthing/20160108-00/?p=92821), [2](https://stackoverflow.com/questions/15169101/how-to-create-a-temporary-file-that-can-be-read-by-a-subprocess) and [3](https://stackoverflow.com/questions/20328422/what-share-mode-is-used-when-files-are-opened-using-open), the usage of the tempfile can work by calling a modified version of `os.open`. The original version of `os.open` sets the `FILE_FLAG_DELETE_ON_CLOSE` flag to the file while opening the files, which causes the weird permission failures to open the file again.",pytorch
3467,peterjc123,pr,2017-11-03T16:52:34Z,Fix MSVC build after major change,"1. Add export macros definition for the export modifier to work
2. Fix some functions or constants that differs from Unix
3. Add some include files and build scipts",pytorch
3469,ngimel,pr,2017-11-03T18:10:15Z,Install magma in cuda 9 docker,,pytorch
3474,ssnl,pr,2017-11-03T21:32:27Z,Signal handling in DataLoader workers; Timeout option,"1. Make DataLoader workers a bit more verbose on bus error and segfault, implemented with C++ signal handlers. Partially addresses https://github.com/pytorch/pytorch/issues/1355
2. Adding timeout option to DataLoader, implemented with SIGALRM. https://github.com/pytorch/pytorch/issues/2474 

Test plan (modified from script by @fmassa in #1595 ):
1. Segfault case:
script: 
```
class DS(object):
    def __getitem__(self, idx):
        import ctypes;ctypes.string_at(0)
        return torch.rand(1000000)
    def __len__(self):
        return 200

ds = DS()
it = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10)

for i, data in enumerate(it):
    print(i)
```
output: 
```
[~] python ds.py
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
[~] # Doesn't hang anymore
```

2. Timeout:
script:
```
import time

class DS(object):
    def __getitem__(self, idx):
        time.sleep(10)
        return torch.rand(1000000)
    def __len__(self):
        return 200

ds = DS()
it = torch.utils.data.DataLoader(ds, batch_size=10, num_workers=10, timeout=2)

for i, data in enumerate(it):
    print(i)
```
output:
```
[~] python ds.py
ERROR: Time out when fetching data in DataLoader.
[~]
```

Since this is deep into multiprocessing and low level signal handling, there might be cases/things I haven't considered. I'll also leave some comment below.

cc @apaszke @colesbury ",pytorch
3476,ngimel,pr,2017-11-03T21:58:51Z,fix copy-paste error in #3263,"I have no idea how it worked on cuda 8, but apparently this fixes failures on cuda 9. cc @colesbury",pytorch
3488,apaszke,pr,2017-11-04T16:04:19Z,Move flattening/unflattening logic of JIT to C,"BatchNorm tests are broken, because they're passing tensors into closures. We should figure out a better story for batch norm anyway, so I just disabled the tests for now.

I also took the chance and simplified parts of our JIT code",pytorch
3518,ssnl,pr,2017-11-06T23:31:33Z,Exposing emptyCache from allocator,"Relevant thread: https://github.com/pytorch/pytorch/issues/1529#issuecomment-339649776

As described in the updated docs, this function will let PyTorch caching allocator release all unused cached memory so that other GPU applications can use those.

Test plan:
```
import torch

x = torch.cuda.DoubleTensor(10000, 10000).normal_()
import pdb; pdb.set_trace()
del x
torch.cuda.empty_cache()
```
Observed that the final call reduces GPU memory usage shown in `nvidia-smi`.

Credit: the implementation is by @tibuch . 

Sorry that I submitted this PR on your behalf. I contacted you on the aforementioned thread, but you didn't reply for several days. Let me know if you want to submit your own PR, and I will close this one. :)
",pytorch
3519,ssnl,pr,2017-11-06T23:34:06Z,Add document for SparseAdam optimizer,Followup on #3137 .,pytorch
3545,ngimel,pr,2017-11-07T21:03:49Z,avoid unnecessary multiplies in derivatives,,pytorch
3579,vfdev-5,pr,2017-11-08T22:50:53Z,Update sampler.py,Add argument's missing doc ,pytorch
3594,peterjc123,pr,2017-11-09T10:54:38Z,Fix build failures in MSVC,"The ATen commits are breaking MSVC builds. Some issues should be addressed.
1. Export some functions and classes
2. Modify the `#ifdef` macro to make it pass build ",pytorch
3595,peterjc123,pr,2017-11-09T11:03:37Z,Enable the build for MSVC 2017 and Ninja ,"The coming of CUDA 9 enables the build for MSVC 2017, so we should support that. And MSVC doesn't support parallel build for custom builds. We can speed up the build of CUDA libs using Ninja. In this PR, i checked the values of some env vars to enable the builds for these two toolchains.",pytorch
3597,apaszke,pr,2017-11-09T12:35:03Z,Move _CompiledMixin to C++,"Moves the JIT hot path to C++. Removes around 10-15us from the LSTM benchmark (155us -> 140us), but should be possible if we manage to remove the `parameters()` call and move the logic for handling implicitly captured variables to C++ (that call costs another 10-15us at least, even for a single module).",pytorch
3604,ssnl,pr,2017-11-09T17:16:56Z,Fix ld* conditions for gemv ger gemm,"Fixes issues raised in comments of https://github.com/pytorch/pytorch/issues/3525.

Docs:
gemv: https://software.intel.com/en-us/mkl-developer-reference-fortran-gemv
ger: https://software.intel.com/en-us/mkl-developer-reference-fortran-ger
gemm: https://software.intel.com/en-us/mkl-developer-reference-fortran-gemm
",pytorch
3617,peterjc123,pr,2017-11-10T01:17:02Z,Enable EXPORT_ALL_SYMBOLS for CMAKE,"If we turn on CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS flag, we don't need to add most decorators by hand.",pytorch
3621,apaszke,pr,2017-11-10T11:46:44Z,Expend autograd profiler docs,cc: @soumith ,pytorch
3631,ngimel,pr,2017-11-10T21:58:17Z,fix selecting deterministic conv algo,Fix for #3614,pytorch
3637,peterjc123,pr,2017-11-11T02:58:43Z,Fix setup scripts for Windows CUDA builds,A change in the link args for MSVC.,pytorch
3645,apaszke,pr,2017-11-11T17:50:39Z,Add torch::hash (C++ hashing utilities) and improve arg checking in JIT,"Fixes a regression in arg checking when it was moved to C++. Also, added a test so we won't miss it again.",pytorch
3647,apaszke,pr,2017-11-11T20:27:49Z,Minor fixes for convolution,"* I got quite nervous when looking at #3631 - we have those lists of cuDNN algorithms, but it's really easy to miss a new algo if it was to be added. I addded `static_assert`s to make sure that we have a full list, and it turned out that there is one algo missing in backward filter, but it's marked as not implemented in cudnn headers.
* Currently, when doing size checks with groups, we divide by the group count instead of multiplying the other side. Since there's a floor after division, this means that we could have accepted inputs of incorrect sizes.

@ezyang let me know if you want to merge this first, or if I should rebase the fixes on top of your thing",pytorch
3650,peterjc123,pr,2017-11-12T02:53:14Z,Fix CUDA builds for Windows,"1. CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS has a limitation, the maximum number of exported functions cannot exceed 65535. So it can't be used in CUDA builds. Removing this.
2. Specify static on an inline function to prevent linking errors. The MSVC uses C89 standard that will expose the inline function if it's not static.",pytorch
3658,apaszke,pr,2017-11-12T14:07:07Z,Cast tensors when loading optimizer state dicts,"Right now optimizers can load state dicts of other optimizers only if all parameters are matching in type and device (in contrast to `nn.Modules`). This is too strict for many use cases, and is addresses in this patch.

The only problem is that optimizer state isn't typed in any way, so code from this PR tries to make reasonable guesses - only state that's bound to certain parameters is casted (with parameter being the template), and we assume that floating point tensors in the state should match the type of parameter (I can't think of better way to handle load_state_dict across sets of parameters with different fp types). All other types are only moved to a different device.

Fixes #2830, #1442.",pytorch
3660,apaszke,pr,2017-11-12T15:21:41Z,Fix cuBLAS arguments for fp16 dot,"Result type has to be fp16 for fp16 dot. See [docs of `cublasDotEx`](http://docs.nvidia.com/cuda/cublas/index.html#cublas-dotEx) (look for ""datatypes combinations currrently supported"").",pytorch
3684,peterjc123,pr,2017-11-14T01:59:39Z,Fix CUDA 9 builds for Windows,"The changes in CUDA 9 breaks the operators here, so some functions need to be added. There's also a fix for the wrong behaviour in cuda setup helper.",pytorch
3686,peterjc123,pr,2017-11-14T02:45:05Z,Fix ssize_t in MSVC,,pytorch
3690,peterjc123,pr,2017-11-14T07:16:42Z,Fix the missing import in cudnn.py for Windows,,pytorch
3691,apaszke,pr,2017-11-14T10:35:12Z,Hotfix for ONNX BatchNorm export,"Note that BatchNorm still doesn't work with the JIT compiler. This makes tracing different than compilation (but only when modules have buffers)!

cc: @dzhulgakov ",pytorch
3692,peterjc123,pr,2017-11-14T14:13:22Z,set CC and CXX only when it's empty,"In that way, we can use sth like clcache to speed up builds.",pytorch
3703,apaszke,pr,2017-11-14T21:12:12Z,Minor JIT improvements,,pytorch
3713,peterjc123,pr,2017-11-15T09:54:57Z,Fix cmake scripts for CUDA and MSVC,,pytorch
3731,peterjc123,pr,2017-11-16T01:43:26Z,For NativeFunctions for MSVC,,pytorch
3756,peterjc123,pr,2017-11-17T14:42:06Z,Fix torch::hash for MSVC,,pytorch
3757,peterjc123,pr,2017-11-17T14:55:03Z,Fix CUDA builds after the NativeFunction change,,pytorch
3760,apaszke,pr,2017-11-17T18:50:38Z,Correct JIT interpreter autograd function,"This makes the interpreter autograd function respect edge pruning in the tracer, check flags of inputs, and create outputs with appropriate flags.

I haven't noticed any significant differences in run time, but these changes are needed for the code to be correct.",pytorch
3766,fritzo,pr,2017-11-17T23:55:43Z,Rename pyro.distributions.Multinomial -> .Categorical,Fixes #3764 ,pytorch
3767,peterjc123,pr,2017-11-18T02:57:05Z,Fix torch::hash for MSVC again,,pytorch
3781,vfdev-5,pr,2017-11-19T16:08:28Z,Update README.md,Typo fix ,pytorch
3782,apaszke,pr,2017-11-19T18:47:58Z,Remove dead code (related to stochastic function) + clean up input buffer,"As in the title. Next commits will remove `is_executable`, since it shouldn't be necessary anymore",pytorch
3783,apaszke,pr,2017-11-19T22:15:51Z,Fix hash.h compile errors in newer compilers,Disable the default `std::hash<T>` overload if `T` is an enum type.,pytorch
3799,apaszke,pr,2017-11-20T22:39:12Z,Always define outputs of ConvBackwardBackward,As in the title. Fixes #3743.,pytorch
3800,apaszke,pr,2017-11-20T22:41:20Z,Always define outputs of ConvBackwardBackward,Backport of #3799.,pytorch
3816,ssnl,pr,2017-11-21T17:37:32Z,Add determinant function on variable; Add backward on svd,"Addresses https://github.com/pytorch/pytorch/issues/3423.

On why not to bp through svd: there is no concrete formula when singular values are not distinct.",pytorch
3817,apaszke,pr,2017-11-21T19:38:15Z,Improve DataChannelMPI,"Remove unnecessary messages and make certain functions in-place.

This commit weakens error checking, but I think it's fine to make
it UB for now, and implement a better asynchronous mechanism later.

This also makes the backend work with CUDA-aware MPI implementations.",pytorch
3829,peterjc123,pr,2017-11-22T06:01:12Z,Fix THP_export for python_variable_indexing.cpp,,pytorch
3831,apaszke,pr,2017-11-22T11:21:54Z,Fix errors in previous DataChannelMPI refactor,"Not sure what happened, I must have forgotten to build after applying the last round of patches. Also, it seems that our contbuilds are missing MPI, so they aren't checking this code paths.

Fixes #3822.",pytorch
3841,fritzo,pr,2017-11-22T19:48:14Z,Implement torch.standard_gamma and distributions.Gamma,"Addresses #3813 

This implements a `torch.standard_gamma()` random number generator and a `distributions.Gamma` distribution that implements the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution). Note that this is named `torch.standard_gamma` to avoid confusion with the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function) that is already implemented as `torch.lgamma`. 

We follow scipy in generating standard Gamma variables `Gamma(alpha, 1)` rather than fully-parameterized `Gamma(alpha, beta)` random variables for two reasons: (1) this partial parameterization makes it easier to implement reparameterized gradients, and (2) the community is split between the scale parameter `theta` and the rate parameter `beta = 1/theta`. This PR uses the `beta` parameter in `distributions.Gamma`, but remain agnostic in `torch.standard_gamma(alpha)`.

## Tested

- Added deterministic tests of shape and `.log_prob()` method
- Added a randomized test of `.sample()` method (also added a test for `Normal.sample()`)

`test_distributions.py` runs in under 1 second.",pytorch
3842,ssnl,pr,2017-11-22T20:31:12Z,Fix padding_idx getting ignored in backward for Embedding(sparse=True),"Fixes https://github.com/pytorch/pytorch/issues/3506

Verified that gradients are identical for sparse and dense in `Embedding(padding_idx=(not None), sparse=True)` with same weight matrix.",pytorch
3853,apaszke,pr,2017-11-23T19:54:30Z,Fix void* wrapping in autograd codegen,"The real reason for this PR is this:

before:

```python
>>> Variable(torch.randn(5, 5)).data_ptr()
True
```

after:

```python
>>> Variable(torch.randn(5, 5)).data_ptr()
31109648
```

It used to work because `void*` would get implicitly casted to a `bool` when searching for an overload of `wrap`. This commit fixes this and adds a bunch of assertions to make sure this won't happen again. I can't think of any way to make argument implicit conversion a build failure in C++.",pytorch
3854,apaszke,pr,2017-11-23T19:55:16Z,Fix void* wrapping in autograd codegen,See #3853. This is a v0.3 backport.,pytorch
3855,apaszke,pr,2017-11-23T19:58:27Z,Remove as much of Python from JIT hot path as possible,"This patch reduces the amount of Python code we have to hit when actually calling JIT-compiled modules or functions to 0, bringing some nice speedups.

Time for single application of a JIT-fused LSTM cell in a loop for 512 steps (forward only - with autograd enabled):
- before - 35us
- after - 26us
- cuDNN (nn.LSTM) - 28us?? The possible reasons for this might be that the JIT cell was specialized to never run backward, so it can save a bunch of memory bandwidth, or I did sth wrong in my benchmarking script). However, cuDNN is the only one of these which is not CPU-bound (only 15us on CPU).",pytorch
3859,apaszke,pr,2017-11-24T12:39:30Z,Fix lint,,pytorch
3885,apaszke,pr,2017-11-26T20:47:22Z,Multiple JIT-related fixes,"- Falling back to AutogradClosure now produces a warning (so that we can at least see if it happens and know that our perf numbers are incorrect)
- Recording the trace should happen before saving outputs (their tracing state needs to be embedded in `SavedVariable`s)
- Fixed handling ops with multiple inputs in `aten_dispatch.cpp` (currently only `cat`). Used to trigger an assertion error before
- Improved the fuser algorithm to do an extra pass and merge neighbouring fusion groups.",pytorch
3888,apaszke,pr,2017-11-26T23:10:06Z,Clean up InputBuffer,"Also accept sparse tensors or corresponding types in VariableType.

Note that this is expected to fail until we merge the new interpreter, because it uncovers bugs that were fixed in that PR.",pytorch
3932,ngimel,pr,2017-11-29T00:30:26Z,use torch.cat in _flatten,Brings time to 21ms from 35ms in #3930,pytorch
3934,peterjc123,pr,2017-11-29T12:09:41Z,Fix wrong arg in operator function for MSVC,,pytorch
3935,peterjc123,pr,2017-11-29T12:10:18Z,Enable ext build for Windows,,pytorch
3937,ssnl,pr,2017-11-29T16:43:32Z,Improve Tensor.scatter_ doc,"Adding more description on the actual formula and size constraint.

Test:
![screenshot 2017-11-29 11 42 45](https://user-images.githubusercontent.com/5674597/33387117-754cb23c-d4fa-11e7-9c93-ce21e6cb93a6.png)
",pytorch
3953,ssnl,pr,2017-11-30T18:57:42Z,Fix CUDA index_fill_ boundary check with small tensor size,"One character PR :)

Fixes #3922 .",pytorch
3954,ssnl,pr,2017-11-30T19:44:22Z,Improve Tensor.new doc,"Addresses #3920 .

Test:
<img width=""730"" alt=""screenshot 2017-11-30 14 43 52"" src=""https://user-images.githubusercontent.com/5674597/33451403-f2669e6c-d5dc-11e7-8b42-dc60e3cc4c1a.png"">
",pytorch
3960,peterjc123,pr,2017-12-01T05:23:33Z,Hide the unknown headers for WIN32,,pytorch
3969,ssnl,pr,2017-12-01T17:33:48Z,Improve docs for torch and torch.Tensor,"Makes the documents overall much more consistent. Changes include:

1. raw docstrings.
2. improves / add some math expressions
3. makes doc for certain methods clearer, e.g. `chunk` and `split`
4. use a consistent format for tensor shape
5. [Tensor, `Tensor`, `tensor`, etc.] -> tensor when not referring the the class
6. fixes several formatting errors caused by newline

",pytorch
3972,ssnl,pr,2017-12-01T21:23:07Z,Implements gradients calculation for trtrs,"Related issue: #440 

cc: @samuela ",pytorch
3974,ssnl,pr,2017-12-01T22:20:30Z,Fix doc change lint errors from #3969,"Fix lint errors from #3969 .
@colesbury ",pytorch
3978,fritzo,pr,2017-12-02T00:57:56Z,Implement reparameterized gradient for Gamma sampler,"Closes #3813

This implements reparameterized gradient for `distributions.Gamma`. The gradient is implemented by directly approximating the reparameterized gradient function `dx/dalpha` following [Knowles (2015)](https://arxiv.org/pdf/1509.01631.pdf). The approximation is accurate to within 0.5% relative error for a wide range of alphas.

## Derivation

First consider the `beta` variable. If `x ~ Gamma(alpha, beta)` then `x / beta ~ Gamma(alpha, 1)`. Since division is already implemented in PyTorch, we can thus reduce our problem to computing a reparameterized gradient of a standard gamma `x ~ Gamma(alpha) = Gamma(alpha, 1)` wrt `alpha`.

This PR implements a function `standard_gamma_grad(x, alpha)` that directly approximates the reparameterized gradient defined (for any continuous univariate distribution) as
```
                d/dalpha cdf(x; alpha)     d/dalpha cdf(x; alpha)
dx / dalpha = - ---------------------- = - ----------------------
                  d/dx cdf(x; alpha)           pdf(x; alpha)
```
This definition is used in the unit tests in `tests/test_distributions.py`, which compute `d/dalpha cdf(x;alpha)` via finite difference of the `scipy.stats.gamma.cdf()` function.

The approximation is split into three regions:
- For small `x` we use a power series approximation of `cdf(x, alpha)`.
  Until `digamma()` is implemented in PyTorch, we use a finite difference of `lgamma()`.
- For large `alpha` we use the approximation
  ```
  standard_gamma_grad(x, alpha) = sqrt(x/alpha)
  ```
- For intermediate x,alpha we use a rational function approximation
   ```
   standard_gamma_grad(x, alpha) = exp(PQ(log(x / alpha), log(alpha)))
   ```
   where `PQ(u,v)` is a rational function of order 2 in u and 3 in v.

For complete derivation, see this [Jupyter Notebook](https://github.com/fritzo/notebooks/blob/master/gamma-reparameterized.ipynb).",pytorch
3993,peterjc123,pr,2017-12-04T06:39:08Z,Enable ninja during python build process for MSVC,,pytorch
4000,peterjc123,pr,2017-12-04T13:52:58Z,[WIP]Improve Dataloader on Windows,"Process creation is very expensive in Windows. So we'd better hold the processes rather than spawning new ones. We should use a different style for loading data rather than the original one that keeps spawning processes.
Still in progress.",pytorch
4003,ngimel,pr,2017-12-04T17:44:28Z,Remove separate nccl installation from Dockerfile9,Base image already contains nccl,pytorch
4009,ssnl,pr,2017-12-04T19:51:30Z,Fix CUDA Multinomial checks,"Fixes #3475 .
Also adds probability non-negative check on CPU.",pytorch
4018,ssnl,pr,2017-12-04T22:55:25Z,Add default PyTorch seeding and worker_init_fn to DataLoader,Fixes #3880 .,pytorch
4021,ngimel,pr,2017-12-05T00:25:35Z,allow cudnn for fp16 batch norm,,pytorch
4022,ngimel,pr,2017-12-05T01:57:23Z,Use integer division to fix failing test,,pytorch
4034,kashif,pr,2017-12-05T15:03:53Z,added AMSgrad optimizer to Adam and SparseAdam,- [x] need to look at SparseAMSGrad version,pytorch
4040,ngimel,pr,2017-12-05T21:35:37Z,slightly simplified math in IndexToOffset,,pytorch
4044,ngimel,pr,2017-12-05T23:07:52Z,handle requires_grad when creating buckets for distributed,Partially addresses #2533 (this issue is also present in 0.3),pytorch
4052,peterjc123,pr,2017-12-06T09:50:57Z,Resolve environmental variable conflict,CMAKE_GENERATOR is used in Windows but for a different purpose. Don't override it.,pytorch
4056,ssnl,pr,2017-12-06T19:04:23Z,"Assert MKL ld* conditions for ger, gemm, and gemv","Issue #3606 .

Changes are mainly:

1. in `THBlas.c`, use assert rather than `if`.
2. in relevant caller functions in `THTensorMath.c`, make tensors contiguous if those checks aren't met.
3. add contiguity checks in 4 legacy kernels (\[Spatial|Volumetric\](Full)?DilatedConvolution) that calls these methods. They are contiguous before, but the logic involves both Python and C++, and is a bit complicated, so I add the check to be clear, and to be safe.",pytorch
4062,ssnl,pr,2017-12-06T23:13:50Z,Allow .view on noncontig tensors when certain conditions are met,"Implements #3653 .

On a high level, a tensor with `size` can be viewed as a `view_size` if we
1. separate dimensions in `size` into chunks where each chunk is ""contiguous"" within itself.
2. can separate `view_size` into same number of chunks, where each chunk pair has matching ""numel"", i.e. number of subspace.

Copying from the doc change in this PR:

  For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions :math:`d, d+1, \dots, d+k` that satisfy the following contiguity-like condition that `\forall i = 0, \dots, k-1`,`stride[i] = stride[i+1] \times size[i+1]`

  Otherwise, :func:`contiguous` needs to be called before the tensor can be viewed.

The diagram in the added test case might help understanding this as well.",pytorch
4095,ssnl,pr,2017-12-08T23:04:15Z,"Add python only default init expression; Implement stft, hann/hamming/bartlett window.","This PR

1. Implements short-time fourier transform #3775 with tests against Scipy.
2. Adds `atan2` for cuda double & half tensors. Refactored `atan2` to `THCNumerics`, Op to `THCTensorMathPointwise.cuh`.
3. Adds support for python only default init in the parsers. This allows us to write argument with a default value that can either cause ambiguity in c++ (e.g., `Scalar p` in `norm`) or have a type that doesn't allow default value `None/NULL/nullptr` (e.g., `int64_t fft_size` in `stft`).
  a. Changes `norm` to also use this in both `Declarations.cwrap` for #1419 . When `Tensor` and `Variable` merge, it should fix the issues in #1419 .
4. Implements three commonly used window functions (Hann, Hamming, Bartlett) with tests against Scipy

On 3: Now in `python_variable_methods.cpp`, if an argument has python only default value, it will be generated like the following. I added the comment for clarification purpose. It won't be in actual code. 
```
static PyObject * THPVariable_stft(PyObject* self, PyObject* args, PyObject* kwargs)
{
  HANDLE_TH_ERRORS
  static PythonArgParser parser({
    ""stft(int64_t frame_length, int64_t hop, int64_t fft_size=None, Tensor window=None, int64_t pad_end=0)"",
  });
  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;
  PyObject* parsed_args[6];
  auto r = parser.parse(args, kwargs, parsed_args);
  if (r.idx == 0) {
    Tensor & self = self_;
    int64_t frame_length = r.toInt64(0);
    int64_t hop = r.toInt64(1);
    const Tensor & window = r.tensor(3);
    int64_t pad_end = r.toInt64(4);
    // fft_size has python only default, so it will be generated last
    int64_t fft_size = r.toInt64WithDefault(2, frame_length);  
    return wrap(dispatch_stft(self, frame_length, hop, fft_size, window, pad_end));
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
```
",pytorch
4106,ngimel,pr,2017-12-11T00:51:10Z,improve performance of maxpooling backwards,,pytorch
4117,fritzo,pr,2017-12-11T18:18:19Z,Implement Dirichlet and Beta distributions,"This Implements `Dirichlet` and `Beta` distributions. The samplers use`Gamma.sample()` and the reparameterized gradients are computed with numerical approximations to the `cdf`. The computed gradients are accurate to within 1% relative error for a wide range of parameters.

For details of the derivation, see this [Jupyter Notebook](https://github.com/fritzo/notebooks/blob/master/beta-reparameterized.ipynb). Math has been reviewed by @martinjankowiak 
",pytorch
4129,fritzo,pr,2017-12-12T05:53:25Z,"Implement .enumerate_support() for Bernoulli, Categorical distributions","## Why?
This method is used heavily in Pyro distributions for summing-out discrete variables.

## How?

The `.enumerate_support()` method returns a Tensor (or Variable) with all possible elementwise values, enumerated over the leftmost dimension. The result can thus be passed to `dist.log_prob()` as if it were a batch of samples. Note that `.enumerate_support()` does not enumerate over the cartesian product: doing so is exponentially expensive, rarely useful, and easy to accomplish efficiently via `itertools.product(dist.enumerate_support())`. In Pyro we originally implemented cartesian product but then switched to this more useful version.

This PR also includes the property `.has_enumerate_support` which is only true for discrete distributions.

Note this PR should be completely forwards-compatible with @neerajprad's upcoming PR to change batch shape https://github.com/probtorch/pytorch/issues/30.",pytorch
4131,neerajprad,pr,2017-12-12T07:19:32Z,Moving distribution classes into a separate package,"This moves the distribution classes defined in `distributions.py` into a separate package. As many of us are working on distributions and continue to add more classes as well as more functionality into each distribution class, this will help us prevent merge conflicts. Utility functions for distributions can be put into `distributions/utils.py`.

While in this case it doesn't confer any major advantages, having multiple classes per module (/file) is standard convention in python. So this should be treated as a non-essential PR and can be closed if we would like to adhere to this convention.

**Testing:** This is purely an internal refactoring, and should not have any impact on existing code / tests. 

**Issue:** [probtorch/#31](https://github.com/probtorch/pytorch/issues/31)",pytorch
4140,neerajprad,pr,2017-12-12T22:47:25Z,Allow for broadcasting of distribution parameters,"This ensures consistent broadcasting of parameters passed to distribution classes:
 - all scalars are upcast to Tensors / Variables. 
 - all Tensors / Variable arguments are broadcasted to the inferred shape in the constructor.
 - the Bernoulli distribution also supports scalar parameters now to keep it consistent with Gamma and Normal.

Note that this is needed for consistent handling of batch and event shapes [probtorch#30](https://github.com/probtorch/pytorch/issues/30). 

**Testing:** Unit test cases added for asserting on valid / invalid examples of parameter broadcasting.

**Issue:** Refer to [probtorch#32](https://github.com/probtorch/pytorch/issues/32) for more details. [Distribution design doc](https://docs.google.com/document/d/1wnABg0cdyaVMr-Xqz_brHBwnPJuzTeSk3Oqko2HHSfE/edit?usp=sharing)

**Note to reviewers** - This change is not backwards compatible. e.g. 
```
Normal(torch.Tensor([[0, 0]]), torch.Tensor([[1], [1]])).sample()
earlier -> Tensor of shape (1, 2); same as mean.size()
now -> Tensor of shape (2, 2); with full broadcasting
```
Also, let us get outstanding PRs, namely #3886 and #4129 merged first, so as to avoid creating further merge conflicts.
",pytorch
4155,ssnl,pr,2017-12-13T19:08:48Z,Improve svd doc,Fix #4152 ,pytorch
4193,neerajprad,pr,2017-12-15T06:43:08Z,Provide full support for distribution shapes,"This allows us to support all the distribution shape semantics for sampling and scoring that we have discussed. 
 - distributions have internal `_batch_shape` and `_event_shape` attributes which are set at the time of construction. 
 - allows for arbitrary sample shapes (removing restriction for only a single dimensional `batch_shape` argument).
 - allows for scoring of arbitrary shaped samples. Some resulting minor change in `log_prob` methods. One such change is the part that calls `torch.multinomial` (which only supports up to 2d tensors) in `Categorical.log_prob`.

**Testing:** Unit test cases added to validate batch/event shapes are correctly determined internally, and the correct shapes for generated samples and log prob scores. Refer to similar [examples](https://docs.google.com/spreadsheets/d/11ULwArylCjdlk4qW2tin0dY2jZ5iRQ0xjtQHvTFmFgs/edit?usp=sharing) discussed.

**Note:** Some minor changes are not backwards compatible. Refer to the tests.

**Ref:** 
 - Issue [probtorch#30](https://github.com/probtorch/pytorch/issues/30)
 - [Distribution design doc](https://docs.google.com/document/d/1wnABg0cdyaVMr-Xqz_brHBwnPJuzTeSk3Oqko2HHSfE/edit?usp=sharing)
",pytorch
4203,apaszke,pr,2017-12-15T22:53:20Z,Allow map_location in torch.load to be a string,"Simplify loading models that are placed on a single device. To enforce all tensors to be loaded on the CPU you can do this:
```python
torch.load('file.pt', map_location='cpu')
```",pytorch
4210,fritzo,pr,2017-12-17T04:28:40Z,Allow value broadcasting in distributions.Distribution,"This relaxes `Distribution._validate_log_prob_arg()` to allow broadcasting. This came up in a simple use case:
```py
p = Normal(Variable(torch.Tensor([0.0])),
           Variable(torch.Tensor([1.0])))
x = torch.arange(-2,2,0.01)
pdf = torch.exp(p.log_prob(x))  # Fails before this PR, works after.
pyplot.plot(x.numpy(), pdf.numpy())
```
This also removes two unused imports of `expand_n` from files in torch.distributions.

[Design Doc](https://docs.google.com/document/d/1wnABg0cdyaVMr-Xqz_brHBwnPJuzTeSk3Oqko2HHSfE/edit)",pytorch
4216,apaszke,pr,2017-12-17T20:57:11Z,Cache DataParallel replicas,"This commits makes DataParallel cache the wrapped module replicas, and helps bring down overhead of `replicate` on ResNet1001 from ~140ms to 42ms. Remaining time is spent mostly in `broadcast_coalesced`, so moving it to C++ is the next step.

This commit really starts to push Python to the limit, which can be seen in two places:
* There's this ugly odict hack, because Python inheritance does weird things, and selects a much slower implementation of `__getitem__` for OrderedDict subclasses than it could. I'm going to post to Python's mailing lists and clarify why is this happening, but I'm not aware of any reason why this hack would not work. This might seem like a silly thing, but removing this hack costs us 30ms at each forward.
* I was forced to change the implementation of `torch.jit.compile` for modules, and implement this poor man's inheritance-like thing (including the `__instancecheck__` hack so these object still appear to belong to subclasses)... I've tried a few other things, but I can't come up with anything else that wouldn't break. The problem is that having one superclass with `__slots__`, and another one in C++ confuses Python, and it complains that it can't figure out how to lay them out in memory. I'm happy to discuss alternative solutions.",pytorch
4226,peterjc123,pr,2017-12-18T14:56:41Z,Add build support for Python 2.7 using MSVC,"Inspired by @Giszy, I found a way to build with Python 2.7 using MSVC without modification to distutils under Windows. This PR adds the build support for Python 2.7 using the latest MSVC compiler. Some networks are able to run now. However, there're still some tests that can't pass in the CPU build, including:
- [ ] crash when excuting `test_torch.py`
- [ ] crash when excuting `test_autograd.py`
- [ ] crash when excuting `test_legacy_nn.py`
- [ ] errors when excuting `test_cuda.py`
- [ ] Although the test passes, I don't know whether `load_lua` is working as it should be, which has been discussed [here](https://github.com/pytorch/pytorch/pull/2941#discussion_r142378429) previously.",pytorch
4243,ssnl,pr,2017-12-18T23:33:43Z,Allow optional int tensor in Variable,"Fixes #4217 .

Tests: script with #4217 now shows expected behavior. The `test_torch.py` test doesn't really test this as it operates on Tensors. But it will cover this once Tensor and Variable merge.",pytorch
4256,fritzo,pr,2017-12-19T20:18:27Z,"Implement .numpy_dtype() method for Tensor, Variable","This implements `Tensor.numpy_dtype()` method equivalent to `Tensor.numpy().dtype` but without creating the intermediate numpy array.

## Why?

I'm trying to avoid NANs in division by adding tiny numbers to tensors. Tiny numbers depend on datatype: float32 vs float64 etc. In Numpy I can add do this with
```py
x = np.zeros(1)
x += np.finfo(x.dtype).tiny
```
PyTorch currently has no way to map PyTorch dataypes to Numpy datatypes. This PR adds a `.numpy_dtype()` method to support the desired behavior via
```py
x = torch.zeros(1)
x += np.finfo(x.numpy_dtype()).tiny
```
See https://discuss.pytorch.org/t/min-positive-value-for-each-tensor-type/11215",pytorch
4261,fritzo,pr,2017-12-19T23:13:54Z,Fix broken test_beta_log_prob in Python 3.6,"Addresses #4260 

Thanks to @ngimel who noticed the breakage due to #4117!

Tested with Numpy 1.13.3 and SciPy 1.0.0:
- Python 2.7
- Python 3.6",pytorch
4262,fritzo,pr,2017-12-20T00:20:12Z,Ensure gamma samples are positive,"This ensures random Gamma samples are positive by clamping results to at least `DBL_MIN` or `FLT_MIN`. This is needed to avoid NANs in `Beta` and `Dirichlet` samples for small `alpha` (e.g. < 0.001).

This is a low-level alternative to `Tensor.numpy_dtype()` in https://github.com/pytorch/pytorch/pull/4256 , which would require a dependency on Numpy's `np.finfo` in torch.distributions.

## Tested

- Added a regression test
- Tested locally with Scipy in Python 2.7
- Tested locally with Scipy in Python 3.6",pytorch
4268,fritzo,pr,2017-12-20T01:55:15Z,Implement .entropy() methods for all distributions,"This adds `.entropy()` methods for all probability distributions.

Math was reviewed by @neerajprad 

This uses a makeshift finite-difference implementation of `digamma()` until #3955 is merged.

## Tested

- Added shape tests of `.entropy()` for all distributions
- Added test infrastructure for additional parameterized tests
- Tested with Scipy on Python 2.7
- Tested with Scipy on Python 3.6
",pytorch
4270,neerajprad,pr,2017-12-20T06:08:07Z,Minor changes to test utils to catch type errors,"Some minor changes to test utils to have consistent behavior between python 2/3 and to catch type mismatch errors.
 - do not swallow `TypeError` in `assertEqual`.
 - override `assertAlmostEqual` to handle iterables, tensors and numeric types.

Tested locally on python 2.7 and 3.6.",pytorch
4272,apaszke,pr,2017-12-20T13:45:02Z,Document some autograd invariants,,pytorch
4276,apaszke,pr,2017-12-20T17:26:35Z,Make the JIT interpreter handle unused inputs correctly,,pytorch
4297,apaszke,pr,2017-12-21T16:00:45Z,Handle repeated inputs and outputs in the JIT,As in the title. Fixes some tracer-related bugs.,pytorch
4300,apaszke,pr,2017-12-21T18:03:07Z,Squash some warnings,"Made the build warning-free for me. They were starting to accumulate and telling them apart from legit warnings and errors in new code was increasingly hard. Most of the things are simple signed/unsigned comparisons, redefinition of a macro, some unused functions, return values that aren't ignored properly. There's one change in the codegen to prevent it from generating unused locals.",pytorch
4314,kashif,pr,2017-12-22T12:35:13Z,fix AMSGrad for SparseAdam,Fix for issue #4271 ,pytorch
4318,ssnl,pr,2017-12-22T18:17:03Z,Fix btrifact for variables,"Fixes #4217 

This PR breaks `btrifact` into `btrifact` and `btrifact_with_info`. The cwrap declarations for for tensor and variable are different in that the `tensor.btrifact` in `TensorMath.cwrap` still allows `info` optional argument, and `var.btrifact` in `Decalaration.cwrap` always sets `info` as NULL. In `variable.py`, a python method `btrifact` is added to support old `info` optional argument behavior, but I added deprecation warning in favor of `btrifact_with_info`.

I also modified `preprocess_declarations.py`'s `is_nullable` for optional tensor arguments with NULL default values. An example of input declarations is the old `btrifact`in `Declaration.cwrap`:
```[[
  name: btrifact
  cname: btrifact
  types:
    - floating_point
  backends:
    - CPU
    - CUDA
  variants:
    - method
    - function
  return: argument 0,1
  arguments:
    - arg: THTensor* result
      output: True
    - arg: THIntegerTensor* pivots
      output: True
    - arg: THIntegerTensor* info
      kwarg_only: True
      default: NULL
    - arg: bool pivot
      kwarg_only: True
      default: ""true""
    - THTensor* self
]]
```

, which gets parsed into the following:

```
{..., 'options': [{'arguments': [..., 
{'kwarg_only': True, 'default': None, 'type': 'THInteger Tensor*', 'name': 'info'}, 
...] ...}
```",pytorch
4321,fritzo,pr,2017-12-22T19:23:02Z,Fix torch.distributions.Gamma.rsample(),"Fixes #4319 

This reverts parts of #4306 f5de5a8 that broke `_standard_gamma_grad()` and adds a regression test. The existing test was skipped on CI due to #4260, and local test failures were ignored.

## Tested
- Added a regression test to ensure methods can run on CI (even despite #4260)
- Ran `test/test_distributions.py` locally with Scipy",pytorch
4328,neerajprad,pr,2017-12-23T01:13:26Z,Adding Uniform distribution to PyTorch,"The implementation is straightforward. A couple of notes:
 - In our discussions, we had discussed throwing an error if `log_prob` got a value outside the distribution's support. This returns `-inf` instead. I thought that throwing an error does not seem to be a good response for a perfectly valid query. For reference, Tensorflow return `-inf` for uniform but throws an error for Bernoulli (scoring `[1]` with `Bernoulli([0.0])` for example). We should be consistent with whatever convention we choose.
 - Modified `sample_n()` to use `torch.Size` instead of a tuple `(n,)`. The reason is that `tensor.new(tuple)` just takes the last dim, as compared to `tensor.new(torch.Size)`. Not sure if that's expected. cc. @apaszke. 

```
In [7]: torch.zeros(2, 2).new((3, 2)).size()
Out[7]: torch.Size([2])

In [8]: torch.zeros(2, 2).new(torch.Size((3, 2))).size()
Out[8]: torch.Size([3, 2])
```",pytorch
4332,vishwakftw,pr,2017-12-23T07:14:40Z,[Feature Request] Noisy SGD,"Based on the paper [Adding Gradient Noise Improves Learning for Very Deep Networks](https://arxiv.org/abs/1511.06807), I have added functionality to incorporate gradient noise.

Please let me know what you think about this.",pytorch
4350,vishwakftw,pr,2017-12-26T18:23:30Z,Adding torch.expm1() and its inplace function,This closes issue https://github.com/pytorch/pytorch/issues/4007 . Please let me know your views. I followed a similar style as done for the implementation of `torch.log1p`.,pytorch
4351,neerajprad,pr,2017-12-26T19:03:46Z,Fix distribution tests due to merge order,"Since #4270 got merged after #4328, all assertions that dealing with infinite values must use the flag (`allow_inf = True`) in `self.assertEqual`.

cc. @apaszke, @fritzo. ",pytorch
4356,neerajprad,pr,2017-12-27T02:07:11Z,Adding the Cauchy distribution to torch.distributions,"This adds the Cauchy distribution to `torch.distributions`. Note that `sample` uses the default implementation since the implementation of `cauchy` (unlike `normal`) in PyTorch does not allow for mu/gamma parameters to be tensors. 

Reviewed in [probtorch#51](https://github.com/probtorch/pytorch/pull/51]).",pytorch
4357,fritzo,pr,2017-12-27T03:01:12Z,Implement OneHotCategorical distribution,"Fixes probtorch#5 (OneHotCategorical)
Fixes probtorch#8 (.shape properties)
Fixes #4353 (fixing random seeds in tests)

This implements a `OneHotCategorial` distribution as a thin wrapper around `Categorical`. This is the most well-tested multivariate distribution so far (`Dirichlet` is the only other multivariate distribution and that is mostly tested by the `Beta` special case). I've fixed a number of issues with `test_distributions.py` to allow easier testing of multivariate distributions.

This also adds `.batch_shape` and `.event_shape` properties, which are now used by `OneHotCategorical` and in tests.

Finally this sets random number seeds in `common.TestCase.setUp()` method as suggested by @ezyang. I've added that in this PR because a nondeterministic test failure came up while testing this locally. This PR also adds maintainer instructions for dealing with randomized statistical tests, as suggested by @ezyang.

Math was reviewed by @neerajprad ",pytorch
4369,fritzo,pr,2017-12-27T21:15:27Z,Improve precision of standard_gamma_grad(),"This implements @martinjankowiak's improved approximation of reparameterized gradients for the standard Gamma distribution, reducing worst-case relative error from .005 to .0005.

See [Derivation](https://github.com/fritzo/notebooks/blob/master/gamma-reparameterized.ipynb)

Math reviewed by @martinjankowiak.",pytorch
4371,vishwakftw,pr,2017-12-28T03:11:08Z,Adding description for Optimizers,"This closes issue https://github.com/pytorch/pytorch/issues/4362 . 

Please let me know your views and suggest improvements if any.",pytorch
4379,vishwakftw,pr,2017-12-28T12:53:00Z,Fix some minor typographical errors,This closes https://github.com/pytorch/pytorch/issues/4376 and fixes a minor nit in the documentation for `torch.expm1` - due to my earlier PR.,pytorch
4385,vishwakftw,pr,2017-12-28T16:53:52Z,Update derivative of expm1,Changes derivative of `expm1` from `grad * self.exp()` to `grad * (result + 1)`.,pytorch
4399,fritzo,pr,2017-12-29T00:42:34Z,Add low-precision digamma() and polygamma() functions,"This implements `digamma()` and `polygamma()` functions on top of @zou3519's PR #3955 which is currently blocked due to licensing issues. This PR implements a low-precision versions of these functions, but keeps all of Richard's wiring. `polygamma(n,x)` is only implemented for `n=0` (`digamma(x)`) and `n=1` (`trigamma(x)`). This implementation is new code and is unencumbered by a license.

## Why?

Users of torch.distributions are currently blocked from differentiating the density of a number of distributions due to lack of `digamma`: `Chi2`, `Gamma`, `Dirichlet`, and `Beta`. The low-precision `digamma()` in this PR is sufficient accuracy for basic statistical applications.

Accuracy: `digamma()` is accurate to at least 7 digits, `trigamma()` is accurate to at least 5 digits. See [this notebook](https://github.com/fritzo/notebooks/blob/master/polygamma.ipynb) for details.

## Tested

- Updated tests in `test_torch` to test more examples but with lower precision
- Ran all Distributions tests, some of which use `digamma()`
- Ran CUDA tests",pytorch
4405,vishwakftw,pr,2017-12-29T10:31:38Z,[WIP] Improvement in derivative computations and change return variable from `destination` to `result` for certain ops,This will close https://github.com/pytorch/pytorch/issues/4402. ,pytorch
4411,vishwakftw,pr,2017-12-30T08:51:36Z,Add Pearson Correlation and Correlation Coefficient,This will effectively close https://github.com/pytorch/pytorch/issues/1254. Finished implementation of Pearson Correlation.,pytorch
4415,vishwakftw,pr,2017-12-31T03:09:08Z,Modify derivatives for efficiency and change `destination` to `result` for consistency,"This closes https://github.com/pytorch/pytorch/issues/4402. Also, the variable names for the return values have been changed from `destination` to `result`.",pytorch
4416,peterjc123,pr,2017-12-31T09:19:00Z,Add quotes and fix ninja on Windows,,pytorch
4421,fritzo,pr,2017-12-31T21:42:12Z,Improve precision of dirichlet_grad() approximation,"This implements @martinjankowiak's improved approximation of reparameterized gradients for the standard Beta and Dirichlet distributions, reducing worst-case relative error from .01 to .001.

See [Derivation](https://github.com/fritzo/notebooks/blob/master/beta-reparameterized.ipynb)

Math reviewed by @martinjankowiak.",pytorch
4429,kashif,pr,2018-01-01T17:54:08Z,Decoupled Weight Decay Regularization in optimizers (added adamw and sgdw among others),"I have added the adamw and sgdw flags to the appropriate optimizers rather than their own ones for issue #3790

~~Instead of defining new optimizers as in  PR #3740 I am fixing the weight decay in the appropriate optimizers.~~

~~My only issue is that now the comparison tests between the older legacy optimizer and this one fails.~~

  
  ",pytorch
4438,ssnl,pr,2018-01-02T16:43:02Z,Fix NLLLoss doc,"The current NLLLoss doc from #4428 is inaccurate on `reduce=False` case. This PR fixes it, and generally improves readability. #3973 can be closed when this is merged.

Screenshot:
![screenshot 2018-01-02 11 39 22](https://user-images.githubusercontent.com/5674597/34491600-10ce03ec-efb2-11e7-9628-73de571ea6ca.png)
",pytorch
4443,apaszke,pr,2018-01-02T18:45:50Z,Move broadcasting code to C++,"Times for 2-GPU training of ResNet1001 (speedups are likely larger for more GPUs):

| Patches         | Time / batch |
|-----------------|--------------|
| -               | 615ms        |
| this PR         | 569ms (-46ms)        |
| #4216           | 499ms (-116ms)       |
| #4216 + this PR | 465ms (-150ms)       |

This improves the perf, and also unlocks new possibilities like moving the loops for `set_` to C++, which might save us creating a lot of short-lived Python wrappers for tensors.",pytorch
4444,ssnl,pr,2018-01-02T19:30:42Z,Fix setting using running stats in InstanceNorm*d,"The original docs about `.training(False)` uses running stats are wrong. [This line](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/instancenorm.py#L27) always uses `training=True` when calling BN backend function. This PR fixes the doc, and adds a method to control whether using running stats or instance stats.

Fixes #4348 . 
",pytorch
4448,neerajprad,pr,2018-01-02T21:18:43Z,Supporting logits as parameters in Bernoulli and Categorical,"This allows for using logits directly as parameters in Bernoulli and Categorical distributions. 

**Details:**
 - Uses the binary cross entropy operation to score Bernoulli samples. This makes the `log_prob` computation differentiable.
 - Categorical normalizes the probs/logits parameters provided in the constructor and stores these as attributes, so that other methods do not need to bother with normalizing unnormalized probabilities/logits.
 - Simplifies entropy computation (and corrects it for the case of Categorical with unnormalized probability values as parameters).",pytorch
4450,fritzo,pr,2018-01-02T22:44:26Z,Declare constraints for distribution parameters and support,"This adds declarations for the constraints on Distribution parameters and support.
This adds `.params` and `.support` attributes to each `Distribution` class. The purpose is to declare the support set of each parameter of a distribution and samples drawn from the distribution.

Constraint declarations can be used to generically chose transformations to unconstrained spaces. For example see this proposed framework: https://github.com/probtorch/pytorch/pull/64

See [Design Doc](https://docs.google.com/document/d/1wnABg0cdyaVMr-Xqz_brHBwnPJuzTeSk3Oqko2HHSfE/edit#heading=h.unemmumtnzfk)",pytorch
4453,peterjc123,pr,2018-01-03T03:45:10Z,Fix multiprocessing and dataloader tests on Windows,"While the other missing components are optional, this one is of much importance. The DataLoader in Windows is not so useful so we have to fix this.",pytorch
4458,apaszke,pr,2018-01-03T17:11:38Z,Guard PyArray_Check with WITH_NUMPY,Otherwise the build fails if `numpy` isn't installed,pytorch
4459,vishwakftw,pr,2018-01-03T17:14:08Z,Implementation of Pareto Distribution,"This closes https://github.com/pytorch/pytorch/issues/4434 .

Thanks to @fritzo. Math checked by @fritzo ",pytorch
4461,apaszke,pr,2018-01-03T17:40:05Z,Fix a leak in JIT interpreter + improve dropout,"Fixes #4396. This wasn't caused by dropout, but by the handling of Python functions in the JIT interpreter (implicit conversions of `PyObject*` to `py::object` that assume borrowing semantics, yay).

While fixing that I also noticed that dropout is unnecessarily cloning the input in `eval` mode.
  ",pytorch
4463,ssnl,pr,2018-01-03T19:02:59Z,Fix dataloader test_worker_seed,"1. Add lock for SynchronizedSeedDataset when incrementing the counter. I mistakenly assumed that `mp.Value` has atomic `+=`. 
2. Add additional os level close stderr for tests that launch failing process. So CI test outputs can be clean.

Test:
Run `python test_dataloader.py` on CI worker for 200 times. No blocks and no messy segfault outputs.

@ezyang 
  ",pytorch
4473,apaszke,pr,2018-01-04T11:48:41Z,Fix template type for std::array size,Should fix #4472.,pytorch
4491,vishwakftw,pr,2018-01-05T02:03:55Z,"Add Slicing capabilities for Sequential, ModuleList and ParameterList",This closes https://github.com/pytorch/pytorch/issues/2174 and https://github.com/pytorch/pytorch/issues/3737,pytorch
4508,peterjc123,pr,2018-01-06T07:26:28Z,"Fixes #4475, Add debug flag for Windows",,pytorch
4511,ssnl,pr,2018-01-06T22:33:54Z,Methods for checking CUDA memory usage,"1. Adds `torch.cuda.memory_cached`, `torch.cuda.max_memory_cached`, `torch.cuda.memory_allocated` and `torch.cuda.max_memory_allocated` to provide per-device memory stats. These will be useful for monitoring and benchmarking.
2. Adds two tests (single/multi-gpu) to test these four methods.

related issue: #1529 
  ",pytorch
4515,peterjc123,pr,2018-01-07T05:15:07Z,Implement demangle in Windows,A missing feature recorded in #4092.,pytorch
4517,vishwakftw,pr,2018-01-07T09:02:04Z,Implementation of Gumbel Distribution,"Math checked and approved by @fritzo .

cc: @apaszke ",pytorch
4522,peterjc123,pr,2018-01-07T11:51:06Z, Fix missing import and enable test for profiler on Windows,,pytorch
4525,fritzo,pr,2018-01-07T18:13:57Z,"Start framework for kl_divergence(-,-) in torch.distributions","This adds a generic function `kl_divergence(-,-)` and a registration decorator
```py
@register_kl(Normal, Normal)
def kl_normal_normal(p, q):
    ...
```
such that `kl_divergence(Normal(0, 1), Normal(0, 2))` will call `_kl_normal_normal()` under the hood.

The implementation has amortized constant lookup time, including failed lookup for `NotImplementedError`. It is important to keep this cheap so that generic inference algorithms can
```py
try:
    kl = kl_divergence(p, q):
except NotImplementedError:
    ...use an approximate method...
```

This also adds a generic `TestKL` to which test cases can be added.

Addresses #4330, probtorch#37
  
  
  
  ",pytorch
4529,neerajprad,pr,2018-01-08T08:46:43Z,Fix return type for Bernoulli enumerate_support,"Fixes [probtorch/pytorch#68](https://github.com/probtorch/pytorch/issues/68). Only the return type for Bernoulli's `enumerate_support` needed change. 
 - We use `FloatTensor` as the default return type of distribution's samples, , as most tensor operations support `FloatTensor` without needing any casting. 
 - If the underlying torch sampler has a different tensor return type, that may be used instead. e.g. in the case of Categorical, we use `torch.multinomial` that returns a `LongTensor`. The return type of `Categorical` is therefore a `LongTensor`. This also helps in cases where the generated sample is used to index into tensors or numpy arrays. However, for the `OneHotCategorical`, we default to `FloatTensor` since the samples are more generally used as masks.",pytorch
4544,ssnl,pr,2018-01-08T22:29:50Z,ATen conv param expansion; InstanceNorm use_running_stats fix,"1. Switched to `conv1d` in `stft`.
2. Added code to do param expansion in `_convolution` in ATen, fixing #4464 
3. @myleott noticed a bug in #4444 where names of a attr and a method collide in `instancenorm.py`. This fixes it.
4. Made an err msg in `check_input_shape_forward` clearer to prevent confusing err msg like 
```
Expected 4-dimensional input for 4-dimensional weight [10], 
but got input of size [11, 1, 32, 32] instead
``` 
from https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083.
",pytorch
4554,fritzo,pr,2018-01-09T06:44:49Z,Add unit test for Dirichlet.rsample(),,pytorch
4563,apaszke,pr,2018-01-09T19:48:04Z,Fix batch norm JIT dispatch,Fixes #4549.,pytorch
4572,fritzo,pr,2018-01-10T02:43:56Z,Add torch.distributions.utils._finfo for numerical stability,"This adds a lightweight `_finfo()` function for use in torch.distributions. Example usage:
```py
>>> _finfo(Variable(torch.HalfTensor())).eps
0.00097656
>>> _finfo(torch.HalfTensor()).tiny
6.1035e-05
>>> _finfo(torch.FloatTensor()).eps
1.19209e-07
>>> _finfo(torch.DoubleTensor()).eps
2.22044604925e-16
```
It is modeled after the [numpy.finfo](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.finfo.html) function. This PR also uses `_finfo` to clamp the `Gamma`, `Beta`, and `Dirichlet` distributions to avoid NANs.
  ",pytorch
4602,fritzo,pr,2018-01-11T02:41:26Z,Fix bug in Dirichlet.rsample(); add tests,"This fixes a bug in `Dirichlet.rsample()` for dim > 2. `Dirichlet.rsample()` was only tested via `Beta.rsample()` using dim=2, so this bug was not caught in previous tests. This PR adds two new tests for `Dirichlet` at n=3. This also numerically stabilizes `torch._C._dirichlet_grad()` by cancelling a factor `(1-x)`.

The bulk of the diff is moving tests around in test_distributions.py. I've done this to keep all `.rsample()` tests together, as they were starting to diverge.

Math and tests reviewed by @martinjankowiak.",pytorch
4604,apaszke,pr,2018-01-11T11:08:23Z,Fix errors in travis config,,pytorch
4614,neerajprad,pr,2018-01-11T21:43:53Z,Allow broadcasting of value x params in Categorical,"Allows for broadcasting of values and parameters across the batch dimensions for Categorical. The other distributions do not need to do explicit broadcasting in `log_prob` and already handle this case.

Fixes #79.",pytorch
4615,apaszke,pr,2018-01-11T21:46:28Z,Implement MM fusion (MM with add reduction tree),"A tree where leaves are matrix multiplies and inner
vertices are adds can be computed as a single mm.
Such subgraph often appear in backward if a single weight
is reused multiple times (e.g. in RNNs).

NOTE: this seems to be slightly slower on the GPU than the
naive implementation, but it's a huge win on the CPU
(think 100x lower overhead)",pytorch
4638,vishwakftw,pr,2018-01-12T17:22:16Z,Addition of KL-Divergences for torch.distributions,"Math checked and approved by @fritzo .

cc: @apaszke ",pytorch
4643,ssnl,pr,2018-01-12T19:17:34Z,Dataloader issues,"1. Fixes `signal.signal` being called on child threads.

2. Sometimes error in dataloader methods causes the trace to be polluted as in #4507 . After investigating this with @ezyang , we realize the reason is that when an exception is thrown, 

    1. sometimes worker processes are terminated before the `Dataloader` is deleted (likely because that the trace contains a reference to the `Dataloader`)

    2. then the workers got SIGTERM from loader before the clean up methods are called in `Dataloader.__del__`

    3. then loader's SIGCHLD handler is triggered, and it sees that workers fail, so it reports a RuntimeError with polluted trace as shown in #4507 .

    As a solution, this PR suppresses SIGTERM from loader in worker SIGTERM handler, and `exit(0)` in such case.

3. Improves some dataloder tests so they use a subclass of `multiprocessing.Process` that tracks the last error, and checks the error message.

4. Prevents unnecessary cuda context initialization in `Dataloader` by `torch.cuda.current_device()` in case `pin_memory=False`.

5. Always call `_remove_worker_pids` in `Dataloader.shutdown` so the set on cpp side in `Dataloder.cpp` is always updated correctly even when `Dataloader.shutdown` errors.

Fixes #4507 .

Verified that windows tests pass.",pytorch
4657,fritzo,pr,2018-01-13T20:00:46Z,Add tests for distribution .entropy() methods,"This adds a monte carlo test to check correctness of `Distribution.entropy()` implementations. Up to now, only the shape of `.entropy()` was tested for correctness.",pytorch
4662,neerajprad,pr,2018-01-14T05:27:28Z,Ensure lazy evaluation for probs and logits,"This makes a few modifications to ensure that `logits` and `probs` are not accessed by a method unless it needs to. e.g. accessing `probs` on a class that was initialized with `logits` only to get the batch shape or event shape. This affects Categorical, OneHotCategorical, Bernoulli and Multinomial.

**Testing:** Added some tests to prevent future regression:
 - `initializing` a distribution with `probs` and calling `sample`, `enumerate_support`, `batch_shape` and `event_shape` should not initialize `logits`.
 - Conversely, `initializing` a distribution with `logits` and calling `log_prob`, `enumerate_support`, `batch_shape` and `event_shape` should not initialize `probs`.",pytorch
4670,apaszke,pr,2018-01-15T11:38:49Z,Fix cast direction in THCBlas,Fixes #4017.,pytorch
4690,apaszke,pr,2018-01-16T20:19:21Z,Improve the engine support for functional graph execution,"Previously the side-effect free grad calculation was performed
using callbacks that could also override the decision to run a
function. However this had a few problems e.g. it forced us to iterate
over pretty much all functions in the graph and drop their buffers.

This patch improves the mechanism, by adding explicit support for this
kind of evaluation in execute(). It's safer, and the algorithm used to
decide which nodes have to be evaluated was replaced with a faster one.

This unblocks #4594 (cc: @prigoyal).
It also finally allows to correctly differentiate parts of the graph, which is useful in various models (cc: @srush)",pytorch
4691,neerajprad,pr,2018-01-16T20:26:41Z,Ensure lazy evaluation for probs and logits,"This makes a few modifications to ensure that `logits` and `probs` are not accessed by a method unless it needs to. e.g. accessing `probs` on a class that was initialized with `logits` only to get the batch shape or event shape. This affects Categorical, OneHotCategorical, Bernoulli, Binomial and Multinomial. 
 - Added `.new()` methods to these distributions that can help with generating tensors/variables of the right type and placing it on the correct CUDA device.

**Testing:** Added some tests to prevent future regression:
 - `initializing` a distribution with `probs` and calling `sample`, `enumerate_support`, `batch_shape` and `event_shape` should not initialize `logits`.
 - Conversely, `initializing` a distribution with `logits` and calling `log_prob`, `enumerate_support`, `batch_shape` and `event_shape` should not initialize `probs`.",pytorch
4706,vishwakftw,pr,2018-01-17T15:49:16Z,Implementation of the Fisher-Snedecor Distribution,"This closes https://github.com/probtorch/pytorch/issues/87. Math checked and approved by @fritzo 

cc: @apaszke ",pytorch
4736,ngimel,pr,2018-01-19T01:47:04Z,update runtime dockerfile,,pytorch
4739,vishwakftw,pr,2018-01-19T03:08:45Z,Adding better KL-Tests,"Joint work with @fritzo.

This PR adds stronger tests for `kl_divergence`. These tests found 3 bugs (now fixed) in the 35 `kl_divergence` pairs we've implemented

cc: @apaszke ",pytorch
4743,apaszke,pr,2018-01-19T17:33:06Z,Scaffolding for symbolic AD in the JIT,,pytorch
4746,ssnl,pr,2018-01-19T19:57:16Z,Heuristic-based autograd execution order,"**tl;dr**: This PR implements the heuristic-based autograd execution order, where the tasks for each thread are ordered by the time the `Function` is created. `Function`s created later are executed earlier.

The current breadth-first (BFS) order can cause huge memory usage in certain models. This was discovered using a real model that uses a `Linear` module multiple times in forward. After `Linear` is decomposed into `Transpose`+`Addmm` (https://github.com/pytorch/pytorch/pull/1935), the BFS orders all `TBackward` tasks at last to execute, causing huge amount of memory occupied by intermediate results. 

With help from @ezyang , @apaszke , @zdevito and @colesbury , I have benchmarked three autograd execution orders:

1. BFS (the current scheme): 
  Within a thread, tasks are fetched from a FIFO queue.
  Sample diff: None.

2. DFS:
  Within a thread, tasks are fetched from a LIFO stack.
  Sample diff: https://github.com/SsnL/pytorch/commit/ac5a97dca0caef88ed0c2d00e9c4b90663a3f2cc

3. HEAP (Heuristic based):
  Within a thread, tasks are fetched from a max-heap, ordered by the time each autograd Function is created.
  Sample diff: https://github.com/SsnL/pytorch/commit/44d91b1ea11b0cda2487d9f0ef68706365aabd94

The benchmark code is applying the above diffs on master at https://github.com/pytorch/pytorch/commit/2dd7039b6badcc362fb6da62a33c49418acd5c5d, with code from [#4511 (Methods for checking CUDA memory usage)](https://github.com/pytorch/pytorch/pull/4511) manually added.

The benchmarked tasks include: ImageNet on ResNet50, Open-NMT, word language model, CycleGAN, the mini-model with which we discovered the issue (*), and a model specifically crafted to make DFS and HEAP perform worse (**).

The benchmark details and results can be found [here](https://github.com/pytorch/pytorch/files/1647739/autograd.pdf). Roughly speaking, the performance for three approaches are similar, except on (*) and (**).

Basing on the results and following discussions, we think it would be reasonable to switch to HEAP ordering, because:

1. There is no substantial slowdown in some common models from benchmark results.

2. All three orders have edges cases that make them bad. But HEAP and DFS need a particular bad way of creating multi-device graph, which should be very uncommon. Yet BFS case can be encountered in real models, especially for models with many Linear layers (T+Addmm). HEAP generally performs slightly better than DFS in benchmark results.

3. This is probably a weaker point. For BFS and DFS, the actual backward order depends on the order of operator args. For HEAP, it instead depends on the creation order of ops, which I think is easier to reason about. Although we probably shouldn't encourage user to tune the ordering, it will be helpful for us to fix some OOM models without releasing new binaries.

Furthermore, after benchmarking on a tiny CPU model, we don't see obvious overheads for maintaining the heap.",pytorch
4759,apaszke,pr,2018-01-20T20:02:32Z,Add code for lambda lifting the backward in JIT's AD,"Added the parts that were missing from yesterday's PR (namely generating not only `df`, but `f` graph + metadata to connect their outputs).

This PR still doesn't support having selective requires_grad, and assumes that all outputs of the first stage are meant to be differentiated. I'll add this to both `differentiate` and `lambdaLiftReverse` in a next patch.",pytorch
4763,apaszke,pr,2018-01-20T23:02:53Z,[WIP] Handle requires_grad in differentiate (JIT AD),"**DO NOT MERGE.** Depends on #4759.

Second commit is the relevant part. It's still missing one part - how are we planning to create zero gradients (I don't think we have any ops that can do that easily).",pytorch
4770,apaszke,pr,2018-01-21T21:00:26Z,Check submodules only in build_deps,Fixes #4768.,pytorch
4812,ssnl,pr,2018-01-23T19:36:55Z,Fix output_nr not incremented correctly,"`output_nr` is not incremented properly in `rebase_history` and `set_history` when some tensors are undefined. This causes the autograd engine incorrectly putting input tensors at wrong indices in `InputBuffer`. See the following extremely simple double backward example that reproduces the bug:

```
import torch
import torch.nn as nn
from torch.autograd import Variable, grad

conv = nn.Conv2d(1, 10, 5)
input = Variable(torch.randn(1,1,32,32))
loss1 = conv(input).sum()
grad_bias, = grad(loss1, conv.bias, create_graph=True)
loss2 = grad_bias.sum()
loss2.backward()
```
Because `input` doesn't require gradient, the `ggW` and `ggb` terms are set to incorrect indices, and it throws this weird error message: 
```
RuntimeError: Expected 1-dimensional input for 1-dimensional weight [10], but got 
input of size [1, 1, 32, 32] instead
```

Why is this not detected in our tests: our double backward tests usually sets all parameters to `requires_grad=True`. Therefore the case where a backward function call returns an undefined tensor is not tested.

An issue has been submitted on testing with more diverse configurations: https://github.com/pytorch/pytorch/issues/4813.

Thanks @ezyang for helping me finding the cause.

Relevant forum post: https://discuss.pytorch.org/t/autograd-grad-dimension-error/12083/8",pytorch
4876,vishwakftw,pr,2018-01-26T18:15:21Z,Addition of ExponentialFamily,"This adds `ExponentialFamily`. `ExponentialFamily` uses concepts of Bregman divergence of the log normalizers of distributions to compute entropy and KL-divergences.

For distributions belonging to an exponential family, this PR adds tests to check implementations of entropy and KL-Divergence functions with both the Monte Carlo and Bregman divergences.

Joint work with @fritzo . Math checked and approved by @fritzo .",pytorch
4879,ssnl,pr,2018-01-26T19:40:49Z,Improve `torch.cuda.empty_cache` documentation,There seems to be a common misconception among some users that `torch.cuda.empty_cache` will increase the amount of GPU memory available and solve their OOM issues. This PR improves the documentation to be clearer on this front.,pytorch
4898,apaszke,pr,2018-01-28T00:16:44Z,Add support for requires_grad in JIT's AD,As in the title.,pytorch
4901,ssnl,pr,2018-01-28T19:48:36Z,Fix doc indentation of `torch.cuda.empty_cache`,Incorrect indentation from #4879 caused the rendered `note` to end early (http://pytorch.org/docs/master/cuda.html#torch.cuda.memory_allocated).,pytorch
4902,fritzo,pr,2018-01-28T20:32:04Z,Implement constraint registry for torch.distributions,"Addresses probtorch#55 probtorch#56
Previous reviews probtorch#108 probtorch#64

This implements two lookup tables `transform_to(-)` and `biject_to(-)`. Each inputs a `Constraint` object and returns a `Transform` object that transforms from `constraints.real` to the given constraint. To perform the reverse transform, use `transform_to(c).inv`. This is the final piece of metadata linking all of the constraints and transforms in torch.distributions.

Here's a [snapshot of the documentation](http://fritzo.org/temp/pytorch/html/distributions.html#module-torch.distributions.constraint_registry).

This design is loosely modeled after Stan's constraint-transform registry. See [Design Doc](https://docs.google.com/document/d/1wnABg0cdyaVMr-Xqz_brHBwnPJuzTeSk3Oqko2HHSfE/edit#heading=h.unemmumtnzfk) for more details.

- [x] added docs
- [x] added tests",pytorch
4916,apaszke,pr,2018-01-29T21:17:05Z,Add formulas for LSTM ops to JIT AD,,pytorch
4922,ssnl,pr,2018-01-30T00:13:24Z,[ready] Layer Normalization,"Commits:
1. Renames `ATen/Check.h` to `ATen/TensorUtils.h` with an additional method `at::maybe_data_ptr` added. #4851 
2. THNN BN code so that `running_mean` and `running_var` can be optional when `training=True`. #4509 
3. ATen changes and cuDNN changes. Still #4509 
4. Python nn.* changes, including changing `InstanceNorm*d`'s `use_running_stats` from #4444 to the new option `track_running_stats` on BN. Improves IN and BN docs.
5. Adds test for the new option for IN and BN. Improves other IN tests.
6. Adds Layer Normalization #1959  .
7. Fixes LRN doc.
8. Functional interface for IN and LN. 
9. Tests for LN. 
10. Fix BN double backward returning undefined tensor when it shouldn't.
11. Fix Jit tests that use wrong dim inputs for BN
12. Add/Improve BN, IN and LN GPU tests with half.
13. Update IN BN docs to be consistent with conv notation; Fix onnx failures.",pytorch
4928,peterjc123,pr,2018-01-30T07:41:59Z,Enable fixed tests again in Windows,The jit test seems to be working after @apaszke 's commit on JIT's fix in #4759. Let's enable the jit tests for Windows again. And then we can mark it completed in #4092.,pytorch
4937,fritzo,pr,2018-01-30T16:36:33Z,Support multivariate TransformedDistributions,"Reviewed by @rachtsingh and @alicanb at probtorch#116

This adds an `.event_dim` attribute to all `Transform`s and correctly handles event shape in `TransformedDistribution.log_prob()` and `ComposeTransform.log_abs_det_jacobian()`. Cases we need to handle are:
- When `TransformedDistribution.base_dist` has a *larger* `event_dim` than its transforms, we need to sum out the rightmost dimensions in the `transform.log_abs_det_jacobian()`s, otherwise there will be a shape error.
- When `TransformedDistribution.base_dist` has a *smaller* `event_dim` than its transforms (e.g. when implementing `MultivariateNormal` as an `AffineOperatorTransform` of univariate `Normal`), we need to sum out the rightmost dimensions of `base_dist.log_prob()`.
- When transforms have differing `event_dim`, we need to sum out all but the largest dim.

This PR also includes fixes to `ComposeTransform.event_dim` and `TransformedDistribution.event_shape` to support multivariate transforms.

This PR was the result of issues that came up in @rachtsingh's probtorch#113 and in @fritzo's refactoring of `InverseAutoregressiveFlow` in Pyro as we build on top of torch.distributions.transforms.

## Tested

- More tests for `TransformedDistribution`
- New tests for `Transform` shapes",pytorch
4941,apaszke,pr,2018-01-30T19:38:31Z,Fix JIT tracing in autograd codegen,`recordTracePost` should be called *before* outputs are saved.,pytorch
4961,vishwakftw,pr,2018-01-31T16:48:31Z,Add KL-divergence for Categorical and OneHotCategorical and stronger tests,"This PR adds two new KL-divergence pairs; one `Categorical, Categorical` and `OneHotCategorical, OneHotCategorical`. 

Additionally, it fixes a bug with Bernoulli KL-divergence (https://github.com/probtorch/pytorch/issues/117), which motivated adding edge case testing for KL-divergences.

Checked and Approved by @fritzo 

cc: @apaszke ",pytorch
4967,ssnl,pr,2018-01-31T20:05:06Z,"Revert ""torch.set_num_threads sets MKL option too""","Reverts pytorch/pytorch#4949

The recent addition of setting MKL num threads in torch.set_num_threads in multiprocessing on cuda CI machines causes segfaults. Our dataloader workers indeed do `torch.set_num_threads(1)` before the loading loop. Let's revert this first to unblock the PRs.",pytorch
4973,apaszke,pr,2018-01-31T22:33:34Z,Improve CUDA softmax performance,"Simple fix with a very large perf benefit for smaller sizes. Below are some plots (`dim_size` = size of the softmaxed dimension, `outer_size` = batch size, z-axis = ratio of old time to new time). In general, as long as `dim_size < 1024` you get at least a 2x speedup with this code, 4x if you fit in 256, and even 12x for sizes around 100 and smaller.

<img width=""435"" alt=""screen shot 2018-01-31 at 23 27 02"" src=""https://user-images.githubusercontent.com/4583066/35651055-9596a8b6-06de-11e8-97d6-32eaa4e74a49.png"">

<img width=""456"" alt=""screen shot 2018-01-31 at 23 27 18"" src=""https://user-images.githubusercontent.com/4583066/35651057-983b9edc-06de-11e8-9134-1b461a9016cc.png"">

I tried playing with some other potential improvements like replacing the blockReduce function with a shuffle-based one, but it gave mixed results (-20% time in some cases, +20% time in other cases).

Thanks to @nikitakit for reporting #4893 (which is fixed in this PR).",pytorch
4976,ssnl,pr,2018-01-31T23:24:28Z,Temporarily disable TestDataLoader.test_segfault,"Other segfaults, e.g. `test_multi_keep/drop` and `test_batch_sampler` are ""fixed"" in #4967 . However, `test_segfault` is still failing intermittently on CI machines, and I can't reproduce it on the CI machine. Let's disable it for now. I'll look more into this tomorrow. ",pytorch
4997,ssnl,pr,2018-02-01T21:21:49Z,[DO NOT MERGE] Re-enable test_segfault,This is to investigate why `test_segfault` was failing on GPU workers.,pytorch
4998,ssnl,pr,2018-02-01T22:08:29Z,README.md for ATen/cudnn,"Add README.md for `ATen/cudnn` to clarify the assumption of these files, and how to use them.

request review from @ezyang ",pytorch
5048,ssnl,pr,2018-02-05T15:58:25Z,Fix blas addmm (gemm) condition check,Fixes #5047 .,pytorch
5050,vishwakftw,pr,2018-02-05T16:45:45Z,Update torch.distributions documentation,"Checked and approved by @fritzo 

Changes:
1. Added documentation for the re-parameterization trick
2. Fix a typographical error in the `ExponentialFamily` documentation",pytorch
5061,ssnl,pr,2018-02-05T20:33:15Z,Broadcast output requires_grad only if corresponding input requires_grad,"This avoids unnecessary computation when finetuning only certain layers with DataParallel.

Fixes #5041 .",pytorch
5065,ssnl,pr,2018-02-06T00:23:26Z,Fix TH compile warnings,"Compiling from scratch prints a lot of warnings from TH files, making it impossible to identify which one is the error that stops compilation. Most of these warnings are unused variables, and comparing signed and unsigned types. This PR fixes all of compile warnings except for unknown pragma, which is suppressed via `-Wno-unknown-pragmas`.",pytorch
5079,vishwakftw,pr,2018-02-06T17:18:11Z,Implementation of the cumulative distribution function and its inverse,"This PR adds the `.cdf` and `.icdf` methods for distributions. @alicanb implemented these in the `Distribution` base class.

Furthermore, this PR adds these methods for `Exponential`, `Normal`, `Cauchy` and for `TransformedDistribution`. `Pareto` and `Gumbel` are converted to `TransformedDistribution` with base classes as `Exponential` and `Uniform` respectively.

Tests for checking the correctness of `.cdf` and `.icdf` methods have also been added. An autograd based check is added to check the correctness of `.cdf` and `.log_prob` as well (this doesn't depend on `TEST_NUMPY`).

Joint work with @fritzo and @alicanb . Checked and approved by @fritzo .",pytorch
5086,fritzo,pr,2018-02-06T20:06:33Z,Ensure Distribution.sample() result is detached,"This fixes a bug in `TransformedDistribution` whereby `.sample()` was accidentally reparameterized. It also fixes a couple other minor `.sample()` bugs and adds two tests to distinguish the behavior of `.sample()` and `.rsample()`.

Previous review by @alicanb at https://github.com/probtorch/pytorch/pull/125",pytorch
5092,apaszke,pr,2018-02-06T22:05:39Z,Support mm batching with transposed weights,,pytorch
5093,ssnl,pr,2018-02-06T22:49:05Z,Fix CPU torch.multinomial with noncontiguous prob tensor,"The CPU code previously uses `resize2d` which does not safely preserve data when input is not contiguous. This PR changes it to `unsqueeze1d` in both CPU and GPU functions, and add strided support for the two functions. As a result, `distributions.Categorical` now doesn't need to call `contiguous()` when the probabilities are not batched.

Also in this PR are 
1. updated tests for `torch.multinomial`,
2. updated doc for `distrbutions.OneHotCategorical` and `distrbutions.Categorical`, and
3. shape tests for `distrbutions.OneHotCategorical` and `distrbutions.Categorical` in unbatched mode.

Fixes #5062 

cc @neerajprad @alicanb for `distributions.*` changes review.",pytorch
5094,ssnl,pr,2018-02-06T23:10:43Z,Change long to int64_t in THCudaBlas_HgemmStridedBatched,Should fix #5091 .,pytorch
5139,ssnl,pr,2018-02-08T19:07:58Z,Conv doc and GPU matmul error message improvements,"Improves Conv*d(Transposed) docs to have correct newline and formatting

Improves CUDA matmul error message by basically copying the CPU error message  (#5135 )

e.g.,
Old doc:
<img width=""740"" alt=""screenshot 2018-02-08 14 05 39"" src=""https://user-images.githubusercontent.com/5674597/35992827-64d3dedc-0cd9-11e8-80a0-6fc6e514acbc.png"">

New doc:
<img width=""744"" alt=""screenshot 2018-02-08 14 05 26"" src=""https://user-images.githubusercontent.com/5674597/35992829-6bf7efb4-0cd9-11e8-8d73-3b7777a79be7.png"">
",pytorch
5140,ssnl,pr,2018-02-08T19:26:40Z,Cherry pick dataloader issue fix to 0.3.1,"Cherry picked these two:
https://github.com/pytorch/pytorch/pull/4812
https://github.com/pytorch/pytorch/pull/4453

cc @soumith ",pytorch
5143,ssnl,pr,2018-02-08T20:43:40Z,Adjust stft result comparison precision to 7e-6,,pytorch
5145,apaszke,pr,2018-02-08T22:09:38Z,Make tree views statically typed in JIT script AST,This should be ready to merge but is slightly incomplete (i.e. we're still missing some expression type views). Will update the PR with them tomorrow,pytorch
5172,fritzo,pr,2018-02-11T02:08:19Z,Fix sign error in TransformedDistribution.cdf() and .icdf(),"Fixes probtorch#127, a sign error in the `.cdf()` method of univariate `TransformedDistribuitons` that reverse direction of input.

I also wrapped some docstrings to fit in 80 characters so they print correctly in Jupyter notebooks.

Note this has a one-line merge conflict with #4950 in test_distributions.py. To resolve the conflict, accept changes from this PR and revert the workaround in #4950.

Previous review by @vishwakftw https://github.com/probtorch/pytorch/pull/128

## Tested

- Added a larger grid of random inputs for `AffineTransform` so that it is much more likely that tests include a direction-reversed example.",pytorch
5175,peterjc123,pr,2018-02-11T04:59:49Z,"About #4990, Makes Window build fail quicker","The reason that it continues to build when error occurs is the use of `exit /b 1`, which is not sufficient for returning error code to python. According to the [post](https://stackoverflow.com/questions/6637468/incorrect-exit-code-in-python-when-calling-windows-script) here, it can be replaced by `exit 1` to generate the correct error code.",pytorch
5178,apaszke,pr,2018-02-11T21:59:30Z,Fix compound assignment in JIT script,cc: @jamesr66a ,pytorch
5184,apaszke,pr,2018-02-12T11:45:29Z,Make Python autograd functions respect grad mode,cc: @fritzo ,pytorch
5190,apaszke,pr,2018-02-12T18:47:30Z,Add Python frontend to the JIT,Its main functionality is translating the Python AST into the TorchScript AST. It doesn't do any type checking.,pytorch
5215,apaszke,pr,2018-02-13T15:57:07Z,Fix GraphExecutor and add more AD formulas,This makes the GraphExecutor match (or even slightly exceed) the performance of our older CompiledFunction code paths.,pytorch
5233,vishwakftw,pr,2018-02-14T10:43:17Z,Include __delitem__ for Sequential,"This PR closes feature request https://github.com/pytorch/pytorch/issues/5206 .

Additions:
1. `__delitem__` for `Sequential` and `ModuleList`
2. Tests for `__delitem__`.

For `ModuleList` after deletion is performed, the `_modules` attribute is recreated to preserve continuous numbering. This is not taken care of in `Sequential`.",pytorch
5238,ssnl,pr,2018-02-14T17:52:36Z,CUDA multinomial fix,"Previously failing on 
```
x = torch.cuda.FloatTensor([[0.5, 0.5], [0, 1]])
x.multinomial(1)
```
by triggering device side assert or returning wrong results (always 1 for 2nd sample).

Cause: I incorrectly used both stride and num_categories in a previous PR. This fixes it.

1. Fix THC multinomial stride usage
2. Improve multinomial test
",pytorch
5257,ngimel,pr,2018-02-15T17:31:02Z,downgrade docker back to 9,"9.1 FROM does not have nccl, also, not everyone has driver sufficient for 9.1. Should also fix #5241 for good, as it no longer adds nvidia repo. ",pytorch
5274,apaszke,pr,2018-02-16T14:58:17Z,Add a print() function to the JIT script,"Additionally:
- add support for calling functions that are not methods in the Python frontend
- add an end-to-end test for the Python frontend
- add a capture_stdout helper for checking that `print` actually works",pytorch
5276,ssnl,pr,2018-02-16T17:36:42Z,Fix  __syncthread in SpatialClassNLLCriterion.cu,"Follow up on #5242 .

Also fixes inaccurate reduceBlock comments introduced in #5238 .

cc @zou3519 ",pytorch
5278,ssnl,pr,2018-02-16T22:17:36Z,[wip] Implements fft ifft rfft irfft,"This is feature-complete. But I need to clean up the commit structure to make it easier to review. Furthermore, I also need to figure out how to skip tests basing on whether MKL is available.

Relevant issue #3775 ",pytorch
5292,malmaud,pr,2018-02-18T04:25:17Z,Tweak 'detach' docstring.,I think it's somewhat more grammatical this way.,pytorch
5298,apaszke,pr,2018-02-19T13:31:45Z,Implement no-attribute dispatch of ATen ops from the JIT,This will be needed to e.g. dynamically specify scalar arguments in JIT script.,pytorch
5304,brettkoonce,pr,2018-02-20T01:45:58Z,"minor sp, underlyhing->underlying",,pytorch
5329,apaszke,pr,2018-02-21T12:56:10Z,Traverse sub-blocks in JIT passes,"Haven't touched the ONNX pass. I'm not up to date with the plans/node definitions for exporting control flow, so I'll leave that to others.",pytorch
5365,peterjc123,pr,2018-02-23T03:54:27Z,Add lazy_init.h into build for Windows and refactor code,,pytorch
5366,peterjc123,pr,2018-02-23T05:33:27Z,Fix wrong argument name,@soumith Sorry for making minor mistakes. Please merge it again for the CPU test to pass.,pytorch
5368,peterjc123,pr,2018-02-23T07:07:44Z,Solves the linking error related to lazy_init for MSVC,"Finally, I tested it using the local build. So it should be okay after this PR. 
@soumith Please review it.",pytorch
5375,peterjc123,pr,2018-02-23T16:48:15Z,better solution for the linking error related to lazy_init for MSVC,replacement of #5368 ,pytorch
5378,ssnl,pr,2018-02-23T17:54:35Z,Add faq on cuda memory management and dataloder worker seeds,"Add faq sections on 
1. memory not seeming to be freed properly
2. dataloader workers return identical random numbers (e.g. due to numpy rng and `fork`).",pytorch
5380,ssnl,pr,2018-02-23T18:07:39Z,Ignore FileNotFoundError when shutting down in data_queue.get,Fixes https://github.com/pytorch/pytorch/issues/5355,pytorch
5393,ssnl,pr,2018-02-23T23:58:34Z,[ready] Add logdet and slogdet,"See commit messages for details of each commit. Notice that the last commit changes the QR based approach to LU based.

Some comments about switching from QR to LU:

1. QR (`geqrf`) is generally more stable than LU (`getrf`).
2. But `getrf` gives specific information about matrix being singular or not. Furthermore, the backward (when det > 0) uses matrix inverse (`getrf` + `getri`), so using `getrf` in forward makes sense.
3. Finally, this is also more consistent with many other libraries (including NumPy and MATLAB) that use LU factorization (`getrf`) to compute determinant. 

Therefore, in this PR, det is computed using `btrtifact` (batched `getrf`). However, the backward still uses `inverse` (`getrf` + `getri`). So future work include:
1. expose `getri` and cache `getrf` results to speed up backward.
2. implement batched `getri` to support batched det methods.",pytorch
5409,apaszke,pr,2018-02-26T11:14:48Z,Add traced module to the JIT,"Depends on #5367. Only review the last commit.

Tests will likely fail, because they also fail in the original PR. The test I added should pass.",pytorch
5416,ssnl,pr,2018-02-26T19:54:10Z,Add Scalar to native_function.yaml doc,Requesting review from @gchanan .,pytorch
5422,ssnl,pr,2018-02-26T23:55:34Z,Fix layer_norm initialization and nn.Module docs,"1. Fix LN initialization. cc @avati
2. Support single int normalized_shape. cc @meder411 
3. Sphinx introduced an annoying feature in 1.7 that automatically inherits docstring. So all our nn module docs now look like:
![image](https://user-images.githubusercontent.com/5674597/36702785-8a167942-1b26-11e8-87dd-3e9c4775dd08.png)
Notice the `forward()` doc. This PR disables this feature by setting `autodoc_inherit_docstrings = False`.
4. Fix some Sphinx warnings.",pytorch
5443,ssnl,pr,2018-02-27T21:43:26Z,"[ready] torch.* doc update for Variable/Tensor merge, and other improvements","Commits:
1. Doc changes
+ Update doc to reflect changes in Variable/Tensor merge (`-> float` functions become `-> Tensor`), and new printing style
+ Remove functions in` torch/functional.py` that are already implemented with native_function. Move their doc to `_torch_docs.py` and `_tensor_docs.py`.
+ Add `set_detault_tensor_type` doc
+ Generally improve doc, including clarifying `torch.gels` (#5431 )

2. Fix `torch.split`. The method behaves differently basing on the type of 2nd arg. 
+ This commit breaks it into two functions in ATen `split` and `split_with_sizes`. The Python API stays the same with the  if-statement in `variable.py`. 
+ To override the `torch.split` functional form, this commit also moves `from .functional import *` to after ATen bindings in `torch/__init__.py`. 
+ Finally, this commit also extends `_add_doc_str` to work on Python functions so `torch.split` doc can remain in `_torch_docs.py`.

partially addresses #5571 

cc @vishwakftw hope that this doesn't conflict with what you are working on. ",pytorch
5450,vishwakftw,pr,2018-02-28T02:59:20Z,[ready] General documentation improvements,"This closes the proposal in https://github.com/pytorch/pytorch/issues/5432.

The idea of the PR is to improve existing documentation.

### Changes made:
1. Modify Tensor operations docs.
2. Modify `torch.nn*` docs (excluding `functional`).

### Types of Changes made:
1. Fix math errors
2. Add formulae
3. Improve description if required

If these changes are acceptable, I could look at the rest of the documentation and clean it up in the same way.",pytorch
5459,ssnl,pr,2018-02-28T16:36:58Z,nn.* doc update after Variable/Tensor merge,"The `nn.*` counterpart of #5443 . Mostly removed Variable wrapper. Also added doc for `nn.RReLU`.

Notice that `torch.randn(*, requires_grad=True)` isn't documented until #5462 is done.",pytorch
5482,ngimel,pr,2018-03-01T02:19:49Z,install pytorch into default conda env,partial fix for #5397 ,pytorch
5488,apaszke,pr,2018-03-01T09:35:45Z,Recompute captures after the parameter is updated,,pytorch
5494,apaszke,pr,2018-03-01T15:48:06Z,Fix doc-push,"Launched the docker container locally and verified that these lines fix the script.

Fixes #5490.",pytorch
5531,fritzo,pr,2018-03-02T19:54:41Z,Fix links in distribution docs (no code changes),"This registers more things in the sphinx docs for torch.distributions:
- adds `:undoc-members:` to indicate which methods each distribution implements
- adds `:show-inheritance:` to link to base classes
- sets `currentmodule` so that base class links work
- adds missing `RelaxedBernoulli` and `RelaxedOneHotCategorical`

## Tested

I generated docs locally and tested that links work.",pytorch
5537,ssnl,pr,2018-03-03T03:15:57Z,[fft] [3 of 3] Implements backward of fft ifft rfft irfft,"Commit structure:
1. move `irfft`'s `signal_sizes` arg to be the last
2. add docs for `fft, ifft, rfft, irfft`; update doc for `stft` to include the newly added `normalized` flag
3. fix typo in window function docs
4. improve gradcheck error message
5. implement backward of `fft, ifft, rfft, irfft`
6. add grad tests for `fft, ifft, rfft, irfft`

Relevant issue is #3775 .",pytorch
5538,ssnl,pr,2018-03-03T03:20:51Z,Fix a TODO in native/README,Add a concrete example for python_default_init in native functions doc.,pytorch
5548,peterjc123,pr,2018-03-04T03:30:05Z,Fix extension test on Windows,,pytorch
5549,peterjc123,pr,2018-03-04T06:06:08Z,"Fixes #5542, api changes for output path on Windows",,pytorch
5550,peterjc123,pr,2018-03-04T08:28:51Z,Make build scripts respect MAX_JOBS on Windows,,pytorch
5574,apaszke,pr,2018-03-05T20:12:48Z,Defer shape analysis failures until runtime,,pytorch
5578,apaszke,pr,2018-03-05T21:39:22Z,Add return statement to the JIT AST,"This also makes the syntax recognized by the parser compatible with Python syntax, so it will allow us to reuse Python functions for script tests as well.",pytorch
5585,peterjc123,pr,2018-03-06T05:31:56Z,Fix memory leak when using multiple workers on Windows,"The memory leak is caused by the difference in using FileMapping(mmap) on Windows. On Windows, FileMapping objects should be closed by all related processes and then it can be released. And there's no other way to explicitly delete it.(Like shm_unlink)
When multiprocessing is on, the child process will create a FileMapping and then the main process will open it. After that, at some time, the child process will try to release it but it's reference count is non-zero so it cannot be released at that time. But the current code does not provide a chance to let it close again when possible.
This PR targets #5590.
Current Progress:
The memory leak when num_worker=1 should be solved. However, further work has to be done for more workers.
Error type 1(unrelated filemapping handle get killed):
```pytb
Traceback (most recent call last):
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\utils\data\dataloader.py"", line 61, in _worker_loop
    data_queue.put((idx, samples))
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\queues.py"", line 344, in put
    self._writer.send_bytes(obj)
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\connection.py"", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\connection.py"", line 280, in _send_bytes
    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)
OSError: [WinError 6] The handle is invalid
```
Error type 2(unrelated event handle get killed):
```pytb
Traceback (most recent call last):
  File ""test.py"", line 22, in <module>
    memory_error()
  File ""test.py"", line 17, in memory_error
    for i in dl:
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\utils\data\dataloader.py"", line 277, in __next__
    idx, batch = self._get_batch()
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\utils\data\dataloader.py"", line 256, in _get_batch
    return self.data_queue.get()
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\queues.py"", line 337, in get
    return _ForkingPickler.loads(res)
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\multiprocessing\reductions.py"", line 86, in rebuild_storage_filename
    storage = cls._new_shared_filename(manager, handle, size)
RuntimeError: Couldn't open shared event: <torch_17608_4052021606_event>, error code: <2> at D:\Projects\pytorch-scripts\pytorch\aten\src\TH\THAllocator.c:245
Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x000002303FB255C0>>
Traceback (most recent call last):
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\utils\data\dataloader.py"", line 341, in __del__
    self._shutdown_workers()
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\utils\data\dataloader.py"", line 322, in _shutdown_workers
    self.data_queue.get()
  File ""C:\Anaconda2\envs\test_new\lib\multiprocessing\queues.py"", line 337, in get
    return _ForkingPickler.loads(res)
  File ""C:\Anaconda2\envs\test_new\lib\site-packages\torch\multiprocessing\reductions.py"", line 86, in rebuild_storage_filename
    storage = cls._new_shared_filename(manager, handle, size)
RuntimeError: Couldn't open shared event: <torch_18984_3257952678_event>, error code: <2> at D:\Projects\pytorch-scripts\pytorch\aten\src\TH\THAllocator.c:245
```",pytorch
5593,apaszke,pr,2018-03-06T19:55:19Z,Fix for a confusion around grammar of Maybe,,pytorch
5619,ssnl,pr,2018-03-07T22:47:32Z,Prefix DataLoaderIter with underscore to discourage subclassing,"Discourage subclassing `DataLoaderIter` so that errors like #5430 hopefully won't happen as often.

@apaszke ",pytorch
5623,ssnl,pr,2018-03-08T01:21:46Z,Fix nn.Module.apply doc formatting,,pytorch
5644,ssnl,pr,2018-03-08T20:00:37Z,Fix CUDA btrifact error message using wrong info type,"`btrifact`'s `info` is always an `IntTensor`. But when printing errors, we treat it as a `real` tensor, causing wrong error messages.

Before:
```python
(Pdb) torch.btrifact(torch.zeros(1,3,3).cuda())
*** RuntimeError: failed to factorize some batch elements (min info == 1435440384, max info
 == 19942608) at /home/ssnl/sftp/pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:811
(Pdb) torch.btrifact(torch.zeros(1,3,3))
*** RuntimeError: failed to factorize batch element 0 (info == 1) at /home/ssnl/sftp/pytorc
h/aten/src/TH/generic/THTensorLapack.c:1020
```
After:
```python
(Pdb) torch.btrifact(torch.zeros(1,3,3).cuda())
*** RuntimeError: failed to factorize some batch elements (min info == 1, max info == 1) at
 /home/ssnl/sftp/pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:811
(Pdb) torch.btrifact(torch.zeros(1,3,3))
*** RuntimeError: failed to factorize batch element 0 (info == 1) at /home/ssnl/sftp/pytorc
h/aten/src/TH/generic/THTensorLapack.c:1020
```",pytorch
5647,apaszke,pr,2018-03-08T20:39:15Z,Fix binary operations with scalars,"If you have a double scalar and float tensor, then we currently allow doing `tensor + scalar` (and correctly cast the scalar), while `scalar + tensor` fails and complains that the types don't match. This commit also unifies ATen C++ operators with Python operators to make sure that they will always have the same semantics (this is useful in the JIT too).

There's a bit of awkwardness when dealing with these situations because we generally want to treat scalars as tensors in autograd (to record their history), but we want 0d tensors to behave like scalars in ATen (to allow implicit type casts). I've played with it for a while, but it's hard to disentangle them and come up with a good solution.",pytorch
5649,ssnl,pr,2018-03-08T21:04:59Z,Fixes the InstanceNorm and LayerNorm momentum doc,Fixes the InstanceNorm and LayerNorm momentum doc. Similar to the the issue with BN in https://github.com/pytorch/pytorch/issues/5625.,pytorch
5685,vishwakftw,pr,2018-03-10T03:35:35Z,[ready] General Documentation Improvements - 2,"This PR fixes some minor errors in the docs due to the previous PR https://github.com/pytorch/pytorch/pull/5450.

The focus of this PR is to cleanup the `torch.nn.functional` docs.",pytorch
5689,apaszke,pr,2018-03-10T12:05:37Z,Minor improvement in AutoGPU usage in CUDA bindings,A minor fix on top of #5655. This lets us reduce the number of `cudaGetDevice`/`cudaSetDevice` calls inside the loop 2x.,pytorch
5691,ssnl,pr,2018-03-10T20:30:04Z,Improve nn.Module doc.,"fix named_modules doc, clarify eval doc",pytorch
5701,vishwakftw,pr,2018-03-12T06:13:01Z,Fix error message in nn.functional.convNd and nn.functional.conv_transposeNd,This closes https://github.com/pytorch/pytorch/issues/5696.,pytorch
5709,ngimel,pr,2018-03-12T17:22:36Z,make CUDA_VERSION available in cudnn/Descriptors.h,Otherwise hmma is not called in rnns on volta. cc @ezyang ,pytorch
5710,ngimel,pr,2018-03-12T17:25:53Z,improve occupancy for cuda rngs,"Currentlty max 64 blocks of 256 threads are launched, which is very low occupancy and performance on Pascal and Maxwell cards. This PR increases the number of blocks to 200, current max that MTGP can handle without generating additional starting parameter sets. Longer term, core THC should probably be moving to philox (distributions are already using it) and aim for 100% occupancy.  ",pytorch
5716,apaszke,pr,2018-03-12T20:38:09Z,Add a Number node to the JIT AST and unify script syntax with Python,"As discussed in the today's meeting. For now we always emit floating point literals as float tensors, and int literals as long tensors. I haven't added the casts yet, but it should be fairly straightforward to do so in the future if we will need it.",pytorch
5743,apaszke,pr,2018-03-13T16:43:40Z,Clean up TraceInput,,pytorch
5750,apaszke,pr,2018-03-13T20:35:42Z,Desugar torch.* and F.* functions in JIT script,"Script functions can now use functions from the torch and functional namespace. It's not 100% robust, because `torch.nn.functional` doesn't have 100% parity with what we have in ATen, but the most important functions are there.",pytorch
5771,peterjc123,pr,2018-03-14T11:06:50Z,Fix debug build failure on Windows,,pytorch
5782,ngimel,pr,2018-03-14T17:52:36Z,fused GLU backward,"uses pointer arithmetic to operate on 5 tensors with pointwiseApply3. Still pretty general because tensors are guaranteed to come from splitting an original tensor, thus can be addressed by pointer arithmetic. ",pytorch
5784,apaszke,pr,2018-03-14T18:40:09Z,Desugar torch.* and F.* functions in JIT script,"Script functions can now use functions from the torch and functional namespace. It's not 100% robust, because torch.nn.functional doesn't have 100% parity with what we have in ATen, but the most important functions are there.

This is a duplicate of #5750 (because CI is irreversibly broken for that PR).",pytorch
5793,apaszke,pr,2018-03-14T22:34:32Z,Fix the rule for Assign in JIT's Python frontend,cc: @jamesr66a ,pytorch
5803,peterjc123,pr,2018-03-15T07:46:10Z,Fix undefined '__func__' for CUDA 8 on Windows,,pytorch
5842,fritzo,pr,2018-03-16T23:48:24Z,[distributions] Avoid in-place ops in BoltzmannTransform,"This avoids the following error in `.backward()`:
```
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
```",pytorch
5843,apaszke,pr,2018-03-16T23:48:40Z,Add support for number and list literals in Python frontend,cc: @jamesr66a ,pytorch
5847,ssnl,pr,2018-03-17T04:55:30Z,Fix typo in faq,,pytorch
5855,ssnl,pr,2018-03-17T18:45:36Z,[fft][1 of 3] build system and helpers to support cuFFT and MKL,"This is the first of three PRs that #5537 will be split into.

This PR adds mkl headers to included files, and provides helper functions for MKL fft and cuFFT.
In particular, on POSIX, headers are using `mkl-include` from conda, and on Windows, it is from a new file @yf225 and I made and uploaded to s3.

cc @soumith @apaszke @ezyang ",pytorch
5856,ssnl,pr,2018-03-17T18:53:18Z,[fft][2 of 3] Forward for fft methods,"This builds upon #5855 , and is the second of three PRs that #5537 will be split into.",pytorch
5859,peterjc123,pr,2018-03-18T12:57:21Z,[WIP] Add windows doc,"The windows document is in a missing state. So we have to add it. Instead of the the basic installation and compilnation, I don't know whether there is sth more to add.",pytorch
5864,apaszke,pr,2018-03-18T22:36:40Z,Fix binary operations with scalars,"A new version of #5647. Here's an example auto-generated function that uses the new path:
```cpp
Tensor Type::add(const Tensor & self, const Tensor & other, Scalar alpha) const {                                                                              
    if (auto new_type = unifyTypes(self, other)) {                                                                                                             
        return (*new_type)->add(self.type() != (**new_type) ? (**new_type).copy(self) : self, other.type() != (**new_type) ? (**new_type).copy(other) : other);
    }                                                                                                                                                          
                                                                                                                                                               
    Tensor b_self, b_other;                                                                                                                                    
    std::tie(b_self, b_other) = expand_outplace(self, other, ""add"");                                                                                           
    return s_add(b_self, b_other, alpha);                                                                                                                      
}                                                                                                                                                              
```

@gchanan @colesbury do you also want me to include the changes to binding code for Python operators from the previous PR (i.e. use C++ operators to dispatch them)?",pytorch
5869,peterjc123,pr,2018-03-19T10:06:51Z,Fix ATen build for MSVC 2015 on Windows,MSVC 2015 is required to build with CUDA 8 on Windows.,pytorch
5882,ssnl,pr,2018-03-19T18:30:36Z,Fix some methods not showing up in doc,"Some methods were missing in `torch/tensors.rst`, and thus don't show up in doc. ",pytorch
5890,apaszke,pr,2018-03-19T21:45:01Z,Add support for subscripts in Python frontend,,pytorch
5891,apaszke,pr,2018-03-19T21:46:56Z,Unify JIT tests to check both frontends,"Depends on #5890. Mosts tests now create a Python function, and will test the Python frontend. Then, they will recover the source of that function and test the string frontend.

I rearranged the tests a bit and isolated all the script-related ones in a `TestScript` class (instead of `TestJit`)",pytorch
5894,ssnl,pr,2018-03-19T23:26:11Z,Better error msg for missing mkl headers,"Fixes #5887 .

Now it shows:
```
-- MKL library found
-- Found a library with BLAS API (mkl).
CMake Error at CMakeLists.txt:389 (MESSAGE):
  MKL header files not found.  If using conda, please run `conda install
  mkl-include`.  Otherwise, please make sure that CMake will search the
  directory containing the header files, e.g., by setting CMAKE_INCLUDE_PATH.


-- Configuring incomplete, errors occurred!
See also ""/home/ssnl/sftp/pytorch/torch/lib/build/aten/CMakeFiles/CMakeOutput.log"".
See also ""/home/ssnl/sftp/pytorch/torch/lib/build/aten/CMakeFiles/CMakeError.log"".

```

cc @zou3519 @ezyang ",pytorch
5909,apaszke,pr,2018-03-20T16:34:54Z,"Revert ""Fix ImportError with requests in model_zoo""","Reverts pytorch/pytorch#5896. I merged the commit because the CI was all green, but only travis run, and the commit broke ONNX tests. We should really get some indication that CI was incomplete...",pytorch
5910,fritzo,pr,2018-03-20T18:40:08Z,[distributions] Support pickling of constraint objects,"This PR adds support for pickling of `Constraint` objects. Previously the pickle-unpickle process broke constraint registration due to our use of hashing by singleton instance. To fix this, we now hash only by `Constraint` type, which is preserved by unpickled. The constraint registration table should remain semantically unchanged in this PR; only the registration syntax changes.

This PR also renames `BotlzmannTransform` to a more familiar `SoftmaxTransform`.

## Tested

- existing unit tests
- verified that constraints can now be pickled when used in Pyro",pytorch
5911,apaszke,pr,2018-03-20T19:04:47Z,Add submodules in the ATen subtree,"Right now the standalone build of ATen is broken, because it gained a few dependencies. It should be enough to add this file manually to register them over there as well. I copied it over to the standalone repo and verified that it builds fine with them.

It would be good if we had a strategy for updating this file that's better than periodical manual updates, but this is at least a start.",pytorch
5927,ssnl,pr,2018-03-21T20:41:37Z,Linearly interpolating upsampling fix,"Add align_corners option to upsampling module & functional when using linearly interpolating modes:

When align_corners=True, it uses the old original upsampling scheme, which gives visually better results,
but doesn't properly align input and output pixels, and thus cause the output vary basing on input.
This PR adds this align_corners option, and changes the default behavior to align_corners=False, with
proper warning if this option is not specified upon using nn.Upsample or nn.functional.upsample to let
be aware of this new change.
Adds tests in test_nn.py for spatial invariance when align_corners=False, and usual module tests for
align_corners=False.

The ratio is basically computed as:
```
ratio = align_corners ? (input_size - 1) / (output_size - 1) : input_size / output_size
```
And src_idx is:
```
if align_corners:
  src_idx = dst_idx * ratio
else:
  src_idx = (dst_idx + 0.5) * ratio - 0.5
```
The `0.5` are used to cast the index to location of the pixel centers.

This also makes the default upsampling behavior consistent with other DL frameworks like tf.

This solves the issue raised in #5511 

cc @Dorimer ",pytorch
5931,fritzo,pr,2018-03-22T05:54:38Z,[distributions] Fix scalar bugs in torch.distributions.transforms etc.,"This attempts to fix a number of edge-case bugs in `Transforms` and `ConstraintRegistry`:

1. ~~Avoids `broadcast_all()` on statically defined transforms, since these must work with all tensor types (float/double, cpu/cuda), and often are defined only via integers, e.g. `AffineTransform(0, -1)`. This PR aims to support `numbers.Number` as `.loc` and `.scale` of `AffineTransform`.~~
  EDIT This will be addressed in a later PR.

2. Support scalars in the `.log_bas_det_jacobian()` method of `identity_transform = ComposeTransform([])`. This had not been updated since PyTorch supported scalars.

This also updates `test_distributions.py` to use more modern PyTorch syntax, and fixes some broken documentation. 

## Tested

- added a few `Transform`s and `TransformedDistribution`s with parameters that are `numbers.Number`",pytorch
5941,malmaud,pr,2018-03-22T15:18:03Z,Verify that 'catch' submodule has been checked out before attempting build.,Fixes #5925.,pytorch
5967,vishwakftw,pr,2018-03-23T16:09:27Z,Fix Multivariate Normal docs,"Brought forward by https://github.com/pytorch/pytorch/issues/5954.

Multivariate normal appears in the sidebar, but not as we scroll downwards.

-----------------------
On a related note: is `:undoc-members:` working as expected for the documentation?",pytorch
5968,ssnl,pr,2018-03-23T16:31:32Z,Group Normalization,Implements group normalization.,pytorch
5969,ssnl,pr,2018-03-23T16:57:25Z,Fix incorrect rendering of Tensor.index_*_ doc examples.,"Currently examples in `index_*_` docs are broken, e.g. http://pytorch.org/docs/master/tensors.html#torch.Tensor.index_add_.  This PR fixes the incorrect `Example:` format in `_tensor_docs.py`.

cc @zou3519 who discovered this together with me",pytorch
5976,vishwakftw,pr,2018-03-24T03:24:11Z,[distributions] Implement Power transform,"This PR implements the Power Transform for distributions. Tests have also been modified accordingly.

Use cases of the Power Transform would be in the implementation of the Kumaraswamy distribution.

Math checked and approved by @fritzo .

cc: @apaszke ",pytorch
5981,peterjc123,pr,2018-03-24T09:12:23Z,Modidy setup docs for Windows,,pytorch
5983,ssnl,pr,2018-03-24T20:07:56Z,Move LayerNorm to ATen; remove tracking_running_stats functionality,"After discussing with @soumith , we decide to remove `track_running_stats` option from LayerNorm as it doesn't make much sense.

cc @soumith ",pytorch
5984,ssnl,pr,2018-03-24T20:13:48Z,Add pip mkl-devel to the error message about mkl header files,"Add `pip install mkl-devel` to the error message when MKL is found but MKL headers are not.

relevant task: #5933 

cc @ezyang ",pytorch
5988,fritzo,pr,2018-03-25T04:11:38Z,"[distributions] Implement MultivariateNormal.mean, .variance","Previous review: https://github.com/probtorch/pytorch/pull/141

Note that these properties are standardized across distributions. Pyro [already wraps](https://github.com/uber/pyro/blob/c1fad2d/pyro/distributions/torch.py#L74) PyTorch's `MultivariateNormal` to provide these properties, and we are merely moving them upstream.

## Tested

- added tests against scipy.stats",pytorch
5989,fritzo,pr,2018-03-25T04:20:04Z,"[distributions] Rename .params to .arg_constraints, fix logic","This PR originated from discussion with @apaszke at https://github.com/pytorch/pytorch/pull/5358#discussion_r174145151

1. Renames `.params` to `.arg_constraints` to make it clear that this dict concerns arguments to the `__init__()` method (as suggested by @apaszke and @alicanb). This avoids confusion with other PyTorch usage of `.parameters` to mean ""the set of leaf variables parameterizing an object"". The distinction is especially important regarding `TransformedDistribution`, some of whose parameters are part of `.base_dist` and other of whose parameters are part of various `.transforms`.
2. Fixes buggy/inconsistent logic whereby `TransformedDistribution.params` was a `Constraint` object (a `constraints.dependent_property`) and referred to leaf params in its `.base_dist`. After this PR `TransformedDistribution.arg_constraints == {}`, since the `.base_dist` will do its own arg validation. This is currently triggering errors when we turn on arg_validation in the Pyro unit test suite https://github.com/uber/pyro/pull/922 (test logs https://travis-ci.org/uber/pyro/jobs/357976836)

Previous review: https://github.com/probtorch/pytorch/pull/142",pytorch
5998,ssnl,pr,2018-03-26T03:15:33Z,Add precision matrix to MultivariateNormal,"Also changed some `.contiguous().view(*)` to `.reshape(*)`.

cc @fritzo ",pytorch
5999,ssnl,pr,2018-03-26T04:05:25Z,Improve docs,"1. Clarify det and svd doc on when backward is not stable
2. Fix some links in nn.functional doc; improve upsampling doc",pytorch
6001,peterjc123,pr,2018-03-26T04:30:11Z,Fixes #5973: Stop printing verbose warnings for MSVC,This PR targets #5973 and aims to make Windows log smaller.,pytorch
6027,ssnl,pr,2018-03-26T22:13:48Z,Use THC cached CUDA device property in certain torch.cuda.* methods,"Getting CUDA device property struct with `cudaGetDeviceProperties` is expensive. THC caches CUDA device property, which is available via `THCState_getDeviceProperties`, which is available via `at::globalContext().getDeviceProperties(device)`, which is available via `torch.cuda.get_device_properties`. This PR changes the two methods that previously calls `cudaGetDeviceProperties` to directly using `torch.cuda.get_device_properties` in Python.

Also fixes ATen compile error when it can't find CUDA.

Fixes https://github.com/pytorch/pytorch/issues/4908. Using the script from that issue, we get roughly 18x speed-up.
```
[ssnl@ ~] python dev.py  # master
0.2826697587966919
0.00034999847412109375
0.0003493785858154297
0.000356292724609375
0.00036025047302246094
0.0003629922866821289
0.00036084651947021484
0.00035686492919921874
0.00036056041717529296
0.0003606319427490234
[ssnl@ ~] python dev.py  # this PR
0.27275662422180175
2.1147727966308594e-05
1.9598007202148438e-05
1.94549560546875e-05
1.9359588623046876e-05
1.938343048095703e-05
2.0074844360351563e-05
1.952648162841797e-05
1.9311904907226562e-05
1.938343048095703e-05
```",pytorch
6035,fritzo,pr,2018-03-27T03:32:33Z,[distributions] Support python floats in AffineTransform,"This avoids promotion from python float to `torch.Tensor` for `AffineTransform`. This appears to be needed so that constraint registration works across CPU and all GPUs.

Previous discussion at https://github.com/pytorch/pytorch/pull/5931/files/3a25db73c8fa4cf5fe53705bb392c711dd95b980#r176361909

## Background

There are three basic types of objects in torch.distributions:
- [Distributions](http://pytorch.org/docs/master/distributions.html#distribution) are flyweight objects constructed from tensor or float args. They always promote float args to tensors.
- [Transforms](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.transforms) are longer-lived objects (sometimes cached; some are static globals). They can take float arguments. This PR makes `AffineTransform` avoid promoting float args to tensors.
- [Constraints](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraints) are long-lived objects. They can take either float or tensor arguments. They do not promote floats to tensors. These are relatively symbolic and are not much more than partially evaluated comparisons, e.g. `constraints.positive` is basically a symbolic version of `lambda x: x > 0` that can be stored in a [ConstraintRegistry](http://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraint_registry) table.

## The Problem

Sometimes we want to apply `transform_to(constraints.positive)` to a `torch.Cuda.FloatTensor`. This is fine since
```py
transform_to(constraints.positive)(x)
    = ExpTransform()(x)
    = x.exp()
```
which works with any tensor type.

Other times we want to apply `transform_to(constraints.greater_than(1.5))` to a `torch.cuda.FloatTensor`. This is problematic before this PR since
```py
transform_to(constraints.greater_than(1.5))(x)
    = ComposeTransform([ExpTransform(), AffineTransform(1.5, 1)])(x)
    = AffineTransform(1.5, 1)(x.exp())
    = t.loc + t.scale * x.exp()  # where t = AffineTransform(1.5, 1)
```
Before this PR, `AffineTransform` would promote `t.loc` and `t.scale` to tensors. This promotion can happen as early as library load time for some transforms, e.g. `transform_to(constraints.unit_interval)`. Therefore before this PR, the second example would error at `t.scale * x.exp()` because `t.scale` is a [default] `torch.FloatTensor` whereas `x.exp()` is a `torch.cuda.FloatTensor`.

## Proposed solution

This PR merely adds support for python floats as the `.loc` and `.scale` parameters of `AffineTransform`. This should suffice for most purposes since only `AffineTransform` and a handful of parameter-free transforms are ever stored in the global `transform_to` and `biject_to` registries.

Alternative solutions include:
- allowing promotion from `torch.FloatTensor` to all other tensor types, e.g. `torch.cuda.FloatTensor`.
- adding a handful of specific parameter-free transforms like `NegateTransform()` in lieu of `AffineTransform(0, -1)`.

## Tested

- added a regression test",pytorch
6070,apaszke,pr,2018-03-28T15:38:08Z,Relax constraints on return statements in the script,"Script functions can now have no return statements, empty
return statements, or return one or more values.

Additionally fix the lexer to always emit TK_NEWLINE before
TK_DEDENT, which simplifies the parser.",pytorch
6079,ssnl,pr,2018-03-28T19:18:18Z,RNN `num_layers` and `dropout` docs and checks,"Improves doc on RNN classes `num_layers` and `dropout` args
Adds value checks for `dropout` arg

Fixes https://github.com/pytorch/pytorch/issues/5985",pytorch
6084,apaszke,pr,2018-03-28T21:32:01Z,Handle broadcasting in the JIT,Fixes #6011.,pytorch
6086,ssnl,pr,2018-03-28T22:05:47Z,Add class-specific error when key mismatch in load_state_dict,"Add _load_state_dict_key_mismatch to nn.Module so that submodules can throw class-specific errors when there is state_dict key mismatch.

Fixes #5602 

cc @apaszke @smessmer @CynthiaLu1119 
",pytorch
6089,ssnl,pr,2018-03-28T22:49:55Z,Update FFT comments ,"Fix typos and an extra obsolete comment from #5856

cc @ezyang ",pytorch
6093,ssnl,pr,2018-03-29T01:00:48Z,Add underscore to nn.init.* and deprecate the original ones,Fixes https://github.com/pytorch/pytorch/issues/5946.,pytorch
6108,ssnl,pr,2018-03-29T15:02:56Z,Set dataloader.batch_size = None when batch_sampler is given,"Set dataloader's `batch_size` and `drop_last` to `None` when `batch_sampler is given. Add a warning in doc for this case. 

Address https://github.com/pytorch/pytorch/issues/5884.",pytorch
6114,ssnl,pr,2018-03-29T18:02:27Z,Avoid generating torch.*_backward_(input|weight|bias),Avoid generating things like `torch.cudnn_convolution_backward_input`,pytorch
6118,ssnl,pr,2018-03-29T20:59:09Z,Fix fft when any of the input dimensions is not aligned,"Discovered this bug while looking into the test fail @colesbury saw. In C2C or C2R transform, when any of the input dimension is not aligned to complex type (`stride % 2 != 0`), the third party library calls give incorrect results. Added those checks and tests for this.

Added test for `ifft+fft == identity`. Empirically this only increased test time by 0.1s.

@ezyang 
",pytorch
6143,orionr,pr,2018-03-30T18:02:45Z,Fix setup_caffe2.py lint error.,"./setup_caffe2.py:235:31: E231 missing whitespace after ','",pytorch
6158,ssnl,pr,2018-03-30T22:09:03Z,Add dtype arg to torch.*_window; Add dtype.is_floating_point,"Add `dtype` arg to `torch.*_window` tensor factory functions
Add `dtype.is_floating_point` property

Requesting a review from @gchanan .",pytorch
6160,orionr,pr,2018-03-30T23:27:47Z,Update from Facebook,* First sync of internal changes to pytorch/caffe2,pytorch
6172,vishwakftw,pr,2018-04-01T16:18:11Z,[distributions] KL-Divergence for Multivariate Normal,"This PR implements the KL-Divergence for Multivariate Normal and tests for the same.

Math reviewed and approved by @fritzo [here](https://github.com/probtorch/pytorch/pull/144).",pytorch
6203,orionr,pr,2018-04-02T20:41:42Z,[caffe2] Remove ShuffleNet from model zoo.,"* No longer supported.

",pytorch
6211,ssnl,pr,2018-04-02T23:08:20Z,Fix sparse embedding backward when input contains only padding_idx,Fixes https://github.com/pytorch/pytorch/issues/5472,pytorch
6244,ssnl,pr,2018-04-03T20:31:55Z,Fix SGD lr check failing on default value,Following up https://github.com/pytorch/pytorch/pull/6000,pytorch
6249,ssnl,pr,2018-04-03T22:14:27Z,Add arg checks in torch.utils.data.Sampler classes,Fixes https://github.com/pytorch/pytorch/issues/6168,pytorch
6251,ssnl,pr,2018-04-03T23:13:44Z,Fix Tensor.__setstate__ for legacy Tensor state,"Fixes https://github.com/pytorch/pytorch/issues/6184

Also changed `variable` in `tensor.py` to `tensor`.

cc @colesbury ",pytorch
6272,vishwakftw,pr,2018-04-04T14:55:40Z,[ready] Implement log2 and log10 in PyTorch,"This completes feature request https://github.com/pytorch/pytorch/issues/6233 .
Tests have been added as well.

cc: @apaszke @soumith ",pytorch
6274,orionr,pr,2018-04-04T16:51:07Z,Add a CODEOWNERS file,"* This will let us require review from owners of aten/ and torch/ while giving wider access (for now) to caffe2/
* This will be adjusted as we work on shared components.

@smessmer, @soumith, @Yangqing, @dzhulgakov, @ezyang please review. Thanks.",pytorch
6275,ssnl,pr,2018-04-04T17:47:49Z,Add tests for old tensor serialization,Add test for the fix in #6251 .,pytorch
6279,ssnl,pr,2018-04-04T21:03:06Z,[DO NOT MERGE] TESTING,,pytorch
6308,peterjc123,pr,2018-04-05T10:59:13Z,[Test only] CPU only build for Windows,,pytorch
6327,ssnl,pr,2018-04-05T22:21:12Z,Add total_length option to pad_packed_sequence,"Add example on how to use pack->rnn->unpack with DataParallel.
Fix incorrect comment on hooks with DataParallel.

Fixes #6299 ",pytorch
6329,crcrpar,pr,2018-04-05T23:53:56Z,fix typo in autograd.rst,,pytorch
6332,peterjc123,pr,2018-04-06T03:31:06Z,Several minor fixes for Windows build,,pytorch
6352,peterjc123,pr,2018-04-06T15:37:49Z,[TEST ONLY] TEST MSVC BUILD,,pytorch
6358,ssnl,pr,2018-04-06T18:21:58Z,Fix Sphinx's incorrect rendering of arg type torch.dtype,"Fix `torch.dtype` getting incorrectly rendered as `torch.dpython:type` by Sphinx.

",pytorch
6367,ssnl,pr,2018-04-06T22:29:49Z,Fix activation images not showing up on official website,"For some reason, Sphinx ***copies*** all images referenced in docs to a folder it automatically creates, `_images`, and ***changes*** all the references to the new path under `_images`. For example, the actual ELU image is at http://pytorch.org/docs/master/_static/img/activation/ELU.png, as indicated by the code https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L280. However, website has a link to `http://pytorch.org/docs/master/_images/ELU.png`.

There doesn't seem to be a way to turn off such behavior. This PR adds `_images` to `html_static_path`, and changes the activation image directory to somewhere out of `_static` so we don't get the same images twice in our static assets.

Fixes #5677 .

cc @pmitros ",pytorch
6383,peterjc123,pr,2018-04-07T14:48:35Z,Another CUDA 8 fix for Windows,,pytorch
6396,peterjc123,pr,2018-04-08T05:30:24Z,"Fixes #6386, Use copies instead of symbolic files",,pytorch
6399,peterjc123,pr,2018-04-08T06:37:11Z,"[WIP] Fixes #6398, fix only one core can be detected on Windows",,pytorch
6401,bstriner,pr,2018-04-08T08:52:26Z,Add CUDA headers,"`lib/include/torch/csrc/cuda/*.h` headers are not copied during the install. Useful for compiling extensions so this adds those headers to the install. Not sure why they were not included already.

Cheers",pytorch
6403,peterjc123,pr,2018-04-08T13:02:12Z,[TEST ONLY] Windows one core,,pytorch
6404,peterjc123,pr,2018-04-08T13:56:57Z,[TEST ONLY] Windows one core,,pytorch
6405,peterjc123,pr,2018-04-08T14:09:21Z,Quote arguments only when possible,,pytorch
6409,ssnl,pr,2018-04-08T19:25:37Z,Fix incorrect error message in convolution_expand_param_if_needed,Fixes #6400 ,pytorch
6416,peterjc123,pr,2018-04-09T06:57:55Z,"Fix #6398, Add MKL threading support for Windows",,pytorch
6423,peterjc123,pr,2018-04-09T16:00:38Z,Skip cpp_extensions test when possible on Windows,We should make sure the test pass even if the test conditions are not satisfied.,pytorch
6429,ssnl,pr,2018-04-09T18:45:18Z,Fix typo in link to sigmoid activation image,Partially fixing #5677 . The more important fix is being done by @zou3519 at pytorch.github.io repo.,pytorch
6430,ssnl,pr,2018-04-09T19:00:23Z,Clarify Embedding padding_idx arg,Addresses #6411 ,pytorch
6438,ssnl,pr,2018-04-09T21:11:35Z,Fix reflection padding boundary checks,Addresses #6432 ,pytorch
6459,peterjc123,pr,2018-04-10T05:18:06Z,[TEST ONLY] MKL UPDATE ON WINDOWS,,pytorch
6476,ssnl,pr,2018-04-10T18:41:49Z,Link relevant FAQ section in DataLoader docs,cc @calvinleenyc ,pytorch
6482,ssnl,pr,2018-04-10T20:19:52Z,Fix THTensor_(take) negative index check,"Fixes #6472 .

Previously we save invalid indices in `invalidIdx` and checks if `invalidIdx < 0` after OMP loop. This in incorrect because the index can be negative. This PR changes to saving the invalid dimension.",pytorch
6489,ssnl,pr,2018-04-10T22:54:25Z,Skip all TestTorch tests in test_cuda.py,"Apparently we run TestTorch tests twice. E.g., one TestTorch test fails in test_cuda.py here: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-py3-clang5-asan-test/2416/console.

This fixes it.

cc @gchanan ",pytorch
6498,ssnl,pr,2018-04-11T06:18:08Z,Fix torch.utils.checkpoint not showing up in doc,"Add checkpoint to index.rst. Order torch.utils.* alphabetically.

cc @prigoyal ",pytorch
6499,peterjc123,pr,2018-04-11T06:30:05Z,Add flag and warning for Python 2.7 users on Windows,,pytorch
6500,ssnl,pr,2018-04-11T06:35:11Z,[ TEST ]  DO NOT MERGE,for CI signal,pytorch
6506,ssnl,pr,2018-04-11T16:07:29Z,Use symbolizer by default in ASAN CI build,"This has no effect until https://github.com/pietern/pytorch-dockerfiles/pull/15 is merged and new PyTorch docker file is built.

I've verified that ASAN works even if `ASAN_SYMBOLIZER_PATH` points to a non-existing file (no symbols of course).",pytorch
6525,ssnl,pr,2018-04-11T20:57:10Z,Fix typos in sampler.py,,pytorch
6526,ssnl,pr,2018-04-11T21:04:36Z,Improve utils.checkpoint docs,Mainly adds links and formatting,pytorch
6528,ssnl,pr,2018-04-11T21:43:36Z,Support arbitrary number of batch dimensions in *FFT,Addresses #6516 .,pytorch
6541,ssnl,pr,2018-04-12T04:55:20Z,Fix regression that STFT has no backward.,"STFT is just a conv1d, so it is differentiable out of the box. This fixes the regression that marked its backward as ""not implemented"". It needs no gradcheck test because if conv1d passes, it also passes. So having a gradcheck is pointless.
",pytorch
6553,ssnl,pr,2018-04-12T16:06:31Z,Restore allow_unused functionality,"Addresses #6535 

I'll do a Variable -> Tensor codemod in a separate PR.",pytorch
6554,apaszke,pr,2018-04-12T16:36:41Z,Trace size-dependent expressions correctly,"This makes the JIT tracer much more robust, by allowing it to record
dependencies on tensor sizes. For example, if you were to trace this
function

```python
def fn(x):
    return x.view(x.size(1), -1)
```

before this patch, then it would embed the actual value of `x.size(1)`
in the trace as a constant, making it very hard to have e.g. batch size
independent traces. Now, this will correctly record the dependency, and
will retrieve the size of x at every run.

cc @colesbury, @gchanan for the arg parser and codegen changes.
cc @dzhulgakov @bddppq who requested this functionality.",pytorch
6568,ssnl,pr,2018-04-12T22:05:39Z,Use THC allocation for CUFFT workspace,"Previously we let cufft handle the workspace allocation. Now we switch to using the THC caching allocator.

cc @ngimel ",pytorch
6569,ssnl,pr,2018-04-12T22:10:50Z,[ TEST ] DO NOT MERGE,test codemod CI,pytorch
6583,apaszke,pr,2018-04-13T09:49:31Z,Fix issues with tracing of Python autograd Functions,Fixes #6576.,pytorch
6597,ssnl,pr,2018-04-13T20:56:03Z,Adds requires_grad flag to torch.from_numpy,,pytorch
6609,malmaud,pr,2018-04-15T05:37:32Z,Fix typo.,,pytorch
6615,fritzo,pr,2018-04-15T20:25:28Z,[distributions] Implement Independent distribution,"Implements an `Independent` distribution to reinterpret batch dims as event dims.

This follows [tensorflow nomenclature](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Independent).

## Why?

This has been requested a couple times on PyTorch slack. We have a similar [ReshapedDistribution](https://github.com/uber/pyro/blob/4d938bc/pyro/distributions/torch_distribution.py#L185) in Pyro.

## Tested

- added some test examples",pytorch
6628,ssnl,pr,2018-04-16T15:16:05Z,Make dtype in .to positional rather than kwarg only,"Currently the `dtype` in `.to(device, dtype=dtype)` is kwarg only. This makes changing both device and dtype quite wordy, and even longer than the less efficient `.to(device).to(dtype)`.

This PR changes the `dtype` in this variant to be positional.",pytorch
6629,ssnl,pr,2018-04-16T16:19:14Z,Add Module.to,"Had to manually parse `*args, **kwargs` :/. This builds on top of #6628 .",pytorch
6639,ssnl,pr,2018-04-16T22:14:54Z,Add BC mechanism to Module.load_state_dict,"1. Add version number to module. Currently all modules have version `0`. Modules without version numbers are considered as having `-1` as their versions.
2. Change `load_state_dict` to call `load_local_state_dict` on each submodule. This method can be overridden for class specific loading.
3. Change `InstanceNorm*d`'s hard error (when `track_running_stats=False` and `running* in state_dict`) to a warning because in most cases the running buffers are not used and manually removing the keys is quite some trouble for users.

Unblocks #6445 .

",pytorch
6641,ssnl,pr,2018-04-17T00:24:37Z,Codemod to update our codebase to 0.4 standard,"1. Change mentions of `Variable` to `Tensor` in doc text and examples
2. Change tests against `Variable` to `Tensor`, e.g. `isinstance`
3. Use the new style tensor factory methods.
4. Update the doc to be more consistent in naming
5. Add data_parallel functionals docs
6. Remove a obsolete test that compares blas results when called on `Variable` and on `Tenors`
7. Remove lowercase `variable`

Test scripts are not completely updated (too many)

Also addresses #6276 ",pytorch
6653,peterjc123,pr,2018-04-17T09:52:57Z,Add documents for Windows,,pytorch
6666,fritzo,pr,2018-04-17T17:49:01Z,[distributions] Skip validation of lazy properties,"Fixes https://github.com/probtorch/pytorch/issues/146
Previous review https://github.com/probtorch/pytorch/pull/147

## The Problem

Currently distributions check each possible constraint in `.arg_constraints`. This is problematic for distributions with multiple alternative parameterizations (e.g. `MultivariateNormal` with `scale_tril`, `covariance_matrix`, and `precision_matrix`) because all possible parameterizations must be constructed and checked. For `MultivariateNormal` this can trigger an expensive matrix inverse.

## Proposed Solution

This PR skips checking of args that are `lazy_property`s, so that only already-instantiated args are checked.

## Tested

- passes existing validation tests",pytorch
6671,ssnl,pr,2018-04-17T18:32:43Z,Fix import error sometimes happening in dataloader when exiting Python,Fixes #6461 ,pytorch
6677,ssnl,pr,2018-04-17T20:54:53Z,Make torch.backends.mkl.is_available() work without importing,"..., so that the behavior is consistent with `torch.backends.cudnn.*`

Currently, 

```python
>>> import torch
>>> torch.backends.cudnn.enabled
True
>>> torch.backends.mkl.is_available()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'torch.backends' has no attribute 'mkl'
>>> import torch.backends.mkl
>>> torch.backends.mkl.is_available()
True
```",pytorch
6679,ssnl,pr,2018-04-17T21:25:40Z,Fix padding and output_padding in ConvTranspose docs,Fixes #6417 ,pytorch
6692,orionr,pr,2018-04-18T01:43:33Z,Update from Facebook,,pytorch
6701,apaszke,pr,2018-04-18T10:26:42Z,Sort declarations when generating Python bindings,"This helps resolve ambiguities in argument parsing according to
any rules we will need.

For now, this allows us to make scalar operations more conservarive
wrt. argument types, but makes them commutative again.

@gchanan ",pytorch
6702,peterjc123,pr,2018-04-18T11:26:04Z,[TEST ONLY]Test debug build for Windows,,pytorch
6711,ngimel,pr,2018-04-18T17:29:12Z,Check for g++ also in check_compiler_ABI,"Otherwise a spurious warning is generated. @goldsborough

",pytorch
6720,neerajprad,pr,2018-04-18T19:54:43Z,Allowing for vectorized counts in Binomial Distribution,"Currently the `Binomial` distribution only allows for integer counts, i.e. `total_count` remains fixed while we can vary the probability tensor. Since we are drawing Bernoullis internally when sampling from Binomial, there is no need to restrict `total_count` to be an integer, and we can support vectorized counts with a small modification. This will be really useful for building beta-binomial regression models, for instance.

Additionally, some of the methods assumed that `total_count > 0`. Made some minor changes to ensure that `total_count = 0` works as expected, and added this special case to the tests.

Hat tip to @fritzo for suggesting this. Math reviewed in https://github.com/probtorch/pytorch/pull/148.

cc. @apaszke ",pytorch
6726,ssnl,pr,2018-04-18T20:27:09Z,Better error message for gels on CUDA,fixes https://github.com/pytorch/pytorch/issues/6723,pytorch
6733,ssnl,pr,2018-04-18T22:41:22Z,Updates module.to doc for the new tensor.to(requires_grad),,pytorch
6737,Yangqing,pr,2018-04-18T23:42:12Z,Add option cache to speed up cmake build,"What this PR does: when we figure out that one dependency is not met, we will update the cached option so that when we run cmake next time on the same machine, we do not incur yet another dependency check. According to @ezyang 's analysis this could account for a nontrivial amount of cmake run time.",pytorch
6751,peterjc123,pr,2018-04-19T08:37:05Z,Fix OpenMP pragma for MSVC,`PRAGMA(ivdep)` and `PRAGMA(simd)` are not supported by MSVC. So we need alternatives for them. Reference [here](https://msdn.microsoft.com/en-us/library/hh923901.aspx).,pytorch
6758,peterjc123,pr,2018-04-19T11:40:13Z,Fix debug build for Windows,,pytorch
6774,ssnl,pr,2018-04-19T17:28:49Z,[docs] Module.to doc udpate and example format update,,pytorch
6786,ngimel,pr,2018-04-19T23:40:20Z,move softmax/logsoftmax to ATen,"THCUNN kernels are mostly unchanged, with minimum changes to types so that more intermediate values are preserved in AccumT. 
Softmax/LogSoftmax from THNN are combined into a single templated function.
Remaining issues
 - in THNN accumulation for float used to be in double, ATen does not support this now, would need something like https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/AccumulateType.cuh, should we add it? With this PR, accumulation for float is in float. 
- in softmax_backward, self argument is added just so that gradient of self can be computed in double backward. Self tensor itself is not necessary neither for softmax_backward, nor for softmax_double_backward. I don't know if there's a better way of handling it. 
- scalar handling is somewhat awkward (I'm checking .dim() == 0, and do .view(1) for scalars), is there a better way of doing it?
- legacy tests are failing, because softmax is no longer legacy. I can rewrite legacy softmax forward to use torch.softmax instead of [backend].SoftMax_UpdateOutput, but what to do with backward? Softmax backward is no longer exposed.",pytorch
6806,fritzo,pr,2018-04-20T16:26:57Z,[distributions] Fix Independent.rsample() and add more tests,This fixes a typo in `Independent.rsample()` reported by @stepelu. It also adds regression tests and fixes a couple shape bugs surfaced in the new tests.,pytorch
6814,fritzo,pr,2018-04-20T20:02:16Z,[distributions] Fix Indepenedent.rsample() and add more tests,Backport of pytorch/pytorch#6806,pytorch
6820,kashif,pr,2018-04-20T21:30:05Z,fixed error message in tools/setup_helpers/mkldnn.py,,pytorch
6843,ssnl,pr,2018-04-21T19:03:41Z,[docs] Update set_default_(tensor_|d)type docs,Previous examples aren't clear.,pytorch
6853,peterjc123,pr,2018-04-23T04:28:49Z,[doc] Minor fixes for Windows docs,,pytorch
6854,bstriner,pr,2018-04-23T06:28:43Z,[WIP] Working MSVC 2015 Build,"Hi everybody!

A few modifications to get pytorch up and running with a Visual Studio 14 2015 Win64 cmake build. This build currently works on my machine but I need to make sure I haven't broken something on another platform. Several different issues addressed. Would appreciate any feedback on solutions.

- Lots of switches changed to ifs. This is most of the changes and nothing too surprising. Some macros were there already, some I added.
- one static_assert had to become an assert. Not sure how the static_assert is supposed to be working because seems like a runtime issue to me.
- weird issues with hashing. Had to add something so the custom template doesn't match things that are hashable to avoid ambiguity. Also had to add a custom specific hash function for 2 classes for some reason.
- weirdest issue is cnt_to_dst_idx_functor. It has const members so the operator= is deleted, but the operator= is being referenced somewhere and causing a compilation issue. I added an operator= so it can compile. The root cause I have no idea about. Should it be assignable or not? If not, where is the operator= coming from?

Would welcome any help from people with other windows rigs to try this build out.

In terms of actually running the build, here is my environment setup. Probably a few more variables than needed but it works. (These are actually the combined environment variables I used for Magma, numpy, mkldnn, pytorch, etc.)
```
""C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\vcvars64.bat""
""C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2018\windows\mkl\bin\mklvars.bat"" intel64 lp64 vs2015
set MKLDNN_HOME=C:\Program Files\Intel(R) MKL-DNN
set MKLDNN_ROOT_DIR=C:\Program Files\Intel(R) MKL-DNN
set MAGMA_HOME=C:\Program Files\MAGMA
set MKL_HOME=%MKLROOT%
set MKLDNN_INCLUDE_DIR=C:\Program Files\Intel(R) MKL-DNN\include
set MKLDNN_LIB_DIR=C:\Program Files\Intel(R) MKL-DNN\lib
set MKLDNN_LIBRARY=C:\Program Files\Intel(R) MKL-DNN\mkldnn.lib
set MKL_LIB_DIR=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2018.2.185\windows\mkl\lib
set OPENMP_LIB_DIR=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2018.2.185\windows\compiler\lib\intel64_win
set TBB_LIB_DIR=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2018.2.185\windows\tbb\lib\intel64_win\vc_mt
set TORCH_CUDA_ARCH_LIST=6.1
set MKL_INCLUDE_DIR=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries_2018.2.185\windows\mkl\include
set CMAKE_LIBRARY_PATH=%MKLDNN_LIB_DIR%;%OPENMP_LIB_DIR%;%TBB_LIB_DIR%;%MKL_LIB_DIR%
set CMAKE_INCLUDE_PATH=%MKL_INCLUDE_DIR%;%MKLDNN_INCLUDE_DIR%
set CMAKE_GENERATOR=Visual Studio 14 2015 Win64
set DISTUTILS_USE_SDK=1
```

Cheers",pytorch
6866,orionr,pr,2018-04-23T16:54:18Z,[aten] Move submodules to third_party,"* Move ATen submodules to root-level third_party
* Update aten_mirror.sh to reflect

Confirmed that PyTorch tests passed.

@ezyang when I run `./tools/aten_mirror.sh` what should `EXTRACTED_REPO` be set to?

Review requested from @ezyang, @zdevito. FYI @Yangqing 
",pytorch
6870,ssnl,pr,2018-04-23T18:26:03Z,Fix SVD backward on non-square matrices when some=False,"Didn't slice `gu` and `gv` properly before. Added test for them.

Fixes #6849 ",pytorch
6872,ssnl,pr,2018-04-23T18:36:14Z,Add torch.get_default_dtype doc,cc @zou3519 @li-roy ,pytorch
6878,Yangqing,pr,2018-04-23T20:47:25Z,Bump benchmark to master,"This is continuation of #6737 in the spirit of reducing cmake time. Related PR at google/benchmark at: https://github.com/google/benchmark/pull/573 . On local laptop this reduces cmake re-run time from 8s to 6s (2s saving).

cc @ezyang ",pytorch
6905,ftynse,pr,2018-04-24T16:50:22Z,[aten] only lookup CuDNN if compiling with CUDA,"ATen can be configured to compile without CUDA support by passing
`-DNO_CUDA=0` to cmake.  However, cmake will look for CuDNN independently
of that flag and may eventually find it.  In cases were compilation
without CUDA support was requested on system with CUDA installed, this
will result in linking errors while building some tests that rely only
on CuDNN being found.

Do not look for CuDNN if `-DNO_CUDA=1` was provided in the cmake call
since it does not make sense to compile with CuDNN if CUDA support was
disabled.

",pytorch
6929,crcrpar,pr,2018-04-25T03:19:57Z,add spectral normalization [pytorch],"related to #5027.
This PR aims at implementing Spectral Normalization in a way similar to `torch.nn.utils.weight_norm`.",pytorch
6943,vfdev-5,pr,2018-04-25T12:18:44Z,Update checkpoint.py,Fix typo in doc string,pytorch
6958,ngimel,pr,2018-04-25T17:47:55Z,Make cuda 9 behave as cuda 8 wrt half conversions,"Cuda 9 is too smart about implicit half conversions, this would disable them so that cuda 8 and cuda 9 behave in the same way wrt half.

",pytorch
7009,apaszke,pr,2018-04-26T20:53:06Z,Add support for type annotations in Python functions,"When a call to a Python function is resolved, we now actually actively look for type annotations/use the source to guide the type checks instead of blindly trusting the user, and assuming that everything is a tensor. Both the fancy Py3.5+ annotation syntax, and the backwards-compatibly `# type:` comments are supported.

As an extra benefit I've cleaned up the `frame_id` mess so that we now pass numbers relative to the current frame everywhere (previously they were absolute wrt. the place where we called `inspect.stack()`). This finally allows us to use `checkScript` for script objects that have references to Python functions surrounding them.

@zdevito @jamesr66a ",pytorch
7049,ssnl,pr,2018-04-27T20:10:05Z,clamp now has subgradient 1 at min and max,addresses #7002 ,pytorch
7054,orionr,pr,2018-04-27T23:07:16Z,Update the video input op in caffe2,"There are multiple fixes to the video input op recently. This is to update
the caffe2 version so that it is up to date.

",pytorch
7130,peterjc123,pr,2018-05-01T09:36:44Z,Add support for MKLDNN on Windows,,pytorch
7140,ssnl,pr,2018-05-01T17:07:51Z,Remove handling workers waiting to put in dataloader.__del__,"Now that each worker has its own queue, each worker has at most 2 batches, which should be far below SimpleQueue's limit.

Also fixes https://github.com/pytorch/pytorch/issues/6932

",pytorch
7143,orionr,pr,2018-05-01T17:55:44Z,WIP: Allow us to build torch and ATen from root CMake,"Very much a WIP, but putting up for people to look at. I'll likely force push to this branch in the future and rewrite things, so heads up.

Also, `tools/build_pytorch_libs.sh` will likely change to have us call root CMake.",pytorch
7159,Yangqing,pr,2018-05-01T22:55:44Z,[build] Remove /torch/lib/THD/cmake in favor of /cmake,We should put all cmake related files in a clean structure. I made a best effort guess where the file were used (in /torch/lib/THD/CMakeLists.txt) but let's see whether there are others. Kicking off a contbuild to test.,pytorch
7162,Yangqing,pr,2018-05-01T23:14:00Z,[build] Simplify /aten/src/TH/cmake in favor of /cmake,"the MKL/BLAS situation is a little bit more complex to figure out at the moment, so leaving it there.

Also checked with @soumith that the ARM special path is no longer needed.",pytorch
7163,orionr,pr,2018-05-01T23:56:38Z,[build] Setup to build ATen from root CMake file,"* Cleanup aten/ cmake files in prep for including it from the root-level cmake

@ezyang and @Yangqing please review. Thanks.",pytorch
7170,Yangqing,pr,2018-05-02T07:30:10Z,[build] Add back the MKL find code,It seems that the FindMKL.cmake code had a major change from #6699 that stripped off all the MKL related commands and only had ideep and mklml left. This leaves MKL_LIBRARIES to be empty among others. This PR ports the aten/src/TH FindMKL.cmake file back into the main FindMKL.cmake codebase for correctness.,pytorch
7188,Yangqing,pr,2018-05-02T18:27:47Z,Reroute aten to use the root cmake system's select_compute_arch.cmake,Kicking off contbuild.,pytorch
7230,ngimel,pr,2018-05-03T06:12:47Z,"clean up runtime dockerfile, use cuda 9 package",,pytorch
7245,apaszke,pr,2018-05-03T16:42:26Z,Add support for __floordiv__ and __rdiv__ for integral tensors,"This commit adds support for `__floordiv__` to all tensors. I checked that it behaves similarly to what NumPy does, i.e. always returns a result of the same type. On the other hand, NumPy seems to follow the Python rules for rounding, while I used the C rules, to make it conform with how our integral division behaves in general.

This also adds support for `__rdiv__` with integral tensors. Previously it raised an error saying that `reciprocal` isn't implemented for them. Ideally we would just have a kernel for this, but I didn't want to make this patch too large, as it's mostly meant to unblock my other work.",pytorch
7252,orionr,pr,2018-05-03T18:14:18Z,Fix USE_ATEN flag in caffe2,"This will be improved with other changes I'm making, but putting this up so USE_ATEN works in caffe2 right now.",pytorch
7262,ssnl,pr,2018-05-03T21:21:18Z,Update the gif for 0.4,Fixes #6180 and #7025 ,pytorch
7264,ssnl,pr,2018-05-03T21:37:12Z,Fix onnx.symbolic.upsample_bilinear2d not considering align_corners,Fixes https://github.com/pytorch/pytorch/issues/6900.,pytorch
7265,vfdev-5,pr,2018-05-03T22:15:02Z,Fix issue #7209 in DataLoader,Addresses the issue #7209,pytorch
7269,fritzo,pr,2018-05-03T23:36:08Z,"[distributions] Fix broadcasting error in LogNormal, TransformedDistribution","This replaces two in-place operations `+=`,`-=` with non-in-place operations `= ... +`, `= ... -` so that small-shaped samples can be scored in `TransformedDistributions` with large-shaped base distribution parameters. The motivating example is `LogNormal`:
```py
LogNormal(torch.zeros(2), 1).log_prob(torch.tensor(0.5))  # fails before this PR
```

## Tested

- added a regression test for `LogNormal` that also exercises `TransformedDistribution`",pytorch
7270,ssnl,pr,2018-05-03T23:40:42Z,Add memory leak check in CUDA tests,"With `torch.cuda.memory_allocated` available, we can test if CUDA methods have memory leaks. This PR adds a wrapper around each CUDA test. It checks if the CUDA memory usage before and after stay constant.

Also move `TEST_CUDA`, `TEST_MULTIGPU`, `TEST_CUDNN`, `TEST_CUDNN_VERSION` to `common_cuda.py`.",pytorch
7283,superbobry,pr,2018-05-04T13:48:39Z,Change the error message in pad_sequence to be more user-friendly,"Before:

```python
>>> pad_sequence([torch.tensor([1]), torch.tensor([2, 3])])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""[...]/torch/nn/utils/rnn.py"", line 301, in pad_sequence
    raise ValueError(""lengths array has to be sorted in decreasing order"")
ValueError: lengths array has to be sorted in decreasing order
```

After:

```python
>>> pad_sequence([torch.tensor([1]), torch.tensor([2, 3])])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""[...]/torch/nn/utils/rnn.py"", line 301, in pad_sequence
    raise ValueError(""lengths array has to be sorted in decreasing order"")
ValueError: sequences must be sorted in the order of decreasing length
```",pytorch
7295,orionr,pr,2018-05-04T18:20:27Z,[build] Make ATen buildable without all Caffe2 by root cmake,"* Allows us to build ATen as a part of libcaffe2.so (statically linked for now) and also disable large parts of the Caffe2 code.
* Next step will be for PyTorch setup.py to depend on a version of libcaffe2.so instead of libATen.so

I ran the following builds to test locally
```
cd ~/local/pytorch
mkdir build; cd build; cmake ..; make -j 8
cd ..; rm -rf build; mkdir build; cd build; cmake -DBUILD_CAFFE2=ON -DBUILD_ATEN=ON -DUSE_CUDA=ON ..
cd ..; rm -rf build; mkdir build; cd build; cmake -DBUILD_CAFFE2=OFF -DBUILD_ATEN=ON -DUSE_CUDA=ON ..
```
Now kicking off contbuild here.

cc @Yangqing, @ezyang and @pjh5 for review. Thanks.",pytorch
7300,orionr,pr,2018-05-04T19:30:47Z,[build] Move all Find*.cmake to cmake/Modules,"* Move remaining `Find*.cmake` files into `cmake/Modules`
* Note that `FindGloo.cmake` and `FindNCCL.cmake` were different, but I'm tempted to take the PyTorch versions, which is what this does. We'll see how Caffe2 builds fare.

c @ezyang, @Yangqing, @pjh5 for review. Thanks.",pytorch
7301,ssnl,pr,2018-05-04T19:39:07Z,Update gif with new logo,preview: https://github.com/SsnL/pytorch/blob/9b6a1183cf639e7006183833d873d91ca54a8d7b/docs/source/_static/img/dynamic_graph.gif,pytorch
7312,ssnl,pr,2018-05-04T23:00:33Z,Add non_blocking to Tensor/Module.to,cc @gchanan ,pytorch
7319,facaiy,pr,2018-05-05T13:56:46Z,add `to` method for PackedSequence,"Fix #7221.

It's my first contribution to pytorch, so any help would be appreciated. Thanks.",pytorch
7368,peterjc123,pr,2018-05-08T05:03:15Z,[TEST ONLY] Accelerate MSVC linking,,pytorch
7382,ngimel,pr,2018-05-08T18:25:51Z,make half overflow checks consistent with other types,,pytorch
7399,orionr,pr,2018-05-09T00:22:48Z,[build] Have PyTorch depend on minimal libcaffe2.so instead of libATen.so,"* ATen will be included as a part of Caffe2 with this pull request
* This will let us share code between Caffe2 and ATen for operator library work

Tested with

```
python setup.py build_deps
python setup.py develop
cd test && python run_test.py
```
as well as

```
tools/cpp_build/build_all.sh
```

cc @ezyang, @pjh5, @Yangqing for review once I figure out the test failure.",pytorch
7421,apaszke,pr,2018-05-09T14:00:11Z,Remove CompiledFunction + clean up JIT tests,"This PR switches all tests we used to have for CompiledFunction to work with GraphExecutors. This improves our coverage, and lets us get rid of legacy code that we'll no longer need.

It also helped uncover some bugs like the fact that the shape propagation pass didn't respect the device of inputs (which might actually be a fix for #7072, although it uses compile, so it's hard to tell).

cc @zdevito ",pytorch
7432,orionr,pr,2018-05-09T18:44:17Z,Update to latest ideep,"Looks like they did a force push, so this pins to the latest master. Seeing how tests go.

Related to https://github.com/intel/ideep/issues/6

cc @ezyang, @Yangqing.",pytorch
7452,orionr,pr,2018-05-10T01:29:44Z,[build] Fix Caffe2 with ATen build,"* I introduced a change at the last minute that made building both at the same time error out
* I'd like to move the add_subdirectory(../aten aten) to one location, but unfortunately it doesn't look like I can

Thanks to @pjh5 for exposing this. cc @ezyang for review.",pytorch
7486,orionr,pr,2018-05-10T23:04:45Z,Fix Caffe2 build with ATen CPU/GPU split,"* Disable ATen tests for now in Caffe2 and reference ATen_cpu and ATen_gpu rather than just ATen.

@ezyang and @pjh5 for review. Thanks.",pytorch
7515,ngimel,pr,2018-05-12T01:32:56Z,put dropout states on the input device,"Fix for #7280. Probably actual bindings in aten should have asserts that all the inputs and dropout states are on the same device, @ezyang if you can point me to a place where you are doing these checks, which at a quick glance I was not able to find, I'd add assert for dropout states in there, if there is no such place, I think it makes sense to add these checks. ",pytorch
7543,peterjc123,pr,2018-05-14T04:01:57Z,[WIP] Fix mingw build for Windows,"1. Change some `WIN32` flags to `_MSC_VER` for those changes that should only apply to MSVC.
2. Add `__MINGW64__` flag for those that should also apply to MINGW-W64.
3. Add the flags for mingw-w64 in `build_pytorch_libs.bat`",pytorch
7563,fritzo,pr,2018-05-15T04:59:06Z,[distributions] Add link to TensorFlow Distributions paper,This both gives credit to the tf.distributions team and links to a great piece of documentation for the torch.distributions library.,pytorch
7624,apaszke,pr,2018-05-16T20:13:50Z,Show skipped distributed tests as skipped,"Previously, tests that have been skipped because their backend was
missing would show up as succeeded, which has been very confusing.

",pytorch
7644,apaszke,pr,2018-05-17T15:18:45Z,Fix UB when converting negative floating values to uint8_t,"See the comment in the code for a detailed description.

cc: @goldsborough @ezyang",pytorch
7672,apaszke,pr,2018-05-18T11:33:21Z,Add a loop unrolling pass to PyTorch JIT,"Examples can be seen in expect files. Current strategy for dealing with iteration counts is to have a single ""mutable"" variable outside of the loop, that would be incremented every time. An alternative would be to have them independent, and still use the builtin value, but with an appropriate multiplier (for the unrolled loop), or offset (for epilogue), but this doesn't seem very useful for now.

Also contains a few minor improvements, like a change in our strategy for generating new unique names, to favor changing `name.n` to `name.(n+1)` (where `(n+1)` is ofc evaluated), instead of appending `.1` (that strategy resulted in names like `y.2.1.1.1.1.1.1` after unrolling). Also, `checkScript` has been using the Python-frontend in optimized mode even if `optimize` was explicitly set to `False`.

It also looks like our DCE pass is unsound at the moment, because it treats PythonOps as side-effect free which is not true in general. I couldn't easily fix it, because ONNX depends on this behavior to implement export of packed sequences.

@zdevito @jamesr66a",pytorch
7703,peterjc123,pr,2018-05-19T14:17:56Z,Fix compile flags for MSVC,"This helps to identity `/DNDEBUG` in compile flags, which doesn't work correctly before this change.",pytorch
7704,peterjc123,pr,2018-05-19T14:32:29Z,Fix Windows doc for import error,About #4518.,pytorch
7708,ssnl,pr,2018-05-19T19:35:34Z,[distributions] Always enable grad when calculating lazy_property,"Fixes #7705 .

@apaszke @fritzo @alicanb ",pytorch
7709,ngimel,pr,2018-05-20T03:32:19Z,fix for cuda 9.2 builds,,pytorch
7710,ssnl,pr,2018-05-20T05:04:55Z,[PyTorch] [gradcheck] change backward() to grad(),"Change backward calls to grad to avoid memory leak from #7343 
Replace unnecessary create_graph=True with retain_graph=True


The memory leak is blocking #7270 .

",pytorch
7718,bstriner,pr,2018-05-20T23:59:24Z,Fix win mkldnn,"Hi Everybody!

Error in windows build if mkldnn is enabled because `build_pytorch_libs.bat` doesn't understand the argument `--with-mkldnn`. This argument works on Linux but not Win. This PR fixes the windows build so it can use MKLDNN.

I synced the command line parsing and the ATen cmake command between the batch script and the bash script. It looks like ATen is now correctly building with MKLDNN on Windows.
- https://github.com/pytorch/pytorch/blob/master/tools/build_pytorch_libs.sh
- https://github.com/pytorch/pytorch/blob/master/tools/build_pytorch_libs.bat

Warning: I just brought in all of the options from Linux build that were missing in the Windows build, so I'm trusting that that code is correct. Would appreciate if anyone can give them a once-over.
- Not sure about whether `ATEN_NO_CONTRIB` and `CMAKE_EXPORT_COMPILE_COMMANDS` should be set or not. The latter shouldn't really matter but the former might make a difference.
- I added the command line parsing for gloo and distributed-mw while I was putting in mkldnn but these are not fully implemented. Maybe should print a warning or something. 
- The other build commands in that script probably also need some work, the MKLDNN issue just happened to be causing a problem in my specific situation. Not sure if there is some good way to synchronize the two or figure out something more OS independent. Why do we need the two shell scripts at all instead of just python? A lot of overhead with environment variables and command line parsing that could be avoided.

Cheers",pytorch
7726,bstriner,pr,2018-05-21T06:36:42Z,Export getCudnnHandle,"Easy access to the CuDNN handle would be super useful for writing extensions. This makes `getCudnnHandle` an `AT_CUDA_API`.

I need access to a `cudnnHandle_t` to build a pytorch wrapper for `cudnnCTCLoss`.

Cheers",pytorch
7749,bstriner,pr,2018-05-22T00:06:56Z,Include cudnn_h,"Add additional include files to the install (ATen/cudnn/*.h)

Cheers",pytorch
7771,ssnl,pr,2018-05-22T20:51:15Z,Fix legacy comment after variable tensor merge,Fixes https://github.com/pytorch/pytorch/issues/7075,pytorch
7776,ngimel,pr,2018-05-22T23:45:01Z,small fixes in fusion_compiler,"Don't call cudaFree unconditionally, guard cudaFree call on cudaFreeMutex, submit kernel to current stream. ",pytorch
7779,ngimel,pr,2018-05-23T00:18:24Z,add launch bounds to im2col and col2im,Hopefully should fix #7680 ,pytorch
7787,vfdev-5,pr,2018-05-23T10:20:48Z,Nit fix run_test.sh -> run_test.py,"

",pytorch
7810,orionr,pr,2018-05-24T15:11:27Z,[build] Add back cpp_build tests for Mac,"These were temporarily removed so we could land the PyTorch with libcaffe2 PR.

Can't test locally right now, since I have slow WiFi, but would run
```
git clean -fdx
./.jenkins/pytorch/macos-build-test.sh
```
Instead I'm going to see if Mac tests pass here.

cc @goldsborough 
",pytorch
7828,ngimel,pr,2018-05-24T23:14:51Z,re-fix 9.2 build,Broken by #7399 ,pytorch
7833,orionr,pr,2018-05-25T00:23:11Z,[build] Fix Caffe2 with BUILD_CAFFE2 and BUILD_ATEN both set,"A last minute change made the code generation on the Caffe2-side not wait for the code generation done within PyTorch. This adds that dependency back now that we don't depend directly on the ATen_cpu interface library (which I removed at the last minute).

Tested with

```
./.jenkins/caffe2/build.sh -DUSE_ATEN=ON
```",pytorch
7842,crcrpar,pr,2018-05-25T08:37:01Z,[Pytorch] add missing document in nn.Embedding,This added the explanation of the optional `_weight` argument in `nn.Embedding`.,pytorch
7845,orionr,pr,2018-05-25T14:55:32Z,[build] Only add BUILD_ATEN/USE_ATEN once to flags,We are currently adding it in the caffe2.groovy as well as the build.sh script. Seeing if this helps our builds pass.,pytorch
7862,orionr,pr,2018-05-25T22:23:07Z,Potential fix for RNN test on MKL,,pytorch
7879,ssnl,pr,2018-05-26T20:24:47Z,Fix error when setting multiple arch in TORCH_CUDA_ARCH_LIST,"On master if you do `TORCH_CUDA_ARCH_LIST=""6.0;6.1;7.0"" python setup.py install`, you will meet this error
```
CMake Error at cmake/public/cuda.cmake:302 (if):
  if given arguments:

    ""6.0"" ""6.1"" ""7.0""

  Unknown arguments specified
Call Stack (most recent call first):
  cmake/public/cuda.cmake:419 (caffe2_select_nvcc_arch_flags)
  cmake/Dependencies.cmake:396 (include)
  CMakeLists.txt:188 (include)
```

If you replace `;` with spaces, it compiles, but doesn't build for the archs you specified. 

This fixes it.

cc @orionr @soumith ",pytorch
7886,thuyen,pr,2018-05-27T21:14:52Z,Fix seeding random module in DataLoader,"Fix #7882 

The  `random` module is seeded [here](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L86). The problem with that line is that `seed` is not an `int` but a `Tensor`. Python then will use `hash(seed)`  to seed the random module ([docs](https://docs.python.org/2/library/random.html#random.seed)). Without a hash function for `Tensor`, the actually seed will be the address of the tensor. And it changes every time!

The line should be changed to: `random.seed(int(seed))`

Test on fllowing script from #7882 
```python
import torch
import random

from torch.utils.data import Dataset, DataLoader

class Data(Dataset):
    def __len__(self):
        return 10000
    def __getitem__(self, index):
        print(index, torch.rand(2, 2).sum().item(), random.uniform(0, 1))
        return 1

seed = 2018
torch.manual_seed(seed)
loader = DataLoader(Data(), num_workers=4, shuffle=True)

for x in loader:
    print('-'*10)
    break
```
## Before
First run
```
4717 2.202341079711914 0.9952153654478976
4607 2.3166141510009766 0.6813692345925851
4194 1.9806793928146362 0.6281118075687344
2595 2.95841383934021 0.8414756141240453
4691 0.9809015393257141 0.7622458327788627
9868 2.521920680999756 0.5253262288522356
7367 2.333574056625366 0.35079311205192487
9490 3.02830171585083 0.16235006783937567
----------
6759 3.1252167224884033 0.4424384676992986
```
Next run
```
4607 2.3166141510009766 0.15198273935290807
4194 1.9806793928146362 0.36414129463658884
4691 0.9809015393257141 0.027569260048619926
4717 2.202341079711914 0.5512619092026773
7367 2.333574056625366 0.7932627754589792
9490 3.02830171585083 0.19395324967791994
9868 2.521920680999756 0.5497794735158222
2595 2.95841383934021 0.782779934368899
----------
6759 3.1252167224884033 0.7098308465010348
```

## After
First run
```
4194 1.9806793928146362 0.28176797222610817
4717 2.202341079711914 0.9839100412778289
4607 2.3166141510009766 0.6780782905745018
4691 0.9809015393257141 0.3976280065444453
9868 2.521920680999756 0.7470951277406424
2595 2.95841383934021 0.3961659556772974
7367 2.333574056625366 0.021153138172570696
9490 3.02830171585083 0.07198172828873439
----------
6759 3.1252167224884033 0.5803779056735119
```
Next run
```
4717 2.202341079711914 0.9839100412778289
4194 1.9806793928146362 0.28176797222610817
4607 2.3166141510009766 0.6780782905745018
2595 2.95841383934021 0.3961659556772974
4691 0.9809015393257141 0.3976280065444453
9868 2.521920680999756 0.7470951277406424
7367 2.333574056625366 0.021153138172570696
9490 3.02830171585083 0.07198172828873439
----------
6759 3.1252167224884033 0.5803779056735119

```",pytorch
7905,ssnl,pr,2018-05-29T03:13:19Z,"Fix SN not backprop via sigma(W), and not reusing W_u",cc @crcrpar @t-vi ,pytorch
7934,ssnl,pr,2018-05-29T21:41:21Z,Fix returning scalar input in Python autograd function,Fixes https://github.com/pytorch/pytorch/issues/7568,pytorch
7936,ssnl,pr,2018-05-29T22:02:45Z,Try to fix TORCH_CUDA_ARCH_LIST for PyTorch again,test CI,pytorch
7952,ssnl,pr,2018-05-30T16:34:33Z,Fix THCUNN SpatialDepthwiseConvolution assuming contiguity,Fixes #7805 ,pytorch
7959,ssnl,pr,2018-05-30T19:46:05Z,Fix EmbeddingBag max_norm option,"1. Fixes #7947 
2. Added entries for `embedding`, `embedding_bag`, `gumbel_softmax` under `nn.functional`
3. Improved embedding and embedding bag docs (both Module and functional forms). It took me way tooo long staring at previous docs to figure out what `offset`, `max_norm`, `scale_grad_by_freq` exactly does.
4. Added test for `max_norm` in testing `EmbeddingBag`.
5. Swapped positional args `input` and `weight` in `F.embedding_bag` with BC fix (discussed with @soumith )

Also, cc @cpuhrsch  the new test with `max_norm` set easily fails with CUDA. Basically any value below 3 seem to fail. But on CPU it is stable even when `max_norm=0.1`. I'll open an issue after this is merged.",pytorch
7967,orionr,pr,2018-05-30T22:24:58Z,Build ONNX for PyTorch version of libcaffe2,"@zdevito will be using this to serialize module state. Let's see how the builds do.

cc @bddppq too.",pytorch
7973,ssnl,pr,2018-05-30T23:27:09Z,Support modules that output scalar in Gather (and data parallel),"Fixes #7956 

cc @colesbury ",pytorch
7975,orionr,pr,2018-05-31T00:00:09Z,Fix the cpp libtorch CUDA build,@goldsborough please review. Thanks.,pytorch
7979,peterjc123,pr,2018-05-31T01:50:05Z,Fix the import part of the windows doc,The root cause of #4518 and #7579 has been found and the doc needs to be repaired.,pytorch
8011,vishwakftw,pr,2018-05-31T23:52:37Z,Example for Transformed Distribution,"Closes https://github.com/pytorch/pytorch/issues/7857

cc: @fritzo ",pytorch
8033,ssnl,pr,2018-06-01T16:08:15Z,propagate nan in some activations,"Fixes #7999 

cc @weiyangfb on your work with `hardshrink`.",pytorch
8043,ssnl,pr,2018-06-01T19:52:34Z,Skip flaky CUDA memory leak check on BN tests on windows,"Can't figure out the 1024 bytes anti-leaks, and it happened three times already. :(

I'll keep trying.

cc @ezyang 

",pytorch
8046,vishwakftw,pr,2018-06-01T20:57:05Z,[ready] Clean up torch.distributions,"This PR removes instances of `Variable` and `Tensor` across the entire distributions suite. I have also removed some dead code, and made minor modifications to `utils.py` inside `distributions`. Tensor notation has also been fixed in the examples.

cc: @fritzo @apaszke ",pytorch
8047,ssnl,pr,2018-06-01T21:36:15Z,Allow parallel_apply to take in list[Tensor],"This has no harm, but makes using `parallel_apply` nicer, and also avoids accidental slicing of Tensor if one accidentally pass in `[tensor0, tensor1]` as `input` instead of `[[tensor0], [tensor1]]`. It is actually particularly easy to make this mistake because `[tensor0, tensor1]` is the result of `scatter(a_single_tensor_arg)`, e.g. #6983 .
",pytorch
8050,apaszke,pr,2018-06-01T21:42:56Z,Add code for TensorBoard visualization of JIT GraphExecutors,"cc @zdevito @jamesr66a 

Example screenshot:

<img width=""1919"" alt=""screen shot 2018-05-24 at 23 35 11"" src=""https://user-images.githubusercontent.com/4583066/40864758-71427c26-65f5-11e8-9f5f-77170d03974c.png"">

Unfortunately has a TF dependency, but I don't think there's any way to use TensorBoard without it installed...",pytorch
8051,ssnl,pr,2018-06-01T22:07:19Z,Better conv error message basing on weight shape,"Fixes #7332 

",pytorch
8058,ngimel,pr,2018-06-01T23:44:55Z,add comparison operators to jit,"Will be necessary for adding activations such as relu, hardtanh, hardshrink, clamp etc. 
Note: now shape_inferenceable balks at any integral type, I think it would make sense to support shape inference for pointwise operations on integral types (that way, it would also be possible to add those to jit). ",pytorch
8073,orionr,pr,2018-06-02T16:26:43Z,Expose proto utils and ONNX,"Looking to support ONNX protobufs exposed from libcaffe2.so to PyTorch.

Needs to link up with @bddppq's changes on ONNX, but this is the Caffe2 protobuf side of the changes.

To test (and see the current failure)

```
python setup.py build_deps
python setup.py develop
cd test
python run_test.py
```

cc @zdevito @Yangqing ",pytorch
8089,ngimel,pr,2018-06-03T20:01:24Z,remove some unnecessary cudaGetDevices,"Removes some (not all) unnecessary cudaGetDevice's. It does not affect usual performance, except in may be some pathological cases, but it makes timings under nvprof slightly less distorted. ",pytorch
8123,ssnl,pr,2018-06-04T18:53:56Z,[WIP] [PyTorch] [STFT] use fft when fft_size == frame_length,"Addresses #7883 
Using script in #7883 , here are the new timings:
```
librosa: 1.918302297592163
(2049, 2647)
torch: 0.34431028366088867
torch.Size([10, 2639, 2049])
```
The output shape difference is expected as explained in https://github.com/pytorch/pytorch/issues/7038 .

",pytorch
8130,ssnl,pr,2018-06-04T20:57:56Z,Move signal window functions to ATen; add Blackman window,"cc @rafaelvalle .

relevant issue https://github.com/pytorch/pytorch/issues/3775",pytorch
8161,vishwakftw,pr,2018-06-05T14:56:48Z,Fix __rshift__ bug,Fixes https://github.com/pytorch/pytorch/issues/8111 .,pytorch
8166,ssnl,pr,2018-06-05T18:13:39Z,Docs for gradcheck and gradgradcheck; expose gradgradcheck,Fixes #6090 ,pytorch
8170,ssnl,pr,2018-06-05T19:52:31Z,[SpectralNorm] don't register original weight as buffer,"Also did some fixes for buffers that require grad

https://github.com/pytorch/pytorch/issues/8160",pytorch
8184,bstriner,pr,2018-06-06T03:37:11Z,Fix protobuf options,"`protobuf_BUILD_SHARED_LIBS` and `protobuf_MSVC_STATIC_RUNTIME` are set in `ProtoBuf.cmake` as regular variables which are wiped out when the library itself declares them as options. This means these values don't propagate to protobuf, so protobuf is building me a static runtime, which then fails when caffe2 tries to link:

libprotobuf.lib(io_win32.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MT_StaticRelease' doesn't match value 'MD_DynamicRelease' in Backtrace.obj [D:\Projects\pytorchtest\pytorch\build\caffe2\caffe2.vcxproj]

This PR passes them as options, just like `protobuf_BUILD_TESTS` and `protobuf_BUILD_EXAMPLES` earlier in that file.

No idea why this isn't failing on CI, but maybe only specific versions of cmake have this issue. I'm seeing this on 3.8 (I know I should update).

Cheers",pytorch
8188,ngimel,pr,2018-06-06T05:14:39Z,use THCThrustAllocator in BCECriterion,,pytorch
8192,apaszke,pr,2018-06-06T09:29:14Z,Add more annotations for arguments in ATen schema,"A completely minor change, but improves readability of ATen schema by a great deal. Our `FunctionSchema` list contains triples with: an offset into our string table, and the number of arguments and returns to be read out from the **current** position in the argument list. This is ok for an automatic parser, but it makes it impossible to associate `FunctionSchema` with its `ArgumentSchema`s automatically. This PR simply adds comments that separate and annotate `ArgumentSchema`s for different functions, which makes them very easy to find.

@zdevito @jamesr66a 

Previously (the output is truncated at some column, because I only copied part of it, not because of the code):
```cpp
{ 210, 1, 0, 0 }, // Argument(""tensors"", at::nullopt, at::nullopt, ListType::ofTensors())   
{ 15, 0, 8, 2 }, // Argument(""dim"", as_tensor(int64_t(0)), AttributeInfo{ AttributeKind::i, 
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 469, 0, 0, 2 }, // Argument(""chunks"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 15, 0, 8, 2 }, // Argument(""dim"", as_tensor(int64_t(0)), AttributeInfo{ AttributeKind::i, 
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
{ 461, 0, 0, 0 }, // Argument(""input"", at::nullopt, at::nullopt, DynamicType::get())        
{ 107, 0, 0, 0 }, // Argument(""weight"", at::nullopt, at::nullopt, DynamicType::get())       
{ 385, 0, 0, 0 }, // Argument(""bias"", at::nullopt, at::nullopt, DynamicType::get())         
{ 208, 0, 0, 1 }, // Argument(""stride"", at::nullopt, AttributeInfo{ AttributeKind::is, at::n
{ 317, 0, 0, 1 }, // Argument(""padding"", at::nullopt, AttributeInfo{ AttributeKind::is, at::
{ 330, 0, 0, 1 }, // Argument(""dilation"", at::nullopt, AttributeInfo{ AttributeKind::is, at:
{ 472, 0, 0, 2 }, // Argument(""transposed"", at::nullopt, AttributeInfo{ AttributeKind::i, at
{ 396, 0, 0, 1 }, // Argument(""output_padding"", at::nullopt, AttributeInfo{ AttributeKind::i
{ 473, 0, 0, 2 }, // Argument(""groups"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
{ 461, 0, 0, 0 }, // Argument(""input"", at::nullopt, at::nullopt, DynamicType::get())        
{ 107, 0, 0, 0 }, // Argument(""weight"", at::nullopt, at::nullopt, DynamicType::get())       
{ 385, 0, 0, 0 }, // Argument(""bias"", at::nullopt, at::nullopt, DynamicType::get())         
{ 208, 0, 0, 1 }, // Argument(""stride"", at::nullopt, AttributeInfo{ AttributeKind::is, at::n
{ 317, 0, 0, 1 }, // Argument(""padding"", at::nullopt, AttributeInfo{ AttributeKind::is, at::
{ 330, 0, 0, 1 }, // Argument(""dilation"", at::nullopt, AttributeInfo{ AttributeKind::is, at:
{ 472, 0, 0, 2 }, // Argument(""transposed"", at::nullopt, AttributeInfo{ AttributeKind::i, at
{ 396, 0, 0, 1 }, // Argument(""output_padding"", at::nullopt, AttributeInfo{ AttributeKind::i
{ 473, 0, 0, 2 }, // Argument(""groups"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 475, 0, 0, 2 }, // Argument(""benchmark"", at::nullopt, AttributeInfo{ AttributeKind::i, at:
{ 476, 0, 0, 2 }, // Argument(""deterministic"", at::nullopt, AttributeInfo{ AttributeKind::i,
{ 462, 0, 0, 2 }, // Argument(""cudnn_enabled"", at::nullopt, AttributeInfo{ AttributeKind::i,
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
```

Now:
```cpp
// Arguments for cat (2 args, 1 returns)                                                    
{ 210, 1, 0, 0 }, // Argument(""tensors"", at::nullopt, at::nullopt, ListType::ofTensors())   
{ 15, 0, 8, 2 }, // Argument(""dim"", as_tensor(int64_t(0)), AttributeInfo{ AttributeKind::i, 
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
// Arguments for ceil (1 args, 1 returns)                                                   
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
// Arguments for chunk (3 args, 1 returns)                                                  
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 469, 0, 0, 2 }, // Argument(""chunks"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 15, 0, 8, 2 }, // Argument(""dim"", as_tensor(int64_t(0)), AttributeInfo{ AttributeKind::i, 
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
// Arguments for cudnn_is_acceptable (1 args, 1 returns)                                    
{ 1, 0, 0, 0 }, // Argument(""self"", at::nullopt, at::nullopt, DynamicType::get())           
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
// Arguments for convolution (9 args, 1 returns)                                            
{ 461, 0, 0, 0 }, // Argument(""input"", at::nullopt, at::nullopt, DynamicType::get())        
{ 107, 0, 0, 0 }, // Argument(""weight"", at::nullopt, at::nullopt, DynamicType::get())       
{ 385, 0, 0, 0 }, // Argument(""bias"", at::nullopt, at::nullopt, DynamicType::get())         
{ 208, 0, 0, 1 }, // Argument(""stride"", at::nullopt, AttributeInfo{ AttributeKind::is, at::n
{ 317, 0, 0, 1 }, // Argument(""padding"", at::nullopt, AttributeInfo{ AttributeKind::is, at::
{ 330, 0, 0, 1 }, // Argument(""dilation"", at::nullopt, AttributeInfo{ AttributeKind::is, at:
{ 472, 0, 0, 2 }, // Argument(""transposed"", at::nullopt, AttributeInfo{ AttributeKind::i, at
{ 396, 0, 0, 1 }, // Argument(""output_padding"", at::nullopt, AttributeInfo{ AttributeKind::i
{ 473, 0, 0, 2 }, // Argument(""groups"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
// Arguments for _convolution (12 args, 1 returns)                                          
{ 461, 0, 0, 0 }, // Argument(""input"", at::nullopt, at::nullopt, DynamicType::get())        
{ 107, 0, 0, 0 }, // Argument(""weight"", at::nullopt, at::nullopt, DynamicType::get())       
{ 385, 0, 0, 0 }, // Argument(""bias"", at::nullopt, at::nullopt, DynamicType::get())         
{ 208, 0, 0, 1 }, // Argument(""stride"", at::nullopt, AttributeInfo{ AttributeKind::is, at::n
{ 317, 0, 0, 1 }, // Argument(""padding"", at::nullopt, AttributeInfo{ AttributeKind::is, at::
{ 330, 0, 0, 1 }, // Argument(""dilation"", at::nullopt, AttributeInfo{ AttributeKind::is, at:
{ 472, 0, 0, 2 }, // Argument(""transposed"", at::nullopt, AttributeInfo{ AttributeKind::i, at
{ 396, 0, 0, 1 }, // Argument(""output_padding"", at::nullopt, AttributeInfo{ AttributeKind::i
{ 473, 0, 0, 2 }, // Argument(""groups"", at::nullopt, AttributeInfo{ AttributeKind::i, at::nu
{ 475, 0, 0, 2 }, // Argument(""benchmark"", at::nullopt, AttributeInfo{ AttributeKind::i, at:
{ 476, 0, 0, 2 }, // Argument(""deterministic"", at::nullopt, AttributeInfo{ AttributeKind::i,
{ 462, 0, 0, 2 }, // Argument(""cudnn_enabled"", at::nullopt, AttributeInfo{ AttributeKind::i,
{ 2, 0, 0, 0 }, // Argument(""result"", at::nullopt, at::nullopt, DynamicType::get())         
```",pytorch
8200,Yangqing,pr,2018-06-06T17:01:02Z,[cmake] deprecate caffe2_* specific cuda function in cmake.,"Was chatting with @pietern about the cuda situation and custom paths, and this is a quick experiment to see how we can consolidate things. In the past, we have CUDA_ARCH_NAME and TORCH_CUDA_ARCH_LIST to select cuda archs, and this tries to:

(1) request one to explicitly pass TORCH_CUDA_ARCH_LIST as a cmake variable instead of env variable. We can make it a fatal error in the future.
(2) request one to deprecate the use of CUDA_ARCH_NAME in favor of TORCH_CUDA_ARCH_LIST.

The reason is that CUDA_ARCH_LIST is the name used in FindCUDA.cmake nowadays so it is better to consolidate.

The caffe2_* functions were originally written by NVidia circa 2014 and similar functionalities have hence been in the official cmake lists, making it better to use the official one.

Test plan: waiting for CI. Do not have cuda on my laptop.",pytorch
8205,ssnl,pr,2018-06-06T18:15:48Z,[DO NOT MERGE] Test cuda mem leak check on windows,test #8044 ,pytorch
8213,ssnl,pr,2018-06-06T21:20:06Z,Temporarily skip CUDA memory leak check on Windows altogether,See #8044 . Still actively investigating.,pytorch
8222,ssnl,pr,2018-06-06T22:58:49Z,Fix broadcast copying device[0] tensor when not using NCCL,"1. Fix broadcast copying device[0] tensor when not using NCCL
2. Avoids potential extra copy in `flatten_dense_tensors`
",pytorch
8225,Yangqing,pr,2018-06-07T00:16:43Z,Allow optional build and installation of native test binaries,"Per request from @smessmer - we add an option to install test.

(1) If BUILD_TEST is on, we build test. It is defaulted off.
(2) if INSTALL_TEST is on, we install test to installation folder. It is defaulted on if BUILD_TEST is on.",pytorch
8232,Yangqing,pr,2018-06-07T04:29:56Z,Remove .gitmodules.aten since it is in .gitmodules now,,pytorch
8239,orionr,pr,2018-06-07T14:43:18Z,Remove core and util warnings,"We'll be exposing caffe2/core/ and caffe2/util/ to PyTorch shortly, so this removes some of the warnings for when we do `-Werror`.

cc @Yangqing @bddppq ",pytorch
8265,Yangqing,pr,2018-06-08T00:11:55Z,[cmake] Make cudnn optional,"This is after discussion with @orionr and @soumith regarding dependency requirements for unified backend. This diff makes it so that we can determine the use of CUDNN at compilation time.

I did not have a chance to test it yet (mac cuda build hangs at the eigen error), so firing off a CI to test first.

@orionr do you know if we have BUILD_CAFFE2=ON and USE_CUDNN=OFF cases in CI?",pytorch
8271,Yangqing,pr,2018-06-08T03:14:01Z,[cmake] Add and export Modules_CUDA_fix,This PR puts the discretional inclusion of Modules_CUDA_fix to public/cuda.cmake and also installs the corresponding files. Manually verified the build process but the installation path is not tested (only did a dummy install and eyeballed that the files are there).,pytorch
8277,peterjc123,pr,2018-06-08T07:21:42Z,Fix the script doesn't stop eariler on error for MSVC and Ninja,Fixes #4990. Uses the error log to determine whether the build process is actually successful or not.,pytorch
8289,Yangqing,pr,2018-06-08T19:51:18Z,Add option USE_NVRTC which defaults to off,"This is further to unblock unified backend because right now libcuda.so has to be explicitly depended upon due to libtorch not separating cpu and cuda builds (as a result, we turn off USE_NVRTC by default).",pytorch
8298,Yangqing,pr,2018-06-09T00:49:22Z,"[forcing function, don't merge yet] Remove BUILD_CAFFE2, aka always build it.",This is not intended to immediately merge but serves as a unit testing to see how close we are with it.,pytorch
8299,Yangqing,pr,2018-06-09T02:43:53Z,[cmake] Remove THC's FindMAGMA,Seems that it is the last item in @orionr 's ongoing PR #7300. After this we should have full cmake Find*.cmake files aggregated under root cmake/.,pytorch
8300,Yangqing,pr,2018-06-09T03:13:49Z,[build] remove the use of NO_CUDA,I wanted to do a check to see how difficult it is to remove deprecated flags. This one deals with NO_CUDA.,pytorch
8309,vishwakftw,pr,2018-06-09T23:48:26Z,Improve error messages,Fixes #8307 .,pytorch
8313,bstriner,pr,2018-06-10T04:01:32Z,Export tensor descriptor,"Hi Guys!

This exports descriptors (TensorDescriptor, ConvolutionDescriptor, etc.). Also re-implements previous PR to include cudnn/*.h in the install, which was knocked out by recent refactoring: https://github.com/pytorch/pytorch/pull/7749

Descriptors.h, Handles.h and Exceptions.h eliminate a lot of boilerplate in cudnn extensions, so this PR makes them available.

Cheers",pytorch
8317,peterjc123,pr,2018-06-10T12:37:36Z, [TEST ONLY] Test caffe2 build error early stopping,,pytorch
8321,bstriner,pr,2018-06-10T20:33:20Z,Remove duplicate descriptors,"This PR removes some duplication in `recurrent_op_cudnn.cc`. Instead of 4 of the same exact descriptor, should work fine with just 1. I don't see any other code that relies on those being 4 separate locations, but if that is what you need you can always allocate additional descriptors as necessary.

Have not fully tested this thing out, just something I noticed when I was reading through the descriptor  code.

Cheers",pytorch
8326,peterjc123,pr,2018-06-11T07:34:17Z,Fix collect_env.py for Windows,,pytorch
8338,orionr,pr,2018-06-11T16:36:59Z,[build] Remove BUILD_CAFFE2 and build everything,"This completely removes BUILD_CAFFE2 from CMake. There is still a little bit of ""full build"" stuff in setup.py that enables USE_CUDNN and BUILD_PYTHON, but otherwise everything should be enabled for PyTorch as well as Caffe2. This gets us a lot closer to full unification.

cc @mingzhe09088, @pjh5, @ezyang, @smessmer, @Yangqing",pytorch
8340,orionr,pr,2018-06-11T17:37:30Z,Fix disabling of USE_CUDNN when not found,cc @goldsborough @smessmer ,pytorch
8344,ssnl,pr,2018-06-11T20:20:23Z,Cache cufft plans,"#8120 
Empirical speed up when running our test 10 times is `9.57s => 7.90s` with cache enabled.",pytorch
8380,orionr,pr,2018-06-12T16:34:35Z,[build] Use SYSTEM For all includes in Dependencies.cmake,"This should help with warnings and errors.

cc @goldsborough @ezyang ",pytorch
8381,orionr,pr,2018-06-12T16:38:00Z,[build] Check CAFFE2_USE_MSVC_STATIC_RUNTIME to set -MD vs -MT in cuda.cmake,"Potential fix for https://github.com/pytorch/pytorch/issues/7962

cc @Yangqing @pietern ",pytorch
8382,orionr,pr,2018-06-12T17:05:02Z,More warning skips,Extracting from https://github.com/pytorch/pytorch/pull/8073,pytorch
8407,ssnl,pr,2018-06-13T00:36:24Z,Fix -g not passed to nvcc when DEBUG=1,"1. See title. Basically deletes `set(CUDA_PROPAGATE_HOST_FLAGS OFF)`, which is a line from 2 yrs ago. Locally it doesn't break things.

2. Reworked a bit how `CUDA_DEBUG` works, and renamed it to `CUDA_DEVICE_DEBUG` to be clearer. Now it is a cmake flag. `build_torch_libs.sh` fetches its value from env.

3. Disable debug mode when `DEBUG=0`. Same for `REL_WITH_DEB_INFO`.",pytorch
8416,Yangqing,pr,2018-06-13T07:02:14Z,[minor] fix old comment to point to the right file,Just comments - no actual content changes.,pytorch
8422,orionr,pr,2018-06-13T13:32:23Z,[WIP] Update AT_API logic to match CAFFE2_API,,pytorch
8427,vishwakftw,pr,2018-06-13T15:00:57Z,"Default the hidden states to 0 in RNNCell, LSTMCell and GRUCell.","Fixes #8420 .

cc: @zou3519 @valsworthen",pytorch
8428,vishwakftw,pr,2018-06-13T15:22:39Z,Expose logsumexp docs," Fixes #8426 .

Also, marks `log_sum_exp` in distributions for internal use to avoid confusion.

",pytorch
8444,orionr,pr,2018-06-13T20:04:11Z,Move libtorch CMakeLists.txt to torch/,"Seeing if this quick move doesn't break anything. We'll be unifying this more with the root-level cmake file in the future.

cc @goldsborough @soumith ",pytorch
8532,orionr,pr,2018-06-14T22:01:28Z,Remove aten project for main build,,pytorch
8535,orionr,pr,2018-06-14T22:22:17Z,[WIP][build] Build cpp_build libtorch from root-level CMakeLists,"Remove separate shell script to build libtorch in cpp_build and rather have it built by root CMakeLists.txt include.

Putting this up to run CI tests. Things will likely break.",pytorch
8567,ssnl,pr,2018-06-15T19:52:22Z,fix lint,,pytorch
8570,orionr,pr,2018-06-15T22:18:47Z,[WIP] Test merge from master into fbsync,,pytorch
8573,ngimel,pr,2018-06-15T23:00:45Z,add relu to jit and exp to autodiff,,pytorch
8615,ssnl,pr,2018-06-18T19:44:42Z,Make NCCL build select NVCC_GENCODE smarter,"Currently our NCCL 1 module always builds for all archs that current CUDA supports. So rebuilding it is extremely slow. This PR refactors the code that retrieves `nvcc_arch_flags` from `cuda.cmake` into a macro, and use it when building NCCL. As a result, NCCL should respect `TORCH_CUDA_ARCH_LIST` if given, and only builds for the current archs found on the machine if not.

Fixes #8405 .

There are also two references to `-DCMAKE_MODULE_PATH=""$BASE_DIR/cmake/FindCUDA""` which are now non-existent. I changed them to pointing to `-DCMAKE_MODULE_PATH=""$BASE_DIR/cmake/Modules_CUDA_fix""`, but I don't actually know the difference. Please let me know if I should revert those changes.",pytorch
8654,orionr,pr,2018-06-19T15:57:18Z,Add CODEOWNERS entry for third_party to track changes,"With our switch to commit-level syncs to the internal code base, I need to review and manually update all third_party changes. Using CODEOWNERS to manage that right now. Will remove once we're doing commit-level syncs.

cc @ezyang ",pytorch
8657,Yangqing,pr,2018-06-19T17:43:06Z,Add CAFFE2_USE_CUDNN guard on context_gpu.cu,Facebook note: this is mirrored in internal diff D8517127,pytorch
8663,vishwakftw,pr,2018-06-19T19:50:38Z,Add a warning in gradcheck if inputs precision < float64,"Fixes #8659 . This PR adds a warning to alert users about the possibility of a failure in the `gradcheck` and `gradgradcheck` functions.

Why warning? Users might alter the values of `eps`, `atol` and/or `rtol` to make their custom tests pass. Throwing errors in such scenarios could be bad.

cc: @SsnL ",pytorch
8671,Yangqing,pr,2018-06-19T22:38:47Z,Remove dangling inclusion path,(introduced in #8389).,pytorch
8682,ngimel,pr,2018-06-20T05:55:22Z,don't do unnecessary copies for bernoulli_,fix for #8673 ,pytorch
8691,orionr,pr,2018-06-20T16:03:51Z,[DO NOT MERGE] Test merge from master into fbsync,Seeing whether CI runs and if we see the correct buttons to import into Facebook code base.,pytorch
8699,vishwakftw,pr,2018-06-20T17:52:58Z,Make torch.eye_ init retain requires_grad as original tensor,"This fixes #8692 

cc: @zou3519 ",pytorch
8706,ssnl,pr,2018-06-20T19:53:17Z,Improve cudnn RNN backward error message in eval mode,"sort of related issue is #7961 

cc @ezyang ",pytorch
8721,ssnl,pr,2018-06-20T23:00:46Z,Fix as_strided_backward,"Partially fixes #8626 
fixes https://github.com/pytorch/pytorch/issues/8649
fixes the immediate issue in https://github.com/pytorch/pytorch/issues/8577 , but we should make these things a hard error. @ezyang commented that this is doable in `setUp` and `tearDown`

@colesbury ",pytorch
8743,peterjc123,pr,2018-06-21T13:43:05Z,Minor fixes for finding CUDNN,Fixes #8485 probably.,pytorch
8750,ssnl,pr,2018-06-21T18:14:46Z,i2h<->h2h in gif,"fixes https://github.com/pytorch/pytorch.github.io/issues/34
fixes #1413

for future reference, this is procedure:
```
1. duplicate last frame to make it last 5 frames (so the total number of frames should be 11)
2. convert -resize 720x404 -delay 100 -loop 0 *.jpeg dynamic_graph.gif
```",pytorch
8754,ssnl,pr,2018-06-21T19:09:37Z,Add ssnl and zou3519 as pytorch doc directory owner,cc @zou3519 ,pytorch
8773,orionr,pr,2018-06-21T23:15:25Z,Move nanopb-generated ONNX to unique file name,Matches changes made during the latest sync.,pytorch
8784,peterjc123,pr,2018-06-22T07:32:53Z,[WIP] Add clang-cl build support for Windows,"In order to enable OpenMP 2.0+ functions on Windows, this PR is made. However, we need to change some flags in ONNX and add build tools on CI for this PR to pass.",pytorch
8794,orionr,pr,2018-06-22T17:58:00Z,[DO NOT CHECK IN] fbsync test,,pytorch
8797,ssnl,pr,2018-06-22T18:40:01Z,[build] Fix for old cmake versions,"Strategy:
1. Move all upstream files to `Module_CUDA_fix/upstream/*`. 
2. Create a custom `Module_CUDA_fix/FindCUDA.cmake` that first includes `upstream/CMakeInitializeConfigs.cmake` and then `upstream/FindCUDA.cmake` so it can be used by submodules too.
3. Remove all instances of `include(CMakeInitializeConfigs)`.

For easier review process, [here](https://github.com/SsnL/pytorch/tree/findcuda_wrapper/cmake/Modules_CUDA_fix)'s how `cmake/Module_CUDA_fix` looks like after this patch. 

@ezyang @soumith @orionr 

cc @li-roy @zdevito ",pytorch
8798,vishwakftw,pr,2018-06-22T19:10:51Z,Add CUDA to logspace and linspace declarations in Declarations.cwrap,"These functions are already implemented, but were not exposed. Fixes https://github.com/pytorch/pytorch/issues/8786 and https://github.com/pytorch/pytorch/issues/1070 .

",pytorch
8799,ssnl,pr,2018-06-22T20:04:09Z,[README.md] Use GitLab URL for CMake,CMake has moved to GitLab. Use that as the URL if people want to submit patch upstream.,pytorch
8803,ssnl,pr,2018-06-22T20:23:36Z,Test that broadcast doesn't copy when dst and src devices are the same,Add test for the fix in https://github.com/pytorch/pytorch/pull/8222 .,pytorch
8819,ngimel,pr,2018-06-23T04:21:28Z,"remove unnecessary headers from SpectralOps, add cuda.h include to deâ€¦","â€¦viceutils

deprecated __ballot warnings recently appeared again, because THCDeviceUtils was somehow included without cuda.h. This PR removes what looks like unnecessary includes in SpectralOps.cu (on my machine compiles w/o them), and explicitly adds cuda.h to THCDeviceUtils.cuh, so that CUDA_VERSION is always defined there. ",pytorch
8822,vishwakftw,pr,2018-06-23T14:23:40Z,Fix CUDA_NVCC_EXECUTABLE from being set to empty,"Fixes #8790 (and partly #8781)

cc: @zou3519 @SsnL ",pytorch
8825,ssnl,pr,2018-06-23T15:47:54Z,Improve convT output_padding docs,https://github.com/pytorch/pytorch/issues/8816 ,pytorch
8834,ssnl,pr,2018-06-24T19:40:47Z,[build] Fix NCCL NVCC_GENCODE w/ multiple archs,"Fixes https://github.com/pytorch/pytorch/issues/8729
and probably https://github.com/pytorch/pytorch/issues/8831

CMake automatically escapes spaces in string into `\[space]` for some reason. So just passing a list will work.",pytorch
8836,bstriner,pr,2018-06-25T00:47:26Z, Add file and line to CUDA_CHECK and CUDNN_CHECK,"Add file and line to messages from `CUDA_CHECK` and `CUDNN_CHECK`. Renamed original functions to `cudaCheck` and `cudnnCheck` and made macros with the original names. Should make debugging slightly friendlier.

Also ran clang-format on the file.

Cheers",pytorch
8850,ssnl,pr,2018-06-25T15:31:17Z,[build] disable test_expect for pinning cmake to 3.5* in dockerfiles repo,,pytorch
8859,vishwakftw,pr,2018-06-25T18:21:38Z,"Document get_device, fixes #8857",cc: @SsnL @zou3519 ,pytorch
8863,ssnl,pr,2018-06-25T19:35:29Z,[build] Raise in cmake when seeing NVCC{9/9.1} + GCC6 combo,"NVCC 9/9.1 cannot compile `std::tuple` header from GCC >= 6.
See: 
1. https://devtalk.nvidia.com/default/topic/1028112/cuda-setup-and-installation/nvcc-bug-related-to-gcc-6-lt-tuple-gt-header-/
2. PyTorch: https://github.com/pytorch/pytorch/issues/3963 #8832 https://github.com/pytorch/pytorch/issues/5136 https://github.com/pytorch/pytorch/issues/3807
3. C2: https://github.com/caffe2/caffe2/issues/1898
4. TF: https://github.com/tensorflow/tensorflow/issues/16246
5. More: https://github.com/kokkos/kokkos/issues/1306",pytorch
8864,ssnl,pr,2018-06-25T19:46:41Z,[build] Use conda cmake in two CI builds,"Now that we are using `apt-get cmake=3.5*` in most builds, make two builds use conda cmake (currently `3.11.1`): `pytorch-linux-xenial-cuda8-cudnn6-py2` and `pytorch-linux-xenial-cuda9-cudnn7-py3`.",pytorch
8869,ssnl,pr,2018-06-25T21:36:43Z,[build] Enable clang-specific warnings only when using clang,"Should fix #8447 

@smessmer on the `c10/CMakeList.txt` change.",pytorch
8877,orionr,pr,2018-06-25T23:07:41Z,Sync from fbsync branch up to db570d40dad186817fd17b16a009a0518e6d84e2,Rebased on top of master,pytorch
8887,orionr,pr,2018-06-26T02:44:57Z,Update from Facebook,,pytorch
8945,vishwakftw,pr,2018-06-27T17:26:03Z,Fix x.pow(0) gradient when x contains 0,This closes https://github.com/pytorch/pytorch/issues/8940 .,pytorch
8948,ssnl,pr,2018-06-27T17:47:10Z,Fix nccl/CMakeLists.txt,Changes (were merged) in #8834 and #8829 (cc @yf225 ) were lost in https://github.com/pytorch/pytorch/commit/9ec0a2aef4fe67e40e4f6d487c01ec614b4af11a#diff-6997846ce6daf0c271e2db9ef0508551. This PR resubmits them.,pytorch
8950,orionr,pr,2018-06-27T18:02:41Z,Remove third_party from CODEOWNERS file,No longer required now that we've switched over to ShipIt on master.,pytorch
8956,orionr,pr,2018-06-27T18:47:50Z,Cleanup of the shipit commit,Some files shouldn't have been added. Minor changes.,pytorch
8965,ssnl,pr,2018-06-27T21:31:07Z,[wip] Speed up permute and expand cases in as_strided_backward,"1. Optimize for permute case: since we sort the strides, we can check if the size & strides match via some permutation (assuming that strides are unique).
2. Optimize for expand case:

    1. expanded input: expanding dim `i` to `size[i]` is just multiplying the #times an address is referred to in input by `size[i]`. So we can just divide gradient by `\prod_{expanded dim i} inp_size[i]`.
    2. expanded output: using a single `sum` over multiple dimensions, rather than doing a `sum` for each one. (same for `squeeze`).

I added a test that asserts combinations of permute, (un)squeeze, expand, and transpose won't allocate the large storage-like tensor. Although it tests the internal behavior of `as_strided_backward` and will need to be modified if further optimizations are done in other cases, I can't find better ways to test that we are actually not copying but only returning a view of the gradient.",pytorch
8990,orionr,pr,2018-06-28T16:24:26Z,Correctly escape operator documentation,"Summary: Looks like we didn't doubly-escape the new operator docs. This takes care of that.

Differential Revision: D8678436

cc @goodlux @MatthewInkawhich @inkawhich 
",pytorch
8992,ssnl,pr,2018-06-28T18:10:45Z,Speed-up multidim sum,"1. Instead of using non `_out` variant, we allocate a buffer and use `_out` variant to write the intermediate results into the buffer.
2. Reduce dimensions in order of decreasing sizes.

Benchmark:
Sum a randn tensor of shape `[200, 1, 30, 40, 20, 1, 50]` along dimensions `[4, 6, 3, 0, 2, 5]`. Averaged across 1000 times:
```
before patch:
CPU: 0.0441 s
CUDA: 0.0273 s

after patch:
CPU: 0.0234 s
CUDA: 0.0047 s
```",pytorch
9004,ssnl,pr,2018-06-28T22:03:31Z,Fix loading 0.4 BN checkpoints,Fixes https://github.com/pytorch/pytorch/issues/8481,pytorch
9007,ssnl,pr,2018-06-28T23:26:36Z,Improve DataLoader worker fail error message,"Tell people to run with num_workers=0 when DataLoader worker failed

",pytorch
9008,ssnl,pr,2018-06-28T23:53:02Z,[wip] Move dropout to ATen; deprecate Dropout*d with FeatureDropout,"TODO: 
1. onnx symbolic
2. added file, so fix fb build",pytorch
9023,peterjc123,pr,2018-06-29T09:25:13Z,Fix CUDA 8 for Windows,"Fix missing functions for MSVC 2015
Inspired by https://github.com/tensorflow/tensorflow/pull/13525",pytorch
9033,ssnl,pr,2018-06-29T14:48:58Z,Fix select backward when wrap dim,"Previous backward was broken when `index=-1` because slicing `[-1:0]` gives empty tensor/list/array.

Added a test.

cc @goldsborough ",pytorch
9036,peterjc123,pr,2018-06-29T17:33:56Z,[TEST ONLY] Error thrown check,,pytorch
9038,vishwakftw,pr,2018-06-29T17:42:34Z,Specify default initialization schemes for modules in docs,"This closes #6906 .

cc: @SsnL @zou3519 ",pytorch
9052,vishwakftw,pr,2018-06-29T22:22:58Z,Implement torch.pinverse : Pseudo-inverse,"1. Used SVD to compute.
2. Tests in test_autograd, test_cuda and test_torch
3. Doc strings in _torch_docs.py and _tensor_docs.py

Closes #6187",pytorch
9057,orionr,pr,2018-06-30T01:09:19Z,Make _C depend on csrc-no-python,"Summary:
Make the `_C` target depend on the `csrc-no-python` target. Also removes the `csrc` target and the with-python version of autogradpp (which is not used). Let me know if we should pick better names here.

I also ran into a nasty linker issue with only one symbol being undefined. It turns out had been given inline linkage in the `.cpp` file, which I believe is an error.

Reviewed By: orionr

Differential Revision: D8705750
",pytorch
9060,peterjc123,pr,2018-06-30T03:37:41Z,Fix build script for Windows,"1. Escape quotes
2. Use file exist logic to determine build success/failure",pytorch
9080,apaszke,pr,2018-07-01T17:35:49Z,[EASY] Use a static random_device in StorageSharing,"I've been cleaning up my email notifications, and noticed that this PR used a stack-allocated `random_device`. This is generally a bad idea due to this sentence from the C++ reference (emphasis mine):

> `std::random_device` may be implemented in terms of an implementation-defined pseudo-random number engine if a non-deterministic source (e.g. a hardware device) is not available to the implementation. **In this case each `std::random_device` object may generate the same number sequence.**

If this is how this object is implemented, then this `rd()` call will give the same result at every call.

cc @yf225 ",pytorch
9082,ssnl,pr,2018-07-01T19:39:22Z,Fix MAGMA svd and eig,"Fixes https://github.com/pytorch/pytorch/issues/9079

There is room for speed-up for both functions (see https://github.com/pytorch/pytorch/issues/9083), but let's get this in to unblock #9052 .",pytorch
9090,peterjc123,pr,2018-07-02T03:01:32Z,Enable the general usage of _download_url_to_file,A requirement for the fix on https://github.com/pytorch/examples/issues/378.,pytorch
9114,ssnl,pr,2018-07-02T20:59:08Z,Test nn.Module on non-contiguous inputs,"1. Let `ModuleTest` raise when they fail on non-contiguous inputs. Fix legacy modules.
2. Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.
3. Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling `.contiguous()` on in both `forward` and `backward`, 
  a. prefix all current `embedding_bag*` functions with `_`, indicating that they require input to be contiguous (there is a check in each function).
  b. create `embedding_bag`, which makes input arguments `.contiguous()`, and calls `_embedding_bag`
3. Make many ATen `embedding*` functions to work on non-contiguous inputs so we don't need to call `input = input.contiguous()` in Python `nn.functional.embedding`.
4. Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with `sparse=True`. Added tests.
5. Update `TensorUtils.cpp` to use `AT_*` macros.

Request:
review from @cpuhrsch on the `Embedding*` changes.
review from @ezyang on ATen sparse & BN changes.
",pytorch
9134,vishwakftw,pr,2018-07-03T13:56:39Z,"Add grid lines for activation images, fixes #9130","1. Add dashed light blue line for asymptotes.
2. RReLU was missing the activation image.
3. make clean in docs will remove the activation images too.

Sample image:
![image](https://user-images.githubusercontent.com/23639302/42224142-5d66bd0a-7ea7-11e8-8b0a-26918df12f7c.png)

",pytorch
9144,vishwakftw,pr,2018-07-03T19:39:04Z,Modify arguments for random_,This fixes #4574 . Arguments have been changed from `from` and `to` to `min` and `max` respectively.,pytorch
9156,vishwakftw,pr,2018-07-04T01:58:37Z,Fix bug in flip(),"Closes #9147
Added a test to prevent regression in test_torch
Added entries in docs

cc @ezyang @weiyangfb ",pytorch
9192,ssnl,pr,2018-07-05T20:38:35Z,Fix TestAutograd.test_pinverse not actually testing,"cc @vishwakftw 

Also added a check if none of the input tensors in `gradcheck` have `requires_grad=True`.",pytorch
9196,ssnl,pr,2018-07-05T22:04:13Z,Make GroupNorm arg order consistent with BN and IN,"Now GroupNorm has `GroupNorm(num_groups, num_channels)`. But for IN and BN, the first arg is `num_channels`. Since GN hasn't been a release yet, I propose to make `num_channels` the first arg.

The functional form doesn't have such issue.",pytorch
9216,vishwakftw,pr,2018-07-06T19:47:41Z,Add reversed(torch.Tensor),Closes https://github.com/pytorch/pytorch/issues/3376,pytorch
9225,vishwakftw,pr,2018-07-06T23:50:44Z,NumPy Scalar to PyTorch Scalar,Fixes https://github.com/pytorch/pytorch/issues/4985 .,pytorch
9238,vishwakftw,pr,2018-07-07T15:03:04Z,Fix standard deviation gradient,"Defined the subgradient of `std` when `result = 0` to be `inf`.

1. Added tests in test_autograd
2. New method std_backward() for computing backward in Functions.cpp

Fixes #4320 .

",pytorch
9239,ssnl,pr,2018-07-07T20:20:10Z,"[docs] Update extension docs, fix Fold/Unfold docs","Commits: 
1. In extension doc, get rid of all references of `Variable` s (Closes #6947 )
    + also add minor improvements 
    + also added a section with links to cpp extension :) @goldsborough 
    + removed mentions of `autograd.Function.requires_grad` as it's not used anywhere and hardcoded to `return_Py_True`.
2. Fix several sphinx warnings
3. Change `*` in equations in `module/conv.py` to `\times`
4. Fix docs for `Fold` and `Unfold`. 
    + Added better shape check for `Fold` (it previously may give bogus result when there are not enough blocks). Added test for the checks.
5. Fix doc saying `trtrs` not available for CUDA (#9247 )
",pytorch
9246,vishwakftw,pr,2018-07-08T17:21:23Z,Added hash for device,"If this is good, I could write some tests to ensure collision doesn't occur within a given range.

Closes #7228 ",pytorch
9254,peterjc123,pr,2018-07-09T06:59:57Z,Fix docs for Windows CUDA 8 builds,Fixes #9200.,pytorch
9259,JerryShih,pr,2018-07-09T08:48:46Z,Fix the build break for python3.7 PyUnicode_AsUTF8AndSize() prototype changing,"https://docs.python.org/3.7/c-api/unicode.html#c.PyUnicode_AsUTF8AndSize
The return type changes from ""char*"" to ""const char*"".

",pytorch
9267,orionr,pr,2018-07-09T14:45:29Z,Make roi_align_rotated_op_test not rely on 1.12.0 numpy.rot90,"Breaking this out of https://github.com/pytorch/pytorch/pull/8338

Use a local version of `np.rot90` with an `axes` argument, since we don't have NumPy 1.12.0 in all of the test environments. Caffe2 conda2-ubuntu16.04, for example, fails. Generally, it seems better to not require a NumPy bump just for this test.

cc @mingzhe09088",pytorch
9268,orionr,pr,2018-07-09T15:13:55Z,Some more changes to support USE_CUDNN=OFF,"Breaking this out of #8338 

More changes required to support USE_CUDNN=OFF. We should be able to land some of our fixes before the big BUILD_CAFFE2 and BUILD_ATEN removal lands.

cc @mingzhe09088 @Yangqing ",pytorch
9269,orionr,pr,2018-07-09T15:20:05Z,Fix Mac CUDA issues,"Breaking this out of #8338

This takes care of failures we saw on Mac CUDA builds when BUILD_CAFFE2 and BUILD_ATEN were removed. Specifically, @smessmer fixed `std::hash` being handled in a weird way by nvcc and I fixed an nvcc template issue by moving `SparseNormalizeOp::RunOnDevice` implementation into the cc file.

cc @mingzhe09088 @smessmer",pytorch
9270,orionr,pr,2018-07-09T15:28:02Z,Fix Eigen issue on OS X with CUDA and nvcc compile,"Breaking this out of #8338

This takes care of the Eigen failure we saw on Mac CUDA builds when BUILD_CAFFE2 and BUILD_ATEN were removed. Fix is to isolate Eigen from headers included by cu files and processed by nvcc. This was worked on with @smessmer.

cc @mingzhe09088 @smessmer @BIT-silence @Yangqing ",pytorch
9274,ssnl,pr,2018-07-09T17:38:05Z,3d conv should use int64_t,"Fixes #9264 .

There can be so many elements in the output of `vol2col` so it overflows `int` range! This PR changes 3d conv to use `int64_t` mostly.

Also fixes some unused var warning (cc @goldsborough )",pytorch
9281,vishwakftw,pr,2018-07-09T19:48:17Z,Add eye_like,"Closes #9244 

Added doc string and tests.",pytorch
9292,vishwakftw,pr,2018-07-10T00:48:34Z,Fix segmentation fault in grad_fn,"Fixes #8774 .

cc: @soumith @SsnL ",pytorch
9300,peterjc123,pr,2018-07-10T08:46:14Z,Fix CUDA 8 build for Windows,Replacement of #9023.,pytorch
9304,vishwakftw,pr,2018-07-10T13:58:55Z,Fix nn.Parameter.to(),Fixes #9303 .,pytorch
9308,ssnl,pr,2018-07-10T16:49:10Z,change stft to have consistent signature with librosa,"Fixes #7883 by using `rfft`.

It's worth noting that this is BC breaking. And it's impossible to detect the change because the two signatures before and after this change supports a common subset of calling patterns, e.g., `stft(Tensor, int, int)`. (some other calling patterns will raise error). 

@soumith and I plan to change the current `stft` interface because it is a bit messy and non-standard. @rafaelvalle suggested us that `librosa` is a good reference API to align with. After discussing with @soumith and @ezyang , and given that `stft` is only out for 1 release, I decide to go with directly changing the signature. Also, my understanding is that most researchers in this field will welcome this change as `librosa` seems to be the golden-standard here. (it doesn't yet support all `pad_mode` but those will become available if added to `F.pad`.)",pytorch
9323,apaszke,pr,2018-07-10T22:12:04Z,Remove legacy code from the JIT,"In particular, get rid of backward tracing and CppOp.",pytorch
9345,kashif,pr,2018-07-11T13:16:35Z,NegativeBinomial distribution,"- [x] implement distribution
- [x] add tests
- [x] docs

cc @ingmarschuster",pytorch
9350,orionr,pr,2018-07-11T17:40:09Z,Fix Eigen issue on OS X with CUDA and nvcc compile,"Summary:
Re-apply #9270

Breaking this out of #8338

This takes care of the Eigen failure we saw on Mac CUDA builds when BUILD_CAFFE2 and BUILD_ATEN were removed. Fix is to isolate Eigen from headers included by cu files and processed by nvcc. This was worked on with smessmer.

Differential Revision: D8794431
",pytorch
9363,apaszke,pr,2018-07-11T22:33:41Z,Implement tensor weak references,Add `WeakTensor` - a `Tensor` counterpart which doesn't keep the data (or any other expensive resources) alive. They can be `.lock()`ed and return `at::optional<Tensor>` if they're still alive.,pytorch
9371,apaszke,pr,2018-07-12T00:40:43Z,Guard gloo algorithm creation with DeviceGuard,Let us avoid creating a context on GPU0 unnecessarily.,pytorch
9374,ssnl,pr,2018-07-12T03:48:48Z,fix unsqueeze doc,"fixes #9348

",pytorch
9377,syed-ahmed,pr,2018-07-12T05:07:49Z,Enable standalone build of ATen,"This PR changes the ATen `CMakeLists.txt` slightly, to enable standalone build of ATen inside PyTorch. Currently, the tests in ATen gets linked to `libcaffe.so libcaffe2.so`. As a result, ATen can't be built standalone without building from the root pytorch directory. I know that there is a big merge happening between caffe2 and pytorch and hence, the purpose of this PR is to really start a conversation on what would be the proper way of migrating the CMakeLists to enable clean builds. We should also follow up on this PR: https://github.com/pytorch/pytorch/pull/7275. For your reference, that PR has the explanation for why `-Wl --no-as-need` is needed. Moreover, without `set(ATen_CUDA_SRCS ${all_cuda_cpp})`, the standalone build will throw unresolved references.

",pytorch
9414,apaszke,pr,2018-07-13T02:37:47Z, Make JIT tracing a thread-local property,"As in the title. Lets us simplify a lot of code.

Depends on #9363, so please review only the last commit.

@zdevito ",pytorch
9451,vishwakftw,pr,2018-07-14T21:01:13Z,[ready] Add multivariate log-gamma (mvlgamma),"1. Add tests in test_cuda, test_torch
2. Add doc strings

Closes https://github.com/pytorch/pytorch/issues/9378 .

cc: @alicanb 

",pytorch
9452,vishwakftw,pr,2018-07-15T00:22:02Z,Implement reshape_as,"1. Added tests
2. Added doc string
3. Remove view_as redundant definition from tensor.py

Closes #9416 
",pytorch
9457,vishwakftw,pr,2018-07-15T14:58:50Z,Add test case for segmentation fault fix in grad_fn,cc: @SsnL ,pytorch
9478,ngimel,pr,2018-07-16T23:34:22Z,fix small literals being flushed to 0 by std::to_string,,pytorch
9491,peterjc123,pr,2018-07-17T09:12:43Z,Remove CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS and fix CUDA 8 build on Windows,Fixes #9092.,pytorch
9492,JerryShih,pr,2018-07-17T09:36:50Z,"Handle the ""spatial"" attribute in onnx BatchNormalization op","If we have this ""spatial"" attribute and its value equals to 1, we could just remove this attribute and convert this op to caffe2 SpatialBN.

",pytorch
9497,ssnl,pr,2018-07-17T15:31:20Z,change stft to have consistent signature with librosa (#9308),"Summary:
Fixes #7883 by using `rfft`.

It's worth noting that this is BC breaking. And it's impossible to detect the change because the two signatures before and after this change supports a common subset of calling patterns, e.g., `stft(Tensor, int, int)`. (some other calling patterns will raise error).

soumith and I plan to change the current `stft` interface because it is a bit messy and non-standard. rafaelvalle suggested us that `librosa` is a good reference API to align with. After discussing with soumith and ezyang , and given that `stft` is only out for 1 release, I decide to go with directly changing the signature. Also, my understanding is that most researchers in this field will welcome this change as `librosa` seems to be the golden-standard here. (it doesn't yet support all `pad_mode` but those will become available if added to `F.pad`.)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9308

Differential Revision: D8806148
",pytorch
9500,ssnl,pr,2018-07-17T16:22:33Z,Use _six for inf and nan,"Things like `float('inf')` are actually quite expensive.
```py
In [1]: import math

In [2]: %timeit -n 200 math.inf
49.3 ns Â± 1.42 ns per loop (mean Â± std. dev. of 7 runs, 200 loops each)

In [3]: %timeit -n 200 float('inf')
194 ns Â± 39.1 ns per loop (mean Â± std. dev. of 7 runs, 200 loops each)
```",pytorch
9505,apaszke,pr,2018-07-17T18:33:50Z,Prepare to stop using attributes in the JIT,"This PR adds machinery to cache the schema in an IR node, and allows lookups of (possibly) constant inputs by their names (instead of position). The new methods are:

- `at::optional<T> get<T>(Symbol name)` - if the argument called name is a constant, then casts it to type `T` and returns it. If it's not constant returns `nullopt`. Raises an error if there's no argument with that name.
- `at::optional<IValue> get<T>(Symbol name)` - like above, but packs the result in an IValue
- `Value* getValue(Symbol name)` - retrieves a `Value*` for an argument (no need to know its position).

All above functions currently inspect the attributes as well, but that's only so that I could start using them in other places in the JIT without disrupting our current functionality. I wanted this diff to be a preparation that doesn't change the semantics too much, and so both the tracer and script create nodes with attributes. The next PR will put that to a stop, and hopefully the changes we need to make to other components will be simpler thanks to what I did here.

One more thing I'd like to do before actually stopping creating the non-attributed nodes is to have a convenient way of creating a schema programmatically, matching nodes against it, and creating them without having to pack inputs into flat argument lists (which is quite error prone).

@zdevito ",pytorch
9508,vishwakftw,pr,2018-07-17T19:55:08Z,Fix byte ordering issue in from_numpy,Fixes https://github.com/pytorch/pytorch/issues/3671 .,pytorch
9538,ssnl,pr,2018-07-18T15:49:02Z,Fix TestAutograd.test_as_strided,"0. Fixes #9479 
1. rewrites `as_strided` as a native function. This is fine because `set_` does the scalar check.
2. allow using `self` in `python_default_init`. Previously `python_variable_methods.cpp` has `self` as an input `PyObject *`, and use `self_` as the unpacked tensor. But `python_torch_functions.cpp` just use `self` as the unpacked tensor, making it impossible to use `self` in `python_default_init`.",pytorch
9562,ssnl,pr,2018-07-18T20:51:19Z,[don't merge] Use int64_t in THNN and THCUNN,"Fixes https://github.com/pytorch/pytorch/issues/9404 

I did a codemod. Likely there are something that should stay int.",pytorch
9566,apaszke,pr,2018-07-18T21:45:03Z,[EASY] Generate uninplaced operators that don't have out-of-place counterparts,"This is needed to unblock the thread-local tracing PR. We trace some in-place ops as out-of-place ops, but not all of them actually have ATen implementations. This is a simple change that adjusts our codemod to pretend that they exist by cloning the modified input and only then applying the inplace op.

For example we trace `x.zero_()` as `y = zero(x)`, which is pretty much `zeros_like`. At some point we should add a pass that translates it, because `zeros_like` would be more efficient than `x.clone().zero_()`, but I'm leaving that for later. Here's how the generated operator looks like:
```cpp
Operator(                                                 
    ""aten::zero(Tensor self) -> Tensor"",                  
    [](Node *node) {                                      
                                                          
      return Operation([=](Stack & stack) {               
        autograd::profiler::RecordFunction record(""zero"");
                                                          
        auto self = peek(stack, 0, 1).toTensor().clone(); 
        auto result = at::zero_(self);                    
        drop(stack, 1);                                   
        pack(stack, std::move(result));                   
        return 0;                                         
      });                                                 
    }                                                     
),                                                        
```

@zdevito ",pytorch
9567,apaszke,pr,2018-07-18T21:52:58Z,Add new ways of matching nodes with schemas in the JIT,"**REVIEW LAST COMMIT ONLY**

As discussed in our yesterday's meeting. Nodes can be now matched to particular overloads using the `matches(...)` function:
```cpp
n->matches(""aten::type_as(Tensor self, Tensor other) -> Tensor"")
```

This also changes the shape prop and peephole passes to use those functions for matching. This fixes a few bugs, makes them much more robust, and prepares us for removal of attributes.

@zdevito ",pytorch
9590,ssnl,pr,2018-07-19T15:33:22Z,Use int64_t for im2col and col2im,Fixes #9404 ,pytorch
9599,ssnl,pr,2018-07-19T19:42:36Z,cherry pick #9500 and #9590 into 0.4.1,cherry pick #9500 and #9590 into 0.4.1,pytorch
9607,ssnl,pr,2018-07-19T21:40:56Z,docs fixes,fixes #9589 #9507 #9502 #9390 #4176 ,pytorch
9618,peterjc123,pr,2018-07-20T05:04:32Z,Fix CUDA 8 build on Windows,#9491 for 0.4.1.,pytorch
9625,vishwakftw,pr,2018-07-20T13:46:08Z,Fix integral type dispatch error message,"This fix will prevent errors like (found in `bincount`)
```
RuntimeError: %s not implemented for '%s'bincounttorch.FloatTensor
```",pytorch
9630,ssnl,pr,2018-07-20T15:05:16Z,Add scatter_add_ doc,"fixes #4176 cc @vishwakftw 

I didn't do `:math:` and `\neg` because I am using double ticks so they render more similarly with `:attr:`.",pytorch
9655,ssnl,pr,2018-07-20T20:19:25Z,Fix dataloader hang when it is not completely iterated,"second trial of https://github.com/pytorch/pytorch/pull/7140

cc @csarofeen Let's see if this works. It passes everything locally.",pytorch
9666,ngimel,pr,2018-07-20T23:42:23Z,add fused dropout kernels,"While waiting for dropout to be fully ported to ATen, here's performance fix for the most common dropout case. Dropout is still in python function, I just added efficient path to it. I could not make inplace work, because generator always generates `return self` for inplace function, and I need to return both original tensor and mask, so inplace goes on the existing pass. Even with non-inplace version, since mask is now a ByteTensor, memory used is just a little larger than for inplace dropout, due to savings on mask. 
Once dropout is moved to aten, these kernels still can be used for efficient implementation.  ",pytorch
9691,ssnl,pr,2018-07-22T15:58:23Z,Cherry pick #9607 #9630 #9635 #9629 into 0.4.1,"Cherry pick #9607 #9630 #9635 #9629 
Fix cherry pick history for #9590 and #9500",pytorch
9700,ssnl,pr,2018-07-23T04:02:16Z,[test ci] Re-enable test_segfault after recent dataloder changes,,pytorch
9721,ssnl,pr,2018-07-23T18:50:48Z,Move Pixel Shuffle to ATen,<del>#9692 </del>,pytorch
9757,vishwakftw,pr,2018-07-24T13:18:42Z,Fix bincount for empty input,Added tests too. Fixes #9756 .,pytorch
9763,apaszke,pr,2018-07-24T14:53:49Z,Make GraphExecutors work on Stacks instead of variable_tensor_lists,"This is blocking the IR operator unification, because I need to be able to pass scalars to backward functions.

@zdevito ",pytorch
9769,ssnl,pr,2018-07-24T16:03:47Z,Use torch::empty before random_ on seed gen,,pytorch
9777,ssnl,pr,2018-07-24T18:49:07Z,Disable test_segfault for NO_MULTIPROCESSING_SPAWN,,pytorch
9804,ssnl,pr,2018-07-25T02:51:43Z,"Revert ""Fix dataloader hang when it is not completely iterated (#9655)""","This reverts commit 9ee513365121cd387e11987c66db6599ac53ded7.

",pytorch
9807,apaszke,pr,2018-07-25T04:43:22Z,Unify IR operator representation (stop using attributes in the JIT),"Based on top of #9763 (first 3 commits belong to that PR). The first commits from this PR are ""Stop using attributes ...""

I tried to separate the changes into fairly meaningful commits. I can't split them up into smaller PRs, because everything starts working and all tests pass only after the whole sequence, but hopefully this will make reviewing somewhat easier.

Known issues/regressions/future tasks:
- `aten::lerp` and `aten::clamp` are no longer fusable
- `CreateAutodiffSubgraphs` needs a rewrite
  - It is much more strict now, and will miss a lot of opportunities, especially when viewing ops are involved. Our previous approach was ""ignore the assumption on shape availability in gradient formulas to determine differentiability, and hope that shape prop will be robust enough to actually deliver them before we differentiate"", which obviously doesn't scale well to more complex cases. We should either work on reducing the size dependency of grad formulas (feasible e.g. for `view`/`reshape`, unfeasible for `squeeze`/`unsqueeze`), or make `CreateAutodiffSubgraphs` integrate some kind of ""I could integrate this node into an AD subgraph, but will I be able to infer the shape of its input"" reasoning (kind of like a limited shape prop, that doesn't infer anything, and only tells if it *could* infer something).
  - It sometimes creates constant-only (or constants + one node) graphs, which is useless
- Broken `aten::add` in auto-batching, because it gained a non-tensor input. I changed the test for pointwise operations to use `aten::mul` instead, but I needed to disable the LSTM cell test. I'm not sure how scalar constants should be implemented in this case, because I don't fully understand our format. cc: @ChunliF 
- Graph import does some hacks to recover type of constants. This code should be removed once we'll gain the ability to export the IR along with value types.
- There's still a fair amount of dead code that can be removed. I didn't want to make this diff any bigger, and removing it is an easy task.
- Graph fuser could be improved to use signature matching (possibly using `OperatorSet`) instead of basing on node kinds.
- Manual constant propagation for the `ListConstruct` node in `torch/onnx/utils.py` should be replaced with a proper constant propagation pass (or we should ensure that the one we have handles at least this case before we remove this code).

@zdevito",pytorch
9819,vishwakftw,pr,2018-07-25T14:16:16Z,Remove deprecated masked_copy,"No tests are affected by this removal.

Closes https://github.com/pytorch/pytorch/issues/1885 and closes #9817 

While I was at it, I also fixed #9876 .

",pytorch
9864,ssnl,pr,2018-07-26T03:26:43Z,Increase BC for PackedSequence ctor,"PackedSequence is never supposed to be created by user, but unfortunately some community repo is already doing this (e.g., [here](https://github.com/huggingface/torchMoji/blob/7c191048ce906fc0404fe156827d97cb990ebecb/torchmoji/model_def.py#L218-L229)). Some change we made break the calling pattern `PackedSequence(data=x, batch_sizes=y)`. This patch adds back support for that.",pytorch
9920,peterjc123,pr,2018-07-27T05:07:16Z,Add essential PATH for the Windows PyTorch loading process,"Fixes #9818. 
It seems original Python doesn't add `[PYTHONPATH]\Library\bin` into `PATH`. We try to add it before dll loading process.",pytorch
9937,apaszke,pr,2018-07-27T15:52:58Z,[TESTING ONLY] Fix ONNX issues that surfaced after unification PR,cc @anderspapitto @bddppq ,pytorch
9948,apaszke,pr,2018-07-27T19:30:09Z,"Add (constant size) TensorLists to JIT, use them in cat and stack nodes",@zdevito ,pytorch
9949,vishwakftw,pr,2018-07-27T19:36:27Z,"[RFC, ready] Batched Inverse","Complete billing of changes:

Related to Batch Inverse:
- [x] Add batched inverse (CPU)
- [x] Add batched inverse (CUDA)
- [x] Modify autograd entry
- [x] Add tests
  - [x] test_autograd
  - [x] test_cuda
  - [x] test_torch
- [x] Modify docs
- [x] Remove `_batch_inverse` in `MultivariateNormal`.
- [x] Allow batch matrices as inputs for negative powers in `matrix_power`

Miscellaneous modifications:
- [x] Move all batch operations to BatchLinearAlgebra.cpp/.cu and provide general framework for adding more batch ops.
- [x] Add a RAII structure for MAGMA queue management.",pytorch
9953,cclauss,pr,2018-07-27T20:53:13Z,Travis CI: Run flake on Python 2.7 and 3.7,"Flake8 will produce different results on Python 2 and 3.  Python 3.7 has __async__ as a reserved word https://github.com/pytorch/pytorch/pull/4999.

",pytorch
9961,ssnl,pr,2018-07-27T22:28:26Z,Move grid sampler to ATen,"Spatial version benchmark

|                           | CPUFloat THNN | CPUFloat ATen | CPUDouble THNN | CPUDouble ATen | CUDAHalf THNN | CUDAHalf ATen | CUDAFloat THNN | CUDAFloat ATen | CUDADouble THNN | CUDADouble ATen |
|---------------------------|---------------|---------------|----------------|----------------|---------------|---------------|----------------|----------------|-----------------|-----------------|
| [1024x1x28x28] zero pad   | 2.19281888s   | 0.21280479s   | 2.52922535s    | 0.23944831s    | 0.17494774s   | 0.06242800s   | 0.31270599s    | 0.03706479s    | 0.40542483s     | 0.07391024s     |
| [1024x1x28x28] border pad | 3.04329610s   | 0.24705672s   | 2.29205394s    | 0.22336411s    | 0.17980361s   | 0.06212497s   | 0.31415701s    | 0.03847790s    | 0.43020391s     | 0.07540464s     |
| [32x3x244x244] zero pad   | 18.29301333s  | 2.18566656s   | 19.01662397s   | 3.51552224s    | 1.72487235s   | 0.28933954s   | 2.02466702s    | 0.18178749s    | 2.63671613s     | 0.41391206s     |
| [32x3x244x244] border pad | 18.72205329s  | 2.02600884s   | 20.13017297s   | 3.25979590s    | 1.96455693s   | 0.33070564s   | 2.18666625s    | 0.19546938s    | 2.91268897s     | 0.38465047s     |


For #9702 

basics: 
+ grid tensors have dimensions `[N, H, W, 2]` (or `[N, D, H, W, 3]` for 3d).
+ input/output tensors have dimensions `[N, C, H, W]` (or `[N, C, D, H ,W]` for 3d)
+ grid sampler maps `input([N, C, inp_H, inp_W]), grid([N, H, W, 2])` to `output([N, C, H, W])` (3d case is similar).

variable naming:
+ `tensor_sH` means the stride of `tensor` at the dimension of `H`.
+ `tensor_ptr_NCH` is a data pointer that always points to the beginning of the `tensor[n][c][h]` slice in the loop.",pytorch
9969,apaszke,pr,2018-07-28T03:28:28Z,Slightly relax the constraints on argument and return types to script functions,"This lays out initial support for taking and returning a richer set
of types than only tensors. Floats and ints are already valid, lists are
straightforward to add, tuples need some discussion.

Based on top of #9948. Review only the last commit.

@zdevito ",pytorch
9971,rasbt,pr,2018-07-28T04:47:41Z,Adds the default value for the amsgrad arg to the Adam docstring,"Minor addition to the docstring of `torch.nn.optim.Adam`, adding the default argument description for the `amsgrad` argument to the docstring for concistency.",pytorch
10020,orionr,pr,2018-07-30T18:43:33Z,Fix Caffe2 with ATen conda build failure,"Extracted from https://github.com/pytorch/pytorch/pull/8338/commits/627624627ef9bfd4ff3594c65f677a2b47bdd569 and in support of https://github.com/pytorch/pytorch/pull/10019

cc @pjh5 @mingzhe09088 @ezyang @smessmer ",pytorch
10045,ssnl,pr,2018-07-31T01:55:46Z,[pyHIPIFY] Substitute std::(max|min|ceil|floor) with ::\1,Encountered some of these problems when doing https://github.com/pytorch/pytorch/pull/9961. I was suggested to make changes here. :) ,pytorch
10051,ssnl,pr,2018-07-31T03:31:28Z,Grid sampler: nearest interpolation & reflection padding,"closes #9702 .

cc @jph00 

Commit structure:

1. Change the index calculation logic. I will explain using 1-D for simplicity.

	Previously we have (in pseudo code):

	```
	// 1. get the float locations from grid
	scalar_t x = from_grid()

	// 2. find the integral surrounding indices
	int x_left = floor(x)
	int x_right = x_left + 1

	// 3. calculate the linear interpolate weights
	scalar_t w_left = x_right - x
	scalar_t w_right = x - x_left

	// 4. manipulate the integral surrounding indices if needed
	// (e.g., clip for border padding_mode)
	x_left = manipulate(x_left, padding_mode)
	x_right = manipulate(x_right, padding_mode)

	// 5. interpolate
	output_val = interpolate(w_left, w_right, x_left, x_right)
	```

	This is actually incorrect (and also unintuitive) because it calculates the 
	weights before manipulate out-of-boundary indices. Fortunately, this 
	isn't manifested in both of the current supported modes, `'zeros'` and 
	`'border'` padding:

	+ `'zeros'`: doesn't clip
	+ `'border'`: clips, but for out-of-bound `x` both `x_left` and `x_right` are 
	  clipped to the same value, so weights don't matter

	But this is a problem with reflection padding, since after each time we reflect,
	the values of `w_left` and `w_right` should be swapped.

	So in this commit I change the algorithm to (numbers corresponding to the 
        ordering in the above pseudo-code)

	```
	1. get float location
	4. clip the float location 
	2. find the integral surrounding indices
	3. calculate the linear interpolate weights
	```

	In the backward, because of this change, I need to add new variables to track
	`d manipulate_output / d manipulate_input`, which is basically a multiplier
	on the gradient calculated for `grid`. From benchmarking this addition doesn't
	cause obvious slow downs.

2. Implement reflection padding. The indices will keep being reflected until 
	they become within boundary.

	Added variant of `clip_coordinates` and `reflect_coordinates` to be used in 
	backward. E.g.,
	```cpp
	// clip_coordinates_set_grad works similarly to clip_coordinates except that
	// it also returns the `d output / d input` via pointer argument `grad_in`.
	// This is useful in the backward pass of grid_sampler.
	scalar_t clip_coordinates_set_grad(scalar_t in, int64_t clip_limit, scalar_t *grad_in)
	```
	For example, if `in` is clipped in `'border'` mode, `grad_in` is set to `0`.
	If `in` is reflected **odd** times in `'reflection'` mode, `grad_in` 
	is set to `-1`.

3. Implement nearest interpolation.

4. Add test cases

5. Add better input checking
  Discussed with @goldsborough for moving `operator<<` of `at::Device`, 
  `at::DeviceType` and `at::Layout` into `at` namespace. (Otherwise 
  `AT_CHECK` can't find them.)

6. Support empty tensors. cc @gchanan

    + Make empty tensors not acceptable by cudnn. 
    + Add `AT_ASSERT(kernel block size  > 0)` if using `GET_BLOCKS`
   + Cache `numel` in `TensorGeometry`
      I was going to use `numel` to test if cudnn descriptor should accept a
      tensor, but it isn't used eventually. I can revert this if needed.

7. Add more test cases, including on input checking and empty tensors

8. Remove an obsolete comment

9. Update docs. Manually tested by generating docs.

",pytorch
10068,vishwakftw,pr,2018-07-31T11:52:04Z,[ready] Add matrix_power,"1. Tests added
2. Doc string added
",pytorch
10095,orionr,pr,2018-07-31T21:26:00Z,Update eigen submodule to fix BUILD_ATEN issue,"Extracted from https://github.com/pytorch/pytorch/pull/8338

Updating Eigen submodule to fix an issue we saw with BUILD_ATEN and BUILD_CAFFE2 removal.

cc @mingzhe09088 @ezyang @smessmer ",pytorch
10108,JerryShih,pr,2018-08-01T03:58:51Z,Update the onnx Gemm op to FC/FCTransposed logic in caffe2 onnx backend,"The broadcast is used by default when the opset version is greater then 6.
#9874
",pytorch
10147,syed-ahmed,pr,2018-08-01T23:17:25Z,Cuda half macros cleanup,"Summary: This PR removes couple of macros throughout TH* as part of the re-factoring effort for ATen. Removing these macros should avoid confusion among developers who are trying to move things from TH* to ATen. This PR is part of the THCNumerics deprecation that I have been working on following up on @mruberry's https://github.com/pytorch/pytorch/pull/9318. I am separating these two commits to see if removal of these macros doesn't upset the pytorch public CI, as well as internal builds.

- Commit https://github.com/pytorch/pytorch/commit/1248de7baf4e37f69aa871520135bc9779dccfda removes the code paths guarded by `CUDA_HALF_INSTRUCTIONS` macro. Since the macro was removed in commit https://github.com/pytorch/pytorch/commit/2f186df52d029073c1745274e74ce8700aa3f0f0, `ifdef CUDA_HALF_INSTRUCTIONS` would return false and hence the code path that is kept after this change is for the false case of `ifdef CUDA_HALF_INSTRUCTIONS`

- Commit https://github.com/pytorch/pytorch/commit/520c99b057e741ccd4c96653650932902fb8b53e removes the code paths guarded by `CUDA_HALF_TENSOR` macro. Since Pytorch now provides support for only CUDA 8.0 and above, `CUDA_HALF_TENSOR` is always true since CUDA 8.0 satisfies `CUDA_HAS_FP16` and hence, the code path that is kept after this change is for the true case of `ifdef CUDA_HALF_TENSOR`.",pytorch
10154,peterjc123,pr,2018-08-02T02:50:06Z,Add import check utility on Windows,,pytorch
10171,ssnl,pr,2018-08-02T18:21:04Z,Use PYTORCH_PYTHON to call generate_code.py,"Probably fixes https://github.com/pytorch/pytorch/issues/8373#issuecomment-409994847

cc @Amir-Arsalan",pytorch
10181,ssnl,pr,2018-08-02T21:24:47Z,Sparse tensor printing; add NotImplemented autograd fn,"Commits:

1. Add autograd function `NotImplemented` (subclass of `Error`) so python `grad_fn` prints nicer. Since `Error` is used in `DelayedError` to implement `@oncedifferentiable`, I can't just change its name. cc @colesbury 

2. Add printing for sparse tensors. Fixes https://github.com/pytorch/pytorch/issues/9412 . cc @weiyangfb @li-roy . 

3. Add tests for sparse printing

Examples:
```diff
  In [2]: x = torch.sparse.FloatTensor(torch.arange(4).view(2,2), torch.randn(2, 2), [10, 10, 2])
  
  In [3]: x
  Out[3]:
- torch.sparse.FloatTensor of size (10,10,2) with indices:
- tensor([[0, 1],
-         [2, 3]])
- and values:
- tensor([[-1.1832, -0.5927],
-         [ 0.0831,  0.2511]])
+ tensor(indices=tensor([[0, 1],
+                        [2, 3]]),
+        values=tensor([[ 1.5081,  0.3451],
+                       [-0.0392,  0.4776]]),
+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo)
  
  In [4]: x.requires_grad_()
  Out[4]:
- torch.sparse.FloatTensor of size (10,10,2) with indices:
- tensor([[0, 1],
-         [2, 3]], grad_fn=<Error>)
- and values:
- tensor([[-1.1832, -0.5927],
-         [ 0.0831,  0.2511]], grad_fn=<Error>)
+ tensor(indices=tensor([[0, 1],
+                        [2, 3]]),
+        values=tensor([[ 1.5081,  0.3451],
+                       [-0.0392,  0.4776]]),
+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, requires_grad=True)
  
  In [5]: x + x
  Out[5]:
- torch.sparse.FloatTensor of size (10,10,2) with indices:
- tensor([[0, 1],
-         [2, 3]], grad_fn=<Error>)
- and values:
- tensor([[-2.3664, -1.1855],
-         [ 0.1662,  0.5021]], grad_fn=<Error>)
+ tensor(indices=tensor([[0, 1],
+                        [2, 3]]),
+        values=tensor([[ 3.0162,  0.6902],
+                       [-0.0785,  0.9553]]),
+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, grad_fn=<AddBackward0>)
  
  In [6]: x.double()
  Out[6]:
- torch.sparse.DoubleTensor of size (10,10,2) with indices:
- tensor([[0, 1],
-         [2, 3]], grad_fn=<Error>)
- and values:
- tensor([[-1.1832, -0.5927],
-         [ 0.0831,  0.2511]], dtype=torch.float64, grad_fn=<Error>)
+ tensor(indices=tensor([[0, 1],
+                        [2, 3]]),
+        values=tensor([[ 1.5081,  0.3451],
+                       [-0.0392,  0.4776]]),
+        size=(10, 10, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo,
+        grad_fn=<NotImplemented>)
  
  In [7]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0), [0])
  
  In [8]: x
  Out[8]:
- torch.sparse.FloatTensor of size (0,) with indices:
- tensor([], size=(0, 2), dtype=torch.int64)
- and values:
- tensor([], size=(2, 0))
+ tensor(indices=tensor([], size=(0, 2)),
+        values=tensor([], size=(2, 0)),
+        size=(0,), nnz=2, layout=torch.sparse_coo)
  
  In [9]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2), [])
  
  In [10]: x
  Out[10]:
- torch.sparse.FloatTensor of size () with indices:
- tensor([], size=(0, 2), dtype=torch.int64)
- and values:
- tensor([-0.0064,  0.8518])
+ tensor(indices=tensor([], size=(0, 2)),
+        values=tensor([ 0.9800, -0.5978]),
+        size=(), nnz=2, layout=torch.sparse_coo)
```",pytorch
10196,ssnl,pr,2018-08-03T04:28:13Z,fix padding doc not rendered correctly,"somehow sphinx doesn't like the previous wording

test plan: built doc",pytorch
10247,peterjc123,pr,2018-08-06T02:38:43Z,Add warning for building PyTorch using Python 2.7 on Windows,Fixes #9232.,pytorch
10251,vishwakftw,pr,2018-08-06T04:48:50Z,Fix min and max for CUDA tensors with inf/-inf,"Add tests as well.

Fixes #10237 ",pytorch
10273,ssnl,pr,2018-08-06T20:38:22Z,[ready] Move bernoulli into ATen,"## Fixes 
+ https://github.com/pytorch/pytorch/issues/10236 : torch.bernoulli's out kwarg is broken
  fixed in moving `bernoulli_out` to ATen
+ https://github.com/pytorch/pytorch/issues/9917 : BUG torch.bernoulli(p.expand(shape)) is broken
  fixed in moving all `bernoulli` ops in ATen to use the modern apply utils methods
+ https://github.com/pytorch/pytorch/issues/10357 : torch.bernoulli inconsistent gpu/cpu results
  fixed by adding CUDA asserts

## Notable changes:

In order to use `curand_uniform4`, I made some changes to `CUDAApplyUtils.cuh`. Specifically, I introduced an optional template parameter `int step` to the `CUDA_tensor_applyN` methods, representing that we want to process `step` values at each time for each of the `N` tensors. 

The calling convention for `step = 1` (default) isn't changed. But if `step > 1`, the given lambda `op` must take in `int n` as its first argument, representing the number of valid values, because there may not be full `step` values at the boundary. E.g., here is what the `bernoulli(self, p_tensor)` call look like:
```cpp

  // The template argument `4` below indicates that we want to operate on four
  // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.
  at::cuda::CUDA_tensor_apply2<scalar_t, prob_t, 4>(
      ret, p,
      [seeds] __device__(
          int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4,
          const prob_t& p1, const prob_t& p2, const prob_t& p3, const prob_t& p4) {
        curandStatePhilox4_32_10_t state;
        curand_init(
            seeds.first,
            blockIdx.x * blockDim.x + threadIdx.x,
            seeds.second,
            &state);
        float4 rand = curand_uniform4(&state);
        switch (n) {
          case 4: {
            assert(0 <= p4 && p4 <= 1);
            v4 = static_cast<scalar_t>(rand.w <= p4);
          }
          case 3: {
            assert(0 <= p3 && p3 <= 1);
            v3 = static_cast<scalar_t>(rand.z <= p3);
          }
          case 2: {
            assert(0 <= p2 && p2 <= 1);
            v2 = static_cast<scalar_t>(rand.y <= p2);
          }
          case 1: {
            assert(0 <= p1 && p1 <= 1);
            v1 = static_cast<scalar_t>(rand.x <= p1);
          }
        }
      }
    );
``` 

## Benchmarking

Benchmarking on `torch.rand(200, 300, 400)` 20 times, each time with 20 loops:

post patch
```
âžœ  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py
torch.bernoulli(x)
6.841588497161865 +- 0.05413117632269859
torch.bernoulli(xc)
0.05963418632745743 +- 0.0008014909108169377
x.bernoulli_()
0.4024486541748047 +- 0.0021550932433456182
xc.bernoulli_()
0.02167394384741783 +- 2.3818030967959203e-05

```

pre-patch
```
âžœ  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py
torch.bernoulli(x)
12.394511222839355 +- 0.0966421514749527
torch.bernoulli(xc)
0.08970972150564194 +- 0.0038722590543329716
x.bernoulli_()
1.654480218887329 +- 0.02364428900182247
xc.bernoulli_()
0.058352887630462646 +- 0.003094920190051198

```",pytorch
10277,ssnl,pr,2018-08-06T20:58:11Z,Propagate NaN through threshold,Fixes https://github.com/pytorch/pytorch/issues/10238,pytorch
10297,orionr,pr,2018-08-07T12:19:58Z,Fix some warnings,"Fixing some compiler warnings while looking at symbol visibility.

cc @smessmer @ezyang ",pytorch
10301,syed-ahmed,pr,2018-08-07T14:43:21Z,Refactor THCNumerics and add common math functions for at::Half,"**Summary**: This PR is a followup of @mruberry's https://github.com/pytorch/pytorch/pull/9318/. It tries to achieve the following:
- Specializing std common math functions for `at::Half` type.
- Create `CUDANumerics.cuh` to contain necessary parts from `THCNumerics.cuh`.
- Update `THCNumerics.cuh` with new usage and comments to  demonstrate the best practice for developers and hence, making way for its deprecation.
- Remove legacy/redundant code path.
- Remove unused CUDA HALF macros (see separate PR https://github.com/pytorch/pytorch/pull/10147)

**Comments**: `CUDANumerics.cuh` contains mathematical functions that are either not in the std namespace or are specialized for compilation with CUDA NVCC or CUDA NVRTC. This header is derived from the legacy `THCNumerics.cuh`. Following are some rationale behind why some functions were kept while others were removed:
- All arithmetic can now be done in ATen using binary cuda kernel  or CUDA tensor pointwise apply (check https://github.com/pytorch/pytorch/pull/8919 and `CUDAApplyUtils`). `at::Half` comparisons rely on implicit conversion to float.
- Functions that are c/c++ standard compliant, have been specialized for user defined types, for instance, the std namespace has been opened up for `at::Half`, that defines math function definitions for `at::Half`. Check `Half-inl.h`
- Some standard compliant functions are specialized here for performance reasons. For instance, `powi` is used for `pow` calculation on integral types. Moreover, `abs`, `isinf`, `isnan` are specialized to save one API call vs when used with std. Although this is subject to change, depending on if we really care about saving one API call.
- Numeric limits such as `max/min` is removed since they call standard defines. Moreover, numeric limits for
`at::Half` is present in `Half-inl.h`. I understood that HIP has some issue with `std::numeric_limits` and this the related github issue I found: https://github.com/ROCm-Developer-Tools/HIP/issues/374. @AlexVlx mentions that the issue can be avoided by launching `std::numeric_limits` in `__device__`. Since, we are launching lambdas with device contexts, I don't see an issue why `std::numeric_limits` won't compile on HIP if launched with device context within a kernel, unless I am not aware of the real reason why max/min was there in THCNumerics in the first place. (Haven't ever tried a build with HIP).

Here are some reference PRs that was handy in refactoring TH into ATen:
- https://github.com/pytorch/pytorch/pull/6786
- https://github.com/pytorch/pytorch/pull/5475
- https://github.com/pytorch/pytorch/pull/9401
- https://github.com/pytorch/pytorch/pull/8689
- https://github.com/pytorch/pytorch/pull/8919",pytorch
10305,apaszke,pr,2018-08-07T15:08:24Z,Move fused RNN kernels into ATen,As in the title. I also did a small refactor that let us loose almost 400 loc. This is a first step in moving the RNN code to C++.,pytorch
10311,ssnl,pr,2018-08-07T16:30:50Z,Add tests for Tensor.* nn.* F.* docs,"Test only for existence for now. I had to skip a lot of them so there a FIXME in the test.

Also I'm not testing torch.* because of namespace issue.",pytorch
10321,fritzo,pr,2018-08-07T18:44:40Z,[distributions] Make more distributions jittable,"This uses @zou3519's new `torch.broadcast_tensors()` #10075 to make `Categorical.log_prob()` and the `*Normal.__init__()` methods jittable. Previously `.log_prob()` was failing due to calls to `torch._C.infer_size()` with errors like
```
    def log_prob(self, value):
        if self._validate_args:
            self._validate_sample(value)
>       value_shape = torch._C._infer_size(value.size(), self.batch_shape) if self.batch_shape else value.size()
E       RuntimeError: expected int at position 0, but got: Tensor
```
After this change I'm able to jit many more of Pyro's tests.

## Questions for reviewers

This assumes that `broadcast_tensors` will create a `value` with stride 0 in the rightmost dimension; that way we never create a huge tensor. Is this assumption valid?

> @zou3519 Yes, broadcast_tensors will create a value with stride 0 in the rightmost dimension because broadcasting expands tensors (as opposed to using repeat, which creates new tensors).",pytorch
10338,vishwakftw,pr,2018-08-08T01:39:20Z,Add matrix_rank,"- Similar functionality as NumPy
- Added doc string
- Added tests

cc: @SsnL 

Closes #10292 ",pytorch
10361,alsrgv,pr,2018-08-08T23:44:49Z,Fix performance of DistributedSampler per #8958,,pytorch
10365,ssnl,pr,2018-08-09T04:45:02Z,Order Loss functions alphabetically in nn.rst,,pytorch
10366,ssnl,pr,2018-08-09T05:04:09Z,Resubmit: Fix dataloader hang when it is not completely iterated,https://github.com/pytorch/pytorch/pull/9655,pytorch
10368,peterjc123,pr,2018-08-09T07:58:12Z,Fix compile flags for MSVC,#7703 after the merge of Caffe2 and PyTorch.,pytorch
10369,peterjc123,pr,2018-08-09T08:04:14Z,Some symbol annotation fixes for Windows,,pytorch
10372,peterjc123,pr,2018-08-09T10:57:58Z,Fix the logic for PATH guess on Windows,,pytorch
10380,Yangqing,pr,2018-08-09T16:08:41Z,Remove caffe1 specific proto,"This was used as a convenient way for us to convert c1 models. Now that conversion is more or less done, we should probably require any users who need to convert c1 models to explicitly install c1. This PR removes the explicit c1 proto (which was copied from c1) in favor of explicit installation.

Note that caffe_translator would still work properly, only difference is that now users need to install c1 separately.",pytorch
10381,ssnl,pr,2018-08-09T16:28:44Z,"Avoid std::thread ctor ""cannot resolve"" error","If an `at::test` function is added, gcc can't figure out the `std::thread(test, -1)` resolution. 

It is not a problem for current code. I bumped into this when playing with native functions. But I think it is good to just prevent it from happening in future by removing `using namespace at;`, especially considering that it is a simple change.",pytorch
10384,apaszke,pr,2018-08-09T18:47:48Z,Move dropout and alpha dropout to ATen,@zdevito @ezyang ,pytorch
10438,Yangqing,pr,2018-08-11T16:29:49Z,Remove caffe namespace GetEmptyStringAlreadyInited,A followup cleanup of #10380 .,pytorch
10444,Yangqing,pr,2018-08-12T02:20:07Z,[unified build] gflags improvement to allow CAFFE2_EXPORTS,"Explanation copied from code:

// Motivation about the gflags wrapper:
// (1) We would need to make sure that the gflags version and the non-gflags
// version of Caffe2 are going to expose the same flags abstraction. One should
// explicitly use caffe2::FLAGS_flag_name to access the flags.
// (2) For flag names, it is recommended to start with caffe2_ to distinguish it
// from regular gflags flags. For example, do
//    CAFFE2_DEFINE_BOOL(caffe2_my_flag, true, ""An example"");
// to allow one to use caffe2::FLAGS_caffe2_my_flag.
// (3) Gflags has a design issue that does not properly expose the global flags,
// if one builds the library with -fvisibility=hidden. The current gflags (as of
// Aug 2018) only deals with the Windows case using dllexport, and not the Linux
// counterparts. As a result, we will explciitly use CAFFE2_EXPORT to export the
// flags defined in Caffe2. This is done via a global reference, so the flag
// itself is not duplicated - under the hood it is the same global gflags flag.",pytorch
10445,Yangqing,pr,2018-08-12T03:02:18Z,[lint] end macros with a semicolon guard,"For the type id related masters, -Wpedantic will give a warning that an unnecessary semicolon is being added to the macro invocation. This is a linting change to make sure that we are properly swallowing the macro, and also, explicitly require this macro to also end with a semicolon as other macros in PT1 do.",pytorch
10447,Yangqing,pr,2018-08-12T06:49:56Z,cuda compiler workaround,,pytorch
10453,peterjc123,pr,2018-08-13T07:18:41Z,Fix FindMKL.cmake for Windows,Targets the issue discussed at https://github.com/pytorch/pytorch/pull/7399#issuecomment-400788971.,pytorch
10481,apaszke,pr,2018-08-13T19:22:38Z,Move RNN implementations to C++,"This is the first of two changes that are supposed to improve how we handle RNNs in the JIT. They still get traced as `PythonOp`s, but now it will be much easier to actually expose them to the JIT as e.g. `aten::lstm`, and ignore the Python interpreter entirely. This needs some symbolic adjustments that will be part of a second PR.

Even when we fix symbolics, there will still be a bit of a problem with statefulness of the cuDNN API (we need a mutable cache for the dropout state, but our IR has no way of representing that).

@zdevito @ezyang ",pytorch
10504,Yangqing,pr,2018-08-14T05:17:33Z,[unified build] build changes to make cpu unified build working.,"Properly annotated all apis for cpu front. Checked with cmake using

cmake -DUSE_ATEN=ON -DUSE_CUDA=OFF -DBUILD_ATEN=ON

and resulting libcaffe2.so has about 11k symbols.",pytorch
10505,vishwakftw,pr,2018-08-14T14:32:23Z,[RFC] Introduce SobolEngine,"`SobolEngine` is a quasi-random sampler used to sample points evenly between [0,1]. Here we use direction numbers to generate these samples. The maximum supported dimension for the sampler is 1111.

Documentation has been added, tests have been added based on @Balandat 's references. The implementation is an optimized / tensor-ized implementation of @Balandat 's implementation in Cython as provided in #9332.

This closes #9332 .

cc: @soumith @Balandat",pytorch
10507,orionr,pr,2018-08-14T15:48:25Z,[unified build] Additional changes to make GPU builds work,"A continuation of https://github.com/pytorch/pytorch/pull/10504 for GPU, torch, etc. builds.

I was testing with

```
# edit caffe2/CMakeLists.txt to have
# target_compile_options(caffe2 PRIVATE ""-fvisibility=hidden"")
# enabled
# and then
FULL_CAFFE2=1 python setup.py build_deps | tee ~/log.txt
cat ~/log.txt | egrep 'undefined refer' | sort | less
```


I'll rebase on master when @Yangqing's changes in 10504 land, but putting up for some testing.

cc @mingzhe09088 @anderspapitto @ezyang ",pytorch
10530,orionr,pr,2018-08-15T04:02:31Z,[build] Remove setup_requires and tests_require from setup.py for FULL_CAFFE2,"In my environment, it looks like setup.py hangs when running

```
FULL_CAFFE2=1 python setup.py build_deps
```

Removing this fixes things, but we might also want to look at `tests_require`, which came over from `setup_caffe2.py`.

cc @pjh5 ",pytorch
10545,orionr,pr,2018-08-15T17:55:27Z,Bump gloo to latest master,Needed by the Gloo development team. Verifying nothing breaks in CI.,pytorch
10556,ssnl,pr,2018-08-15T22:03:59Z,[don't review] test TEST_NUMPY,,pytorch
10565,Yangqing,pr,2018-08-16T01:06:39Z,lint for github PR #10504,"Summary: TSIA

Reviewed By: mingzhe09088

Differential Revision: D9354482
",pytorch
10581,apaszke,pr,2018-08-16T14:25:56Z,Remove all cuDNN specific inputs to RNN functions,"This is still not the final PR, but it removes all blockers for actually using the RNN functions directly in the JIT. Next patch should be final, and will actually remove the symbolic_override code, and change it to proper symbolics for those ATen functions. Turns out the symbolic code can be also cleaned up a bit, and I'll do that too.

@zdevito @ezyang 
@colesbury (for minor DispatchStub.h) changes

#### Commit message:

There was no way to handle those in the JIT for now, and they turned
out to be completely unnecessary. It should make the Python and C++
module code much simpler too, since all the logic is now centralized
in the native functions.

The downside is that RNN modules no longer own their dropout buffers,
which are shared per-device instead (with appropriate locking and
synchronization). This might appear as a perf regression at first, but
in reality it's highly unlikely that anyone will want to run cuDNN RNNs
on the same GPU in parallel.",pytorch
10621,ssnl,pr,2018-08-17T15:18:24Z,Fix dropout fused kernel applied in eval mode,"fixes https://github.com/pytorch/pytorch/issues/10584

cc @apaszke ",pytorch
10624,orionr,pr,2018-08-17T16:02:44Z,[unified build] More fixes for hidden visibility,"Some more `ATEN_API` additions for hidden visibility.

Running CI tests to see what fails to link.

cc @Yangqing @mingzhe09088 @ezyang ",pytorch
10625,vishwakftw,pr,2018-08-17T16:39:19Z,[ready] Remove __hash__ for torch.Tensor,"This closes #7733 . 

Opened after making all tests pass in `test/` locally. Some tests have slight modifications.

Test plan: Will test on `pytorch/tutorials` to confirm if removal is clean. Edit: Tutorials build successful. ",pytorch
10626,apaszke,pr,2018-08-17T17:37:33Z,Allow tracing functions that take tuples of tensors as inputs,"I'll make it possible to return tuples in a subsequent PR.

@zdevito",pytorch
10627,apaszke,pr,2018-08-17T17:37:54Z,[EASY] Ignore stack frames coming from python3 object file,@goldsborough ,pytorch
10637,apaszke,pr,2018-08-17T19:55:25Z,Allow tracing functions that take tuples of tensors as inputs,"And return tuples.

@zdevito",pytorch
10638,apaszke,pr,2018-08-17T20:17:36Z,Stop using symbolic override for tracing RNNs,"This disables the symbolic override hacks and makes tracing emit the recently added ATen ops for RNNs (`aten::lstm`, `aten::gru`, ...). I managed to reuse pretty much all of the translation code for their symbolics.

@zdevito ",pytorch
10639,apaszke,pr,2018-08-17T21:18:45Z, Minor cleanup for tracing,"Stacked on top of #10637.

`get_trace_graph` was the only place where we explicitly used the
tracer from Python, which was annoying during some refactors.
This patch implements it in terms of the usual `trace` function.

@zdevito ",pytorch
10662,apaszke,pr,2018-08-18T22:48:16Z,Fix cuDNN dropout state cache,"Minor fix for the cuDNN cache. Previously we would skip the event reinitialization when an RNN function would be called on GPU 0, and then on GPU 1, but it would be in eval mode on GPU1. That would cause us to skip event re-initialization, and cause an incorrect resource handle error when trying to record the event.

@soumith ",pytorch
10666,ssnl,pr,2018-08-19T17:44:27Z,[docs] some improvements on distributed docs,,pytorch
10671,ssnl,pr,2018-08-19T20:29:41Z,Import DistributedSampler in utils/data/__init__,There is no reason that user should do an extra import to use DistributedSampler.,pytorch
10686,ssnl,pr,2018-08-20T15:21:04Z,fix typo,,pytorch
10692,orionr,pr,2018-08-20T18:12:00Z,[WIP][unified build] More changes for hidden visibility,"Let's run CI tests to see what fails given the changes that just landed in https://github.com/pytorch/pytorch/pull/10624

cc @mingzhe09088 @ezyang @Yangqing ",pytorch
10752,orionr,pr,2018-08-21T21:08:54Z,[unified build] Default hidden visibility,"Flipping to hidden visibility one more time. Let's see what fails.

cc @mingzhe09088 @pjh5 @Yangqing ",pytorch
10760,ssnl,pr,2018-08-22T00:30:06Z,Fixes *fft docs,"cc @cranmer  

fixes #10751",pytorch
10763,ssnl,pr,2018-08-22T01:48:29Z,[wip][test ci] Vectorized cpu grid sampler 2d forward,,pytorch
10764,JerryShih,pr,2018-08-22T02:27:19Z,[caffe2] Support caffe2 LeakyRelu op in caffe_translator.,Convert Relu layer to LeakyRelu op if we have 'negative_slope' value in caffe Relu layer.,pytorch
10765,peterjc123,pr,2018-08-22T02:46:59Z,Fixes errors when linking caffe2 statically,"Fixes #10746 and #10902.
The linking error is caused by the wrong usage of dllexport/dllimport. We don't need them if the implementation of the function is available from the external side.",pytorch
10770,JerryShih,pr,2018-08-22T08:38:54Z,[WIP][caffe2] Extend caffe2 FC/FCTranspose op to handle 2d bias.,"Currently, FC/FCTranspose only accepts 1d bias and broadcasts it to 2d matrix. If we have 2d bias, we could just bypass it to FC/FCTranspose op.

",pytorch
10771,orionr,pr,2018-08-22T15:27:35Z,Remove protobuf require and use requirements.txt,"In prep for making FULL_CAFFE2 default, users shouldn't be required to have protobuf installed.

cc @pjh5 ",pytorch
10772,orionr,pr,2018-08-22T15:47:49Z,[build] Remove nanopb since we've switched to protobuf,"We no longer use nanopb in PyTorch (or Caffe2) so removing. All protobuf manipulation should go through standard protobuf, which is statically linked inside libcaffe2.so by default.

cc @zdevito @pjh5 @ezyang @Yangqing ",pytorch
10775,ssnl,pr,2018-08-22T16:13:24Z,Resubmit #8322 with scipy version check,,pytorch
10780,ssnl,pr,2018-08-22T18:16:45Z,Pin NumPy and SciPy version in one CI build,,pytorch
10786,rohan-varma,pr,2018-08-22T18:55:42Z,[docs] added num_directions explanation to docstrings,"Resolving [https://github.com/pytorch/pytorch/issues/10741](https://github.com/pytorch/pytorch/issues/10741). The current docs use `num_directions` quite a bit, without any explanation for them. `num_directions` is set to 2 if the RNN is bidirectional, or 1 otherwise. This change simply adds that to the docs. ",pytorch
10809,JerryShih,pr,2018-08-23T03:26:47Z,[caffe2] Handle 'axis' parameter for softmax op in caffe-caffe2 translator.,The axis information doesn't pass to caffe2 op.,pytorch
10822,apaszke,pr,2018-08-23T16:34:45Z,Remove Node::invalidateSchema,"The schema_ field is a private and internal cache for nodes, and no
methods meant to manipulate it should be publicly visible. This call
wasn't even necessary at its call site, since removeInput will reset the
schema by itself.

@zdevito @jamesr66a 
",pytorch
10833,ssnl,pr,2018-08-23T21:06:07Z,Make torch.cuda.* take device objects; Update distributed docs,"Commits:

1. Make `torch.cuda.*` take device objects
2. Update `torch.distributed` docs to emphasize calling `torch.cuda.set_device` before `init_process_group`

",pytorch
10843,apaszke,pr,2018-08-24T01:41:10Z,Prevent JIT from overspecializing to every single size configuration,"Summary of changes:

- Renamed `TensorType` to `CompleteTensorType` and added `TensorType` which records only the scalar type, number of dimensions, and device of a value. The argument behind the rename is to encourage people to use `CompleteTensorType` less, as most passes will only have limited information available. To make transition easier `complete_type->cast<TensorType>()` works, and makes our passes work with both kinds of specialization if they don't need extra the extra detail.
- Shape analysis can process graphs with both `CompleteTensorType` and `TensorType`.
- Fuser was a part that heavily relied on full shape information being available. Now, we simply try to fuse the largest possible graphs, and have to do run-time checks to make sure they match the code we generate. If they don't, we fall back to regular interpretation. The shape checks are implementing using an optimized method exploiting algebraic properties of shapes with broadcasting, and the relations of broadcasting with pointwise ops. A full written proof of correctness of the shape checking algorithm is included in a comment in `fusion_compiler.cpp`.",pytorch
10844,apaszke,pr,2018-08-24T01:45:06Z,Prevent JIT from overspecializing to every single size configuration,"Please review the expects carefully to make sure there are no regressions. I tried to go over them one by one when they changed, but it's sometimes easy to miss finer details.

Summary of changes:

- Renamed `TensorType` to `CompleteTensorType`. Added a new `TensorType` which records only the scalar type, number of dimensions, and device of a value. The argument behind the rename is to encourage people to use `CompleteTensorType` less, as most passes will only have limited information available. To make transition easier `complete_type->cast<TensorType>()` works, and makes our passes work with both kinds of specialization if they don't need extra the extra detail.
- Renamed `ArgumentSpec` to `CompleteArgumentSpec`. Added a new `ArgumentSpec`, which matches argument only at the level of the new `TensorType`.
- Shape analysis can process graphs with both `CompleteTensorType` and `TensorType`.
- Fuser was a part that heavily relied on full shape information being available. Now, we simply try to fuse the largest possible graphs, and have to do run-time checks to make sure they match the code we generate. If they don't, we fall back to regular interpretation. The shape checks are implementing using an optimized method exploiting algebraic properties of shapes with broadcasting, and the relations of broadcasting with pointwise ops. A full written proof of correctness of the shape checking algorithm is included in a comment in `graph_fuser.cpp`.

@zdevito @ezyang @mruberry @ngimel @csarofeen",pytorch
10865,orionr,pr,2018-08-24T18:47:26Z,[build] Completely remove the nanopb submodule,"After making changes internally, really remove the nanopb submodule.

Finalizes https://github.com/pytorch/pytorch/pull/10772

cc @pjh5 @ezyang ",pytorch
10867,apaszke,pr,2018-08-24T19:43:26Z,Make it possible to disable JIT using env variables,@zdevito ,pytorch
10894,vishwakftw,pr,2018-08-27T07:37:37Z,Bag of Distributions doc fixes,"- Added `__repr__` for Constraints and Transforms.
- Arguments passed to the constructor are now rendered with :attr:

Closes https://github.com/pytorch/pytorch/issues/10884",pytorch
10903,ssnl,pr,2018-08-27T15:36:58Z,"Check min version of numpy, scipy, and librosa in common.py",,pytorch
10949,apaszke,pr,2018-08-28T15:34:25Z,Don't flatten output lists in the JIT IR,"Operators like aten::chunk used to return a number of tensors, but
now return a list. To make it easier to do shape prop through
aten::chunk and fuse it, I've also introduced prim::ConstantChunk,
which behaves like the previous implementation (has a variable length
output list).

The downside of this PR is that the introduction of more lists to the IR causes the LSTM and MiLSTM graphs to be considered as non-differentiable by the graph executor. I verified that they are still optimize correctly, and my next patch (that changes how the specializations/differentiation works) will restore those.

@zdevito 

",pytorch
10954,ssnl,pr,2018-08-28T16:22:29Z,Set -Wno-stringop-overflow only with GCC >=7,`stringop-overflow` is added in GCC 7.,pytorch
10977,apaszke,pr,2018-08-28T21:41:05Z,Change specialization rules in GraphExecutors,"**Review last commit only.** Stacked on top of #10949.

This commit fixes a number of issues connected to caching
differentiability status of graphs inside graph executors,
and changes the rules for optimization of differentiable subgraphs.
Previously every one of those was instantiated as a separate graph
executor, but now they are simply heavier-optimized graph regions,
and graph executors are only instantiated for their backward.

@zdevito",pytorch
10980,ssnl,pr,2018-08-28T22:29:39Z,[Ready] Vectorize grid sample 2d CPU kernels,"This PR vectorizes the CPU grid sample 2d forward and backward kernels. Specifically, 



 1. add `.data()` in `TensorAccessor`
 2. support non-void return value for declaring CPU kernel stub
 2. add `bool at:: geometry_is_contiguous(IntList sizes, IntList strides)`
1. The following vectorized CPU primitives are added: 

    + `gather<scale>(baseaddr, vindex)`: `result[i] = baseaddr[vindex[i] * scale]`
    + `mask_gather<scale>(src, baseaddr, vindex, mask)`: `result[i] = mask[i] ? baseaddr[vindex[i] * scale] : src[i]`. 
    + comparison ops
    + binary logical ops
    + `min(a, b)`
    + `cast<dst_t, src_t>(src_vec)`: changing dtype but keeping the bit representation
    + `blendv(a, b, mask)`: `result[i] = mask[i] ? b[i] : a[i]`.
    + ctor with multiple values (i.e., `setr`)
    + `arange(start = 0, step = 1)`: constructs a vector with values specified by the arange parameters
    + `convert_to_int_of_same_size(vec)`: convert floating point vector to corresponding integral type of same size
    + `interleave2(a, b)` & `deinterleave2(x, y)`: interleave or deinterleaves two vectors. E.g., for `interleave`:
        ```
        inputs:
          {a0, a1, a2, a3, a4, a5, a6, a7}
          {b0, b1, b2, b3, b4, b5, b6, b7}
        outputs:
          {a0, b0, a1, b1, a2, b2, a3, b3}
          {a4, b4, a5, b5, a6, b6, a7, b7}
        ```

  2. Grid sample CPU kernel implementations are described in the following note (also in `GridSampleKernel.cpp`:

  ```
   NOTE [ Grid Sample CPU Kernels ]
  
   Implementation of vectorized grid sample CPU kernels is divided into three
   parts:
  
   1. `ComputeLocation` struct
      Transforms grid values into interpolation locations of the input tensor
      for a particular spatial dimension, basing on the size of that dimension
      in input tensor, and the padding mode.
```
```cpp
      template<typename scalar_t, GridSamplerPadding padding>
      struct ComputeLocation {
        using Vec = Vec256<scalar_t>;
  
        // ctor
        ComputeLocation(int64_t size);
  
        // Given grid values `in`, return the interpolation locations after
        // un-normalization and padding mechanism (elementwise).
        Vec apply(const Vec &in) const;
  
        // Similar to `apply`, but also returns `d apply(in) / d in`
        // (elementwise).
        // this is often used in gradient computation.
        std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;
      };
```
```
   2. `ApplyGridSample` struct
      Owns N `ComputeLocation` structs, where N is the number of spatial
      dimensions. Given N input grid vectors (one for each spatial dimension)
      and spatial offset, it gets the interpolation locations from
      `ComputeLocation`s, applies interpolation procedure, and then writes to
      the output (or grad_input & grad_grid in backward).
```
```cpp
      template<typename scalar_t, int spatial_dim,
               GridSamplerInterpolation interp,
               GridSamplerPadding padding>
      struct ApplyGridSample {
  
        // ctor
        ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);
  
        // Applies grid sampling (forward) procedure:
        //   1. computes interpolation locations from grid values `grid_x` and
        //      `grid_y`,
        //   2. interpolates output values using the locations and input data
        //      in `inp_slice`, and
        //   3. writes the first `len` values in the interpolated vector to
        //      `out_slice` with spatial offset being `offset`.
        //
        // This assimes that `grid_x` and `grid_y` all contain valid grid
        // values \in [-1, 1], even at indices greater than `len`.
        //
        // The `*_slice` argument namess mean samples within a batch (i.e.,
        // with the batch dimension sliced out).
        void forward(TensorAccessor<scalar_t, 3>& out_slice,
                     const TensorAccessor<scalar_t, 3>& inp_slice,
                     int64_t offset, const Vec& grid_x, const Vec& grid_y,
                     int64_t len) const;
  
        // Applies grid sampling (backward) procedure. Arguments semantics
        // and strategy are similar to those of `forward`.
        void backward(TensorAccessor<scalar_t, 3>& gInp_slice,
                      TensorAccessor<scalar_t, 3>& gGrid_slice,
                      const TensorAccessor<scalar_t, 3>& gOut_slice,
                      const TensorAccessor<scalar_t, 3>& inp_slice,
                      int64_t offset, const Vec& grid_x, const Vec& grid_y,
                      int64_t len) const;
      }
```
```
   3. `grid_sample_2d_grid_slice_iterator` function
      Among the tensors we work with, we know that the output tensors are
      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in
      backward), we need to randomly read `input` anyways, and `grad_output`
      usually comes from autograd and is often contiguous. So we base our
      iterating strategy on the geometry of grid.
      `grid_sample_2d_grid_slice_iterator` function provides an abstract to
      efficiently iterates through a `grid` slice (without batch dimension).
      See comments of that function on the specific cases and strategies used.
```
```cpp
      template<typename scalar_t, typename ApplyFn>
      void grid_sample_2d_grid_slice_iterator(
        const TensorAccessor<scalar_t, 3>& grid_slice,
        const ApplyFn &apply_fn);

      // `apply_fn` is a function/lambda that can be called as if it has
      // declaration:
      //   void apply_fn(const Vec256<scalar_t>& grid_x,
      //                 const Vec256<scalar_t>& grid_y,
      //                 int64_t spatial_offset, int64_t len);
```
```
      `apply_fn` will be called multiple times, and together cover the entire
      output spatial space. Therefore, e.g., to implement forward 2d grid
      sample, we can do
```
```cpp
      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);
  
      for (int n = 0; n < input_accessor.size(0); n++) {
        grid_sample_2d_grid_slice_iterator(
          grid_accessor[n],
          [&](const Vec256<scalar_t>& grid_x, const Vec256<scalar_t>& grid_y,
              int64_t spatial_offset, int64_t len) {
            grid_sample.forward(out_accessor[n], input_accessor[n],
                                spatial_offset, grid_x, grid_y, len);
          });
      }
   ```",pytorch
10983,Yangqing,pr,2018-08-29T02:27:22Z,Add flush to logging messages higher than INFO.,This probably fixes the logging test error that @orionr is encountering - haven't tested locally but wanted to send out a PR to kick off CI.,pytorch
10998,orionr,pr,2018-08-29T15:15:57Z,[build] Windows raw string fix,"Breaking this out of https://github.com/pytorch/pytorch/pull/8338

@mingzhe09088's fix of the docstrings for Windows builds. Unfortunately some versions of Windows seem to try and parse the `#` inside the string as a pre-processor declaration. We might need to change this to something else later, but want to get this landed first.

cc @mingzhe09088 @Yangqing ",pytorch
10999,orionr,pr,2018-08-29T15:19:09Z,[build] Disable smart_tensor_printer_test without glog,"Breaking out of https://github.com/pytorch/pytorch/pull/8338

This test fails once we start building with `-DUSE_GLOG=OFF` since the non-glog logging case doesn't support flushing or streaming to the right location. For now, we just disable this test in that case.

cc @Yangqing @mingzhe09088 ",pytorch
11000,orionr,pr,2018-08-29T15:21:56Z,[build] Cleanup BUILD_DOCS cmake section,"Breaking out of https://github.com/pytorch/pytorch/pull/8338

cc @mingzhe09088 @Yangqing ",pytorch
11008,ssnl,pr,2018-08-29T17:48:05Z,[doc] Add doc for Tensor.digamma_?,"follow up for #10967 

@zou3519 @vishwakftw ",pytorch
11020,Yangqing,pr,2018-08-29T20:39:29Z,"In default, use third party eigen. Added new flag USE_SYSTEM_EIGEN_INSTALL to control.","TSIA. @apaszke pointed out that it might be better to use third party folder in default, since system Eigen may often be out of date and does not have the version we need to compile successfully.",pytorch
11024,ssnl,pr,2018-08-29T20:54:31Z,Add has_lapack flag,"Currently our `skipIfLapack` has uses a try-catch block and regex match the error message. It is highly unreliable. This PR adds `hasLAPACK` and `hasMAGMA` on ATen context, and expose the flags to python.

Also fixes refcounting bug with `PyModule_AddObject`. The method steals reference, but we didn't `Py_INCREF` in some places before calling it with `Py_True` or `Py_False`.",pytorch
11034,orionr,pr,2018-08-29T22:20:19Z,[build] Allow same flags when glog is used or not,"Extracted from https://github.com/pytorch/pytorch/pull/8338

cc @mingzhe09088",pytorch
11037,orionr,pr,2018-08-29T22:36:32Z,[build] Export MPI functions,"Potential fix for https://github.com/caffe2/caffe2/issues/2551#issuecomment-417124872

cc @Yangqing @mingzhe09088 ",pytorch
11040,ssnl,pr,2018-08-29T23:17:31Z,Make torch.randint have default dtype int64,cc @gchanan @apaszke ,pytorch
11045,orionr,pr,2018-08-30T00:10:37Z,[build] Inject GetEmptyStringAlreadyInited once for static proto,"I've been seeing a lot of warnings about multiple declarations of this. Hopefully this fixes it.

cc @Yangqing @mingzhe09088 @ezyang ",pytorch
11082,apaszke,pr,2018-08-30T14:56:12Z,Expose arbitrary cpp autograd functions to Python,"This is needed because the JIT declares some custom autograd functions.

@colesbury ",pytorch
11083,apaszke,pr,2018-08-30T15:06:35Z,Add entry for torch/lib/pythonX.Y in .gitignore,"I've had `torch/lib/python3.6` show up as part of the build for some time now. It's not ignored which means I need to be extra careful about checking in files, or I end up with a thousand of them in my index.",pytorch
11088,apaszke,pr,2018-08-30T15:57:16Z,Warn about non-traceable behavior when tracing,@zdevito ,pytorch
11090,apaszke,pr,2018-08-30T16:10:30Z,Disable -Werror on macOS test build,"cc @goldsborough 
",pytorch
11091,ssnl,pr,2018-08-30T16:25:37Z,Fix max and min with inf on CUDA,"Fixes #10237 #11084 

cc @vishwakftw ",pytorch
11092,ssnl,pr,2018-08-30T16:55:44Z,Fix relying on UB in test_data_parallel_nested_output,We shouldn't reply on plain `dict` ordering. Example failure: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda8-cudnn6-py3-test1/8417/console,pytorch
11100,apaszke,pr,2018-08-30T18:33:57Z,Fix a bug in addmm fusion in the JIT,"Fixes #10839.

@zdevito ",pytorch
11106,ssnl,pr,2018-08-30T20:33:39Z,[Doc] Improvements on conv/pool/fold/stft/ParamDict docs,"Also fixes some incorrect formula rendering.

Test plan:

make html",pytorch
11110,apaszke,pr,2018-08-30T21:24:13Z,Lower trivial differentiable subgraphs,@zdevito ,pytorch
11142,apaszke,pr,2018-08-31T14:55:08Z,Hotfix for JIT tests,Because master is broken.,pytorch
11150,ngimel,pr,2018-08-31T18:17:27Z,"return aten::gt to the list of fusable operations, add expected graphs",Fixes one of #11118 issues. ,pytorch
11161,orionr,pr,2018-08-31T21:33:58Z,[build] Support environments with no libprotobuf,"Just pulling this out of https://github.com/pytorch/pytorch/pull/10611

Make sure we can support environments where we don't have libprotobuf installed when we link-local protobuf.

cc @goldsborough @Yangqing ",pytorch
11177,ssnl,pr,2018-09-01T20:46:46Z,Fix compile warnings,,pytorch
11185,vishwakftw,pr,2018-09-02T12:13:41Z,Document torch.allclose,"- Modify torch.autograd.gradcheck to use torch.allclose instead
- Expose doc strings

Closes #10355

",pytorch
11187,ssnl,pr,2018-09-02T17:21:53Z,remove allclose from test_doc skipped list,,pytorch
11194,vishwakftw,pr,2018-09-03T05:15:43Z,Switch SVD on CPU from gesvd to gesdd,- Added a note to the doc string for `svd`.,pytorch
11208,ssnl,pr,2018-09-03T22:33:46Z,Fix some more compile warnings,,pytorch
11209,ssnl,pr,2018-09-04T01:22:55Z,Allow passing through arguments to unittest,"Example:
```sh
python run_test.py -i sparse -- TestSparse.test_factory_size_check -f
```

With this, the `--verbose` option is redundant (one can call `python run_test.py -- -v` instead of `python run_test.py -v`). But since this is (probably) a frequently used flag, I didn't remove the existing easier-to-use option.

cc @ezyang ",pytorch
11221,orionr,pr,2018-09-04T15:11:10Z,[build] Cleanup dependency of distributed flags,"Now that we're building everything together, making all distributed flags conditional of USE_DISTRIBUTED being set.

cc @pietern @teng-li @cpuhrsch ",pytorch
11224,apaszke,pr,2018-09-04T16:11:36Z,Port PackedSequences functions to C++,@zdevito ,pytorch
11231,neerajprad,pr,2018-09-04T18:02:56Z, Optional expand=True kwarg in distribution.enumerate_support,"This adds an optional `expand=True` kwarg to the `distribution.expand_support()` method, to get a distribution's support without expanding the values over the distribution's `batch_shape`. 
 - The default `expand=True` preserves the current behavior, whereas `expand=False` collapses the batch dimensions.

e.g.
```python
In [47]: d = dist.OneHotCategorical(torch.ones(3, 5) * 0.5)

In [48]: d.batch_shape
Out[48]: torch.Size([3])

In [49]: d.enumerate_support()
Out[49]:
tensor([[[1., 0., 0., 0., 0.],
         [1., 0., 0., 0., 0.],
         [1., 0., 0., 0., 0.]],

        [[0., 1., 0., 0., 0.],
         [0., 1., 0., 0., 0.],
         [0., 1., 0., 0., 0.]],

        [[0., 0., 1., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 1., 0., 0.]],

        [[0., 0., 0., 1., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 1., 0.]],

        [[0., 0., 0., 0., 1.],
         [0., 0., 0., 0., 1.],
         [0., 0., 0., 0., 1.]]])

In [50]: d.enumerate_support().shape
Out[50]: torch.Size([5, 3, 5])

In [51]: d.enumerate_support(expand=False)
Out[51]:
tensor([[[1., 0., 0., 0., 0.]],

        [[0., 1., 0., 0., 0.]],

        [[0., 0., 1., 0., 0.]],

        [[0., 0., 0., 1., 0.]],

        [[0., 0., 0., 0., 1.]]])

In [52]: d.enumerate_support(expand=False).shape
Out[52]: torch.Size([5, 1, 5])
```

**Motivation:**
 - Currently `enumerate_support` builds up tensors of size `support + batch_shape + event_shape`, but the values are *repeated* over the `batch_shape` (adding little in the way of information). This can lead to expensive matrix operations over large tensors when `batch_shape` is large (see, example above), often leading to OOM issues. We use `expand=False` in Pyro for message passing inference. e.g. when enumerating over the state space in a Hidden Markov Model. This creates sparse tensors that capture the markov dependence, and allows for the possibility of using optimized matrix operations over these sparse tensors. `expand=True`, on the other hand, will create tensors that scale exponentially in size with the length of the Markov chain.
 - We have been using this in our [patch](https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py) of `torch.distributions` in Pyro. The interface has been stable, and it is already being used in a few Pyro algorithms. We think that this is more broadly applicable and will be of interest to the larger distributions community.

cc. @apaszke, @fritzo, @alicanb 
",pytorch
11245,apaszke,pr,2018-09-04T20:32:45Z,Improve error message to include return types too,Fixes #11057.,pytorch
11246,apaszke,pr,2018-09-04T22:09:08Z,Treat numerical differences as warnings instead of errors when tracing,"Also, make `torch.isclose` work with integral tensors and refactor `_check_trace` a bit.

@zdevito ",pytorch
11248,ngimel,pr,2018-09-04T22:29:19Z,add persistent rnns with conservative criteria,Persistent rnns provide much better performance on V100 with half input data for a variety of cases. ,pytorch
11253,ssnl,pr,2018-09-04T23:22:56Z,[sparse] Autograd get_indices/values and sparse_coo ctor,"TODO: docs

Closes #11232 ",pytorch
11257,ssnl,pr,2018-09-05T01:38:02Z,Fix some more warnings,Found these when compiling the new master with gcc 7.3,pytorch
11263,neerajprad,pr,2018-09-05T04:19:18Z,Fix to distribution.__repr__ with lazy attributes,"`__repr__` currently fails for distributions with lazy attributes in PyTorch master, throwing a `KeyError`. This fixes the issue. 

**Additionally:**
 - Added `logits` to `arg_constraints` for distributions that accept either `probs` or `logits`. This is both to have `__repr__` display the `logits` param when available, and to be able to do validation checks (e.g. NaN checks) when the logit parametrization is used. @fritzo, @alicanb - I think there were reasons why we had not done so in the first place, but I am unable to recall now. It passes all the tests, but let me know if there is something that I am missing at the moment.
 - There are certain distributions, e.g. `OneHotCategorical` which won't show any parameters because it uses a `categorical` instance under the hood and neither `logits` / `probs` in `arg_constraints` are present in the instance's `__dict__`. This isn't addressed in this PR. 

cc. @vishwakftw, @fritzo, @nadavbh12, @apaszke 
",pytorch
11266,Yangqing,pr,2018-09-05T05:41:39Z,[build] Windows DLL build with Caffe2 code,"This is an experimental build on top of what @orionr and @mingzhe09088 built.

Essentially, the idea is that we will need separate *_API versions for different shared libraries. If this theory is right, I'll try to clean up the design a bit and document it properly.",pytorch
11288,apaszke,pr,2018-09-05T18:38:41Z,"Improve support for tracing sizes, add more tracer warnings","Many constructors like `torch.zeros` or `torch.randn` didn't support
size tracing correctly which is fixed by this pass. Same issue has been
fixed in legacy tensor constructors.

Additionally, new tensor constructors, which do not participate in
tracing (most notably `torch.tensor`, `torch.as_tensor` and
`torch.from_numpy`) raise a warning when they are used.

Finally, entering a traceable operation disables the tracing in its body.
This is needed because

@zdevito ",pytorch
11295,Yangqing,pr,2018-09-05T20:41:25Z,[do not merge] temporary fix: trying to patch generated headers to use const instead of constexpr,"This is an experiment to see if we can temporarily patch protobuf gen files to avoid having constexpr, which nvcc complains at this moment on windows machines (see https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-build/19383/console)",pytorch
11303,orionr,pr,2018-09-05T22:26:01Z,[build] Roll nomnigraph build into caffe2,"We need to remove nomnigraph from the list of public libraries in order to support libtorch extensions. Easiest way to do this is to include it into the Caffe2 source like all other caffe2/core/ code.

However, because the headers are in a different place, we need to include them for linked libraries (pybind, tests, etc).

On an upside, this means that nomnigraph is now default hidden visibility too.

FYI @peterjc123 @xkszltl @goldsborough @bwasti @Yangqing ",pytorch
11312,orionr,pr,2018-09-05T23:49:13Z,Fix conv gradient conversion,Fix Windows build failure after https://github.com/pytorch/pytorch/pull/10744 landed.,pytorch
11320,ssnl,pr,2018-09-06T04:43:58Z,Fix more warnings,also a missing space in fft error message,pytorch
11321,orionr,pr,2018-09-06T04:46:45Z,[build] Remove FULL_CAFFE2 flag,"Continuing @pjh5's work to remove FULL_CAFFE2 flag completely.

With these changes you'll be able to also do something like

```
NO_TEST=1 python setup.py build_deps
```
and this will skip building tests in caffe2, aten, and c10d. By default the tests are built.

Also, BUILD_BINARIES and USE_OPENCV are OFF by default and BUILD_TEST is ON by default.

cc @mingzhe09088 @Yangqing ",pytorch
11329,themightyoarfish,pr,2018-09-06T08:00:21Z,WIP: Reproducibility note,"This adds a Note on making experiments reproducible. 

It also adds Instructions for building the Documentation to `README.md`. Please ping if I missed any requirements.

I'm not sure what to do about the submodule changes. Please advise.",pytorch
11334,orionr,pr,2018-09-06T16:00:39Z,[build] Force third_party Eigen from setup.py,We shouldn't use system Eigen in any cases when building with setup.py. If people want to use system Eigen (not from third_party) they can build with CMake for now.,pytorch
11341,neerajprad,pr,2018-09-06T18:55:37Z,Add .expand() method to distribution classes,"This adds a `.expand` method for distributions that is akin to the `torch.Tensor.expand` method for tensors. It returns a new distribution instance with batch dimensions expanded to the desired `batch_shape`. Since this calls `torch.Tensor.expand` on the distribution's parameters, it does not allocate new memory for the expanded distribution instance's parameters. 

e.g.
```python
>>> d = dist.Normal(torch.zeros(100, 1), torch.ones(100, 1))
>>> d.sample().shape
  torch.Size([100, 1])
>>> d.expand([100, 10]).sample().shape
  torch.Size([100, 10])
```

### Motivation

We have already been using the `.expand` method in Pyro in our [patch](https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py#L10) of `torch.distributions`. We use this in our models to enable dynamic broadcasting. This has also been requested by a few users on the distributions slack, and we believe will be useful to the larger community. 

Note that currently, there is no convenient and efficient way to expand distribution instances:
 - Many distributions use `TransformedDistribution` (or wrap over another distribution instance. e.g. `OneHotCategorical` uses a `Categorical` instance) under the hood, or have lazy parameters. This makes it difficult to collect all the relevant parameters, broadcast them and construct new instances.
 - In the few cases where this is even possible, the resulting implementation would be inefficient since we will go through a lot of broadcasting and args validation logic in `__init__.py` that can be avoided.

The `.expand` method allows for a safe and efficient way to expand distribution instances. Additionally, this bypasses `__init__.py` (using `__new__` and populating relevant attributes) since we do not need to do any broadcasting or args validation (which was already done when the instance was first created). This can result in significant savings as compared to constructing new instances via `__init__` (that said, the `sample` and `log_prob` methods will probably be the rate determining steps in many applications).

e.g.
```python
>>> a = dist.Bernoulli(torch.ones([10000, 1]), validate_args=True)

>>> %timeit a.expand([10000, 100])
15.2 Âµs Â± 224 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

>>> %timeit dist.Bernoulli(torch.ones([10000, 100]), validate_args=True)
11.8 ms Â± 153 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

cc. @fritzo, @apaszke, @vishwakftw, @alicanb ",pytorch
11344,rasbt,pr,2018-09-06T20:47:45Z,typo/grammar fixes,"Fixes some minor grammar issues in the code base.

PS: I was actually looking for the following one but couldn't find it via grepping in this repo:

![screen shot 2018-09-06 at 3 27 39 pm](https://user-images.githubusercontent.com/5618407/45184280-1e16a980-b1ec-11e8-9cb1-87a96738bdd1.png)

Any idea in which file this issue is raised?",pytorch
11358,apaszke,pr,2018-09-07T01:45:25Z,Improve shape analysis to cover all most commonly used ops,"[Here's a list](https://gist.github.com/apaszke/f0821840bdcc67a977832dc58acc1b85) of ops that are in `register_aten_ops.cpp`, but aren't supported in shape prop. Everything else should work now.",pytorch
11367,peterjc123,pr,2018-09-07T04:12:13Z,Remove symbol export annotations in THC/generic/*.cu,"We use these annotations during function declarations, not definitions. See the description of compiler error [C2491](https://msdn.microsoft.com/en-us/library/62688esh.aspx) for more details.",pytorch
11374,peterjc123,pr,2018-09-07T09:33:23Z,[TEST ONLY] Test CUDA 8 build on Windows,,pytorch
11375,vishwakftw,pr,2018-09-07T09:42:58Z,[WIP] Fix issues with certain heterogeneous types in lists during tensor creation,"TODO:
- tests

Closes #9963 ",pytorch
11377,vishwakftw,pr,2018-09-07T14:04:27Z,Fix issues with certain heterogeneous types in lists during tensor creation,Closes #9963 ,pytorch
11388,Yangqing,pr,2018-09-07T17:41:11Z,[third-party] Update googletest to release-1.8.1,This is mainly to pick up the change https://github.com/google/googletest/commit/20074be19a9c1d6568a4554da2fee3f2e3dbac09 to avoid polluting the CMAKE_DEBUG_POSTFIX variable. cc @orionr .,pytorch
11393,ssnl,pr,2018-09-07T18:57:14Z,Update variable view note,,pytorch
11406,syed-ahmed,pr,2018-09-07T22:21:00Z,Updates FindCUDA.cmake to 3.12.2 upstream version,"This PR is just a copy-paste of the upstream FindCUDA.cmake. Since, cublas_device is deprecated in CUDA >= 9.2, this change is necessary for build.

Related: https://gitlab.kitware.com/cmake/cmake/merge_requests/2298
",pytorch
11407,orionr,pr,2018-09-07T22:31:25Z,Constexpr std::move / std::forward for C++11,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#11407 Constexpr std::move / std::forward for C++11**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D9724805/)

std::move and std::forward in C++11 aren't constexpr (they are in C++14).
This caused a build issue @orionr was working on.
It should be fixed by this diff

Differential Revision: [D9724805](https://our.internmc.facebook.com/intern/diff/D9724805/)",pytorch
11415,orionr,pr,2018-09-07T23:58:25Z,[build] Lockdown NO_TEST=1 for tests even more,"Skip torch tests as well when NO_TEST=1 environment variable is set. Also remove the separate ATen code path for not being built with Caffe2, since it will always be built with Caffe2.

cc @teng-li ",pytorch
11416,Yangqing,pr,2018-09-08T00:59:13Z,[build] re-enable USE_MPI,"The previous error was caused by mpi_test not depending on MPI_CXX_LIBRARIES. This might solve the problem. 

Not tested locally - waiting for CI test.",pytorch
11417,Yangqing,pr,2018-09-08T01:01:52Z,[build] Update gtest and remove the macro guide on gtest from #11321,"Last PR seems to have test failures, re-issuing.",pytorch
11421,ssnl,pr,2018-09-08T07:01:45Z,Add matrix power,"@vishwakftw Your patch needed some updates because the default native function dispatches changed from `[function, method]` to `[function]`. The CI was run before that change happened so it still shows green, but the internal test caught it.

I did some changes when rebasing and updating so I didn't just force push to your branch. Let's see if this passes CI and internal test. If it does, let me know if you want me to force push to your branch or use this PR instead.

Note to reviewers: patch was already approved at #10068 .

cc @yf225 ",pytorch
11429,Yangqing,pr,2018-09-08T20:44:14Z,[test] Add gtest dependency on aten tests.,"@ezyang delivering my promise to you :)

Basically, now aten tests can use gtest as part of our test harness unification effort. I also converted one test (atest.cpp) to show how one can do this.
",pytorch
11432,ssnl,pr,2018-09-09T00:27:06Z,Only join started dataloader workers,"`Process.start()` actually take some time as it needs to start a
process and pass the arguments over via a pipe. Therefore, we
only add a worker to self.workers list after it started, so
that we do not call `.join()` if program dies before it starts,
and `__del__` tries to join it but will get:
    AssertionError: can only join a started process.

Example trace when such error happens:
```py
[unrelated]
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 500, in __iter__
    return _DataLoaderIter(self)
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 292, in __init__
    w.start()
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/process.py"", line 112, in start
    self._popen = self._Popen(self)
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/context.py"", line 277, in _Popen
    return Popen(process_obj)
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/popen_fork.py"", line 20, in __init__
    self._launch(process_obj)
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/popen_fork.py"", line 70, in _launch
    self.pid = os.fork()
KeyboardInterrupt
Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fa704d5aa60>
Traceback (most recent call last):
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 398, in __del__
    self._shutdown_workers()
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 392, in _shutdown_workers
    w.join()
  File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/process.py"", line 139, in join
    assert self._popen is not None, 'can only join a started process'
AssertionError: can only join a started process
```

No test because hard to reliably trigger.",pytorch
11434,themightyoarfish,pr,2018-09-09T08:03:38Z,WIP: Include note on cudnn determinism in each function backed by cudnn,"Ping @ezyang 
This addresses your comment in #114. Strangely, when running the doc build (`make html`) none of my changes are actually showing, could you point out what I'm doing wrong?

Once #11329 is merged it might make sense to link to the reproducibility note everywhere.",pytorch
11444,ssnl,pr,2018-09-09T20:22:32Z,Use AT_CHECK and AT_ERROR,,pytorch
11457,vishwakftw,pr,2018-09-10T15:13:35Z,More use of AT_CHECK and AT_ERROR,"Considering these increase the size of the message stack, I didn't touch the code outside `ATen/native`

cc: @soumith @SsnL 
",pytorch
11462,orionr,pr,2018-09-10T17:39:38Z,"[build] Flags for LMDB, LevelDB, and Caffe2 ops","Add flags for LMDB and LevelDB, default `OFF`. These can be enabled with

```
USE_LMDB=1 USE_LEVELDB=1 python setup.py build_deps
```

Also add a flag to build Caffe2 ops, which is default `ON`. Disable with

```
NO_CAFFE2_OPS=1 python setup.py build_deps
```

cc @Yangqing @soumith @pjh5 @mingzhe09088 ",pytorch
11466,ssnl,pr,2018-09-10T18:15:49Z,Check for maximum numel in NCCL broadcasting,"NCCL1 uses `int` as its numerical type for fields like `count`, which makes broadcasting tensors larger than `2 << 31 - 1` impossible, and raises opaque error `invalid arguments`. NCCL2 greatly increase the limit on many platforms by using `size_t`. This patch statically detects this type, and raises properly if the broadcast tensor exceeds the limit.

No test because I don't think our test suite should broadcast big tensors.",pytorch
11472,ssnl,pr,2018-09-10T19:06:07Z,Fix katex math rendering,I'm 80% sure that this fixes the math bug. But I can't repro locally so I don't know.,pytorch
11483,orionr,pr,2018-09-10T21:08:59Z,[WIP][build] Enable BUILD_TEST for Windows,"We'll work through the test build failures on Windows in this PR.

cc @mingzhe09088 @Yangqing ",pytorch
11488,orionr,pr,2018-09-10T21:33:58Z,[build] Remove separate ATen build target,"ATen has had a separate build target in the past, but with our move to a root-level CMakeLists.txt file this makes less sense and is harder to maintain. Also, as we blend code between Caffe2 and ATen this will become even less maintainable.

Talked to @ezyang about this, but also cc @zdevito, @Yangqing, and @soumith. If this is too difficult, I will revert, but want to see if we can simplify for now.",pytorch
11491,apaszke,pr,2018-09-10T23:22:28Z,Make .to() methods native functions (to fix JIT tracing),,pytorch
11506,apaszke,pr,2018-09-11T01:54:23Z,Add support for tracing strings,"This enabled `torch.einsum` both in tracing and in script mode. It's used all over Pyro at the moment, and is needed for any use of the JIT in there.

Fixes #11157.

@zdevito @fritzo @neerajprad ",pytorch
11517,orionr,pr,2018-09-11T15:18:47Z,[build] Copy protos on install same as develop,"This is a potential fix for https://github.com/pytorch/pytorch/issues/11453 and https://github.com/pytorch/pytorch/issues/11074 worked through with @pjh5 . Turns out we had some protos copy code that was in the .sh file that was removed. Better to have it in setup.py, though, same as for develop.

cc @ezyang ",pytorch
11525,apaszke,pr,2018-09-11T17:09:48Z,Remove time prefix from rsync,"This fails with zsh saying ""time: command not found"".

cc @soumith

",pytorch
11531,ssnl,pr,2018-09-11T17:59:31Z,Fix some more doc,,pytorch
11539,apaszke,pr,2018-09-11T19:56:12Z,Allow tracing random functions (only when using default generators),"Fixes #11504.

@zdevito, @neerajprad, @fritzo ",pytorch
11541,apaszke,pr,2018-09-11T20:04:02Z,Release GIL when calling into JIT interpreter,,pytorch
11542,fritzo,pr,2018-09-11T20:04:06Z,[distributions] Ensure .enumerate_support() methods are jittable,"This works around #11535 by avoiding `arange(n, out=x)` and `eye(n, out=x)` in `torch.distributions`. I've confirmed that the `.enumerate_support()` methods are now jittable.",pytorch
11545,apaszke,pr,2018-09-11T20:46:47Z,Improve tracer warnings,"Also, fix a performance bug in `ensureUnique`. Previously it formatted the warning string even though we weren't tracing, so all that work would *always* happen in the hot path and be for nothing.

A sample of how the new warnings look like:
```
tmp.py:4: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Pytho
n values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  int(x)
tmp.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this fun
ction to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might caus
e the trace to be incorrect.
  torch.tensor([1.])
tmp.py:6: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause t
he trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other ha
nd, if all other views use the same memory, but are disjoint (e.g. are outputs of torch.split), this might still be safe.
  torch.split(y, 2, dim=1)[0].add_(2)

```",pytorch
11560,fritzo,pr,2018-09-12T01:44:14Z,[jit] Ensure most Distribution methods are jittable,"This adds tests in tests/test_distributions.py to ensure that all methods of `Distribution` objects are jittable.

I've replaced a few samplers with jittable versions:
- `.uniform_()` -> `torch.rand()`
- `.exponential_()` -> `-(-torch.rand()).log1p()`
- `.normal_()` -> `torch.normal(torch.zeros(...), torch.ones(...), ...)`

Some jit failures remain, and are marked in test_distributions.py
- `Cauchy` and `HalfCauchy` do not support sampling due to missing `.cauchy_()`
- `Binomial` does not support `.enumerate_support()` due to `arange` ignoring its first arg.
- `MultivariateNormal`, `LowRankMultivariateNormal` do not support `.mean`, `.entropy`

## Questions for reviewers

- [x] Currently some tests fail (I've skipped those) due to unavailability of `aten::uniform` and `aten::cauchy` in the jit. Can someone suggest how to add these? I tried to add declarations to `torch/csrc/ir.cpp` and `torch/csrc/passes/shape_analysis.cpp`, but that resulted in ""Couldn't find operator"" errors.
- [x] There are still lots of `TracerWarning`s that something doesn't match something. I'm not sure whether these are real.",pytorch
11563,JerryShih,pr,2018-09-12T02:38:16Z,Update OpenMP cmake setting for xcode 9 compiler(AppleClang 9.0),"Fix the link OpenMP link error for AppleClang 9.0 compiler.

Built with the following command:
python setup.py build develop

The error message:

```
Undefined symbols for architecture x86_64:
  ""___kmpc_critical"", referenced from:
      _THFloatTensor_addmm in THTensorMath.cpp.o
      _THDoubleTensor_addmm in THTensorMath.cpp.o
      _THByteTensor_addmm in THTensorMath.cpp.o
      _THCharTensor_addmm in THTensorMath.cpp.o
      _THShortTensor_addmm in THTensorMath.cpp.o
      _THIntTensor_addmm in THTensorMath.cpp.o
      _THLongTensor_addmm in THTensorMath.cpp.o
      ...
  ""___kmpc_end_critical"", referenced from:
      _THFloatTensor_addmm in THTensorMath.cpp.o
      _THDoubleTensor_addmm in THTensorMath.cpp.o
      _THByteTensor_addmm in THTensorMath.cpp.o
      _THCharTensor_addmm in THTensorMath.cpp.o
      _THShortTensor_addmm in THTensorMath.cpp.o
      _THIntTensor_addmm in THTensorMath.cpp.o
      _THLongTensor_addmm in THTensorMath.cpp.o
      ...
  ""___kmpc_end_reduce_nowait"", referenced from:
      _.omp_outlined..270 in THTensorMoreMath.cpp.o
      _.omp_outlined..271 in THTensorMoreMath.cpp.o
      _.omp_outlined..273 in THTensorMoreMath.cpp.o
      _.omp_outlined..275 in THTensorMoreMath.cpp.o
      _.omp_outlined..43 in THTensorEvenMoreMath.cpp.o
      _.omp_outlined..44 in THTensorEvenMoreMath.cpp.o
      _.omp_outlined..46 in THTensorEvenMoreMath.cpp.o
      ...
  ""___kmpc_end_serialized_parallel"", referenced from:
      at::native::embedding_renorm_cpu_(at::Tensor&, at::Tensor const&, double, double) in Embedding.cpp.o
      at::native::_embedding_bag_dense_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, bool, long long) in EmbeddingBag.cpp.o
      at::native::softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::log_softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::native::log_softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::TensorIterator::for_each(std::__1::function<void (int, char**, long long const*, long long)> const&) in TensorIterator.cpp.o
      ...
  ""___kmpc_for_static_fini"", referenced from:
      _.omp_outlined..9 in Embedding.cpp.o
      _.omp_outlined. in EmbeddingBag.cpp.o
      _.omp_outlined. in GridSampler.cpp.o
      _.omp_outlined..42 in GridSampler.cpp.o
      _.omp_outlined..44 in GridSampler.cpp.o
      _.omp_outlined..45 in GridSampler.cpp.o
      _.omp_outlined..47 in GridSampler.cpp.o
      ...
  ""___kmpc_for_static_init_4"", referenced from:
      _.omp_outlined. in init.cpp.o
      _.omp_outlined..35 in init.cpp.o
      _.omp_outlined..36 in init.cpp.o
      _.omp_outlined..37 in init.cpp.o
      _.omp_outlined..49 in init.cpp.o
      _.omp_outlined..52 in init.cpp.o
      _.omp_outlined..220 in init.cpp.o
      ...
  ""___kmpc_for_static_init_8"", referenced from:
      _.omp_outlined..9 in Embedding.cpp.o
      _.omp_outlined. in EmbeddingBag.cpp.o
      _.omp_outlined. in GridSampler.cpp.o
      _.omp_outlined..42 in GridSampler.cpp.o
      _.omp_outlined..44 in GridSampler.cpp.o
      _.omp_outlined..45 in GridSampler.cpp.o
      _.omp_outlined..47 in GridSampler.cpp.o
      ...
  ""___kmpc_for_static_init_8u"", referenced from:
      _.omp_outlined..203 in init.cpp.o
      _.omp_outlined..207 in init.cpp.o
      _.omp_outlined..209 in init.cpp.o
      _.omp_outlined..210 in init.cpp.o
  ""___kmpc_fork_call"", referenced from:
      at::native::embedding_dense_backward_cpu(at::Tensor const&, at::Tensor const&, long long, long long, bool) in Embedding.cpp.o
      at::native::embedding_renorm_cpu_(at::Tensor&, at::Tensor const&, double, double) in Embedding.cpp.o
      at::native::_embedding_bag_dense_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, bool, long long) in EmbeddingBag.cpp.o
      at::native::grid_sampler_2d_cpu(at::Tensor const&, at::Tensor const&, long long, long long) in GridSampler.cpp.o
      at::native::grid_sampler_3d_cpu(at::Tensor const&, at::Tensor const&, long long, long long) in GridSampler.cpp.o
      at::native::grid_sampler_2d_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, long long) in GridSampler.cpp.o
      at::native::grid_sampler_3d_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, long long) in GridSampler.cpp.o
      ...
  ""___kmpc_global_thread_num"", referenced from:
      at::native::embedding_renorm_cpu_(at::Tensor&, at::Tensor const&, double, double) in Embedding.cpp.o
      at::native::_embedding_bag_dense_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, bool, long long) in EmbeddingBag.cpp.o
      at::native::softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::log_softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::native::log_softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::TensorIterator::for_each(std::__1::function<void (int, char**, long long const*, long long)> const&) in TensorIterator.cpp.o
      ...
  ""___kmpc_push_num_threads"", referenced from:
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 1, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 1, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      ...
  ""___kmpc_reduce_nowait"", referenced from:
      _.omp_outlined..270 in THTensorMoreMath.cpp.o
      _.omp_outlined..271 in THTensorMoreMath.cpp.o
      _.omp_outlined..273 in THTensorMoreMath.cpp.o
      _.omp_outlined..275 in THTensorMoreMath.cpp.o
      _.omp_outlined..43 in THTensorEvenMoreMath.cpp.o
      _.omp_outlined..44 in THTensorEvenMoreMath.cpp.o
      _.omp_outlined..46 in THTensorEvenMoreMath.cpp.o
      ...
  ""___kmpc_serialized_parallel"", referenced from:
      at::native::embedding_renorm_cpu_(at::Tensor&, at::Tensor const&, double, double) in Embedding.cpp.o
      at::native::_embedding_bag_dense_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long, bool, long long) in EmbeddingBag.cpp.o
      at::native::softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::log_softmax_cpu(at::Tensor const&, long long) in SoftMax.cpp.o
      at::native::softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::native::log_softmax_backward_cpu(at::Tensor const&, at::Tensor const&, long long, at::Tensor const&) in SoftMax.cpp.o
      at::TensorIterator::for_each(std::__1::function<void (int, char**, long long const*, long long)> const&) in TensorIterator.cpp.o
      ...
  ""_omp_get_max_threads"", referenced from:
      _THGetNumThreads in THGeneral.cpp.o
      caffe2::Caffe2SetOpenMPThreads(int*, char***) in init_omp.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 0, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 1, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> >, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 1, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 1, false, float, 1, false, 0>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Transpose<Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::Stride<0, 0> > const>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      void Eigen::internal::parallelize_gemm<true, Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> >, long>(Eigen::internal::gemm_functor<float, long, Eigen::internal::general_matrix_matrix_product<long, float, 0, false, float, 0, false, 0>, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1> const, 0, Eigen::OuterStride<-1> >, Eigen::Map<Eigen::Matrix<float, -1, -1, 0, -1, -1>, 0, Eigen::OuterStride<-1> >, Eigen::internal::gemm_blocking_space<0, float, float, -1, -1, -1, 1, false> > const&, long, long, long, bool) in math_cpu.cc.o
      ...
  ""_omp_get_num_procs"", referenced from:
      _THGetNumCores in THGeneral.cpp.o
  ""_omp_get_num_threads"", referenced from:
      _.omp_outlined. in Embedding.cpp.o
      _.omp_outlined. in SoftMax.cpp.o
      _.omp_outlined..35 in SoftMax.cpp.o
      _.omp_outlined..37 in SoftMax.cpp.o
      _.omp_outlined..38 in SoftMax.cpp.o
      _.omp_outlined..46 in SoftMax.cpp.o
      _.omp_outlined..47 in SoftMax.cpp.o
      ...
  ""_omp_get_thread_num"", referenced from:
      _.omp_outlined. in Embedding.cpp.o
      _.omp_outlined. in SoftMax.cpp.o
      _.omp_outlined..35 in SoftMax.cpp.o
      _.omp_outlined..37 in SoftMax.cpp.o
      _.omp_outlined..38 in SoftMax.cpp.o
      _.omp_outlined..46 in SoftMax.cpp.o
      _.omp_outlined..47 in SoftMax.cpp.o
      ...
  ""_omp_in_parallel"", referenced from:
      _THFloatTensor_copy in THTensorCopy.cpp.o
      _THDoubleTensor_copy in THTensorCopy.cpp.o
      _THByteTensor_copy in THTensorCopy.cpp.o
      _THCharTensor_copy in THTensorCopy.cpp.o
      _THShortTensor_copy in THTensorCopy.cpp.o
      _THIntTensor_copy in THTensorCopy.cpp.o
      _THLongTensor_copy in THTensorCopy.cpp.o
      ...
  ""_omp_set_num_threads"", referenced from:
      _THSetNumThreads in THGeneral.cpp.o
      caffe2::Caffe2SetOpenMPThreads(int*, char***) in init_omp.cc.o
ld: symbol(s) not found for architecture x86_64
```",pytorch
11564,ssnl,pr,2018-09-12T03:00:44Z,Fix py37 warning on using collections.abc.*,fixes #10540 for pytorch code. caffe2 still has a few callsites,pytorch
11571,themightyoarfish,pr,2018-09-12T06:56:57Z,Typo fix in randomness.rst,"""need to be"" -> ""need not be""",pytorch
11577,orionr,pr,2018-09-12T16:28:38Z,[build] Fix libnccl linkage error,Potential continuation of @soumith's fix at https://github.com/pytorch/pytorch/pull/11553,pytorch
11586,apaszke,pr,2018-09-12T17:54:52Z,Implement requires_grad propagation in the JIT,"Previously, we would pretty much assume that all floating point tensors do require grad, which might result in some unnecessary compute.

I don't really like the fact that `TensorType` uses `tensor.is_variable() && tensor.requires_grad()` to infer the value of `requires_grad`, but changing constants to keep variables turns out to be pretty hard. I got halfway there, but it would still need some more work. ",pytorch
11589,neerajprad,pr,2018-09-12T18:25:48Z,Disallow scalar parameters in  Dirichlet and Categorical,"This adds a small check in `Dirichlet` and `Categorical` `__init__` methods to ensure that scalar parameters are not admissible. 

**Motivation**
Currently, `Dirichlet` throws no error when provided with a scalar parameter, but if we `expand` a scalar instance, it inherits the empty event shape from the original instance and gives unexpected results. 

The alternative to this check is to promote `event_shape` to be `torch.Size((1,))` if the original instance was a scalar, but that seems to add a bit more complexity (and changes the behavior of `expand` in that it would affect the `event_shape` as well as the `batch_shape` now). Does this seem reasonable? cc. @alicanb, @fritzo. 

```python
In [4]: d = dist.Dirichlet(torch.tensor(1.))

In [5]: d.sample()
Out[5]: tensor(1.0000)

In [6]: d.log_prob(d.sample())
Out[6]: tensor(0.)

In [7]: e = d.expand([3])

In [8]: e.sample()
Out[8]: tensor([0.3953, 0.1797, 0.4250])  # interpreted as events

In [9]: e.log_prob(e.sample())
Out[9]: tensor(0.6931)  # wrongly summed out

In [10]: e.batch_shape
Out[10]: torch.Size([3])

In [11]: e.event_shape
Out[11]: torch.Size([])  # cannot be empty
```

Additionally, based on review comments, this removes `real_vector` constraint. This was only being used in `MultivariateNormal`, but I am happy to revert this if we want to keep it around for backwards compatibility.",pytorch
11594,ssnl,pr,2018-09-12T19:55:59Z,Skip flaky distributed tests,"context: https://github.com/pytorch/pytorch/issues/11582

cc @pietern @teng-li ",pytorch
11598,ssnl,pr,2018-09-12T21:24:09Z,Add jit doc entry to sidebar,cc @zdevito @apaszke ,pytorch
11599,ssnl,pr,2018-09-12T21:37:52Z,Only join pin_memory_thread after it started,"Same reason as in #11432 .

Example error:
```
Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fa06963cf28>
Traceback (most recent call last):
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 405, in __del__
    self._shutdown_workers()
  File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 401, in _shutdown_workers
    self.pin_memory_thread.join()
AttributeError: '_DataLoaderIter' object has no attribute 'pin_memory_thread'
```",pytorch
11602,ssnl,pr,2018-09-12T22:09:45Z,Remove now unnecessary check in function dtor,cc @resistor ,pytorch
11605,JerryShih,pr,2018-09-12T22:26:28Z,[caffe2] Change the mutex protection scope in AtomicFetchAddOp.,"There is a potential data racing in AtomicFetchAddOp. The Output(n) call includes some memory operations. If we run the AtomicFetchAddOp object simultaneously in different threads, we might hit the problem.

I saw a test failed in ""[caffe2/python/operator_test/atomic_ops_test.py](https://github.com/pytorch/pytorch/blob/d4e05f4e1e276055cd3d3e1a2a1e186e6c6405ee/caffe2/python/operator_test/atomic_ops_test.py#L31)"" for this problem. And I can reproduce locally.
Here is the call stack for the crash:
It calls Output(n) simultaneously in different threads. Then, cause the assertion checking in aten.
```
[I plan_executor.cc:511] Step init took 0.000375997 seconds.
libc++abi.dylib: terminating with uncaught exception of type at::Error: refcount_.load() == 0 ASSERT FAILED at ../aten/src/ATen/core/intrusive_ptr.h:83, please report a bug to PyTorch. Tried to destruct an intrusive_ptr_target that still has intrusive_ptr to it (~intrusive_ptr_target at ../aten/src/ATen/core/intrusive_ptr.h:83)
frame #0: caffe2::TensorImpl::~TensorImpl() + 134 (0x10ffa6386 in libcaffe2.dylib)
frame #1: void caffe2::Blob::Destroy<caffe2::Tensor>(void*) + 91 (0x10ffa5f1b in libcaffe2.dylib)
frame #2: caffe2::Blob::GetMutableTensor(at::DeviceType) + 378 (0x10ffa5e1a in libcaffe2.dylib)
frame #3: caffe2::fb::(anonymous namespace)::AtomicFetchAddOp::RunOnDevice() + 149 (0x1102f7c65 in libcaffe2.dylib)
frame #4: caffe2::Operator<caffe2::CPUContext>::Run(int) + 90 (0x10ff9f8ea in libcaffe2.dylib)
frame #5: caffe2::SimpleNet::Run() + 447 (0x11016215f in libcaffe2.dylib)
frame #6: caffe2::(anonymous namespace)::ExecuteStepRecursive(caffe2::(anonymous namespace)::ExecutionStepWrapper&) + 889 (0x11017efc9 in libcaffe2.dylib)
frame #7: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, caffe2::(anonymous namespace)::ExecuteStepRecursive(caffe2::(anonymous namespace)::ExecutionStepWrapper&)::$_5> >(void*) + 156 (0x11018581c in libcaffe2.dylib)
frame #8: _pthread_body + 340 (0x7fff64bea661 in libsystem_pthread.dylib)
frame #9: _pthread_body + 0 (0x7fff64bea50d in libsystem_pthread.dylib)
frame #10: thread_start + 13 (0x7fff64be9bf9 in libsystem_pthread.dylib)

Process 16280 stopped
* thread #5, stop reason = signal SIGABRT
    frame #0: 0x00007fff64a22b66 libsystem_kernel.dylib`__pthread_kill + 10
libsystem_kernel.dylib`__pthread_kill:
->  0x7fff64a22b66 <+10>: jae    0x7fff64a22b70            ; <+20>
    0x7fff64a22b68 <+12>: movq   %rax, %rdi
    0x7fff64a22b6b <+15>: jmp    0x7fff64a19ae9            ; cerror_nocancel
    0x7fff64a22b70 <+20>: retq
Target 0: (Python) stopped.
```


",pytorch
11607,neerajprad,pr,2018-09-12T23:06:46Z,Adding .expand method for TransformedDistribution," This PR:
 - adds a `.expand` method for `TransformedDistribution` along the lines of #11341.
 - uses this method to simplify `.expand` in distribution classes that subclass off of `TransformedDistribution`.
 - restores testing of `TransformedDistribution` fixtures.
 - fixes some bugs wherein we were not setting certain attributes in the expanded instances, and adds tests for `.mean` and `.variance` which use these attributes.

### Motivation:

There are many cases where users directly use `TransformedDistribution` rather than subclassing off it. In such cases, it seems rather inconvenient to have to write a separate class just to define a `.expand` method. The default implementation should suffice in these cases.

cc. @fritzo, @vishwakftw, @alicanb",pytorch
11609,orionr,pr,2018-09-12T23:17:18Z,Only reference ONNX through onnx_pb.h,"I think this is needed to land https://github.com/onnx/onnx/pull/1407 without CI errors.

cc @mingzhe09088 @houseroad ",pytorch
11634,vishwakftw,pr,2018-09-13T12:44:35Z,Fix issue 10492,"- pass infos vector by reference
- checkErrors takes infos vector by reference
- modified gesv tests to not cause infs or nans sporadically
- also clean up error messages

cc: @zou3519 

Closes #10492


",pytorch
11640,orionr,pr,2018-09-13T15:01:19Z,[easy] Improve pybind11 message,Improving the message based on https://github.com/pytorch/pytorch/issues/11570,pytorch
11646,ssnl,pr,2018-09-13T16:43:34Z,Sphinx is case sensitive,,pytorch
11654,apaszke,pr,2018-09-13T18:15:46Z,Minor JIT improvements,"- Disable addmm fusion. The reason for this is explained in the comment.
- Tiny change in `stack.h` that lets us avoid constructing an unnecessary temporary `IValue` on the (C++) stack (it will only get created on the interpreter stack directly).
- Fixed a correctness issue in requires grad propagation",pytorch
11699,vishwakftw,pr,2018-09-14T13:26:23Z,Fix gesv and gels docs,"Closes #9935 and closes #5431 .

cc: @zou3519 @soumith ",pytorch
11715,ssnl,pr,2018-09-14T19:31:46Z,Add distributed get_backend,"I have no idea how to run distributed tests locally so I'll let CI do this. Hopefully everything still works with `IntEnum`.

cc @mcarilli ",pytorch
11718,ssnl,pr,2018-09-14T21:45:20Z,Prevent raising KeyboardInterrupt in worker,"Current behavior is that each process (main and workers) will print trace from `KeyboardInterrupt`. And the main process will also print 
```
RuntimeError: DataLoader worker (pid 46045) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with nm_workers=0 may give better error trace.
```
due to our SIGCLD handler. ",pytorch
11719,ngimel,pr,2018-09-14T22:06:25Z,dtype option for softmax,"Add dtype argument to softmax/log_softmax functions. 
Computing softmax in fp32 precision is necessary for mixed precision training, and converting output of the previous layer into fp32 and then reading it as fp32 in softmax is expensive, memory and perf-wise, this PR allows one to avoid it. 
For most input data/dtype combinations, input data is converted to dtype and then softmax is computed. If input data is half type and dtype is fp32, kernels with the corresponding template arguments are called. ",pytorch
11729,peterjc123,pr,2018-09-15T08:15:29Z,Fix CUDA 8 build on Windows,"Tested via https://github.com/pytorch/pytorch/pull/11374.
Upstream PR: https://gitlab.kitware.com/cmake/cmake/merge_requests/2391",pytorch
11740,ssnl,pr,2018-09-16T17:21:50Z,Fix empty embedding bag on cuda,Fixes https://github.com/pytorch/pytorch/issues/11739,pytorch
11773,apaszke,pr,2018-09-17T21:27:36Z,Improve autograd profiler performance,"To illustrate the benefits of this commit, I'll use the time/iter I got from one of the JIT benchmarks on my machine.

| Run                                          | Time                    |
|----------------------------------------------|-------------------------|
| No profiler                                  | 45ms                    |
| With profiler                                | 56ms                    |
| Use `clock_gettime` instead of `std::chrono` | 48ms                    |
| Touch all pages on block allocation          | 48ms (less jitter)      |
| Use `const char*` instead of `std::string`   | 47ms (even less jitter) |",pytorch
11781,ngimel,pr,2018-09-17T23:29:32Z,fix half grad assignment,"currently grad assignment for half type fails with a misleading RuntimeError
```
RuntimeError: torch.cuda.sparse.HalfTensor is not enabled.
```",pytorch
11798,vishwakftw,pr,2018-09-18T14:41:41Z,Guard NumPy usage using USE_NUMPY,"All usages of the `ndarray` construct have now been guarded with `USE_NUMPY`. This eliminates the requirement of NumPy while building PyTorch from source.

Fixes #11757 

cc: @apaszke ",pytorch
11799,orionr,pr,2018-09-18T15:14:33Z,[build] Don't build Detectron ops with NO_CAFFE2_OPS=1,cc @apaszke ,pytorch
11801,apaszke,pr,2018-09-18T15:35:18Z,Eliminate no-op adds and muls in peephole pass,Because we emit a lot of them in our symbolic AD. This brings down the backward time of an LSTM I'm testing from 14.2ms to 12.5ms (a 15% improvement).,pytorch
11809,apaszke,pr,2018-09-18T18:32:30Z,Stop moving constants into DifferentiableSubgraphs,"Or even taking them as inputs. This prevents optimizations to happen
either inside the differentiable subgraphs, or in the surrounding graph.

",pytorch
11814,Yangqing,pr,2018-09-18T19:22:20Z,remove protobuf inclusion in core/logging.h,This should not be there since logging does not depend on protobuf.,pytorch
11820,Yangqing,pr,2018-09-18T20:25:36Z,Remove inclusion of caffe2 pb,"Probably not needed, but fwiw.",pytorch
11821,ssnl,pr,2018-09-18T20:27:14Z,Minor data loader doc improvements,,pytorch
11830,ssnl,pr,2018-09-18T21:35:34Z,Rename DistBackend -> Backend,"Also add docs for get_backend, Backend, and reduce_op

fixes #11803 

cc @teng-li @pietern @apaszke ",pytorch
11863,apaszke,pr,2018-09-19T15:48:06Z,Specialize ArgumentSpecs on tuple elements too,"This is pretty important because a common situation of passing LSTM hidden states as a tuple completely trashes performance of a network.

Cleans up all our propagation/undef specialization passes, at a cost of increased complexity of `ArgumentSpec` and `GraphExecutor`. An alternative would be to simply flatten all tuple inputs to a graph ahead of time, but that might just end up being confusing in the future (you never know if you're working with a graph that can have tuple or not).",pytorch
11880,peterjc123,pr,2018-09-20T02:03:20Z,Sync FindCUDA.cmake with upstream cmake repo,Upstream PR: https://gitlab.kitware.com/cmake/cmake/merge_requests/2391/diffs,pytorch
11882,vishwakftw,pr,2018-09-20T02:24:10Z,Free MAGMA queues after use,"This PR is a minor change, just adds a simple `magma_queue_destroy` function to the implementation of `Gesv`.

Also, I have replaced calls for obtaining handles with those already written in ATen.
```
THCState_getCurrentSparseHandle(at::globalContext().getTHCState()) --> getCurrentCUDASparseHandle()
THCState_getCurrentBlasHandle(at::globalContext().getTHCState()) --> getCurrentCUDABlasHandle()
```

cc: @soumith @zou3519 ",pytorch
11903,ssnl,pr,2018-09-20T19:54:10Z,Kill self_ty in TYPE_DERIVED_DEFINITION_NATIVE,This allows us to call the type argument with name other than `self_ty`. @ezyang ,pytorch
11909,apaszke,pr,2018-09-20T21:56:19Z,"Pop stashed IntList in resize_, warn about its usage when tracing.","Differential Revision: D9979595
",pytorch
11910,apaszke,pr,2018-09-20T22:02:58Z,Stop tracing _out overloads,"Summary: They aren't recognized anywhere in the JIT

Differential Revision: D9979968
",pytorch
11919,Yangqing,pr,2018-09-21T01:16:54Z,Consolidate c++11 flag,Seems that we have a huge amount of places setting c++11 manually - this puts it in one place with the new cmake (post 3.1) command.,pytorch
11920,Yangqing,pr,2018-09-21T01:18:55Z,[minor] remove a remaining todo line deletion in THD cmake,TSIA,pytorch
11939,Yangqing,pr,2018-09-21T17:27:41Z,[build][c10] set up c10 scaffolding. Move macros proper first.,,pytorch
11985,ssnl,pr,2018-09-23T02:11:10Z,Prevent hanging in data loader altogether,"Test plan:

Trained 16000 LeNets (total) on MNIST on 160 processes. Previously some of them will either hang during training or during program exiting. Now every process finishes successfully.

```
    # NOTE [ Data Loader Multiprocessing Shutdown Logic ]
    #
    # Preliminary:
    #
    # Our data model looks like this (queues are indicated with curly brackets):
    #
    #                main process                              ||
    #                     |                                    ||
    #               {index_queue}                              ||
    #                     |                                    ||
    #              worker processes                            ||     DATA
    #                     |                                    ||
    #            {worker_result_queue}                         ||     FLOW
    #                     |                                    ||
    #      pin_memory_thread of main process                   ||   DIRECTION
    #                     |                                    ||
    #               {data_queue}                               ||
    #                     |                                    ||
    #                data output                               \/
    #
    # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if
    #      `pin_memory=False`.
    #
    #
    # Terminating multiprocessing logic requires very careful design. In
    # particular, we need to make sure that
    #
    #   1. The iterator gracefully exits the workers when its last reference is
    #      gone.
    #
    #      In this case, the workers should be gracefully exited because the
    #      main process may still need to continue to run, and we want cleaning
    #      up code in the workers to be executed (e.g., releasing GPU memory).
    #      Naturally, we implement the shutdown logic in `__del__` of
    #      DataLoaderIterator.
    #
    #      We delay the discussion on the logic in this case until later.
    #
    #   2. The iterator exits the workers when the problem ends
    #
    #      We set all workers and `pin_memory_thread` to have `daemon=True`.
    #      Doing so means that when the program ends, it shuts the workers down
    #      with a SIGTERM. `pin_memory_thread` will exit too, but by a different
    #      mechanism.
    #
    #      You may ask, why don't we just not set the workers as daemonic, and
    #      gracefully exit using the same logic as we have in `__del__` when the
    #      iterator gets deleted (see 1 above)? The answer requires a bit
    #      understanding of Python multiprocessing design. As of Python 3.7, for
    #      reasons I have yet to understand, in a subprocess, Python runs the
    #      given function (e.g., the `target` argument passed to a `mp.Process`)
    #      using this pattern (unrelated code removed for clarity):
    #
    #          # These are run the sub-process
    #          try:
    #              user_provided_function()
    #          finally:
    #              multiprocessing.util._exit_function()
    #
    #      In `_exit_function`, Python joins all non-daemonic subprocesses of
    #      this process (which is a subprocess of a Python process itself), and
    #      sends SIGTERM to the daemonic ones. Therefore, if a DataLoader is
    #      used in a subprocess (i.e., used in `user_provided_function` above),
    #      and an error is raised containing frames that references the
    #      DataLoaderIter (Python exception traces keeps local objects in
    #      relevant frames alive), workers will be joined in `_exit_function`
    #      before the `__del__` is called (which starts the shutdown logic). And
    #      unfortunately the DataLoaderIter process will hang. E.g., such errors
    #      can be timeout, or arbitrary error if users hold a reference to an
    #      iterator.
    #
    #      For context, `_exit_function` is also registered as an `atexit` call.
    #      So I really don't understand the need to do this in a finally block
    #      The code dates back to 2008 and there is no comment on the original
    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both
    #      the finally block and the `atexit` registration) that explains this.
    #
    #      Another choice is to just shutdown workers with logic in 1 above
    #      whenever we see an error in `next`. This isn't ideal because
    #        a. It prevents users from using try-catch to resume data loading.
    #        b. It doesn't prevent hanging if users have references to the
    #           iterator.
    #
    #   3. All processes exit if any of them die unexpectedly (e.g., error,
    #      SIGKILL).
    #
    #      As shown above, the workers are set as daemonic children of the main
    #      process. However, automatic cleaning-up of such child processes only
    #      happen if the parent process exits gracefully (e.g., SIGTERM). So we
    #      must ensure that each process will exit even the process that should
    #      send/receive data to/from it were killed, i.e.,
    #
    #        a. A process won't hang when getting from a queue.
    #
    #           Even with carefully designed data dependencies (i.e., a `put()`
    #           always corresponding to a `get()`), hanging on `get()` can still
    #           happen when data in queue is corrupted (e.g., due to
    #           `cancel_join_thread` or unexpected exit).
    #
    #           For child exit, we register SIGCHLD handler on main process,
    #           which checks if any of the workers fail in the (Python) handler.
    #           See DataLoader.cpp.
    #
    #           For `.get()` calls where the sender(s) is not the workers, we
    #           guard them with timeouts, and check the status of the sender
    #           when timeout happens:
    #             + in the workers, the `ManagerWatchdog` class checks the main
    #               process status.
    #             + if `pin_memory=True`, when getting from `pin_memory_thread`,
    #               check `pin_memory_thread` status periodically until `.get()`
    #               returns or see that `pin_memory_thread` died.
    #
    #        b. A process won't hang when putting into a queue;
    #
    #           We use `mp.Queue` which has a separate background thread to put
    #           objects. The background thread is usually automatically joined
    #           when the process exits.
    #
    #           However, in case that the receiver has ended abruptly while
    #           reading from the pipe, the join will hang forever. Therefore,
    #           for both `worker_result_queue` (worker -> main process/pin_memory_thread)
    #           and each `index_queue` (main process -> worker), we use
    #           `q.cancel_join_thread()` in sender process before any `q.put` to
    #           prevent this automatic join.
    #
    #           Moreover, having all queues called `cancel_join_thread` makes
    #           implementing graceful shutdown logic in `__del__` much easier.
    #           It won't need to get from any queue, which would also need to be
    #           guarded by periodic status checks.
    #
    #           Note that this may leave corrupted data in the queue, but we
    #           don't care about the data anyways once we are shutting down.
    #
    #
    # Now let's get back to 1:
    #   how we gracefully exit the workers when the last reference to the
    #   iteartor is gone.
    #
    # To achieve this, we implement the following logic along with the design
    # choices mentioned above:
    #
    # [pin_memory_thread] and [worker processes]
    #   When getting from queues,
    #     if get a `None`, exit.
    #     if get anything else or time out, check `done_event`,
    #        if set, keep getting until see the `None`, then exit.
    #        otherwise, process the data.
    #
    # [main process]
    #   In the DataLoader Iter's `__del__`
    #     a. Set `done_event` (shared with `pin_memory_thread` and workers).
    #
    #        Note: from here on, the workers & `pin_memory_thread` may exit at
    #              any time after they receive `None`.
    #
    #     b. Exit `pin_memory_thread`
    #          i.   Put `None` in `worker_result_queue`.
    #          ii.  Join the `pin_memory_thread`.
    #
    #     c. Exit the workers.
    #          i.   Put `None` in each worker's `index_queue`.
    #          ii.  Join the workers.
    #
    #        Note: This has to be after (b) because it may leave corrupted data
    #              in `worker_result_queue`, which `pin_memory_thread` reads
    #              from.
    #
    #   Note: If `pin_memory=False`, there is no `pin_memory_thread` and (b)
    #         can be omitted
    #
    # NB: `done_event`s isn't strictly needed. E.g., we can just check for
    #     `None` from `index_queue`, but it allows us to skip wasting resources
    #     processing indices already in `index_queue` if we are already shutting
    #     down.
```

Original desc:

In `DataLoaderIter` `__del__`, ensure that `None` is sent to `pin_memory_thread` before joining workers.


Trace when interrupted at such a hang:
```
 Exception ignored in: <function _DataLoaderIter.__del__ at 0x7facf66760d0>
 Traceback (most recent call last):
   File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 412, in __del__
     self._shutdown_workers()
   File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 408, in _shutdown_worke

     self.pin_memory_thread.join()
   File ""/private/home/ssnl/miniconda3/lib/python3.7/threading.py"", line 1032, in join
     self._wait_for_tstate_lock()
   File ""/private/home/ssnl/miniconda3/lib/python3.7/threading.py"", line 1048, in _wait_for_tstate_lock
     elif lock.acquire(block, timeout):
 KeyboardInterrupt

```

The 1st commit solves majority of the hang, but uncovers another problem:
```
36: Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f214fa412f0>
36: Traceback (most recent call last):
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 416, in __
del__
36:     self._shutdown_workers()
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 401, in _s
hutdown_workers
36:     self.worker_result_queue.join_thread()
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py"", line 145, in join_thread
36:     self._jointhread()
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/util.py"", line 189, in __call__
36:     res = self._callback(*self._args, **self._kwargs)
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/multiprocessing/queues.py"", line 192, in _finalize_join
36:     thread.join()
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/threading.py"", line 1032, in join
36:     self._wait_for_tstate_lock()
36:   File ""/private/home/ssnl/miniconda3/lib/python3.7/threading.py"", line 1048, in _wait_for_tstate_lock
36:     elif lock.acquire(block, timeout):
36: KeyboardInterrupt
```",pytorch
11992,crcrpar,pr,2018-09-23T14:00:58Z,[PyTorch] make InstanceNorm1d raise an error if the input is 2D,"Resolves #11991 .

Any comment is welcome.",pytorch
12017,syed-ahmed,pr,2018-09-24T16:11:35Z,Miscellaneous updates for CUDA 10,"This PR has some updates related to CUDA 10.

- https://github.com/pytorch/pytorch/commit/c2195e98647fec9d8227ecb85de28f2ca9a8e29a ensures that the repo successfully builts on CUDA 10. Addresses https://github.com/pytorch/pytorch/issues/11888
- https://github.com/pytorch/pytorch/commit/423d8d3524e29d20d9f7298702f8c068f5c8ad46 follows up on the cufft max plan number bug: https://github.com/pytorch/pytorch/issues/11089, which has been fixed in CUDA 10.",pytorch
12019,Yangqing,pr,2018-09-24T18:25:09Z,[c10] Unify all *_EXPORT and *_IMPORT macros across c++ backend,"TSIA. Right now we should basically use C10_EXPORT and C10_IMPORT for explicitly marking dllexport and dllimport, as a continued effort of the C10 unification.

This is a codemod by mechanically doing the following change:

CAFFE2_{EXPORT,IMPORT} -> C10_{EXPORT,IMPORT}
AT_CORE_{EXPORT,IMPORT} -> C10_{EXPORT,IMPORT}
AT_DISABLE_COPY_AND_ASSIGN -> C10_DISABLE_COPY_AND_ASSIGN

",pytorch
12027,orionr,pr,2018-09-24T23:08:01Z,Discover MKLDNN settings in CMake and not setup.py,@yinghai what do you think about this? Can you test?,pytorch
12038,vishwakftw,pr,2018-09-25T04:26:22Z,Fix warnings emitted when testing distributions,"The earlier tests had around 80 warnings, and now there are 6 warnings: these are due to JIT

The changes remove the wrapping of a Tensor by a Tensor constructor, which emits warnings due to the changes in https://github.com/pytorch/pytorch/pull/11061 .

",pytorch
12051,apaszke,pr,2018-09-25T16:01:17Z,Enable tracing of tensor factories with an out argument,,pytorch
12052,apaszke,pr,2018-09-25T16:01:28Z,"Fix ONNX bug, add symbolic for full",,pytorch
12053,orionr,pr,2018-09-25T16:48:03Z,"Unify versions across setup.py, libtorch, and libcaffe2","This unifies our versions across setup.py, libtorch, and libcaffe2. CMake has a default version (bumped to 1.0.0) that can be overridden by setup.py. The versions are also printed as a part of cmake/Summary.cmake to make sure they are correct.

cc @Yangqing @ezyang @soumith @goldsborough @pjh5 ",pytorch
12077,Yangqing,pr,2018-09-26T06:23:46Z,Move registry fully to c10,"This does 6 things:

- add c10/util/Registry.h as the unified registry util
  - cleaned up some APIs such as export condition
- fully remove aten/core/registry.h
- fully remove caffe2/core/registry.h
- remove a bogus aten/registry.h
- unifying all macros
- set up registry testing in c10

Also, an important note that we used to mark the templated Registry class as EXPORT - this should not happen, because one should almost never export a template class. This PR fixes that.",pytorch
12108,syed-ahmed,pr,2018-09-26T18:22:35Z,Use atomicAdd from cuda_fp16 header when building with CUDA 10,"An efficient atomicAdd for halfs has been added in `cuda_fp16.h` in CUDA 10: 
```__CUDA_FP16_DECL__ __half atomicAdd(__half *address, __half val);```

Through this change, PyTorch will be able to utilize efficient atomicAdd when building with CUDA 10.",pytorch
12132,Yangqing,pr,2018-09-27T09:05:53Z,Remove ATen/Error.h and use ATen/core/Error.h instead.,"Summary: TSIA. No code change involved.

Reviewed By: bwasti

Differential Revision: D10083237
",pytorch
12144,Yangqing,pr,2018-09-27T20:11:30Z,[wip] move flags to c10,still influx.,pytorch
12148,fritzo,pr,2018-09-27T22:16:33Z,[distributions] Fix broadcasting bug in StudentT,"This fixes a broadcasting error with the `StudentT` distribution

## Tested

- [x] added a regression test
- [x] strengthened parameter broadcasting tests",pytorch
12158,cclauss,pr,2018-09-28T05:17:50Z,Docs: Change cuda(async) â€”> cuda(non_blocking),"@goldsborough Modify the docs to match the changes made in #4999
",pytorch
12173,apaszke,pr,2018-09-28T18:07:29Z,Add a serial path to backward engine.,"Many graphs execute on a single device only, and our multithreaded
solution is an overkill for them. This commit attempts to detect
that and will use the user thread to execute backward in that case.

Note that there are still many optimizations that can be applied to
the single-threaded path (e.g. it doesn't need mutexes), but I decided
to leave them for the future.

NB: This makes it impossible to trigger a parallel backward (it will be serialized) nested in a serial backward, but I don't think this will ever cause a problem, as nested graphs tend to be simple.

This patch greatly improves stability of our performance in many environments, because the forward and backward thread switching generally plays badly with CPU frequency and turbo heuristics.

Epoch times for word language model:
```
130 ms +- 23 ms (some epochs taking even longer than 150ms)
108 ms +- 4 ms  (max epoch time is 113ms)
```

@colesbury ",pytorch
12176,apaszke,pr,2018-09-28T18:55:37Z,"Improve JIT docs, add a docstring for torch.jit.script",,pytorch
12194,peterjc123,pr,2018-09-29T03:00:08Z,[TEST ONLY] Test MSVC build without ninja,,pytorch
12311,Yangqing,pr,2018-10-04T01:47:27Z,[experimental] Remove CAFFE2_UNIQUE_LONG_TYPEMETA,CAFFE2_UNIQUE_LONG_TYPEMETA has been a tricky variable defined only from cmake - this is an experiment to remove it and see what exact compilers need that one set.,pytorch
12329,Yangqing,pr,2018-10-04T17:06:45Z,[c10] hip minor fix for c10,TSIA,pytorch
12332,Yangqing,pr,2018-10-04T17:26:45Z,revert test/expect files,Linter added newline to the expect files in #12144 . This reverts it.,pytorch
12348,ssnl,pr,2018-10-04T23:34:29Z,Stop warnings on AT_DECLARE_TENSOR_TYPE(.);,"e.g.,
```
â”‚../aten/src/ATen/core/TensorTypeIdRegistration.h:101:43: warning: extra â€˜;â€™ [-Wpedantic]
â”‚ AT_DECLARE_TENSOR_TYPE(SparseCUDATensorId);
```",pytorch
12354,Yangqing,pr,2018-10-05T01:06:04Z,[c10] Move exception to C10,"There are still a few work to be done:

- Temporarily disabled AT_WARN, as we move logging, that can be handled by error logging.
- A few header files are still being plumbed through
- caffe2::EnforceNotMet aliasing is not done yet
- need to unify the macros. See c10/util/Exception.h",pytorch
12356,Yangqing,pr,2018-10-05T01:09:15Z,Remove dangling cmake check for long typemeta,TSIA,pytorch
12379,syed-ahmed,pr,2018-10-05T18:13:51Z,[CUDA 10 fixes] Adds launch bounds for CTC loss kernel,Fixes https://github.com/pytorch/pytorch/issues/12324,pytorch
12380,vishwakftw,pr,2018-10-05T18:29:42Z,[ready] Introduce chain_matmul,"- This was one of the few functions left out from the list of functions in
  NumPy's `linalg` module
- `multi_mm` is particularly useful for DL research, for quick analysis of
  deep linear networks
- Added tests and doc string",pytorch
12400,orionr,pr,2018-10-05T21:28:40Z,Set proper scope on nodes added by JIT,"Summary:
In order to support tensorboardX and other visualization tools, we need to make sure a non-empty scope is set on all nodes added by the JIT. This attempts to do this, but is still a WIP.

This is a new version of https://github.com/pytorch/pytorch/pull/10749

Differential Revision: D10224380
",pytorch
12446,peterjc123,pr,2018-10-08T06:07:09Z,Add the x64 msvc toolchain into PATH,A possible fix for the problem stated in #12410.,pytorch
12451,JerryShih,pr,2018-10-08T09:58:09Z,Remove the protobuf library in pytorch linking list.,"There will be a link error when the caffe2 doesn't use its protobuf under third_party. The pytorch will always link that protobuf. The pytorch doesn't use the protobuf directly. We could remove it from
the list.",pytorch
12453,ssnl,pr,2018-10-08T16:05:19Z,Fix a bunch of warnings in TestNN,,pytorch
12472,benoitsteiner,pr,2018-10-08T23:15:12Z,torch.finfo and torch.iinfo to mimic the numpy equivalent,This pull request intends to provide the functionality requested in https://github.com/pytorch/pytorch/issues/10742 by adding a new torch.finfo and torch.iinfo API.,pytorch
12487,vishwakftw,pr,2018-10-09T17:01:22Z,Expunge torch.utils.trainer.*,cc: @apaszke ,pytorch
12502,ssnl,pr,2018-10-09T21:36:01Z,Differentiable and Non-differentiable Views,"Fixes https://github.com/pytorch/pytorch/issues/11390. 

This PR introduces the idea of non-differentiable views. A non-differentiable is a view that shares storage with the base variable, but gradient should never flow through the view relation. This includes:
1. .detach()
2. Views created when GradMode is disabled
3. Views that are non-differentiable by nature, e.g., `sparse_tensor.indices()` (This is being added in #11253. I base #11253 on this and update the note in that PR.)

See the note in this PR for details.


cc @colesbury @apaszke @gchanan ",pytorch
12517,vishwakftw,pr,2018-10-10T04:50:39Z,compute_uv for SVD,"Adds a `compute_uv` argument that defaults to `True` for optionally computing the singular vectors during SVD.


Closes https://github.com/pytorch/pytorch/issues/12420 .",pytorch
12518,orionr,pr,2018-10-10T05:22:40Z,Support USE_CUDNN for Windows,"Fixes https://github.com/pytorch/pytorch/issues/12495

cc @peterjc123 @mingzhe09088 ",pytorch
12543,orionr,pr,2018-10-10T18:41:22Z,Add USE_FFMPEG flag for setup.py and R2Plus1D,"Needed for https://github.com/facebookresearch/R2Plus1D/pull/46
",pytorch
12548,orionr,pr,2018-10-10T21:32:08Z,DO NOT CHECKIN: Bisect Windows failure,cc @yf225 ,pytorch
12553,syed-ahmed,pr,2018-10-10T23:01:23Z,[CUDA10 fixes] Adds max plan number for CUDA 10 cufft plan cache array,"@SsnL As per your review in https://github.com/pytorch/pytorch/pull/12017/, I added a max plan number for CUDA 10 path. Our internal cuFFT team couldn't suggest a number since the limit depends on host/device memory. That is, a plan allocates some buffers on the device and also creates objects for the plans on the host side. I raised this number to 4x arbitrarily per you suggestion. ",pytorch
12568,orionr,pr,2018-10-11T15:07:40Z,"Back out ""[c10][NFCI] Move jit/type, function_schema, and utils/functional to ATen/core""","Summary:
Second attempt at D10324615

Original commit changeset: b71eeec98dfe
Original commit changeset #2: 1af6400ae0c1

Differential Revision: D10338168
",pytorch
12574,syed-ahmed,pr,2018-10-11T18:19:46Z,[CUDA10 fixes] Update Gloo submodule to resolve __CUDA_DEPRECATED warning,Gloo was updated with `type` usage for cudaPointerAttributes which resolves the `__CUDA_DEPRECATED` warnings in our CUDA 10 CI. This PR brings in that change.,pytorch
12587,ssnl,pr,2018-10-11T23:53:24Z,Try to prevent occasional timeout in test_proper_exit,,pytorch
12600,ngimel,pr,2018-10-12T05:53:19Z,don't copy weight gradients in rnn,"This PR gets rid of unnecessary copy of weight gradients in cudnn rnn. Also removes unnecessary check for  input size when deciding whether to use persistent rnn, and adds doc string explaining when persistent rnn can be used. cc @ezyang ",pytorch
12665,ssnl,pr,2018-10-15T18:39:43Z,Update fft docs for new cache size,Follow up of #12553 ,pytorch
12671,ssnl,pr,2018-10-15T19:56:54Z,Fix SpectralNorm with DataParallel,"There were two problems with SN + DP:

1. In SN, the updated _u vector is saved back to module via a `setattr`. However, in DP, everything is run on a replica, so those updates are lost.
2. In DP, the buffers are broadcast via a `broadcast_coalesced`, so on replicas they are all views. Therefore, the `detach_` call won't work.

Fixes are:
1. Update _u vector in-place so, by the shared storage between 1st replica and the parallelized module, the update is retained
2. Do not call `detach_`.
3. Added comments in SN about the subtlety.
4. Added a note to the DP doc on this particular behavior of DP.

cc @crcrpar @taesung89 @t-vi @yaoshengfu

Fixes https://github.com/pytorch/pytorch/issues/11476",pytorch
12688,Yangqing,pr,2018-10-15T23:09:12Z,[caffe2] minor fix,This seems to be a typo that never got caught - no actual functionality changes.,pytorch
12699,vishwakftw,pr,2018-10-16T05:35:57Z,Rename potrf to cholesky,"This PR performs a renaming of the function `potrf` responsible for the Cholesky
decomposition on positive definite matrices to `cholesky` as NumPy and TF do.

Billing of changes
- make potrf cname for cholesky in Declarations.cwrap
- modify the function names in ATen/core
- modify the function names in Python frontend
- issue warnings when potrf is called to notify users of the change

cc: @zou3519 

",pytorch
12700,ssnl,pr,2018-10-16T07:32:19Z,Simply exit DataLoader when Python is dying,"I struggled with yet another DataLoader hang for the entire evening. After numerous experiments, I realized that it is unsafe to do anything when Python is shutting down. We also unfortunately implement our DataLaoder cleaning-up logic in `__del__`, a function that may or may not be called during shutdown, and if called, may or may not be called before core library resources are freed. 

Fortunately, we are already setting all our workers and pin_memory_thread as daemonic. So in case of Python shutting down, we can just do a no-op in `__del__` and rely on the automatic termination of daemonic children.

An `atexit` hook is used to detect Python exit.",pytorch
12714,Yangqing,pr,2018-10-16T15:57:25Z,Using c10 namespace across caffe2.,"Summary:
This is a short change to enable c10 namespace in caffe2. We did not enable
it before due to gflags global variable confusion, but it should have been
mostly cleaned now. Right now, the plan on record is that namespace caffe2 and
namespace aten will fully be supersets of namespace c10.

Most of the diff is codemod, and only two places of non-codemod is in caffe2/core/common.h, where

```
using namespace c10;
```

is added, and in Flags.h, where instead of creating aliasing variables in c10 namespace, we directly put it in the global namespace to match gflags (and same behavior if gflags is not being built with).

Reviewed By: dzhulgakov

Differential Revision: D10390486
",pytorch
12734,ssnl,pr,2018-10-16T23:01:59Z,Add tensor.to(options),Actual usage coming in #11253 ,pytorch
12747,ssnl,pr,2018-10-17T04:28:12Z,Print grad_fn name when erroring from SavedVariable,"Now that we print `grad_fn`, there is no reason to not print where the error comes from. It helps debugging which in-place op should be changed to out-of-place.",pytorch
12771,orionr,pr,2018-10-17T16:16:04Z,[build] Support cmake3 for 14.04 and CentOS,"Fix https://github.com/caffe2/caffe2.github.io/issues/24

cc @pjh5 @anderspapitto @soumith ",pytorch
12792,Yangqing,pr,2018-10-17T22:38:02Z,remove ATen/Error.h and ATen/core/Error.h,"Summary:
This is a follow up diff after D10238910.

Only non-codemod change is the removal of ATen/Error.h and ATen/core/Error.h. Other files are basically changing the inclusion path + clang format for inclusion order.

Reviewed By: bddppq

Differential Revision: D10437824
",pytorch
12814,crcrpar,pr,2018-10-18T09:51:08Z,fix typo,,pytorch
12847,benoitsteiner,pr,2018-10-19T00:07:14Z,Added a default constructor for torch.finfo.,,pytorch
12850,ssnl,pr,2018-10-19T00:47:55Z,Fix randomness.rst formatting,,pytorch
12853,ssnl,pr,2018-10-19T01:42:00Z,Try to fix randomness.rst formatting again,,pytorch
12864,ssnl,pr,2018-10-19T05:52:20Z,Add DistributedDataParallelCPU to doc,,pytorch
12870,Randl,pr,2018-10-19T11:54:16Z,Rephrase unclear error message for shape mismatch,I spent a couple of minutes trying to understand which shape corresponds to checkpoint and which one to the model,pytorch
12881,Yangqing,pr,2018-10-19T17:39:47Z,Moving logging from caffe2 to c10.,"Summary:
TSIA. This should not change any functionality.

Remaining work:
- change the build script to deprecate use of CAFFE2_USE_MINIMAL_GOOGLE_GLOG and use a C10 macro instead.
- Unify the exception name (EnforceNotMet -> Error)
- Unify the logging and warning APIs (like AT_WARNING)

Reviewed By: dzhulgakov

Differential Revision: D10441597
",pytorch
12897,Yangqing,pr,2018-10-20T00:52:59Z,Remove unsafecoalesce op,"Summary:
UnsafeCoalesce Op is used during memonger days when we try to coalesce operators
for better efficienct computation kernels. It creates a little bit of an unsafe
underlying memory storage pattern.

With the new tensor unification I am not sure if it is still safe for us to do
so, so I propose we delete it for the sake of safety.

Differential Revision: D10475980
",pytorch
12903,ssnl,pr,2018-10-21T01:28:28Z,[jit] Add note on traced module train/eval behavior,,pytorch
12916,ssnl,pr,2018-10-21T19:14:30Z,Use the newer one of cmake and cmake3.,"On my devgpu, `cmake` is newer than `cmake3`. Using `cmake3` causes compilation to fail. Instead of blindly using `cmake3`, we pick the newer of the two.",pytorch
12917,ssnl,pr,2018-10-21T19:57:59Z,Add torch.jit.enabled,Fixes #12907 ,pytorch
12938,Yangqing,pr,2018-10-22T16:11:43Z,Remove CAFFE2_USE_MINIMAL_GOOGLE_GLOG,"Summary:
We will be using C10_USE_MINIMAL_GLOG. Also, this will be in exported flags,
so dependent libraries won't need to define it.

Reviewed By: smessmer, BIT-silence

Differential Revision: D10468993
",pytorch
12958,Yangqing,pr,2018-10-23T00:21:18Z,Remove at::Optional,"Summary: TSIA - this is an ongoing diff to fully move to c10 namespace.

Differential Revision: D10494123
",pytorch
12961,Yangqing,pr,2018-10-23T00:34:19Z,Remove nullopt from native_parse.py,"Summary:
According to zdevito - this is not used at all, so we are removing it for safety.

It is also possible that this native_parser.py will completely go away in the
near future.

Reviewed By: zdevito

Differential Revision: D10501616
",pytorch
12972,orionr,pr,2018-10-23T02:11:42Z,[build] Use cmake3 if it exists and cmake isn't sufficient,"A tweak to https://github.com/pytorch/pytorch/pull/12916 that only uses cmake3 when cmake isn't good enough. Hopefully fixes the issue @zdevito saw.

cc @zdevito @SsnL  ",pytorch
12990,orionr,pr,2018-10-23T17:16:28Z,[build] Move a number of ATen checks out of Dependencies.cmake,cc @Yangqing @mingzhe09088 @anderspapitto @mingzhe09088 ,pytorch
12991,Yangqing,pr,2018-10-23T17:36:50Z,"Back out ""Revert D10494123: [c10] Remove at::Optional""","Summary:
Previous commit missed a file in test/cpp, which did not have a fbcode internal
target and slipped off contbuild. Will export to oss and ensure that things
build this time.

Differential Revision: D10511254
",pytorch
13001,ssnl,pr,2018-10-23T19:29:58Z,[sparse] Autograd indices/values and sparse_coo ctor,Reopen of #11253 after fixing bug in index_select,pytorch
13009,zheng-xq,pr,2018-10-23T22:18:36Z,Save datatype for input/output records to model.,"Summary:
* Save datatype for input/output records to model.
* Make schema.from_column_list support empty list as input argument.

Differential Revision: D10505047
",pytorch
13013,Yangqing,pr,2018-10-23T22:29:54Z,Add Modules_CUDA_Fix folder to installed folder,This is used to patch our cmake cuda scripts - should be in the installation script.,pytorch
13044,kashif,pr,2018-10-24T12:20:03Z,fixed docs for Student-T distribution,"added loc and scale args.

",pytorch
13070,syed-ahmed,pr,2018-10-24T20:04:59Z,Refactor Random Number Generators in ATen,"## Summary:
The purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (`THCRNGState, THCState, THCRandom_Init` etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR does the following:
- Clarifies generator concepts and creates a unified generator class that can serve multiple backends (CPU, CUDA etc.).
- Refactors CUDA kernels with ATen generator logic and removes THC generator code.
- Refactors CPU generator to use `std::mt19937_64` instead of custom mt19937 in TH.
- Removes `curandStateMTGP32` and `curandStatePhilox4_32_10` and replaces with native Philox4_32_10 implementation.
- Misc:
  - Fixes bug in `incr_n` function of Philox engine from fusion compiler (bug also exists currently in curand) where `nhi` wasn't being checked for overflow.
  - Fixes improper scaling of random number to produce uniform distribution in fusion compiler (i.e. random numbers were sampled from `{0-2^24}` and then divided by 2^24 to avoid bias that was caused by dividing with 2^32).
  - Fixes uses of arbitrary increment number for `next_philox_seed` in `Distributions.cu` with proper philox offset calculation.
  - Fixes hardcoded generator related code in several python files used for code generation, such as `function_wrapper.py` etc.
  - Fixes generator front-end python bindings to include device kwarg and removes `default_generator` python module.
  - Removes creation of generator from Types.
  - Updates documentations and comments and adds documentation for `torch.Generator` api.

## Guide for Reviewers
Unfortunately, this PR touches more files than it originally planned on. Hence, following is a small guide on what I think should help you review the ~100 files in this PR.
#### _Definitions_
- **Generator**: An object which manages the state of the algorithm that produces pseudo random numbers. For instance, Generator class in ATen is responsible for handling seeds, creation of generators, etc.
- **Engine**: An object which contains the implementation of the algorithm that produces pseudo random numbers. For instance, for the CPU side, we are using a 64 bit Mersenne Twister (`std::mt19937_64`) and for the CUDA side we are using a Philox Engine (`Philox4_32_10`).
- **CUDA Grids, Blocks etc.**: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy
#### Current State of Generators
##### _What is the deal with https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/GeneratorDerived.h?_
- This template file takes in the `$` variables from `gen.py` to produce `CPUGenerator.h` and `CUDAGenerator.h`. This file forces the two sides (CPU and CUDA) to have the same API, however, we have a contradiction when we pass `THGenerator` as a member only for the CPU side, but it is not present in the CUDAGenerator. Then it begs the question that, if we are to diverge from these two sides to be same, why not modify on two separate files, `CPUGenerator.h` and `CUDAGenerator.h`, rather than modifying these two files through a complicated `gen.py` script. **tl-dr**: I have removed this logic and it turns out we don't need to maintain two Generator headers since, we have defined an engine and a generator to be independent objects.
##### _Generator State vs Generator_
- CPUGenerator in ATen currently has a `THGenerator` object. This `THGenerator` objects are used as inputs to some kernels in `Declarations.cwrap`. In addition, `function_wrapper.py` processes these declarations from what it looks like - swapping `THGenerator` with `Generator`. As a result of this mix-match, `THGenerator` and `CPUGenerator` are basically treated as the same thing (this was also noted somewhere in one of the linked issues).
- `CUDAGenerator` in ATen doesn't have the funky replacement of `THCGenerator` with Generator. Instead, `function_wrapper.py` and `ProcessorSpecificPlugin.py` processes the `Declarations.cwrap` to remove `Generator` input in function signatures for the CUDA side. As a result, `CUDAGenerator.cpp` just makes direct calls to `THCRandom` functions and doesn't really serve a purpose.
- In TH/THC, the design is consistent. That is, for the CPU side, there is `THGenerator` and for the CUDA side, there is `THCGenerator`. Both of these structs have the same definition as how we are defining what a Generator is. In addition, both of these structs only sets/gets things for their engines. Currently TH/THC both uses Mersenne Twister engines by default. TH uses custom implementation of mersenne twister and THC initializes `curandStateMTGP32`. Some kernels in ATen, uses `curandStatePhilox4_32_10` while using the `THCRandom_getGenerator()` to feed the `THCGenerator` and use it in the `curandStatePhilox4_32_10` - (uses seed and philox_seed_offset from that struct). **tl-dr**: In this PR, I have removed all usage of `THCGenerator` and `THGenerator` and tried to improve the symmetry of the design for both sides.
##### _Generator registry, default generators, torch.Generator api and how front-end calls the backend_
- Currently `Context.h` creates a `generator_registry` which is an array of unique pointers to `Generator` objects (the size of which is equivalent the number of `COMPILE_TIME_MAX_DEVICE_TYPES` from `DeviceType.h`. There is one for CUDA, initialized through the `initCUDAGenerator` in `CUDAHooks`, there is one for CPU, initialized in `Context.cpp`. This `generator_registry` is meant to hold singletons, i.e. the default generator which is used by apis like `torch.randn`, `torch.randperm` etc. It is to be noted, there is a need for having default generators if a user intends to use the running state of an engine throughout their application. Currently, a default generator is instantiated in the front end for the CPU side (residing in `torch/csrc/Module.cpp`) and is added as `default_generator` python module, which is then imported in `torch/random.py`. For the cuda side, there is no module like this and we are just calling `THCRandom` functions as methods of the cuda module (torch/csrc/cuda/Module.cpp) which are used in `torch/cuda/random.py`. Lastly, `Generator.h`, `Generator.cpp` in `torch/csrc/` creates a `torch.Generator` module, which gives the user the ability to create a Generator object other than the default generator. The purpose of this api seems to give the user the ability to ""fork"" RNG states. That is, if someone wanted to run their `torch.randn` function without incrementing the state of the `default_generator`, they could do that by using a `torch.Generator` object as an input to the function. **tl-dr**: In this PR, I extended the functionality of the `Generator.cpp` module to have a `device` keyword and `default` keyword, such that creation of default/non-default generators for different device and their backends can happen through the same python module (rather than doing funky `default_generator` module and also releasing a unique pointer through Types in `torch/csrc/Generator.cpp`). 
##### _Philox Offset Calculation_
- `Distributions.cu` uses arbitrary (may be found through experimentation?) numbers like 20 and 10, when incrementing `philox_seed_offset`, however, there is way to calculate this number before launching a kernel, by figuring out the number of elements per thread using the grid size, block size and loop unrolling number (as demonstrated by @ngimel in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu#L108). The kernels in `Distributions.cu` utilize `CUDApplyUtils.cuh` which does loop unrolling through the `step` variable and and tries to ensure that a kernel works on one element per thread. However, for `CUDA_VERSION < 9000`, `CUDApplyUtils.cuh` adjusts `grid_size` to run with full occupancy (check the `grid.x` assignment in that file) and hence, runs a kernel in a grid-stride manner (that is, it's not one element per thread). Hence, an arbitrary philox offset increment could mean that the kernels in `Distribution.cu` might be broken, however, that might not necessarily be the case if the increment is high enough (but then you might ask how high is high enough)? Hence, **tl-dr**: In this PR, I included a `incrementPhiloxOffset` function which calculates an increment number given some inputs and I use this function in all the kernels requiring the offset increment. 
##### _Mersenne Twister concerns_
Mersenne twister engine has 19937 states. [This article](- http://www.pcg-random.org/posts/cpp-seeding-surprises.html) explains why it's a bad idea to seed a mersenne twister with a single 32 bit. **tl-dr**:  In this PR, I have kept the same functionality as was before, seeding a Mersenne twister engine but from the std library. The bias problems of seeding this engine still remains. 

#### Core Files to Review
- `aten/src/ATen/core/Generator.h`: Re-defines backend API for Generators. Unifies CPU and CUDA Generator structs, i.e. `GeneratorState` is equivalent to the union of `THCGenerator` and `THGenerator`.
- `aten/src/ATen/core/Generator.cpp`: Implements Generator.h.
- `aten/src/ATen/cuda/PhiloxRNGEngine.h`: Creates a header file out of Runtian's Philox RNG implementation in fusion compiler. Adds some device functions for common distributions.
- `aten/src/ATen/native/cuda/Distributions.cu`: Corrects `next_philox_seed` usages. Please review grid size calculations, since it is being done differently in `CUDAApplyUtils.cuh`
- `aten/src/ATen/test/cpu_generator_test.cpp`: Adds unit tests and shows usage for using Generators in CPU.
- `aten/src/ATen/test/cuda_generator_test.cpp`: Adds unit tests and shows usage for using Generators in CUDA.
- `torch/src/Generator.cpp`: Extends torch.Generator api to have `device` and `default` keywords.
- `torch/cuda/random.py`: Modifies the front-end cuda random.py with new Generator python module. Keeps functionality same as before, but calls to backend is different.

I have added individual comments to rest of the files in this PR.

The core functionality was verified through PyTorch's test suite. Some tests needed some adjustments with manual seeding and increasing their tolerance values.

Here are some plots of some common distributions after the patch: https://gist.github.com/syed-ahmed/fb580ea89bd0b36572395db61573a20f

Requesting review from: @ezyang @SsnL @mruberry @ngimel @mcarilli and any other code owners.

#### Future work related to this PR:
- **Templatized and CPU version of Philox**: Currently we are utilizing cuda vectors which can't be used in host. Hence, we need a vector in CPU to hold Philox counter and state variables. Moreover, Philox4_32_10 is found to be good in an older GPU generation (GTX 580). We may find new keys and round numbers that may work better for newer GPUs. Hence, having a templatized Philox might give a way to experiment with different parameters of Philox.
- **Do we need mersenne twister for cpu side if there is a Philox cpu implementation?**: While mersenne twister has better statistical properties than Philox, having philox on the cpu side would mean we will get multi threading on the cpu generator. Another issue with mersenne twister is deterministically seeding it. Since, we would want to maintain a `manual_seed` function which takes in 64-bit seed, it makes sense to have an engine which doesn't have as large a state than the `mt19937` (see the bias problem mentioned above). On the design side, if we get rid of mersenne twister on the CPU side, we won't be able to use std random functions and distributions (which might not be a big deal, since TH has most of the random functions we need, already implemented, currently kept as device functions in PhiloxRNGEngine.h) or we could switch to a 64-bit std::random engine, such as knuth's lcg. On the contrary, the philox engine state consists of only two 64-bit keys. Hence, we could also avoid bias problems by using philox cpu engine. 
- **Moving random tensors to ATen**: Use Sam's PR (https://github.com/pytorch/pytorch/pull/10273) as a guide to how to move the tensors and use the new usages from this PR.
- **Implement poisson kernel for CUDA**: Currently the only instance of curand is curand_poisson. Curand poisson uses a combination of three algorithms (knuth, normal approximation and gammainc approximation) to give poisson randoms. Can we find a better way to do this?

#### PR and Issues resolution:
- https://github.com/pytorch/pytorch/issues/11340
- https://github.com/pytorch/pytorch/issues/1614
- https://github.com/pytorch/pytorch/issues/12083
- https://github.com/pytorch/pytorch/issues/13867

#### Helpful background material:
- https://isocpp.org/files/papers/n3551.pdf
- http://www.thesalmons.org/john/random123/papers/random123sc11.pdf
- https://kristerw.blogspot.com/2017/05/seeding-stdmt19937-random-number-engine.html ",pytorch
13098,Yangqing,pr,2018-10-25T02:13:07Z,remove namespace aliasing,"Summary:
As optional.h file move is already done, this diff explicitly removes
the aliasing to enforce the use of ""c10::optional"" in new code, or after
we explicitly do ""using namespace c10"", the use of unnamespaced ""optional"".

Technical reason: for unification we have followed the practice of moving
namespaces along the diffs. Such as D10513246 and D10050771. The enforcement
makes future better engineering easier: all explicit use of c10:: in namespace
caffe2 and namespace at can be mechanically removed, instead of having to do
one-by-one object checks.

Counterargument about possible breakage of dependent oss code: this is not
alone for this diff, as almost all diffs for c10 folder have this side effect.
It is NOT the task for this diff to ensure possible breakage as it is already
by definition going to be in a broken stage. No complaints have been filed,
meaning that this is a moot point. Eventually with the namespace inclusion
of c10 from caffe2 and at space, this concern will be gone.

Reviewed By: teng-li

Differential Revision: D10849120
",pytorch
13100,peterjc123,pr,2018-10-25T03:03:53Z,Fix linking errors on Windows,"1. Removes the flag ""/FORCE:UNRESOLVED"" that shouldn't be used.
2. Fix the code logic for ONNX_BUILD_MAIN_LIBS on Windows
3. Add a patch for protobuf using CMake",pytorch
13140,Yangqing,pr,2018-10-25T21:08:48Z,Fix python2 and python 3 compatibility found by lint.,"Summary:
This is an example about the benefit of proper facebook linter. The old code
was not python 2.x (actually, pre-python 3.3) compatible. Note that FileExistsError
is added in Python 3.3:

https://stackoverflow.com/questions/20790580/python-specifically-handle-file-exists-exception

Differential Revision: D10858804
",pytorch
13141,Yangqing,pr,2018-10-25T21:20:07Z,arc lint torch/utils,"Summary: This is an example diff to show what lint rules are being applied.

Reviewed By: mingzhe09088

Differential Revision: D10858478
",pytorch
13146,ssnl,pr,2018-10-25T22:22:32Z,Add tensor.to(options),"@ezyang on the template hack
@smessmer on SFINAE of the `TensorOptions(Device)`
@goldsborough on the C++ API test changes
@zdevito on the `jit` codegen changes

",pytorch
13167,benoitsteiner,pr,2018-10-26T17:05:25Z,Created a transformer to convertr caffe2 NetDef into ONNX models.,"Reviewed By: abadams

Differential Revision: D11296189
",pytorch
13220,peterjc123,pr,2018-10-28T03:26:45Z,Fix the improper use of windows-native slashes,Trying to fix #12510.,pytorch
13228,apaszke,pr,2018-10-28T22:56:26Z,Re-enabled mm+add tree batching in the JIT,"I've had to generously increase the range of the CreateADSubgraphs pass, because even though it collapses the RNN loop to a single differentiable subgraphs and a few other nodes, the range uses the distances in the original graph...

cc @zdevito @zou3519 ",pytorch
13250,ssnl,pr,2018-10-29T14:34:01Z,Test scripts only run cases defined in the running script,"1. Refactors `TestTorch` into `TestTorchMixin` (subclass of `object`) and `TestTorch` (subclass of `TestCase`, MRO `(TestCase, TestTorchMixin)`, only defined if `__name__ == '__main__'`). So other scripts won't accidentally run it.
2. Adds an assertion in `load_tests` that each script only runs cases defined in itself.

cc @yf225 @ezyang ",pytorch
13256,Yangqing,pr,2018-10-29T16:48:22Z,"move macros to C10, merge AT_ASSERT and AT_ASSERTM to C10_ASSERT and clearly state intention of ASSERT vs ENFORCE","Summary: TSIA. See comment about future plans of ASSERT and explanation.

Differential Revision: D12815769
",pytorch
13306,peterjc123,pr,2018-10-30T12:40:03Z,[WIP] Fix LNK4049 warnings on Windows,,pytorch
13314,ssnl,pr,2018-10-30T16:23:07Z,Make gels error message nicer,,pytorch
13320,Yangqing,pr,2018-10-30T17:16:13Z,Remove net_simple_async,"Summary:
simple_async has been deprecated via the network override rule for a while,
and we should be able to safely remove it.

This also clears up 2 tech debts:
(1) in rnn executor, rely on the executor override to get the right net.
(2) clearly mark checkExecutorOverride as a potential change to net_type by making it c++ style guide compliant.

Differential Revision: D12840709
",pytorch
13350,ssnl,pr,2018-10-30T22:47:04Z,Fix more spectral norm bugs,"Problems with SN and DP after #12671 : 
1. in eval mode, `weight_orig` is not getting correct gradient #12737 .

    Fix: keep `v` vector around as a buffer and always calculate `W = W_orig / (u @ W_orig @ v)` even in eval.

2. in training mode, the `weight` buffer of the parallelized module is never updated, if someone touches `weight_orig` and/or `weight` and makes them not sharing storage. So in `eval` the weight used is wrong.

    Fix: Make `weight` not a buffer anymore and always calculate it as above.

3. #12671 changed SN to update `u` in-place to make DP work correctly, but then it breaks backward through two forwards (e.g., the common GAN loss `D(real) - D(fake)`) because the vectors needed to backprop the 1st forward is changed in the 2nd forward. 

    Fix: This PR clones `u` and `v` before using them.

To maintain BC, I added a hook interface for producing and loading state_dict. This is ugly and we should really have better interface for spectral_norm. But for the purpose to fix this issue, I make this patch. Even if we have a better interface, BC mechanism for legacy loading legacy state_dict still needs to be done.

cc @t-vi @crcrpar ",pytorch
13352,ssnl,pr,2018-10-30T22:53:07Z,Fix autograd with buffers requiring grad in DataParallel,"Causing a problem with spectral norm, although SN won't use that anymore after #13350 .",pytorch
13360,Yangqing,pr,2018-10-31T01:01:15Z,Add back fb logging dependency in caffe2_cpu,"Summary:
This is caught by salexspb - during the last refactor of coping with aten/core
dependency we removed caffe2_cpu_internal, but accidentally dropped support
for fb init (see D9977191). This adds the dependency back for anyone who is
using caffe2_cpu.

Differential Revision: D12853348
",pytorch
13361,Yangqing,pr,2018-10-31T01:03:37Z,"Move Half.{h, cpp} and Half-inl.h to c10","Summary: att

Reviewed By: Yangqing

Differential Revision: D12853472
",pytorch
13370,Yangqing,pr,2018-10-31T04:46:24Z,net_simple_refcount type to help experimentation with dynamic allocation.,"Summary:
This diff adds a new net type (simple_refcount) that does one thing: for all
intermediate results produced by a net, it will keep refcount about internal
usage, and when it finishes its consumption, the net will delete the blob
content to mimic the case of dynamic allocation. In fact, this would also be
the behavior when we go functional: anything that is not explicitly marked as
input or output will be up to the executor for lifetime management.

See the comments in net_simple_refcount.cc for details.

Differential Revision: D12855489
",pytorch
13377,zheng-xq,pr,2018-10-31T07:58:38Z,Enable junk fill for the default CPU allocator,"Summary: Enable junk fill for the default CPU allocator. The first diff only enables this for the tests. A second diff will change the default of zero-fill to false.

Differential Revision: D10866512
",pytorch
13387,apaszke,pr,2018-10-31T14:20:21Z, Stop depending on static analysis of tensor types in graph fuser,"Built on top of #13108, so please review only the last commit.

This makes the graph fuser ignore input types (device/scalar type) when considering graphs for fusion, making it much more robust to shape-prop failures. Those properties are now checked at run time, as part of the kernel validation. This should enable graph fusions in `jit_premul` and `jit_multilayer` timelines in our benchmarks.

One regression is that I've disabled fusions of comparison ops (and `type_as`). That's because there's really no good way to ensure that those are really valid, and are a source of bugs (I filed #13384).

cc @ngimel @mruberry @zdevito @zou3519 ",pytorch
13415,benoitsteiner,pr,2018-10-31T20:22:10Z,Improved the caffe2 to ONNX conversion,"Summary:
Made the SSA transformation idempotent. This ensures that if a caffe2 graph is already in SSA form, the name of the ONNX models inputs/outputs match these of the caffe2 graph.
Avoid evaluating the model by running it if the shapes of all the blobs are given in the value_info map. This makes the conversion a lot faster in this case.wq

Differential Revision: D12871168
",pytorch
13416,ssnl,pr,2018-10-31T20:47:18Z,"Fix pytest, make it work with run_test.py","Fixes #13326 

Also now you can use `run_test.py` with `pytest`. E.g., 
```
python run_test.py -vci distributed -pt
```

Yes it works with `distributed` and `cpp_extension`.

cc @zou3519 @vishwakftw ",pytorch
13419,ssnl,pr,2018-10-31T21:05:13Z,Rename elementwise_mean to mean,Closes #12459 ,pytorch
13421,ssnl,pr,2018-10-31T21:23:39Z,Make gels error message nicer,cc @vishwakftw ,pytorch
13429,benoitsteiner,pr,2018-10-31T21:50:02Z,Improved the caffe2 to ONNX export,"Summary:
Made the SSA transformation idempotent. This ensures that if a caffe2 graph is already in SSA form, the name of the ONNX models inputs/outputs match these of the caffe2 graph.
Avoid evaluating the model by running it if the shapes of all the blobs are present in the value_info map. This speeds up the conversion and decrease its memory usage in the case of medium to large nets.

Differential Revision: D12873354
",pytorch
13453,vishwakftw,pr,2018-11-01T16:49:43Z,[Ready] Make potrs batched,"- This is a straightforward PR, building up on the batch inverse PR, except for one change:
  - The GENERATE_LINALG_HELPER_n_ARGS macro has been removed, since it is not very general and the resulting code is actually not very copy-pasty.

Billing of changes:
- Add batching for `potrs`
- Add relevant tests
- Modify doc string

Minor changes:
- Remove `_gesv_single`, `_getri_single` from `aten_interned_strings.h`.
- Add test for CUDA `potrs` (2D Tensor op)
- Move the batched shape checking to `LinearAlgebraUtils.h`",pytorch
13456,apaszke,pr,2018-11-01T17:17:33Z,Batch more matrix multiplies,"This handles the input pre-multiplication in RNNs, yielding pretty significant speedups in backward times. This pass depends on loop unrolling, so we'll batch only as many elements as the unrolling factor allows.

cc @mruberry @ngimel @zou3519 @zdevito ",pytorch
13466,ssnl,pr,2018-11-01T18:51:33Z,Fix lint,,pytorch
13470,ssnl,pr,2018-11-01T20:02:03Z,requires_grad=False when no_grad(),"This allows native functions to do computation conditioning on whether gradient is needed. 

For example, SVD backward needs U and V tensors, but if we just need the singular values and the decomposed tensor doesn't require grad, we can safely skip computing U and V. Empirically, this speeds up nuclear norm calculation for tenors with `requires_grad=False` a lot.

```py
# CUDA_LAUNCH_BLOCKING=1

x = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cuda()

%timeit -r 100 x.norm('nuc')

# pre patch: 21.3 ms Â± 1.99 ms per loop (mean Â± std. dev. of 100 runs, 1 loop each)
# post patch: 9.77 ms Â± 295 Âµs per loop (mean Â± std. dev. of 100 runs, 1 loop each)

x.requires_grad_()

%timeit -r 100 x.norm('nuc')

# pre patch: 25.4 ms Â± 2.24 ms per loop (mean Â± std. dev. of 100 runs, 1 loop each)
# post patch: 24.9 ms Â± 1.46 ms per loop (mean Â± std. dev. of 100 runs, 1 loop each)

x = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cpu()

%timeit -r 100 x.norm('nuc')

# pre patch: 3.73 ms Â± 396 Âµs per loop (mean Â± std. dev. of 100 runs, 1 loop each)
# post patch: 1.86 ms Â± 52.7 Âµs per loop (mean Â± std. dev. of 100 runs, 1 loop each)

x.requires_grad_()

%timeit -r 100 x.norm('nuc')

# pre patch: 3.3 ms Â± 478 Âµs per loop (mean Â± std. dev. of 100 runs, 1 loop each)
# post patch: 3.55 ms Â± 434 Âµs per loop (mean Â± std. dev. of 100 runs, 1 loop each)
```

",pytorch
13474,ssnl,pr,2018-11-01T20:25:34Z,Fix half_tensor.bernoulli_(double),Fixes https://github.com/pytorch/pytorch/issues/12431,pytorch
13483,ssnl,pr,2018-11-01T22:22:49Z,Fix handling all empty bags in CUDA embedding bag,Fixes https://github.com/pytorch/pytorch/issues/11847,pytorch
13553,apaszke,pr,2018-11-03T23:23:27Z,Append parameters when checking graphs for TorchScript Methods,"Also, add an assertion in the GraphExecutor to make sure we don't
access memory out of bounds.

",pytorch
13594,ssnl,pr,2018-11-05T21:45:42Z, Give broadcast_coalesced tensors different version counters,"In `broadcast_coalesced`, since multiple variables can be ""views"" of a big flattened tensor, they can share the same version counter. However, this base flat tensor is not exposed and they don't share any memory locations, so this is not necessary. Furthermore, it can cause problems, e.g., when two buffers are broadcast together in `DataParallel` and one of them is modified in-place during `forward` but the other is needed in backward, autograd engine will complain.

Fixing the bug discovered at https://github.com/pytorch/pytorch/pull/13350#issuecomment-436011370

edit: This is a very real problem. E.g., consider using Spectral Norm + Batch Norm together.",pytorch
13713,vishwakftw,pr,2018-11-08T09:11:24Z,"Fix torch.dist for infinity, zero and minus infinity norms","Fixes #13559 

cc: @SsnL @zou3519 ",pytorch
13733,orionr,pr,2018-11-08T19:23:42Z,[WIP] Run Caffe2 tests as a part of PyTorch CircleCI,"A new version of https://github.com/pytorch/pytorch/pull/13454 from @pjh5 

Still lots of debugging required.",pytorch
13769,ssnl,pr,2018-11-09T09:42:59Z,Suggest git submodule update --init --recursive,"We now have submodules that have submodules

",pytorch
13780,ssnl,pr,2018-11-09T18:33:03Z,Make USE_SYSTEM_NCCL not a lie,maybe fixes #13764 ,pytorch
13801,LaurentMazare,pr,2018-11-10T01:57:34Z,Apply weight-decay before momentum in the SGD optimizer.,"While trying to understand why two implementations of the same model, one in Python, one using the C++ api (via some [ocaml wrappers](https://github.com/LaurentMazare/ocaml-torch)) did not perform equally well, I noticed that the Python and C++ implementation of SGD slightly differ on weight decay.

- In the [Python version](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L91-L93) weight decay is applied *before* momentum (and so momentum applies to the weight decay).
- In the C++ implementation the weight decay is applied *after* momentum.

In the couple computer-vision models I have looked at the Python version performs a little better so this PR tweaks the C++ implementation to perform weight-decay *before* momentum. This is possibly caused by having more regularization - maybe increasing the weight decay while keeping the current code would hold the same improvements however a nice advantage of this change is to put the C++ and Python version in line. After this change my Python and C++/ocaml models performed similarly when using the same weight-decay parameter. 

Maybe there was some real reason to have weight decay after momentum in the C++ version but I haven't found any.",pytorch
13859,JerryShih,pr,2018-11-12T19:54:02Z,Update int8_simd.h,"If we use clang with sse4 support, we will have the function redefinition
error between [1] and [2]. This patch try to add some checkings to fix this
problem.

I just turn on USE_NATIVE_ARCH with clang, then I hit the redefinition error.

[1]
caffe2/operators/quantized/int8_simd.h
[2]
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_sse.h

",pytorch
14017,vishwakftw,pr,2018-11-15T06:24:20Z,[Ready] Batched cholesky decomposition,"Implements batching for the Cholesky decomposition.

Performance could be improved with a dedicated batched `tril` and `triu` op, which is also impeding autograd operations.

Changes made:
- batching code
- tests in `test_torch.py`, `test_cuda.py` and `test_autograd.py`.
- doc string modification
- autograd modification
- removal of `_batch_potrf` in `MultivariateNormal`.",pytorch
14056,syed-ahmed,pr,2018-11-16T01:36:10Z,Fix CUDA_tensor_apply1 base case,"I got some build errors when modifying the `bernoulli_tensor_cuda_kernel` in my Generator refactor https://github.com/pytorch/pytorch/pull/13070. Turns out the functions signature for `CUDA_tensor_apply1` was a little wrong. This PR fixes it. Following is the code and error I was getting before this patch:

Code:
```
template<typename scalar_t, typename prob_t>
void bernoulli_tensor_cuda_kernel(
    at::Tensor& ret, const at::Tensor& p,
    std::pair<uint64_t, uint64_t> seeds) {
  // The template argument `4` below indicates that we want to operate on four
  // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.
  at::cuda::CUDA_tensor_apply2<scalar_t, prob_t, 4>(
      ret, p,
      [seeds] __device__(scalar_t& v1, const prob_t& p1) {
      at::cuda::Philox4_32_10 engine(
                                seeds.first,
                                blockIdx.x * blockDim.x + threadIdx.x,
                                seeds.second);
      auto x = at::cuda::standard_uniform_distribution(engine);
      assert(0 <= p1 && p1 <= 1);
      v1 = static_cast<scalar_t>(x <= p1);
    }
  );
}
```

Error:
```
ov 15 23:43:03 /var/lib/jenkins/workspace/aten/src/ATen/cuda/CUDAApplyUtils.cuh(236): error: no suitable conversion function from ""const lambda [](uint8_t &)->void"" to ""int"" exists
Nov 15 23:43:03           detected during:
Nov 15 23:43:03             instantiation of ""void at::cuda::<unnamed>::ApplyOp1<Op, scalar, IndexType, ADims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar, IndexType> &, const Op &, int, IndexType, Offsets...) [with Op=lambda [](uint8_t &)->void, scalar=uint8_t, IndexType=unsigned int, ADims=1, remaining_steps=1, Offsets=<>]"" 
Nov 15 23:43:03 (282): here
Nov 15 23:43:03             instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply1<Op,scalar,IndexType,ADims,step>(at::cuda::detail::TensorInfo<scalar, IndexType>, IndexType, Op) [with Op=lambda [](uint8_t &)->void, scalar=uint8_t, IndexType=unsigned int, ADims=1, step=1]"" 
Nov 15 23:43:03 (735): here
Nov 15 23:43:03             instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply1<scalar,step,Op>(at::Tensor, Op, at::cuda::TensorArgType) [with scalar=uint8_t, step=1, Op=lambda [](uint8_t &)->void]"" 
Nov 15 23:43:03 (774): here
Nov 15 23:43:03             instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply1<scalar,Op>(at::Tensor, Op, at::cuda::TensorArgType) [with scalar=uint8_t, Op=lambda [](uint8_t &)->void]"" 
Nov 15 23:43:03 /var/lib/jenkins/workspace/aten/src/ATen/native/cuda/Distributions.cu(118): here
Nov 15 23:43:03             instantiation of ""void <unnamed>::bernoulli_scalar_cuda_kernel<scalar_t>(at::Tensor &, double, std::pair<uint64_t, uint64_t>) [with scalar_t=uint8_t]"" 
Nov 15 23:43:03 /var/lib/jenkins/workspace/aten/src/ATen/native/cuda/Distributions.cu(227): here
Nov 15 23:43:03 
```",pytorch
14083,vishwakftw,pr,2018-11-16T05:29:46Z,Fix randint docs,"Closes #14079

cc: @SsnL ",pytorch
14092,ssnl,pr,2018-11-16T09:06:58Z,Add missing space in stft doc,,pytorch
14156,vishwakftw,pr,2018-11-18T02:00:19Z,Remove debugging code in test_cholesky_batched,"They didn't turn up in my tests because I use pytest which doesn't
print debug statements if the tests pass

cc: @bddppq 
",pytorch
14180,ssnl,pr,2018-11-19T06:02:38Z,c10d Automatically retry on EINTR,"Probably fixes https://github.com/pytorch/pytorch/issues/14170

Actually I probably shouldn't retry all `SYSCHECK` calls. I'll leave to the reviewers to decide.
",pytorch
14182,JerryShih,pr,2018-11-19T07:20:16Z,[pytorch] Remove protobuf dependency in pytorch cmake file.,"Currently, pytorch doesn't dependent on protobuf. So, we don't need to include the protobuf dir in pytorch cmake file.
And if we build caffe2 without custom-protobuf[1], we will have the protobuf mismatched problem.

[1]
https://github.com/pytorch/pytorch/blob/92dbd0219f6fbdb1db105386386ccf92c0758e86/CMakeLists.txt#L65",pytorch
14218,peterjc123,pr,2018-11-20T06:21:07Z, Some minor fixes for Windows build script,"1. Fix execution failure when some of the paths are not defined
2. Users can now optionally override install dir by setting `CMAKE_INSTALL_PREFIX`",pytorch
14324,ssnl,pr,2018-11-22T17:39:50Z,Update cuda.get/set_rng_state doc,"Now that `cuda.get/set_rng_state` accept `device` objects, the default value should be an device object, and doc should mention so.",pytorch
14485,apaszke,pr,2018-11-28T18:21:54Z,Reduce broadcasted inputs in derivative code,"Previously symbolic AD formulas assumed that no broadcasting happened,
and would return gradients of incorrect shapes (possibly leading to
silent errors later).

Fixes a few bugs (known and unknown):
- #11736
- ArgumentSpec didn't compute the input types correctly [(it didn't advance the offset for non-tensor args)](https://github.com/pytorch/pytorch/pull/14485/files#diff-4fd3157a056596aefb8cdf41022a208bR153)
- Symbolic AD could suffer from use after free (dangling pointers in grad map), because [`EliminateDeadCode` could have removed nodes](https://github.com/pytorch/pytorch/pull/14485/files#diff-25d33ad1ed6855684dec79d927ca6142L781) that referenced gradients of certain values.
- Undefined behavior in `aten::size`

During my tests I've also found a few new problems, and I have opened issues for them:
- FusionGroup seems to think that cat nodes broadcast their inputs (#14483)
- `prim::ConstantChunk` derivative formula doesn't handle undefined inputs (#14484)

This patch unfortunately deoptimizes some of our code (Fusion doesn't happen past chunk nodes, and outputs more tensors only because we have to get their size). I know how to fix those issues, but wanted to fix this terrible bug quickly.

cc @zou3519 @zdevito @ngimel ",pytorch
14498,Yangqing,pr,2018-11-28T21:18:46Z,[third_party] Update FP16 to latest master,TSIA - fp16 cmake had a bug that is fixed in https://github.com/Maratyszcza/FP16/pull/9 .,pytorch
14503,apaszke,pr,2018-11-28T22:08:12Z,Broadcast prim::FusedConcat inputs independently when checking kernels,"Fixes #14483.

cc @zou3519 @mruberry ",pytorch
14534,boeddeker,pr,2018-11-29T08:10:22Z,Concatenate directly into shared memory when constructing batches for numpy,"Since #1323 tensors are shared with shared memory, but this feature is not active for numpy.
This PR fix this.",pytorch
14558,apaszke,pr,2018-11-29T19:15:13Z,Reenable all forward-pass fusions that worked before the AD fix,"Dealing with so many `aten::size` calls (in particular calls on elements computed inside fusion groups) requires us to do some extra graph processing in the fuser (to compute the sizes by explicit broadcasts, instead of writing the intermediate tensors only to check their size). This restores the forward expects of LSTM and MiLSTM to a single big kernel. Unfortunately the backward is much harder, because as long as we can't prove that the reductions are unnecessary (or if we can't distribute them over the op), we will not be able to fuse them.",pytorch
14611,ssnl,pr,2018-11-30T08:10:32Z,[DataLoader] Put indices when receiving any batch from worker ...,"... rather than when returning correct batch.

Currently, in `__next__` all out-of-order batches are cached in a dict. New indices (i.e., tasks) are given to workers only when a batch is retrieved from the dict or the workers. So if a needed batch is taking much longer than other outstanding batches, other workers won't be given new indices and will just be idle, until the cached batches are being asked and returned in future `__next__` calls. 

This patch puts indices whenever the main process gets a (possibly out-of-order) batch from workers. This should potentially speed up dataloading with imbalanced workloads.
",pytorch
14612,ssnl,pr,2018-11-30T09:31:37Z,"[DataLoader] add method to get related info (e.g., dataset) in worker  ","This is now based on top of #15331 . Please only review the last commit.

Add `torch.utils.data.get_worker_info`. I'm copying the docstring here
```
    Returns the information about the current
    :class:`~torch.utils.data.DataLoader` iterator worker process.
     When called in a worker, this returns an object guaranteed to have the
    following attributes:

    * :attr:`id`: the current worker id.
    * :attr:`seed`: the random seed set for the current worker. This value is
      determined by main process RNG and the worker id. See
      :class:`torch.utils.data.DataLoader`'s documentation for more details.
    * :attr:`dataset`: the copy of the dataset object in **this** process. Note
      that this will be a different object in a different process than the one
      in main process.

     When called in the main process, this returns ``None``.

     .. note::
        When used in a :attr:`worker_init_fn` passed over to
        :class:`~torch.utils.data.DataLoader`, this method can be useful to
        set up each worker process differently. E.g., the :attr:`worker_init_fn`
        can use the worker ``id`` to configure the ``dataset`` object to only
        read a specific fraction of a sharded dataset.
```

Issue:  #13023, ``More Flexible `worker_init_fn` ``",pytorch
14640,JerryShih,pr,2018-11-30T21:13:12Z,wip: fix openmp link error,Try to fix the openmp link error like https://github.com/pytorch/pytorch/issues/14539.,pytorch
14665,peterjc123,pr,2018-12-01T07:51:32Z,Fix CUDA 8 build on Windows,"Fixes #14663.
Test for CUDA 8 is running here: https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=54",pytorch
14668,ssnl,pr,2018-12-01T10:49:38Z,[DataLoader] Refactor dataloader.py,"As I am working on tasks in https://github.com/pytorch/pytorch/issues/13023, I realized how unreadable the code is because all functions to be run in multiprocessing must be at top global level. Adding more functionalities to `dataloader.py` will only make things worse. 

So in this PR, I refactor `dataloader.py` and move much of it into `data._utils`. E.g., the `_worker_loop` and related methods are now in `data._utils.worker`, signal handling code in `data._utils.signal_handling`, collating code in `data._utils.collate`, etc. This split, IMHO, makes code much clearer. I will base my future changes to DataLoader on top of this.

No functionality is changed, except that  I added `torch._six.queue`.",pytorch
14669,ssnl,pr,2018-12-01T11:04:46Z,[DataLoader] default_collate should collate bool list to byte tensors,"Based on #15331 . Review only the last commit.

Fixes https://github.com/pytorch/pytorch/issues/14507.",pytorch
14700,ssnl,pr,2018-12-03T14:53:14Z,Convert int8 numpy array to CharTensor,"When rewriting `default_collate`, I noticed that `from_numpy` and `as_tensor` and `tensor` all do not work on `np.int8` arrays.",pytorch
14705,ssnl,pr,2018-12-03T15:30:11Z,"[wip, test CI] Add IterableDataset","1. Add `IterableDataset`.
2. Support non batched loading of traditional map-like dataset. This is useful in doing bulk loading.
3. So we have three data loader mods: `Iterable` (newly added), `Map` (newly added), and `MapWithBatchedRead` (old).

    1. `Iterable` if the `dataset` is an instance of `IterableDataset`
    2. `Map` if `batch_size` is `None`
    3. `MapWithBatchedRead` is chosen otherwise.

3. Refactor `DataLoaderIter` into two classes, `_SingleProcessDataLoaderIter` and `_MultiProcessingDataLoaderIter`. Rename some methods to be more generic, e.g., `get_batch` -> `get_data`.
4. Add `torch.utils.data.get_worker_info` which returns worker information in a worker proc (e.g., worker id, dataset obj copy, etc.) and can be used in `IterableDataset.__iter__` and `worker_init_fn` to do per-worker configuration.
5. Add `ChainDataset`, which is the analog of `ConcatDataset` for `IterableDataset`.
6. Add `convert_fn`, which is meant to convert each loaded data into tensors. For the `MapWithBatchedRead` mode, fetched data is first converted using `convert_fn` and then collated. This shouldn't be much slower (if any) than the old approach of only using `collate_fn`.",pytorch
14710,ssnl,pr,2018-12-03T17:54:27Z,Allow converting CharTensor to np arrays,"The other direction of #14700 

cc @soumith ",pytorch
14738,peterjc123,pr,2018-12-04T06:29:27Z,Update magma to 2.4.0 for Windows,,pytorch
14740,ngimel,pr,2018-12-04T07:20:50Z,add scalar comparison operations and rand_like to autodiff,,pytorch
14743,ssnl,pr,2018-12-04T08:05:05Z,Support torch.load with encoding,"Addresses a common compatibility issue when loading Py2 checkpoints in Py3 regarding to bytes.

E.g., 
[1] https://github.com/pytorch/pytorch/issues/5994,
[2] https://github.com/CSAILVision/places365/issues/25,
[3] https://discuss.pytorch.org/t/how-to-load-a-saved-model-trained-on-pytorch-0-3-1-python-2-7-on-pyorch-1-0-python-3-7/31212",pytorch
14752,apaszke,pr,2018-12-04T12:55:15Z,Disable randn_like fusion in the JIT,"Fixes #14674. We won't have time for a proper fix before the release, so at least disable fusion of nodes that trigger incorrect behavior.",pytorch
14758,apaszke,pr,2018-12-04T18:27:21Z,Improvements for symbolic AD,"**Review only the last commit.**

This commit adds a few optimizations to AD, that let us dramatically
reduce the number of sizes we capture from forward.

We now:
- collapse chains of SumToSize
- avoid capturing sizes of tensors that are captured anyway
- more aggressively DCE the reverse code
- run CSE on the primal code to deduplicate `aten::size` calls

cc @zou3519 @zdevito 
",pytorch
14770,apaszke,pr,2018-12-04T20:57:49Z,Use AT_WARN for warnings in the JIT,"Previously their implementation dispatched to prim::Print, which kept
printing the warnings.

",pytorch
14773,apaszke,pr,2018-12-04T21:13:52Z,Don't DCE PythonOp,,pytorch
14800,ssnl,pr,2018-12-05T11:37:12Z,[doc] fix stft arg types,,pytorch
14834,syed-ahmed,pr,2018-12-06T02:21:11Z,[Deprecate THCState] Move cudaDeviceProp to ATen,"This PR moves `deviceProperties` from `THCState` struct to `CUDAContext` in ATen and hence, takes one more step towards removing `THCState`.",pytorch
14837,JerryShih,pr,2018-12-06T02:57:39Z,"Add ""-fopenmp"" in pytorch link setting for non-AppleClang platform only.","The AppleClang compiler complains about the ""fopenmp"" option.
""clang: error: unsupported option '-fopenmp'""

",pytorch
14856,ngimel,pr,2018-12-06T20:46:19Z,use datatype dependent tolerance in data parallel tests,,pytorch
14880,vishwakftw,pr,2018-12-07T06:28:26Z,Expose torch.roll function and method,"Fixes #14859 .

cc: @zou3519 ",pytorch
14898,orionr,pr,2018-12-07T17:37:07Z,Add __init__.py so files get picked up on install,"This will let us install tests and other Caffe2 python code as a part of running Caffe2 tests in PyTorch.

Broken out of https://github.com/pytorch/pytorch/pull/13733/

cc @pjh5 @yf225 ",pytorch
14900,orionr,pr,2018-12-07T17:43:00Z,Add CAFFE2_API to video processing functions,"Extracted from https://github.com/pytorch/pytorch/pull/13733

Some tests were failing because these methods didn't have an export.

cc @pjh5 @yf225 ",pytorch
14920,syed-ahmed,pr,2018-12-07T23:58:00Z,Alignas Array struct,"This PR aligns the Array struct such that cuda vector performance improvements can be utilized.

I tested this by using it on our Philox header. Note how the vector store instruction gets used for cuda vector types and when using alignas on Array, vs when not using alignas on Array.

With cuda vector type (uint4, uint2, float4): https://godbolt.org/z/UaWOmR
With alignas: https://godbolt.org/z/Eeh0t5
Without alignas: https://godbolt.org/z/QT63gq
",pytorch
14929,ssnl,pr,2018-12-08T08:03:23Z,_get_device_index supports parsing device strings,,pytorch
14935,vishwakftw,pr,2018-12-08T15:19:13Z,Remove deprecated info argument in btrifact,As specified in title.,pytorch
14964,vishwakftw,pr,2018-12-09T15:54:37Z,[ready for review] torch.btrifact for tensors with greater than 3 dimensions,"Motivation:
- Earlier, `torch.btrifact` could not handle tensors with greater than 3 dimensions. This is because of the check:
>   AT_CHECK(THTensor_(nDimension)(a) == 3, ""expected 3D tensor, got size: "", a->sizes());

What is in this PR?:
- Move `btrifact` to ATen
- Remove relation to TH/THC. 
- Handle tensors with more than three dimensions
- Tests
- Docs modifications: added a note about the non-pivoting variant.

[blocked due to old magma-cuda binaries]",pytorch
14976,peterjc123,pr,2018-12-10T04:20:41Z,Refactor hotpatch_vars and apply it to libtorch,Fixes #14801.,pytorch
15000,orionr,pr,2018-12-10T17:12:04Z,[build] Install cpp tests when built,"This is broken out of https://github.com/pytorch/pytorch/pull/13733/

We want to install cpp tests so they can ultimately be runnable from that location for Caffe2 tests run from PyTorch builds.

cc @pjh5 @yf225 @anderspapitto ",pytorch
15046,ssnl,pr,2018-12-11T05:57:22Z,Allow converting char tensor to numpy; add [fi]info.min,"https://github.com/pytorch/pytorch/pull/14710 with test fixed.

Also added `finfo.min` and `iinfo.min` to get castable tensors.

cc @soumith ",pytorch
15049,vishwakftw,pr,2018-12-11T06:15:11Z,Fix derivative for mvlgamma,"Fixes #15015.

Added tests to validate derivative.",pytorch
15085,syed-ahmed,pr,2018-12-11T23:56:29Z,Removes THCNumerics usages in RNN.cu,"We don't need THCNumerics here since at::Half can be implicitly converted to float and the cuda math dispatches are handled by `/usr/local/cuda/include/crt/math_functions.hpp` and `cmath`. ATen should be free of THCNumerics after this and when porting kernels from THC, one should not use THCNumerics.

Should close: https://github.com/pytorch/pytorch/issues/11878",pytorch
15109,vishwakftw,pr,2018-12-12T05:23:28Z,Fix bincount for non-contiguous inputs on CPU,Fixes #15058.,pytorch
15139,ngimel,pr,2018-12-12T21:54:48Z,add erf and erfc to fuser/autodiff,,pytorch
15146,apaszke,pr,2018-12-12T23:28:09Z,Add support for batch_norm fusion to the JIT,"We don't support reductions yet, but simply decomposing batch_norm
into a kernel that computes the stats, and the fusing everything else
with ReLU and following pointwise ops provides nice speedups.

Note that this is only limited to inference mode for now, because we
don't support convolutions and batch norm in AD, so the fuser isn't
applied to those parts.

This commit gives us a 7% end-to-end speedup for ResNet50 with batch size 32. Note that this only applies to inference mode at the moment due to lack of AD support for CNN operations (I'll be adding that soon), and not to the standard `torchvision` models, because they use in-place ops which aren't supported by the fuser (we need a way of proving that de-inplacing them is safe).

cc @zou3519 @zdevito @mruberry @ngimel 

",pytorch
15148,brettkoonce,pr,2018-12-13T00:03:05Z,docs: minor spelling tweaks,,pytorch
15155,syed-ahmed,pr,2018-12-13T04:12:56Z,[wip] [Deprecate THCState] Moves p2pAccessEnabled to ATen,"This PR moves `p2pAccessEnabled` table from THCState to ATen, and hence works towards removing THCState. Previously, we were maintaining a lookup table of p2p connections. These connections were established during cuda init in groups of 8. That is, for instance, if you have a 16 gpu system and assuming all devices can access each other via p2p in that system, the current code sets up p2p connections between {0,1,2...7} and {8,9,10,..15} during lazy initialization. if a user want a connection from 1 to 10 let's say, and even though a p2p connection from 1 to 10 might be possible, the current code would not do a P2P memcpy, since Copy.cu reads from the p2pAccessEnabled table established at initialization. 

In the following figure, left is a copy from device 0 to 10, and right is from 0 to 1, which shows that we can't do p2p memcpy from 0 to 10, due to the connections being established in groups of 8. 
![image 1](https://user-images.githubusercontent.com/8906225/49915065-a8eced00-fe48-11e8-8ba1-1ea89d6decdb.png)

The current max number of simultaneous p2p connections that a device can have is 8 and even though if this number was increased in the future, our Copy.cu code would not allow someone from copying to devices outside their groups with P2P memcpy (since we use the conditional p2p_enabled in there which reads from the global p2pAccessEnabled table).

In this PR, rather than maintaining a lookup table, I created a CUDAP2PState struct, which establishes a p2p connection during construction and releases the connection when the connection is no longer needed. I have found that there is no significant overhead of not caching if a connection was established between two devices.",pytorch
15157,peterjc123,pr,2018-12-13T05:55:03Z,Fix the missing caffe2 proto files for Windows,Fixes #15156,pytorch
15165,vishwakftw,pr,2018-12-13T09:59:13Z,Remove _finfo; replace _finfo usage with torch.finfo,"This PR removes the usage of _finfo defined in torch.distributions.utils and changes the call sites
to use torch.finfo instead

cc: @fritzo @neerajprad 
",pytorch
15257,vishwakftw,pr,2018-12-15T06:58:09Z,"[ready for review] Batched upper triangular, lower triangular","Changelog:

- Implements `triu` and `tril` for batches of 2D tensors.
- Remove TH/THC binding for `tril`
- Fix CUDA implementation
- Update docstrings for tril and triu.
- Remove mask-based `triu` and `tril` in cholesky forward and backward.
- Remove batched tril in torch.distributions.utils

Test plan:
- Add tests for tril and triu for CPU and CUDA.

Fixes #15016, fixes #15226 and closes #14071

Acknowledgements:
- Thanks to @t-vi whose implementation I used as a reference.",pytorch
15286,vishwakftw,pr,2018-12-16T16:41:13Z,Make btriunpack work for high dimensional batches and faster than before,"Changelog:
- Optimize btriunpack by using `torch.where` instead of indexing, inplace operations instead of out place operations and avoiding costly permutations by computing the final permutation over a list.

Test plan:
- Added tests for btriunpack in test_torch.py (and a port to test_cuda.py)

This should help unblock testing in #14964 . I created a separate PR so that reviewing can be done efficiently.
",pytorch
15331,ssnl,pr,2018-12-18T05:47:09Z,[DataLoader] Refactor dataloader.py,"Same as #14668, and was approved there. 

@ailzhang , please apply this patch to Horizon's `data_streamer.py`: https://gist.github.com/SsnL/020fdb3d6b7016d81b6ba1d04cc41459 Thank you!

Below is the original description at #14668:

As I am working on tasks in https://github.com/pytorch/pytorch/issues/13023, I realized how unreadable the code is because all functions to be run in multiprocessing must be at top global level. Adding more functionalities to `dataloader.py` will only make things worse. 

So in this PR, I refactor `dataloader.py` and move much of it into `data._utils`. E.g., the `_worker_loop` and related methods are now in `data._utils.worker`, signal handling code in `data._utils.signal_handling`, collating code in `data._utils.collate`, etc. This split, IMHO, makes code much clearer. I will base my future changes to DataLoader on top of this.

No functionality is changed, except that  I added `torch._six.queue`.",pytorch
15333,peterjc123,pr,2018-12-18T06:42:24Z,Fix failed type cast in Windows Debug Build,Fixes #15330 ,pytorch
15334,vishwakftw,pr,2018-12-18T07:07:42Z,Rename potrs to cholesky_solve,"Changelog:
- Renames `potrs` to `cholesky_solve` to remain consistent with Tensorflow and Scipy (not really, they call their function chol_solve)
- Default argument for upper in cholesky_solve is False. This will allow a seamless interface between `cholesky` and `cholesky_solve`, since the `upper` argument in both function are the same.
- Rename all tests
- Create a tentative alias for `cholesky_solve` under the name `potrs`, and add deprecated warning to not promote usage.

Test plan:
- Nothing much, all the tests are there as is. They should pass.

cc: @zou3519 

",pytorch
15340,vfdev-5,pr,2018-12-18T13:14:18Z,Implement 'to' on ScriptModules,"Following #6008
Fixes ""Implement 'to' on ScriptModules #7354""

cc @zdevito ",pytorch
15403,apaszke,pr,2018-12-19T19:18:12Z,Add special ops for BatchNorm symbolic differentiation,"The main problem there is with differentiating batch norm statically
is that we make a lot of complex run-time decisions about the backend
we choose. Then, the autograd derivatives are implemented for every
backend separately, which makes sense, because they might be saving
buffers containing different values. To resolve the issue, the forward
op returns an index of the chosen backend, and the backward function
takes it as an argument, such that it knows how to interpret the buffers.

",pytorch
15404,apaszke,pr,2018-12-19T19:21:03Z,Stop leaving garbage files after running test_jit.py,,pytorch
15425,JerryShih,pr,2018-12-20T06:22:18Z,Add OpenMP library checking for pytorch build.,Raise a FATAL_ERROR if there is no OpenMP for pytorch compiling.,pytorch
15430,vishwakftw,pr,2018-12-20T08:58:08Z,Make argument size checking consistent across CPU and CUDA for torch.gesv,"There is an inconsistency in the size of arguments for gesv, which is fixed in this PR.

Changelog:
- Replicate check in CPU as done for CUDA
- Fix argument ordering (minor) in CUDA checking

Fixes #15328 

cc: @jacobrgardner

",pytorch
15461,syed-ahmed,pr,2018-12-21T01:58:22Z,[CUDA10/Turing fixes] Resolves ptxas warnings when compiling for CUDA_ARCH 750 and a memoryType deprecation warning,"When compiling for `TORCH_CUDA_ARCH_LIST=7.5` we were getting ptxas warnings (https://github.com/pytorch/pytorch/issues/14310). This was because we had some hardcoded values when using launch_bounds in kernels. The maximum number of threads per multiprocessor is 1024 for Turing architecture (7.5) but 2048 for previous architectures. The hardcoded launch_bounds in the kernel were requesting for 2048 threads when compiling for Turing and hence were generating the warning.

This PR adds a macro that checks for the bounds on the launch bounds value supplied. The max number of threads per block across all architectures is 1024. If a user supplies more than 1024, I just clamp it down to 512. Depending on this value, I set the minimum number of blocks per sm. This PR should resolve https://github.com/pytorch/pytorch/issues/14310. The gradient computation being wrong reported in that PR is probably due to the faulty card.",pytorch
15468,vishwakftw,pr,2018-12-21T05:50:30Z,Enable running collect_env.py without building PyTorch,"Closes #15346 

cc: @ezyang @zou3519 ",pytorch
15499,peterjc123,pr,2018-12-22T06:27:44Z,Fix the compare logic in function `overflows` for MSVC,Fixes https://github.com/pytorch/pytorch/issues/15497.,pytorch
15500,peterjc123,pr,2018-12-22T08:03:28Z,Fix the iterator category for torch::data::Iterator,"Try to fix https://github.com/pytorch/pytorch/issues/14410.
Additional info: From this [page](https://stackoverflow.com/questions/14062297/canonical-way-to-define-forward-output-iterator), If we change it into `input_iterator_tag`, it doesn't mean the `output_iterator_tag` is lost.",pytorch
15510,vishwakftw,pr,2018-12-23T17:25:33Z,Remove TH/THC link for gesv,"This PR removes the TH/THC binding for gesv.

Changelog:
- Remove TH/THC binding
- Port single matrix case to ATen
- Enable test_gesv for CUDA as well

Test plan:
- Existing tests should pass. This will indicate if the port/removal is successful.

Stack (lower number to be popped off first):
1. #15430 (merged)
2. #15510
3. #15595",pytorch
15542,apaszke,pr,2018-12-26T14:20:51Z,Support fusion of mutable operators,"This is a next step in making fusions in ResNets possible. The only piece
we're missing apart from my open PRs is the ability to differentiate convolutions, which I'm hoping to implement soon (using a method similar to the one I used for BN).

Note that while the method is valid, the fusions usually will only
happen in traced graphs. That is because while tracing, no matter
how the expression looks like, we will always assign the `Value*` of
the mutable _output_ to the tensor. On the other hand, the code
usually contains expressions like `x.relu_()`, which when scripted
does not overwrite the value of `x` with the return from that call
(which is exactly the same at run-time, but a different `Value*`).
This means that in this case there are no data dependencies that
our fuser could follow, and it will never even consider such a node
for fusion, even though it would result in a nice group.

**tl;dr** The following function is currently perfectly fusable when
traced, but not when scripted:

```python
def f(x):
    y = x * 2
    y.relu_()
    return y * 2
```

cc @suo @zdevito @mruberry 

",pytorch
15560,peterjc123,pr,2018-12-27T05:19:53Z,Make the warning suppression safer,Address the problem introduced in https://github.com/pytorch/pytorch/pull/15499#issuecomment-450038494.,pytorch
15595,vishwakftw,pr,2018-12-28T14:04:37Z,Remove TH/THC link for cholesky,"Changelog:
- Remove TH/THC binding
- Port single matrix case to ATen

Test plan:
- Existing tests should pass. This will indicate if the port/removal is successful.

This is the second in the series of linalg ports, previous ones are listed below:
- #15510 : Remove TH/THC link for gesv (merged)
",pytorch
15606,peterjc123,pr,2018-12-29T07:03:42Z,[WIP] Fix the disabled test in #15578 on Windows,,pytorch
15631,apaszke,pr,2018-12-30T13:57:48Z,Add support for NHWC cuDNN convolutions,"At a cost of relaxing the constraint that our convolution ops always return contiguous inputs. Basically we attempt to infer that the input is in NHWC format, and in that case we return an output in NHWC as well. NHWC is 10-20% faster than NCHW in fp16.",pytorch
15633,apaszke,pr,2018-12-30T18:45:17Z,Simplify cat fusion,"That makes that definition of a ""fusable node"" much simpler,
as we don't need to keep considering whether something has to be an
""exit node"" at every step. The fuser now tries to maximize the
pointwise fusions first, and proceeds to prepending chunks and appending
concats only once a fix point is reached.

This patch not only makes the fuser much simpler to reason about,
making it siginifcantly easier to implement features like SumToSize
fusion, to improve performance of derivative graphs.

cc @zou3519 @mruberry ",pytorch
15653,ngimel,pr,2019-01-02T03:57:09Z,initialize with ident value in global reduction,Fixes #15647. cc @colesbury.,pytorch
15665,ssnl,pr,2019-01-02T16:57:47Z,"[DataLoader] Fix Windows worker exit detection, fix test_proper_exit","Currently, in `test_proper_exit`,
1. we do not kill the correct input `pid` in the `kill_pid` function
https://github.com/pytorch/pytorch/blob/fe15d6a2c231a7bc1b32781217ed336ccf9adff7/test/test_dataloader.py#L325-L329
2. the Windows command that detects process status doesn't actually work
https://github.com/pytorch/pytorch/blob/fe15d6a2c231a7bc1b32781217ed336ccf9adff7/test/test_dataloader.py#L641-L646
3. `worker_error` and `worker_kill` cases (sometimes?) are not tested because the workers may exit naturally due to the pre-fetching mechanism and a too small `dataset size / batch size`.

In this PR, I, in separate commits:
1. Install `psutil` (a python package specifically built for process monitoring) on some CI builds. (Linux builds installation are done in https://github.com/pietern/pytorch-dockerfiles/pull/29 https://github.com/pietern/pytorch-dockerfiles/pull/30  https://github.com/pytorch/ossci-job-dsl/pull/36 and https://github.com/pytorch/pytorch/pull/15795).
2. Rewrite `test_proper_exit` with `psutil` so we 

    1. do not rely on the hacky `is_process_alive` https://github.com/pytorch/pytorch/blob/fe15d6a2c231a7bc1b32781217ed336ccf9adff7/test/test_dataloader.py#L640-L653
   2. increase the #task per worker so `worker_error` and `worker_kill` properly trigger 
   3. test error message content to ensure that the loader exits with correct message corresponding to each exiting scenario.

3. Fix Windows data loader not having any mechanism to detect worker failures.",pytorch
15691,vishwakftw,pr,2019-01-03T04:40:28Z,Remove TH/THC link for cholesky_solve,"Changelog:
- Remove TH/THC binding
- Port single matrix case to ATen

Test plan:
- Existing tests should pass. This will indicate if the port/removal is successful.

This is the third in the series of linalg ports, previous ones are listed below:
- #15510 : Remove TH/THC link for gesv (merged)
- #15595 : Remove TH/THC link for cholesky (merged)

",pytorch
15704,vishwakftw,pr,2019-01-03T17:34:01Z,Add is_floating_point to docs,"Fixes #15700 .

Changelog:

- Expose torch.*.is_floating_point to docs

cc: @zou3519 ",pytorch
15706,syed-ahmed,pr,2019-01-03T18:41:56Z,[WIP] Fixes selection of cuDNN algorithm ,"Work in progress. Will soon update!

Summary from different conversations:

```
Everywhere where just algo is now used, algo+math type will have to be. cdesc will also have to be modified according to the returned math type. cudnnGet* does not return math type, so there are 2 options: 1) keep cudnnGet and assume math type based on data type 2) use cudnnGet*_v7, which will require more code changes.
```
```
So the behavior of `cudnnFindConvolutionForwardAlgorithmEx`  is that it will try all algo's and all math type.  The fastest math type can be different from the input `convDesc->mathType`.

The output value in `perfResults[0]` contains the fastest algo and mathType combination. User needs to update `convDesc->mathType` to reflect this finding.

So, since we are also observing that algo1 with default math actually has quite a bit of speed up in v7.4, the find algo is now correctly returning algo1 instead of 0 with mathtype=0 combination.  But if the user does not update the mathType of convDesc, they will end up with the slowest choice of algo1 + tensor_op.

In the API logs of both versions of pytorch, the last `cudnnConvolutionForward` call is done with `val=CUDNN_TENSOR_OP_MATH`.  This suggests that in both versions, pytorch is doing suboptimal calls.
```

#### Nvprof:
- Before: ![before](https://user-images.githubusercontent.com/8906225/50870515-313daf80-136d-11e9-8f1e-b7c3f90c16bc.png)
- After: ![after](https://user-images.githubusercontent.com/8906225/50870534-3f8bcb80-136d-11e9-83ac-80f9aa05a38a.png)

#### Timing:
- Current pytorch nightly container: 0.006510331630706787
- With this patch: 0.004045917987823487
- Speedup: ~37x

#### Code used in analysis:
```
import torch
import time
import torch.nn as nn
print(""cudnn version"", torch.backends.cudnn.version())

conv = nn.Conv2d(16,256,(3,3),dilation=(2,2), padding=(1,1)).cuda().half()
torch.backends.cudnn.benchmark=True

input = torch.randn(64,16,56,56, device=""cuda"").half()
out=conv(input)
gO=torch.rand_like(out)
torch.cuda.synchronize()

s = time.time()
for i in range(100):
   out = conv(input.detach())
   out.backward(gO)
torch.cuda.synchronize()
e = time.time()
print((e-s)/100)
```",pytorch
15766,ngimel,pr,2019-01-06T05:41:36Z,use all_weights instead of _parameters in _flat_weights in rnn,Fixes #15749 ,pytorch
15769,peterjc123,pr,2019-01-06T07:17:53Z,Enable torch static build on Windows,,pytorch
15795,ssnl,pr,2019-01-07T16:16:12Z,Bump CircleCI docker version to 278,"Just changing the version number doesn't seem to work. I needed to also fix macos brew parallel conflict

should this merge together with https://github.com/pytorch/ossci-job-dsl/pull/36 ?",pytorch
15820,JerryShih,pr,2019-01-08T07:27:24Z,Update the cmake build configuration for AppleClang compiler,This pr try to merge the https://github.com/pytorch/pytorch/pull/11563 again and fix the linking error in https://github.com/pytorch/pytorch/pull/14837.,pytorch
15851,syed-ahmed,pr,2019-01-09T00:33:52Z,Remove support for CUDNN 6,"This PR aims to remove support for cuDNN 6. 

CC: @soumith @ngimel ",pytorch
15868,peterjc123,pr,2019-01-09T08:18:54Z,Unify flags and environmental variable when building LibTorch/PyTorch,Fixes #15858.,pytorch
15873,ssnl,pr,2019-01-09T12:35:29Z,Fix macos build,"macos builds are broken now with the following error:
```
/usr/local/Homebrew/Library/Homebrew/config.rb:39:in `initialize': no implicit conversion of nil into String (TypeError)
	from /usr/local/Homebrew/Library/Homebrew/config.rb:39:in `new'
	from /usr/local/Homebrew/Library/Homebrew/config.rb:39:in `<top (required)>'
	from /usr/local/Homebrew/Library/Homebrew/vendor/portable-ruby/2.3.7/lib/ruby/2.3.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /usr/local/Homebrew/Library/Homebrew/vendor/portable-ruby/2.3.7/lib/ruby/2.3.0/rubygems/core_ext/kernel_require.rb:55:in `require'
	from /usr/local/Homebrew/Library/Homebrew/global.rb:25:in `<top (required)>'
	from /usr/local/Homebrew/Library/Homebrew/brew.rb:13:in `require_relative'
	from /usr/local/Homebrew/Library/Homebrew/brew.rb:13:in `<main>'
Exited with code 1
```

No recent commits look suspicious, and I can even reproduce locally on my macbook, so it might be related to some new `brew` updates. Empirically, calling `brew update` first seems to fix this.


Example error build: https://circleci.com/gh/pytorch/pytorch/534392?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link",pytorch
15876,orionr,pr,2019-01-09T16:04:32Z,Allow for registration after GlobalInit,"Summary: Build changes made it so some .so libraries are now registered after GlobalInit is called. Although this shouldn't be common, it also shouldn't be explicitly excluded. These changes allow for late Caffe2 registration, but also warn in that case.

Differential Revision: D13608186
",pytorch
15878,vishwakftw,pr,2019-01-09T16:31:31Z,Fix log_prob for Gumbel distribution,"Fixes https://github.com/pytorch/pytorch/issues/15681

Changelog:
- Add hard-coded implementation of log_prob

Test plan:
- Existing tests should pass

cc: @fritzo 
There seems to be a precision issue (reported above) in the Gumbel `log_prob` function computed using the transforms, which is why I have hard-coded it.",pytorch
15881,syed-ahmed,pr,2019-01-09T19:30:51Z,Fixes selection of cuDNN algorithm,"#### Summary:
This PR updates the logic for using cudnnGet* and cudnnFind*. Current version of cudnn find and get (v7) returns a pair of best algorithm and the convDesc mathType. While we were using the returned algorithm, we didn't update the mathType. As a result, we ended up with a slow choice of algorithm and math type. Without this patch, we are seeing a 10x regression in group convolutions.

Changelist:
- Changed the template arguments to be `perf_t` instead of `algo_t` to unify cudnnFind and cudnnGet. Both cudnnFind and cudnnGet have the same purpose and hence, it made sense to unify them and get rid of `getAlgorithm`.
- Used cudnnGet*_v7 everywhere cudnnGet* was being used.
- Removed all cudnn6 paths (This PR depends on https://github.com/pytorch/pytorch/pull/15851)

CC: @ngimel @csarofeen for review

#### Nvprof:
- Before: On the left is the timeline for cudnnFind and on the right it is for cudnnGet. In the cudnnFind timeline, the first few kernel calls are for the find algorithm. It is seen here that the algo chosen is not the fastest one (because chosen algo1 is run with mathType 1). ![before](https://user-images.githubusercontent.com/8906225/50870515-313daf80-136d-11e9-8f1e-b7c3f90c16bc.png)
- After: Compared to the timeline above, when the mathType is updated, we see that the fastest algo is chosen indeed. (In 7.4 algo1+mathType 0 is giving better perf) ![after](https://user-images.githubusercontent.com/8906225/50870534-3f8bcb80-136d-11e9-83ac-80f9aa05a38a.png)

#### Timing:
- Current pytorch nightly container: 0.006510331630706787
- With this patch: 0.004045917987823487
- Speedup: ~1.5x

#### Code used in analysis:
```
import torch
import time
import torch.nn as nn
print(""cudnn version"", torch.backends.cudnn.version())

conv = nn.Conv2d(16,256,(3,3),dilation=(2,2), padding=(1,1)).cuda().half()
torch.backends.cudnn.benchmark=True

input = torch.randn(64,16,56,56, device=""cuda"").half()
out=conv(input)
gO=torch.rand_like(out)
torch.cuda.synchronize()

s = time.time()
for i in range(100):
   out = conv(input.detach())
   out.backward(gO)
torch.cuda.synchronize()
e = time.time()
print((e-s)/100)
```",pytorch
15897,apaszke,pr,2019-01-09T23:20:19Z,JIT Batch Norm fusion,"Resubmit of #15146, which has been accidentally reverted.",pytorch
15910,ssnl,pr,2019-01-10T02:13:57Z,Fix lint,,pytorch
15929,Randl,pr,2019-01-10T18:48:03Z,Add backward pass notes for eig() and symeig(),,pytorch
15955,vishwakftw,pr,2019-01-11T03:52:12Z,Add backend checks for batch norm,"Fixes #15826 

Changelog:
- Add backend checks in `batch_norm_cpu` and `batch_norm_cuda`
- Modify check in `checkBackend` to pass on undefined tensors.

cc: @soumith ",pytorch
15962,neerajprad,pr,2019-01-11T15:41:59Z,Fix numerical stability in binomial.log_prob,"This issue was discovered by @fehiepsi in https://github.com/uber/pyro/issues/1706 with the `log_prob` computation for Binomial, ~and can be seen with `torch.float32` when we have a combination of low probability value and high `total_count` - a test is added to capture this (since scipy only uses float64, the comparison is done using relative tolerance).~ 

The problem is in the code that tries to pull out the minimum values amongst the logits (written by me earlier, presumably to avoid numerical instability issues), but it is not needed.

EDIT: After a few attempts, I have been unable to reliably show that the change is more numerically stable, and have removed my previous test which fails on linux. The reason is that the issue manifests itself when `total_count` is high and `probs` is very low. However, the precision of `lgamma` when `total_count` is high is bad enough to wash away any benefits. The justification for this still stands though - (a) simplifies code (removes the unnecessary bit), (b) is no worse than the previous implementation, (c) has better continuity behavior as observed by @fehiepsi in the issue above.



cc. @fehiepsi, @alicanb, @fritzo ",pytorch
15964,ssnl,pr,2019-01-11T17:43:59Z,libshm retry on EINTR,fixes https://github.com/pytorch/pytorch/issues/14314,pytorch
15985,ssnl,pr,2019-01-12T06:07:46Z,Add cuda.reset_max_memory_*,Addresses #15968 ,pytorch
15986,ssnl,pr,2019-01-12T06:32:14Z,Fix c10d checking errno unconditionally,"In #15964, I learned that `errno` is only meaningful if the function call fails. E.g., on some macos, a successful `fork()` sets `errno` to `EINVAL` in child process. This commit changes the `SYSCALL` macro so error checking is only done when an error happens. This means checking whether `rv == -1` for most calls, but is checking `rv == nullptr` for `inet_ntop`.

Now `SYSCALL` accepts a second argument `success_cond`, which should be an expression returning whether the call succeeded. `SYSCHECK_ERR_RETURN_NEG1` is the shorthand for checking if rv is `-1`.

Any suggestion on better macro names is welcomed.",pytorch
15989,peterjc123,pr,2019-01-12T09:09:31Z,Fix static build on Windows,"Tested locally. It could be now be started by running `set EXTRA_CAFFE2_CMAKE_FLAGS= -DTORCH_STATIC=1` before build. If we want to make sure it works, then maybe we should add it into CI.",pytorch
16006,ssnl,pr,2019-01-14T09:16:45Z,Add IS_PYTORCH_CI flag for testing,"Use case:
Some data loader tests rely on `psutil` (a third party lib). So they are guarded by `skipIf`. But we want to always test them on CI envs. With `IS_PYTORCH_CI`, we can raise if `psutil` is not found.",pytorch
16008,peterjc123,pr,2019-01-14T12:54:34Z,Generate PDB files for better debugging on Windows,"1. Unify `build_pytorch_libs.bat`, `setup.py` and `torch/CMakeLists.txt` on the debugging flags with the `CMAKE_BUILD_TYPE` being `Debug`, `Release` and `RelWithDebInfo`.
2. Install PDBs through CMake if they are generated.

Reference:
1. CMake PDB install: https://gitlab.kitware.com/cmake/cmake/issues/18393#note_459199
2. About debugging flags https://stackoverflow.com/a/4662345
3. MSDN page about /DEBUG flag: https://docs.microsoft.com/en-us/cpp/build/reference/debug-generate-debug-info?view=vs-2017
4. MSDN page about /Z{i/I/7}: https://docs.microsoft.com/en-us/cpp/build/reference/z7-zi-zi-debug-information-format?view=vs-2017

Work to do:
- [x] Test the changes work in Release config through this PR
- [ ] <del> Test debug build through https://github.com/pytorch/pytorch/pull/16009 </del>
- [x] Test release build with debugging symbols through #16013

Difficulties:
- [x] Replace /Zi flags with /Z7 (which will be added if DEBUG or RelWithDebInfo is used), as it is not supported by sccache
- [x] Resolve `LINK : fatal error LNK1210: exceeded internal ILK size limit; link with /INCREMENTAL:NO` in the debug build 
- [ ] DEBUG build blocked by a MSVC bug. In order to resolve it, we'll need to update the MSVC in CI: https://developercommunity.visualstudio.com/content/problem/225957/fatal-error-lnk1318-unexpected-pdb-error-ok-0.html",pytorch
16009,peterjc123,pr,2019-01-14T13:05:59Z,[TEST ONLY] Windows Debug Test,,pytorch
16013,peterjc123,pr,2019-01-14T14:07:31Z,[TEST ONLY] RelWithDebInfo Test,,pytorch
16067,vishwakftw,pr,2019-01-16T06:54:27Z,Fix error message formatting in AT_CHECK/AT_ERROR,"Changelog:

- Fix formatting for error messages in prelu, EmbeddingBag, RNN

Fixes https://github.com/pytorch/pytorch/issues/16043
",pytorch
16071,peterjc123,pr,2019-01-16T10:24:13Z,Fix the caffe2_gpu linkage with torch on Windows,"Fixes https://github.com/pytorch/pytorch/issues/15992.
Inspired by https://docs.microsoft.com/en-us/cpp/build/reference/optimization-best-practices?view=vs-2017. But this PR needs to be tested.",pytorch
16073,themightyoarfish,pr,2019-01-16T11:22:41Z,Correct sphinx-note in symeig (wrong indentation),,pytorch
16082,ngimel,pr,2019-01-16T19:49:58Z,update pytorch docker to cuda 10,,pytorch
16087,ngimel,pr,2019-01-16T21:32:22Z,[jit] reenable rand_like fusion when there is no broadcast,"Reenables rand_like fusion if no tensor is broadcasted in the fusion group. This is a sufficient but not necessary condition for fused rand_like to produce correct results, and it has an unpleasant side effect of falling back to non-fused path if rand_like was optimistically included in the fusion group, but there is a broadcast in the fusion group not necessarily related to rand_like. E.g. before this PR, if the network had (biasAdd -> relu -> dropout), fuser could fuse biasAdd and relu, now it will try fusing the whole thing (if dropout is expressed via rand_like) and fall back every time. 
",pytorch
16102,peterjc123,pr,2019-01-17T05:43:53Z,"cherry-pick PRs: 15157, 15333, 15499, 15500, 15560, 16071, 15697",,pytorch
16131,syed-ahmed,pr,2019-01-18T00:31:27Z,Cherry Picks of #15851 and #15881,,pytorch
16133,peterjc123,pr,2019-01-18T01:58:47Z,Cherry-pick #15524 into v1.0.1,"Summary:
Please refer to issue #15919
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16092

Differential Revision: D13712897

Pulled By: soumith

fbshipit-source-id: edcd1ed3504f1fa1af841a1757616382c745958f

",pytorch
16137,vishwakftw,pr,2019-01-18T04:38:30Z,"[v1.0.1] Cherry pick fixes for linalg ops, mvlgamma grad and gumbel log prob",cc: @soumith ,pytorch
16145,ngimel,pr,2019-01-18T07:37:08Z,improve performance of unique with inverse indices,"Partial fix for #15804, only w/o dim. 
For @jcjohnson benchmarking script I'm getting the following results on V100:
Before:
```
unning with N = 10000, M = 10000
cuda (no inverse): 0.98 ms
cpu (no inverse): 0.96 ms
cuda (with inverse): 1.07 ms
cpu (with inverse): 1.76 ms

Running with N = 10000, M = 100000
cuda (no inverse): 0.76 ms
cpu (no inverse): 1.53 ms
cuda (with inverse): 1.23 ms
cpu (with inverse): 3.02 ms

Running with N = 100000, M = 100000
cuda (no inverse): 1.28 ms
cpu (no inverse): 11.22 ms
cuda (with inverse): 69.76 ms
cpu (with inverse): 20.28 ms

Running with N = 100000, M = 1000000
cuda (no inverse): 0.78 ms
cpu (no inverse): 18.78 ms
cuda (with inverse): 133.45 ms
cpu (with inverse): 34.09 ms

Running with N = 500000, M = 500000
cuda (no inverse): 1.43 ms
cpu (no inverse): 61.13 ms
cuda (with inverse): 3315.18 ms
cpu (with inverse): 104.57 ms

Running with N = 500000, M = 5000000
cuda (no inverse): 0.86 ms
cpu (no inverse): 96.44 ms
cuda (with inverse): 5209.93 ms
cpu (with inverse): 176.10 ms
```
After
```
Running with N = 10000, M = 10000
cuda (no inverse): 1.04 ms
cpu (no inverse): 0.94 ms
cuda (with inverse): 0.64 ms
cpu (with inverse): 1.76 ms

Running with N = 10000, M = 100000
cuda (no inverse): 0.77 ms
cpu (no inverse): 1.55 ms
cuda (with inverse): 0.58 ms
cpu (with inverse): 2.79 ms

Running with N = 100000, M = 100000
cuda (no inverse): 1.30 ms
cpu (no inverse): 14.15 ms
cuda (with inverse): 1.63 ms
cpu (with inverse): 20.90 ms

Running with N = 100000, M = 1000000
cuda (no inverse): 0.82 ms
cpu (no inverse): 18.63 ms
cuda (with inverse): 0.61 ms
cpu (with inverse): 33.52 ms

Running with N = 500000, M = 500000
cuda (no inverse): 1.51 ms
cpu (no inverse): 59.81 ms
cuda (with inverse): 1.23 ms
cpu (with inverse): 110.69 ms

Running with N = 500000, M = 5000000
cuda (no inverse): 0.92 ms
cpu (no inverse): 104.26 ms
cuda (with inverse): 0.84 ms
cpu (with inverse): 187.12 ms
```
",pytorch
16157,vishwakftw,pr,2019-01-18T17:14:47Z,"[v1.0.1] Cherry pick #15955, #16067","I missed these in the earlier cherry pick.

cc: @soumith ",pytorch
16249,ssnl,pr,2019-01-23T03:57:46Z,Make test_proper_exit more robust,"1. Improve error message for better debugging info
2. Increase timeout
3. Also apply the windows worker failure detection mechanism on non-Windows platforms, for better robustness

Attempt to fix #14501 

cc @ezyang ",pytorch
16337,ssnl,pr,2019-01-24T22:08:13Z,Fix slogdet sign requiring grad when input requires grad,"The real fix for https://github.com/pytorch/pytorch/issues/15605.

This is sort of BC breaking because now
```py
In [1]: import torch

In [2]: a = torch.randn(3, 3, requires_grad=True)

In [3]: a.slogdet()
Out[3]: (tensor(1.), tensor(0.1356, grad_fn=<SlogdetBackward>))

In [4]: a.slogdet()[0].requires_grad
Out[4]: False
```
while before this patch ` a.slogdet()[0]` requires grad with `grad_fn=<SlogdetBackward>`. But any use of backproping through this value will meet the error in #15605 so I don't think this is a problem.",pytorch
16376,ngimel,pr,2019-01-25T19:13:26Z,add sparse gradients in-place if possible,,pytorch
16384,ssnl,pr,2019-01-25T22:20:50Z,Fix cuFFT plan cache size on CUDA 10 cannot be set to > 4096,Doc doesn't need to be changed. Also clarifies two inaccurate comments.,pytorch
16389,ssnl,pr,2019-01-25T23:16:37Z,Add stack & cat support for CPU Half,"Fixes https://github.com/pytorch/pytorch/issues/6968

Needed for #14705 ",pytorch
16393,syed-ahmed,pr,2019-01-26T00:28:41Z,[Wip] Resolves upsample deprecation warnings in tests,,pytorch
16403,vishwakftw,pr,2019-01-26T11:02:46Z,Switch to CUDA implementation if batch size >= 65536 for affine_grid,"Changelog:

- Append a condition that switches to the native CUDA implementation for affine_grid

Fixes #16365 

cc: @fmassa 

",pytorch
16404,ssnl,pr,2019-01-26T13:54:34Z,gitignore gdb history,,pytorch
16412,peterjc123,pr,2019-01-27T04:34:36Z,Fix issues on Windows brought by #16289,This one needs to be merged ASAP because the CUDA build for Windows is skipped at this time.,pytorch
16420,peterjc123,pr,2019-01-27T14:52:08Z,Don't install PDB for Windows static build of caffe2_observers,Fixes #16292.,pytorch
16423,syed-ahmed,pr,2019-01-27T22:07:59Z,Report the slowest 10 tests when using pytest,This flag is useful in identifying if a test is taking way too long like the ones in the following snippet when running the test suite with pytest. https://github.com/pytorch/pytorch/blob/9757ad35b0b56cf955f294e751de9b437f9bb4ff/test/common_utils.py#L814-L835,pytorch
16425,ssnl,pr,2019-01-28T01:11:14Z,Add torch.backends.openmp.is_available(); fix some cmake messages,"1. add `torch.backends.openmp.is_available()`
2. Improve various `cmake` outputs
3. Fix LDFLAGS not respected by `caffe2_pybind11_state_*` targets
4. Fix `MKL` warning message, and QUIET flag.
5. Fix various typos",pytorch
16426,ssnl,pr,2019-01-28T01:23:27Z,Clear cmake cache when --cmake,"Also, because sometimes we have `CMakeCache.txt` but cmake errored out so I'm adding the existence of `'build.ninja'` as another criterion of rerunning cmake.",pytorch
16427,ssnl,pr,2019-01-28T02:25:53Z,[wip] Make get/set_num_threads always consistent,"Previously, we have this inconsistent behavior:
1. `set_num_threads` is no-op if neither OpenMP nor MKL is used.
2. `get_num_threads` always returns `1` if OpenMP is not used.

This bothers users. E.g., https://discuss.pytorch.org/t/set-num-threads-not-working-in-python3/35306

After this patch, 
1. `set_num_threads` sets OMP and MKL values if they are used, and raises if the new value `!= 1` and neither is used.
2. `get_num_threads` returns a valued based on this list of decreasing priority: OpenMP, MKL, and 1.

So a successful `set_num_threads(X)` always makes following `get_num_threads()` return `X`. 

However, this is BC breaking in two ways:
1. `set_num_threads` now may error. But this only happens if MKL is not used, and our binary always ships with MKL, so this is not a big issue.
2. `get_num_threads` now returns MKL value if OpenMP is not used. I think the previous behavior is a bug because `set_num_threads` still sets MKL value in this case. So I think we should make this change as well.",pytorch
16431,ssnl,pr,2019-01-28T03:20:36Z,[dont review] [test ci] dbg rocm,seeing some weird rocm behavior in https://github.com/pytorch/pytorch/pull/16389. trying to isolate the cause.,pytorch
16441,ssnl,pr,2019-01-28T14:11:19Z,add new build files to gitignore; test that build doesn't leave repo dirty,,pytorch
16451,peterjc123,pr,2019-01-28T17:37:24Z,More windows fixes towards the code refactor,Fixes #16446.,pytorch
16483,syed-ahmed,pr,2019-01-29T03:50:05Z,"Revert ""Fixes selection of cuDNN algorithm (#15881)""","There is a regression in cudnnGet*_v7. This reverts commit 2ebb8fd443d930e1f5d3d933aad81dc4a5c7b7c2.

",pytorch
16484,syed-ahmed,pr,2019-01-29T04:05:01Z,"Revert ""Fixes selection of cuDNN algorithm (#15881)""","There is a regression in cudnnGet*_v7 that causes slowdown in resnet50 training. I am opening a bug with cuDNN team about this. This reverts commit 38374468832e307ca741901870914857a836dd5d.

@ezyang :crying_cat_face: ",pytorch
16489,vishwakftw,pr,2019-01-29T05:51:39Z,Allow list and tuples to be passed as output_size to max_unpool1d,"Changelog:
- Modify concantenation of [1] to a tuple by using cases for list and non-list types.

Test plan:
- Added a test case to compare the results between `list` and `tuple` (here `torch.Size()`)

Fixes https://github.com/pytorch/pytorch/issues/16486
",pytorch
16496,peterjc123,pr,2019-01-29T11:39:00Z,Add some smoke tests for Windows,,pytorch
16545,peterjc123,pr,2019-01-30T05:12:16Z,Enable USE_NINJA in build_pytorch_libs.py if it is in PATH,"It is required to fix the nightly conda builds.
cc @zdevito @ezyang @soumith ",pytorch
16565,ssnl,pr,2019-01-30T17:20:03Z,add new build files to gitignore; test that build does not leave git repo checkout dirty,"These appear when I run
```
MACOSX_DEPLOYMENT_TARGET=10.13 CC=clang CXX=clang++ NO_CUDA=1 NO_DISTRIBUTED=1 BUILD_CAFFE2_OPS=0 DEBUG=1 python3 setup.py develop --cmake
```",pytorch
16583,apaszke,pr,2019-01-30T23:27:58Z,Don't keep unnecessary saved_inputs alive,"Fixes #16577.

This greatly improves memory efficiency of certain ops like Dropout2d. Previously, they were implemented as `input * mask` where mask never requires_grad, but we didn't use that knowledge in forward, and (in case of a in-place dropout) kept input.clone() for the backward, when it would simply get ignored.

This patch tries to address this situation by emitting some guards for stores like this, but only if they are as simple, as checking if a single value requires_grad.

Interestingly, the same optimizations apply to methods like bmm, baddmm, etc., but _not to mm nor addmm_, because of how their derivatives are defined. Apparently they unnecessarily use `mat1` to compute the derivative of `mat1` just to improve the error message in case `mat1` was sparse. I'd like to apply this optimization to that case, but I don't want to loose the nicer error message, so if anyone has any ideas for solutions, please let me know...

Full list of operators affected by this patch:
* _nnpack_spatial_convolution
* addbmm
* addcdiv
* addcmul
* addmv
* addr
* baddbmm
* bmm
* cross
* div
* dot
* fmod
* ger
* index_add_
* mul
* mv
* scatter_add_",pytorch
16604,syed-ahmed,pr,2019-01-31T07:34:12Z,[CPU] Refactor Random Number Generators in ATen - Part 1/n,"This PR originated from https://github.com/pytorch/pytorch/pull/13070.

## Summary:
The purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (`THCRNGState, THCState, THCRandom_Init` etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR is the first part of more refactoring effort surrounding RNGs and only handles the PyTorch front-end and CPU backend for now. It does the following:
- Clarifies generator concepts by reviewing Generator, CPUGenerator and CUDAGenerator classes.
- Moves mt19937 from TH to aten as MT19937RNGEngine.h and also moves distributions from THRandom.cpp to DistributionsHelper.h. Adds PhiloxRNGEngine.h engine and adds unit tests for it.
- Fixes hardcoded generator related code in several python files used for code generation, such as `function_wrapper.py` etc.
- Fixes generator front-end python bindings to include device kwarg and default kwarg
- Removes creation of generator from Types.
- Updates documentations and comments and adds documentation for `torch.Generator` api.",pytorch
16626,ngimel,pr,2019-01-31T22:41:33Z,blacklist fft algorithms for strided dgrad,,pytorch
16657,apaszke,pr,2019-02-01T12:09:32Z,Make tuple checks faster,"As the comment indicates, the issue is only present in some versions of
Python 2, so we should be able to use heavily optimized PyTuple_Check in
most cases, and skip allocation of the strings, and unnecessary lookups
on object's type.

cc @ezyang @zasdfgbnm ",pytorch
16687,vishwakftw,pr,2019-02-02T02:12:08Z,Fix issue with scalars and __rpow__,"Changelog:

- Modify __rpow__ function in tensor.py to adapt to scalars

Test plan:

- Add scalar entry in test_rpow
- Add CUDA port

Fixes #16685 

",pytorch
16735,apaszke,pr,2019-02-04T23:06:03Z,Add support for fusion of half batch norm with float stats,"Fixes #16642.

cc @ngimel ",pytorch
16736,apaszke,pr,2019-02-04T23:06:10Z,Don't throw in operator== for TypeMeta and ScalarType,cc: @ezyang ,pytorch
16772,fritzo,pr,2019-02-05T19:21:38Z,Support multiple inheritance in torch.distributions,"This adds calls to `super().__init__()` in three classes in torch.distributions.

This is needed when `Distribution` and `Transform` objects are used with multiple inheritance, as e.g. combined with `torch.nn.Module`s. For example
```py
class MyModule(torch.distributions.Transform, torch.nn.Module):
    ...
```
cc  @martinjankowiak @esling who have wanted to use this pattern, e.g. in #16756",pytorch
16807,vishwakftw,pr,2019-02-06T17:06:14Z,Remove redundant wrappers in torch.distributions,"Changelog:
- Remove torch.distributions.multivariate_normal._batch_diag : same functionality is provided by torch.diagonal
- Remove torch.distributions.lowrank_multivariate_normal._batch_vector_diag : same functionality is provided by torch.diag_embed

Test plan:
- All existing tests should pass

",pytorch
16844,dlibenzi,pr,2019-02-07T16:44:35Z,Add recognition for XLA device types.,,pytorch
16905,musikisomorphie,pr,2019-02-08T20:21:06Z,Improve the Sparse matrix multiplication computational speed #16187,"Instead of converting coo to csr format of the sparse matrix in the original implementation, in my revision I directly use coo format for sparse dense matrix mutliplication. 
On my linux machine it is 5 times faster than the original code:

```
(original code)
SIZE: 15000 DENSITY: 0.01 DEVICE: cpu
torch: 0.39403 seconds
np:    0.00496674 seconds
torch/np: 79.3338

----------------------------------------

(my update)
SIZE: 15000 DENSITY: 0.01 DEVICE: cpu
torch: 0.0812583 seconds
np:    0.00501871 seconds
torch/np: 16.1911

```

Further code feedback and running time tests are highly welcomed. I will keep revise my code if needed.",pytorch
16913,dlibenzi,pr,2019-02-09T04:22:41Z,"Allow the variable type association to depend on backend+scalar_type,â€¦","When an XLA tensor registration happen, the mapping between typed ID and variable type does not allow",pytorch
16927,dlibenzi,pr,2019-02-10T04:40:56Z,Allow the XLA tensors to transfer to CPU.,,pytorch
16928,dlibenzi,pr,2019-02-10T04:44:24Z,Allow the XLA tensors to transfer to CPU.,,pytorch
16937,dlibenzi,pr,2019-02-10T19:27:03Z,Enable to XLA tensor transfer to CPU to work from the Python side,,pytorch
16946,dlibenzi,pr,2019-02-11T02:19:41Z,Register layout for XLA backend.,,pytorch
16959,ssnl,pr,2019-02-11T16:11:46Z,Fix allow_inf in assertEqual,"@gchanan pointed out in https://github.com/pytorch/pytorch/pull/16389 that `allow_inf` is treating `-inf` and `inf` as equal. This fixes it.

Also fixing #16448 since it's near and 2.1 has released.",pytorch
16961,ssnl,pr,2019-02-11T16:33:44Z,Fix missing CircleCI GPG key,"I'm seeing a bunch of apt gpg key errors on CI with the following message:
```
An error occurred during the signature verification. The repository is not 
updated and the previous index files will be used. GPG error: 
https://packagecloud.io trusty InRelease: The following signatures couldn't 
be verified because the public key is not available: 
NO_PUBKEY 4E6910DFCB68C9CD
```

Most of the times apt will reuse the old cached version, but sometimes this results in a build failure: https://circleci.com/gh/pytorch/pytorch/758366?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link.

This should hopefully fix it.
",pytorch
16964,vishwakftw,pr,2019-02-11T17:45:53Z,Dispatch the correct legacy function for geqrf_out and ormqr_out,"This fixes the segfault.

Changelog:
- Modify the function calls in LegacyDefinitions for `geqrf_out` and `ormqr_out`

Test Plan:
- Modified `test_ormqr` with `out` passed in the function.
- Added a `geqrf` test for CPU and CUDA.

Fixes #16957 

cc: @gchanan 

",pytorch
17011,ngimel,pr,2019-02-12T18:00:19Z,Speed-up adaptive average pooling for the common case of size=1 output,"When adaptive pooling has to produce a single pixel feature map, it is faster to do so by calling .mean(). Backward calls a pretty inefficient cuda kernel with atomics, which becomes ridiculously slow for halfs. For half this PR provides approx 30x speed-up for adaptive average pooling, which results in 30% end-to-end speed-up on senet. Improvements are smaller for float, but still significant (approx 5x). 
Also this PR unifies handling of 3d (no batch dimension) and 4d tensors, using negative dimension indices. 
cc @ezyang for review. ",pytorch
17016,syed-ahmed,pr,2019-02-12T18:50:16Z,blacklist fft algorithms for strided dgrad,Applies https://github.com/pytorch/pytorch/pull/16626 from v1.0.1,pytorch
17045,vishwakftw,pr,2019-02-13T05:21:29Z,Fix mvlgamma doc,"Changelog:
- Fix the constant in the docs

Test plan:
- Successful doc build

cc: @fehiepsi 

",pytorch
17160,boeddeker,pr,2019-02-15T09:56:20Z,Feature request: torch.broadcast_to,"I would like to have `broadcast_to`. Numpy has also a `broadcast_to` (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.broadcast_to.html#numpy.broadcast_to).
`torch.broadcast_tensors` can solve some cases, but `broadcast_to` can solve more cases.

This PR is a suggestion for a python implementation.

Related to https://github.com/pytorch/pytorch/issues/8076 (`broadcast_tensors`).",pytorch
17182,ngimel,pr,2019-02-15T23:56:25Z,Add sparse gradient option to `gather` operation,"This PR allows `gather` to optionally return sparse gradients, as requested in #16329. It also allows to autograd engine to accumulate sparse gradients in place when it is safe to do so. 
I've commented out size.size() check in `SparseTensor.cpp` that also caused #17152, it does not seem to me that check serves a useful purpose, but please correct me if I'm wrong and a better fix is required. 
Motivating example:
For this commonly used label smoothing loss function
```
def label_smoothing_opt(x, target):
    padding_idx = 0
    smoothing = 0.1
    logprobs = torch.nn.functional.log_softmax(x, dim=-1, dtype=torch.float32)
    pad_mask = (target == padding_idx)
    ll_loss = logprobs.gather(dim=-1, index=target.unsqueeze(1), sparse = True).squeeze(1)
    smooth_loss = logprobs.mean(dim=-1)
    loss =  (smoothing - 1.0) * ll_loss - smoothing * smooth_loss
    loss.masked_fill_(pad_mask, 0)
    return loss.sum()
```
backward goes from 12.6 ms with dense gather gradients to 7.3 ms with sparse gradients, for 9K tokens x 30K vocab, which is some single percent end-to-end improvement, and also improvement in peak memory required. 
Shout-out to core devs: adding python-exposed functions with keyword arguments through native_functions.yaml is very easy now!

cc @gchanan @apaszke ",pytorch
17191,peterjc123,pr,2019-02-16T03:29:57Z,Fix dll loading process in newer Python on Windows,Fixes https://github.com/pytorch/pytorch/issues/17051.,pytorch
17192,peterjc123,pr,2019-02-16T03:57:54Z,Fix dll loading process in newer Python on Windows (for 1.0.1),,pytorch
17214,peterjc123,pr,2019-02-17T03:55:29Z,Fix dll loading issue for Caffe2 and Windows (for 1.0.1),,pytorch
17215,peterjc123,pr,2019-02-17T03:56:58Z,Fix dll loading issue for Caffe2 and Windows,,pytorch
17255,ssnl,pr,2019-02-19T16:46:40Z,Improve error message w/ size inference on empty tensors ,,pytorch
17258,bharatr21,pr,2019-02-19T17:07:27Z,[DOC] Fix #17218 by updating documentation,Fix Issue #17218 by updating the corresponding documentation in [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss),pytorch
17259,ssnl,pr,2019-02-19T17:25:12Z,Fix cuda softmax backward with empty input,Fixes #17256 ,pytorch
17293,peterjc123,pr,2019-02-20T04:20:18Z,[TESTING] Debug windows import torch crash,,pytorch
17330,ngimel,pr,2019-02-21T01:09:59Z,fix double backward for half softmax/logsoftmax,"Fix for #17261, @SsnL do you have tests for it in your other PR? If not, I'll add to this. Example from #17261 now does not error out (and same for log_softmax). ",pytorch
17341,peterjc123,pr,2019-02-21T03:58:58Z,Enable MAX_JOBS for using Ninja on Windows,,pytorch
17348,vishwakftw,pr,2019-02-21T09:26:23Z,"Move lerp to ATen, add functionality for tensor weights","Changelog:
- Remove TH/THC bindings
- Add tensor weights for `lerp`
- Modify derivatives appropriately

Test plan:
- Added tests for new functionality in `test_autograd`, `test_torch`, `test_cuda`.

Closes #17207 

cc: @fmassa @SsnL ",pytorch
17358,vishwakftw,pr,2019-02-21T17:17:39Z,Fix reduction='none' in poisson_nll_loss,"Changelog:
- Modify `if` to `elif` in reduction mode comparison
- Add error checking for reduction mode

Test plan:
- Added tests in `test_nn.py`

Fixes https://github.com/pytorch/pytorch/issues/17347.

",pytorch
17363,ssnl,pr,2019-02-21T18:18:43Z,Fix DataParallel(cpu_m).cuda() not working by checking at forward,Fixes #17362 ,pytorch
17390,vishwakftw,pr,2019-02-22T05:34:45Z,USE_ --> BUILD_ for CAFFE2_OPS and TEST,cc: @soumith ,pytorch
17421,vfdev-5,pr,2019-02-22T22:48:46Z,fix code block typo,,pytorch
17435,vishwakftw,pr,2019-02-23T18:58:32Z,Remove deprecated outplace init methods in init.py,"Changelog:
- Remove deprecated methods in torch/nn/init.py
- Remove corresponding tests

Closes #17429.
",pytorch
17440,musikisomorphie,pr,2019-02-24T06:58:54Z,fix different round behavior on CPU and GPU   #16498,"Convert gpu round behavior to half-to-even, consistent with torch cpu version and numpy.",pytorch
17442,musikisomorphie,pr,2019-02-24T07:45:50Z,convert cuda round to half to even,,pytorch
17443,musikisomorphie,pr,2019-02-24T07:56:11Z,[WIP] fix different round behavior on CPU and GPU #16498,"@xxtemp, @colesbury, @bhushan23, @zou3519,  convert gpu round behavior to half-to-even, consistent with torch cpu version and numpy. You feedback are welcomed.
See #16498",pytorch
17488,neerajprad,pr,2019-02-26T01:25:39Z,Fix underflow issue with dirichlet sample,"Addresses #15738, using @fritzo's suggestion. This adds a `torch._sample_dirichlet` method in `Distributions.cpp` and `Distributions.cu`. 
 - For CPU, this leads to no perf hit since all we do is to promote the `alpha` to double when getting the gamma samples (the gamma sampler anyways uses `accscalar_t`(double for CPU)) and cast it back to float32 on return.
 - I have added an analogous method for CUDA as well, but the default sampler for CUDA uses scalar_t for efficiency, so I have kept it as that. With this, I do not see the bias towards 1 as reported in #15738 with `float32`, but there is a spurious mode at 0.5, as would be expected. Users would need to explicitly use `float64` for GPU to not see the spurious mode at 0.5. (EDIT: see note below, it appears that the bias issue is still there for certain builds).

Added some tests and checked that there is no perf regression. My experience with C++ is very limited, so apologies in advance if I missed something basic. cc. @ailzhang, @fritzo, @fmassa ",pytorch
17494,peterjc123,pr,2019-02-26T05:41:25Z,Fix linking errors when building dataloader test binaries on Windows,Fixes #17489.,pytorch
17559,dlibenzi,pr,2019-02-28T00:59:15Z,Allow users to build in DEBUG=1 mode without getting:,"lib/libcaffe2.so: undefined reference to `c10::Half::from_bits'

",pytorch
17567,peterjc123,pr,2019-02-28T05:17:23Z,Revert #17191 and #17215 that no longer apply on Windows,"They are previously merged to resolve #17051. However, since it was resolved by the upstream, and it was causing some issues like https://github.com/abjer/tsds/issues/8, I think it's time to revert these changes.",pytorch
17570,musikisomorphie,pr,2019-02-28T10:32:52Z,Merge pull request #1 from pytorch/master,"aa

",pytorch
17593,ssnl,pr,2019-02-28T20:49:22Z,Make jit.trace check_inputs behavior consistent with doc,,pytorch
17607,peterjc123,pr,2019-03-01T06:09:27Z,Update magma to 2.5.0 for Windows,,pytorch
17608,peterjc123,pr,2019-03-01T06:18:56Z,Fix the missing Windows CPU job in the build status section,"It will be better to split the CPU job on CI. But unluckily, we are out of Windows machines.
cc, @davidbrownellWork @yf225",pytorch
17661,dlibenzi,pr,2019-03-04T18:45:12Z,Turn the Half::from_bits into a constexpr function to avoid unresolveâ€¦,"â€¦d symbol errors when building in DEBUG mode.

",pytorch
17675,peterjc123,pr,2019-03-05T02:40:11Z,Enable apex on Windows,,pytorch
17705,ssnl,pr,2019-03-06T04:04:10Z,fix typo in hub doc,,pytorch
17706,peterjc123,pr,2019-03-06T05:01:09Z,Enable using CMD when building cpp extensions on Windows,,pytorch
17707,peterjc123,pr,2019-03-06T05:11:03Z,Add check for x64 Python before setup,Fixes https://github.com/pytorch/pytorch/issues/17657.,pytorch
17717,ssnl,pr,2019-03-06T15:54:53Z,Update ModuleDict doc about order,,pytorch
17719,ssnl,pr,2019-03-06T19:16:28Z,fix exp fam. formula,,pytorch
17728,musikisomorphie,pr,2019-03-06T21:56:38Z,Merge pull request #1 from pytorch/master,"aa

",pytorch
17731,ssnl,pr,2019-03-06T23:09:38Z,Further improvements of nn.container docs,,pytorch
17791,peterjc123,pr,2019-03-08T02:39:02Z,[TESTING] Windows test restore,,pytorch
17799,peterjc123,pr,2019-03-08T13:59:21Z,Add /MD to prevent linking errors on Windows,,pytorch
17847,dlibenzi,pr,2019-03-10T02:22:56Z,Enable autograd to recognize the XLA backend as one providing multiple devices,"â€¦e devices, while not being CUDA/HIP.

",pytorch
17851,ssnl,pr,2019-03-10T17:37:13Z,fix faq typo,,pytorch
17863,peterjc123,pr,2019-03-11T09:31:46Z,Prevent VS2017 from emitting ambiguous symbol errors (second time),,pytorch
17864,peterjc123,pr,2019-03-11T10:18:32Z,"Revert ""Add check for x64 Python before setup (#17707)""",This reverts commit 08fb9021da32e73bd7dec73104eea6a76dd44439.,pytorch
17865,peterjc123,pr,2019-03-11T11:27:35Z,Try 32 bit PyTorch on Windows,,pytorch
17866,vishwakftw,pr,2019-03-11T12:52:19Z,Deprecate torch.pstrf,"Changelog:
- Add deprecation warning to torch.pstrf

Test plan:
- No changes made to tests, they should pass as usual

Please refer to discussion in #12908 

",pytorch
17904,vishwakftw,pr,2019-03-12T05:34:51Z,Expose alias multinomial methods to ATen,"This PR exposes the multinomialAliasSetup and multinomialAliasDraw methods.

cc: @neerajprad 

",pytorch
17916,peterjc123,pr,2019-03-12T14:31:08Z,Simplify env creation when running Windows tests,Fixes https://github.com/pytorch/pytorch/issues/13465.,pytorch
17954,peterjc123,pr,2019-03-13T02:51:01Z,Fix Windows test CI,,pytorch
18008,peterjc123,pr,2019-03-14T04:10:08Z,Add magma debug version for Windows,,pytorch
18025,vishwakftw,pr,2019-03-14T18:02:24Z,Add batched version of trtrs,"- Remove single batch TH/THC implementations
- Remove `_batch_trtrs_lower` from `multivariate_normal`
- Add tests for batched behavior
- Modify trtrs_backward to accommodate for batched case
- Modify docs

In a future PR, this will be renamed to `triangular_solve`.",pytorch
18060,vishwakftw,pr,2019-03-15T16:00:06Z,Rename gesv to solve,"Changelog:

- Renames `gesv` to `solve` to remain consistent with `cholesky_solve`.
- Rename all tests, fix callsites
- Create a tentative alias for `solve` under the name `gesv`, and add a deprecated warning to not promote usage.

Test Plan:

- All tests should pass to confirm the patch is correct

",pytorch
18079,ssnl,pr,2019-03-15T20:01:11Z,Customized pin_memory for PackedSequence,fixes https://github.com/pytorch/pytorch/issues/18078,pytorch
18087,ngimel,pr,2019-03-15T22:35:09Z,Allow fusion of float function arguments,"so that functions like `def fn(x, p:float)` can be fused. Fixes #9940 and #11186. Fuses only float (not integer) arguments to simplify assembling arguments for fusion launch. 
CPU fusion is disabled in CI and this won't be tested, but I tested it locally. 
cc @t-vi, @apaszke ",pytorch
18109,lutzroeder,pr,2019-03-17T03:19:20Z,Fix Caffe2 operator schemas (#15462) (#13229),"@Maratyszcza @harouwu @yinghai

This is broken since #13065. `c_str()` returns a pointer that isn't permanent.",pytorch
18115,bharatr21,pr,2019-03-17T17:37:13Z,Corrected type of 'swap' in torch.nn.TripletMarginLoss,Fix #16428 by correcting type of 'swap' from `float` to `bool`,pytorch
18116,vishwakftw,pr,2019-03-17T17:51:21Z,"Add backend checks to solve methods (gesv, cholesky_solve)","Changelog:
- Incorporate a simple backend check in the linearSolveCheckInputs function in LinearAlgebraUtils.h

Test plan:
- Add a test to see if this works as expected

Fixes: #18105 

",pytorch
18117,bharatr21,pr,2019-03-17T18:21:27Z,Added the exception of ignore_index,"Fix #17801 to add an exception regarding `ignore_index` in the documentation for `torch.nn.CrossEntropyLoss` and `torch.nn.NLLLoss`

If any other files/functions are hit, I'd be glad to incorporate the changes there too! ðŸ˜Š ",pytorch
18118,ssnl,pr,2019-03-17T18:53:12Z,update exp. family doc,sphinx doesn't understand hyphen. it does not merge the two halves together in html.,pytorch
18176,peterjc123,pr,2019-03-19T13:44:16Z,Enable 32 bit CPU build on Windows,,pytorch
18213,vishwakftw,pr,2019-03-20T09:27:26Z,Rename trtrs to triangular_solve,"Changelog:
- Renames `trtrs` to `triangular_solve` to remain consistent with `cholesky_solve` and `solve`.
- Rename all tests, fix callsites
- Create a tentative alias for `triangular_solve` under the name `trtrs`, and add a deprecation warning to not promote usage.
- Move `isnan` to _torch_docs.py
- Remove unnecessary imports

Test Plan:
- All tests should pass to confirm that the patch is correct.

Stack:
1. #18025 - Add batched version of trtrs
2. #18213 - Rename trtrs to triangular_solve",pytorch
18217,peterjc123,pr,2019-03-20T10:45:54Z,Correct cmake flags passing,"Fixes #18214.

According to the CMake manual, we should pass the arguments first, and put the directory as the last element. Otherwise, these flags may not be passed correctly.

Reference: 
1. https://cmake.org/cmake/help/latest/manual/cmake.1.html#synopsis
2. https://stackoverflow.com/a/27169347",pytorch
18375,ngimel,pr,2019-03-23T00:06:39Z,change dropout lowering in symbolic_script,"Dropout is now eligible for fusion, and generated fused kernels are just as fast as dropout in ATen. Change its lowering in symbolic script so that it can actually be fused. Still special-cased for cuda, because without fusion this lowering is less efficient than current (bernoulli_ * input). Testing is covered by the test case that @ailzhang added (test_dropout_cuda). ",pytorch
18418,peterjc123,pr,2019-03-25T05:48:45Z,Append c10 libs to TorchConfig.cmake,Fixes #18416.,pytorch
18435,vishwakftw,pr,2019-03-25T17:38:10Z,Rename `btrifact*` to `lu`,"Changelog:

- Renames `btrifact` and `btrifact_with_info` to `lu`to remain consistent with other factorization methods (`qr` and `svd`).
- Now, we will only have one function and methods named `lu`, which performs `lu` decomposition. This function takes a get_infos kwarg, which when set to True includes a infos tensor in the tuple.
- Rename all tests, fix callsites
- Create a tentative alias for `lu` under the name `btrifact` and `btrifact_with_info`, and add a deprecation warning to not promote usage.
- Add the single batch version for `lu` so that users don't have to unsqueeze and squeeze for a single square matrix (see changes in determinant computation in `LinearAlgebra.cpp`)

Test Plan:

- All tests should pass to confirm that the patch is correct.

",pytorch
18449,ssnl,pr,2019-03-25T20:37:32Z,Improve numerical precision of (s)logdet,Fixes https://github.com/pytorch/pytorch/issues/18448 and https://github.com/pytorch/pytorch/issues/18450,pytorch
18458,ssnl,pr,2019-03-25T23:52:33Z,Enable printing to stderr for test_proper_exit for better debugging,related to https://github.com/pytorch/pytorch/issues/16608,pytorch
18464,peterjc123,pr,2019-03-26T03:47:25Z,Add export annotations for functions in c10,Fixes #18461.,pytorch
18479,ssnl,pr,2019-03-26T15:40:32Z,fix #16448,"Fixes #16448 

@bddppq ",pytorch
18511,malmaud,pr,2019-03-27T02:57:25Z,More type stubs,"Added stubs for:

* The `device` module
* The `cuda` module
* Parts of the `optim` module
* Began adding stubs for the `autograd` module. I'll annotate more later but `no_grad` and friends are probably the most used exports from it so it seemed like a good place to start.

This would close #16996, although comments on that issue reference other missing stubs so maybe it's worth keeping open as an umbrella issue.

The big remaining missing package is `nn`.

Also added a `py.typed` file so mypy will pick up on the type stubs. That closes #17639. ",pytorch
18512,benoitsteiner,pr,2019-03-27T03:32:48Z,Improved onnx export for 3 onnx ops.,"Summary:
Ceil and Floor have been supported since version 6 of ONNX: export them using the native onnx ops instead of an Aten op.
Similarly, support for the Where op has been added in version 9, so we don't need to wrap these op in an Aten op.

Differential Revision: D14635130
",pytorch
18529,vishwakftw,pr,2019-03-27T17:36:43Z,Rename `btriunpack` to `lu_unpack`,"Changelog:
- Renames `btriunpack` to `lu_unpack` to remain consistent with the `lu` function interface.
- Rename all relevant tests, fix callsites
- Create a tentative alias for `lu_unpack` under the name `btriunpack` and add a deprecation warning to not promote usage.

Test Plan:
- All tests should pass to confirm that the patch is correct

Note to reviewers:
- Please review the [last commit](https://github.com/pytorch/pytorch/pull/18529/commits/3022ff7ba12a9669a2e6a4d4956ebcb163b8ae64) only.
- Stack:
    1. #18435 : Rename `btrifact*` to `lu`
    2. #18529 : Rename `btriunpack` to `lu_unpack`",pytorch
18681,peterjc123,pr,2019-04-01T08:23:54Z, Some fixes for the build script on Windows,Fixes https://discuss.pytorch.org/t/pytorch-build-from-source-on-windows/40288/13?u=peterjc123.,pytorch
18683,peterjc123,pr,2019-04-01T10:30:43Z,Unify caffe2 and libtorch build scripts on Windows,"`scripts/build_windows.bat` is the original way to build caffe2 on Windows, but since it is merged into libtorch, the build scripts should be unified because they actually do the same thing except there are some different flags.

The follow-up is to add the tests. Looks like the CI job for caffe2 windows is defined [here](https://github.com/pytorch/ossci-job-dsl/blob/master/src/jobs/caffe2.groovy#L906). Could we make them a separate file, just like what we've done in `.jenkins/pytorch/win-build.sh`? There's a bunch of things we can do there, like using ninja and sccache to accelerate build.

cc @orionr @yf225 ",pytorch
18726,vishwakftw,pr,2019-04-02T13:47:31Z,Rename btrisolve to lu_solve,"Changelog:
- Rename `btrisolve` to `lu_solve` to remain consistent with names of solve methods (`cholesky_solve`, `triangular_solve`, `solve`)
- Fix all callsites
- Rename all tests
- Create a tentative alias for `lu_solve` under the name `btrisolve` and add a deprecation warning to not promote usage

Test Plan:
- All tests should pass to confirm that the patch is correct

",pytorch
18790,peterjc123,pr,2019-04-03T05:42:04Z,Make it possible for users for select /Zi or /ZI over /Z7 when using MSVC,"Fixes https://github.com/pytorch/pytorch/issues/18701.

",pytorch
18792,peterjc123,pr,2019-04-03T05:56:46Z,Test static build on Windows,,pytorch
18816,ngimel,pr,2019-04-03T21:22:51Z,try to make at::cat in mm_tree_reduction operate on contig tensors,"Sometimes at::cat gets transposed inputs and goes on a slow path. Also, make jit_premul lstm benchmark add bias to the whole input tensor to avoid separate reduction kernels in the backward pass. ",pytorch
18840,peterjc123,pr,2019-04-04T03:12:40Z,[test]Unify caffe2 and libtorch build scripts on Windows,,pytorch
18871,mkolod,pr,2019-04-04T18:29:44Z,More numerically stable lerp,"The C++ and CUDA implementations of the lerp are not numerically stable. This is discussed on Wikipedia [here](https://en.wikipedia.org/wiki/Linear_interpolation#Programming_language_support). I checked the GPU SASS output and there's no overhead from using the more precise implementation, from Kepler all the way to Turing. I haven't looked at CPU ASM though.",pytorch
18883,ssnl,pr,2019-04-04T20:07:03Z,fix lint in optim doc,,pytorch
18963,malmaud,pr,2019-04-05T20:26:04Z,Type annotations for `util.data`.,"I haven't had a chance to rigorously try these out yet so don't merge yet. 
Closes #18725.",pytorch
18991,cbalint13,pr,2019-04-07T00:16:41Z,AVX2 with GCC9 fix.,"Dear All,


The proposed patch fixes the test code snippets used in cmake infrastructure, and implicit failure to set properly the ```CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS``` flag. The libcaffe2.so will have some ```UND``` avx2 related references, rendering it unusable.

* Using GCC 9 test code from cmake build infra always fails:
```
$ gcc  -O2 -g -pipe -Wall -m64 -mtune=generic -fopenmp -DCXX_HAS_AVX_1 -fPIE -o test.o -c test.c -mavx2
test.c: In function â€˜mainâ€™:
test.c:11:26: error: incompatible type for argument 1 of â€˜_mm256_extract_epi64â€™
   11 |     _mm256_extract_epi64(x, 0); // we rely on this in our AVX2 code
      |                          ^
      |                          |
      |                          __m256 {aka __vector(8) float}
In file included from /usr/lib/gcc/x86_64-redhat-linux/9/include/immintrin.h:51,
                 from test.c:4:
/usr/lib/gcc/x86_64-redhat-linux/9/include/avxintrin.h:550:31: note: expected â€˜__m256iâ€™ {aka â€˜__vector(4) long long intâ€™} but argument is of type â€˜__m256â€™ {aka â€˜__vector(8) floatâ€™}
  550 | _mm256_extract_epi64 (__m256i __X, const int __N)
      |  

$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/9/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,objc,obj-c++,ada,go,d,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --enable-cet --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 9.0.1 20190328 (Red Hat 9.0.1-0.12) (GCC)
```
",pytorch
19052,peterjc123,pr,2019-04-09T02:37:28Z,Fix default CXX for Windows in cpp_extensions.py,Fixes https://github.com/pytorch/pytorch/issues/19017.,pytorch
19089,malmaud,pr,2019-04-10T00:21:55Z,Stubs for torch.nn,Closes https://github.com/pytorch/pytorch/issues/18724,pytorch
19113,crcrpar,pr,2019-04-10T15:30:03Z,[PyTorch] fix the docstring of `RandomSampler`,"fix
- the order of `Arguments` in `RandomSampler` doc
- the meaningless check of `replacement`'s type.",pytorch
19116,vishwakftw,pr,2019-04-10T16:46:18Z,Modify Cholesky derivative,"The derivative of the Cholesky decomposition was previously a triangular matrix.

Changelog:
- Modify the derivative of Cholesky from a triangular matrix to symmetric matrix

Test Plan:
- Add tests to validate that the derivative of the input to matrix function which uses and doesn't use the Cholesky decomposition are the same.

Ref: #18825

cc: @jacobrgardner",pytorch
19152,zheng-xq,pr,2019-04-11T06:19:35Z,Add a fast path for batch-norm CPU inference.,"Summary:
Adding a fast path for batch-norm CPU inference when all tensors are contiguous.
* Leverage vectorization through smiple loops.
* Folding linear terms before computation.
* For resnext-101, this version gets 18.95 times faster.

Differential Revision: D14889728

== Benchmark Results ==

batch_norm: data shape: [1, 256, 3136], bandwidth: 22.26 GB/s
batch_norm: data shape: [1, 65536, 1], bandwidth: 5.57 GB/s
batch_norm: data shape: [128, 2048, 1], bandwidth: 18.21 GB/s",pytorch
19187,peterjc123,pr,2019-04-12T06:00:27Z,One more fix for #18790,,pytorch
19188,peterjc123,pr,2019-04-12T06:09:36Z,[TEST] Test non permissive build for MSVC,,pytorch
19228,ssnl,pr,2019-04-13T03:33:13Z,Add IterableDataset,"This is a modified version of https://github.com/pytorch/pytorch/pull/14705 since commit structure for that PR is quite messy.

1. Add `IterableDataset`.
3. So we have 2 data loader mods: `Iterable` and `Map`.

    1. `Iterable` if the `dataset` is an instance of `IterableDataset`
    2. `Map` o.w.

3. Add better support for non-batch loading (i.e., `batch_size=None` and `batch_sampler=None`). This is useful in doing things like bulk loading.
3. Refactor `DataLoaderIter` into two classes, `_SingleProcessDataLoaderIter` and `_MultiProcessingDataLoaderIter`. Rename some methods to be more generic, e.g., `get_batch` -> `get_data`.
4. Add `torch.utils.data.get_worker_info` which returns worker information in a worker proc (e.g., worker id, dataset obj copy, etc.) and can be used in `IterableDataset.__iter__` and `worker_init_fn` to do per-worker configuration.
5. Add `ChainDataset`, which is the analog of `ConcatDataset` for `IterableDataset`.
7. Import torch.utils.data in `torch/__init__.py`
9. data loader examples and documentations
10. Use `get_worker_info` to detect whether we are in a worker process in `default_collate`


Closes https://github.com/pytorch/pytorch/issues/17909, https://github.com/pytorch/pytorch/issues/18096, https://github.com/pytorch/pytorch/issues/19946, and some of https://github.com/pytorch/pytorch/issues/13023",pytorch
19300,ssnl,pr,2019-04-16T03:56:51Z,Add device-specific cuFFT plan caches,Fixes https://github.com/pytorch/pytorch/issues/19224,pytorch
19306,ThisIsIsaac,pr,2019-04-16T11:54:42Z,Bilinear Upsampling increased throughput,"changed `UpsampleBilinearKernel` s.t. the throughput increased 40~50%.

I tested locally with my local test code -- **not pytorch's provided test code** -- because I am having a build problem ( which I made an issue about [here](https://github.com/pytorch/pytorch/issues/19184)). I tested with various tensor sizes and across all the sizes, it should a significant increase in throughput.

1. added `__restrict__`
2. instead of launch as many threads as there are output elements, I launched only `output_height * output_width` may threads and had each thread iterate through the channel and batch dimension.",pytorch
19310,ssnl,pr,2019-04-16T15:41:06Z,Fix lint,,pytorch
19340,bharatr21,pr,2019-04-17T05:57:45Z,[DOC] Fix missing doc out= for torch.cumprod,Fix #19255 by adding the `out=None` argument for `torch.cumprod` missing [here](https://pytorch.org/docs/master/torch.html#torch.cumprod) also added the docstring for `out` in torch.cumsum which was missing [here](https://pytorch.org/docs/master/torch.html#torch.cumsum),pytorch
19346,bharatr21,pr,2019-04-17T08:59:48Z,[DOC] Add rst entry for nn.MultiheadAttention,Fix #19259 by adding the missing `autoclass` entry for `nn.MultiheadAttention` from [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L676),pytorch
19379,ngimel,pr,2019-04-17T23:47:22Z,improve dim sort performance,"We are already using custom comparators for sorting (for a good reason), but are still making 2 sorting passes - global sort and stable sorting to bring values into their slices. Using a custom comparator to sort within a slice allows us to avoid second sorting pass and brings up to 50% perf improvement. 
@t-vi I know you are moving sort to ATen, and changing THC is discouraged, but #18350 seems dormant. I'm fine with #18350 landing first, and then I can put in these changes. 
cc @umanwizard for review. ",pytorch
19392,peterjc123,pr,2019-04-18T04:49:06Z,Sync FindCUDA/select_computer_arch.cmake from upstream,"1. Fixes auto detection for Turing cards.
2. Adds Turing Support",pytorch
19419,ssnl,pr,2019-04-18T19:50:23Z,Fix missing import sys in pin_memory.py,@kostmo pointed this out in #15331. Thanks :),pytorch
19421,ssnl,pr,2019-04-18T20:19:08Z,Fix no SIGCHLD checking in DataLoaderIter._shutdown_workers,"Also

1. Bump multiprocessing test timeout following python core tests
2. Fix one type of flakiness in `test_proper_exit`. 
3. Add trace reporting when loader process hangs in `test_proper_exit` using `faulthandler`.
3. Give `test_proper_exit` another try. 

I'll heavily retest this.",pytorch
19498,vishwakftw,pr,2019-04-19T17:28:54Z,Rename potri to cholesky_inverse,"Changelog:
- Rename `potri` to `cholesky_inverse` to remain consistent with names of `cholesky` methods (`cholesky`, `cholesky_solve`)
- Fix all callsites
- Rename all tests
- Create a tentative alias for `cholesky_inverse` under the name `potri` and add a deprecation warning to not promote usage

Test Plan:
- All tests should pass to confirm that the patch is correct

Fixes #19459 
",pytorch
19537,ssnl,pr,2019-04-20T15:26:31Z,fix lint,,pytorch
19538,ssnl,pr,2019-04-20T17:46:26Z,Move cuFFT plan cache note outside Best Practices,"I mistakenly put it there.

",pytorch
19573,ssnl,pr,2019-04-22T19:31:31Z,add torch.cuda.synchronize(device=None),fixes https://github.com/pytorch/pytorch/issues/19509,pytorch
19597,ssnl,pr,2019-04-23T01:34:36Z,fix nn.Sequential doc,,pytorch
19606,ssnl,pr,2019-04-23T06:18:24Z,Remove unnecessary printing from tests,,pytorch
19690,ngimel,pr,2019-04-24T19:58:17Z,use relative path to load libthnvrtc,"We had a few hard to repro cases where very occasionally libthnvrtc failed to be loaded due to what looked like garbled dladdr return in `info.dli_fname` field. We could not root cause why this is happening, but this workaround avoids the problem altogether. $ORIGIN is already added to RPATH as the first search location, so dlopen(""libthnnvrtc.so"") will look for libthnvrtc in the caller (`libtorch.so.1`) directory, which was the purpose of the previous code that was getting `libtorch.so.1` directory using dladdr. 
```
root@4ec0aab027a0:/opt/conda/lib/python3.6/site-packages/torch/lib# readelf -d ./libtorch.so.1 | grep RPATH
 0x000000000000000f (RPATH)              Library rpath: [$ORIGIN:/usr/local/cuda/lib64:/opt/conda/lib]
```
Hopefully, same should be happening on Mac. 
cc @zdevito @ezyang 
",pytorch
19754,ssnl,pr,2019-04-25T18:19:00Z,update F.grid_sample doc for clarity,"https://github.com/pytorch/pytorch/issues/19717

",pytorch
19789,syed-ahmed,pr,2019-04-26T01:45:59Z,[CUDA 10] Resolve host_define.h warnings,"Eigen was updated with the commit needed to get rid of this warning that plagued the CI. This PR bumps third_party/eigen to that commit head.
```
warning: #warning ""host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead."" [-Wcpp]
```",pytorch
19810,orionr,pr,2019-04-26T17:56:54Z,Install TensorBoard for doc generation,"In order to have `torch.utils.tensorboard.SummaryWriter` rendered in the documentation at the bottom of https://pytorch.org/docs/master/tensorboard.html we need to have TensorBoard installed.

This change makes it so our pinned version of `tb-nightly` is used for doc generation same as it is used for running tests at https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/test.sh#L45-L52

Eventually we'll use a pinned version of `pip install tensorboard`, but it's not on the release channel yet.

cc @kostmo @soumith @ezyang ",pytorch
19814,orionr,pr,2019-04-26T20:50:02Z,Fix the install of TensorBoard for doc generation,"One more fix for https://github.com/pytorch/pytorch/pull/19810

We now know that we are running with python3, so no need to check python version. The quotes were probably causing problems here.

cc @ezyang @soumith @zou3519 ",pytorch
19823,peterjc123,pr,2019-04-27T11:16:07Z,Test latest cmake,,pytorch
19824,peterjc123,pr,2019-04-27T14:52:27Z,Fix conda build for Windows,Let's test it before merging.,pytorch
19837,peterjc123,pr,2019-04-28T08:01:09Z, Fix conda build for Windows for v1.1.0,,pytorch
19898,crcrpar,pr,2019-04-29T02:18:53Z,Port adaptive_avg_pool3d to ATen,Resolves #18065.,pytorch
19904,ssnl,pr,2019-04-29T04:28:42Z,Update multiprocessing note now that shared CUDA tensors are refcounted,The mp notes are not updated after https://github.com/pytorch/pytorch/pull/16854. (The torch.multiprocessing page is.),pytorch
19914,peterjc123,pr,2019-04-29T14:06:28Z,Add magma for CUDA 10.1 to Windows docs,,pytorch
19915,orionr,pr,2019-04-29T15:28:19Z,Improve torch.utils.tensorboard docs,"This adds method details and corrects example on the page that didn't run properly. I've now confirmed that it runs in colab with nightly.

For those with internal access the rendered result can be seen at https://home.fburl.com/~orionr/pytorch-docs/tensorboard.html

cc @lanpa, @soumith, @ezyang, @brianjo ",pytorch
19917,syed-ahmed,pr,2019-04-29T16:55:54Z, [CUDA 10] Resolve host_define.h warnings,"Eigen was updated with the commit needed to get rid of this warning that plagued the CI. This PR bumps third_party/eigen to that commit head.
```
warning: #warning ""host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead."" [-Wcpp]
```",pytorch
19933,syed-ahmed,pr,2019-04-29T22:03:55Z,Fix warnings coming from bernoulli and dirichlet kernels,"Fixes the following warnings in CI:
```
/tmp/pip-req-build-un327jey/aten/src/ATen/native/cuda/Distributions.cu(123): warning: pointless comparison of unsigned integer with zero
              detected during instantiation of ""void <unnamed>::bernoulli_tensor_cuda_kernel<scalar_t,prob_t>(at::Tensor &, const at::Tensor &, std::pair<uint64_t, uint64_t>) [with scalar_t=uint8_t, prob_t=uint8_t]""
    (238): here
```
```
/tmp/pip-req-build-un327jey/aten/src/ATen/native/cuda/Distributions.cu:220:116: warning: 'c10::ScalarType detail::scalar_type(const at::Type&)' is deprecated [-Wdeprecated-declarations]
    /tmp/pip-req-build-un327jey/aten/src/ATen/Dispatch.h:46:1: note: declared here
```",pytorch
19979,neerajprad,pr,2019-04-30T22:27:07Z,Remove deprecated tensor constructors in torch.distributions,This removes the deprecated `tensor.new_*` constructors (see #16770) from `torch.distributions` module.,pytorch
20013,orionr,pr,2019-05-01T16:06:07Z,Update TensorBoard docs to specify data type (#19959),"Merge doc changes from https://github.com/pytorch/pytorch/pull/19959 to v1.1.0

cc @zou3519 @lanpa @soumith ",pytorch
20025,yanboliang,pr,2019-05-01T19:57:27Z,Delete TensorImpl::GetDevice(),"Summary: Delete TensorImpl::GetDevice() and clean all its call sites.

Differential Revision: D15170917

",pytorch
20038,orionr,pr,2019-05-02T00:14:20Z,[tensorboard] Remove in-memory scalars and add comments,"This takes care of some outstanding review comments for https://github.com/pytorch/pytorch/pull/16196/

Specifically:
1. Add comment about kind
2. Add comment about GraphPy
3. Remove ONNX version comment
4. Remove scalar_dict from SummaryWriter and all history functions

cc @lanpa @ezyang",pytorch
20091,bharatr21,pr,2019-05-03T05:39:56Z,DOC: Update web documentation of geometric_ to be consistent with Tensor behaviour,Fix #19940 by updating web doc to reflect Tensor behaviour which will reflect [here](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.geometric_),pytorch
20110,vfdev-5,pr,2019-05-03T20:44:23Z,Formula typo fix,"T_{cur + 1} -> T_{cur} + 1

",pytorch
20115,orionr,pr,2019-05-03T21:48:17Z,[tensorboard] Add logging import and failing MLP,"Add logging import and a failed MLP model that confirms that we don't fail `add_graph` when graph optimization fails.

This addresses part of https://github.com/pytorch/pytorch/issues/18903

cc @lanpa @ezyang @natalialunova ",pytorch
20127,ssnl,pr,2019-05-04T07:14:21Z,Improve nn.ActivationCls repr of inplace,,pytorch
20131,ssnl,pr,2019-05-04T21:09:25Z,update nn.init.calculate_gain doc example,,pytorch
20150,ssnl,pr,2019-05-06T05:43:19Z,DataLoader: add error detection for worker_init_fn,"This is an attempt to isolate unrelated changes from #19228 for easier review.


",pytorch
20158,peterjc123,pr,2019-05-06T12:08:53Z,Fix the warning if the wrong gcc is used with nvcc,Fixes https://github.com/pytorch/pytorch/issues/11886,pytorch
20166,ssnl,pr,2019-05-06T16:18:35Z,Improve test_proper_exit error printing,This doesn't have `strace` yet. But still have `faulthandler` to print stack traces at hanging. Also part of an attempt to isolate changes from #19228 .,pytorch
20172,ssnl,pr,2019-05-06T17:31:32Z,Disable worker_kill & holder_iter_reference combination in test_proper_exit,"cc @nairbv 
All failures I have seen are of this combination. So let's just disable it for all cases. After #20063 I find it failing for py3 once.",pytorch
20182,ngimel,pr,2019-05-06T21:49:20Z,fix WAR race,was flagged by racecheck. ,pytorch
20203,vfdev-5,pr,2019-05-07T09:07:14Z,Fixes #20124,"Fixes #20124

Description:
Code wraps `optimizer.step()` method to detect whether user is following new pattern or old pattern. In case of old pattern detected, a UserWarning is raised. Documentation is also updated to reflect the change:

![Screen Shot 2019-05-07 at 11 05 17](https://user-images.githubusercontent.com/2459423/57287527-04e63580-70b8-11e9-9ddd-5d159ef0ed2f.png)
 

cc @SsnL, @bado-lee",pytorch
20205,peterjc123,pr,2019-05-07T10:10:07Z,Fix cuda and cudnn libraries search process on Windows,Fixes #20202 ,pytorch
20244,fritzo,pr,2019-05-07T20:12:40Z,Fix memory leak in torch._dirichlet_grad(),"Fixes https://github.com/pyro-ppl/pyro/issues/1853

This fixes a memory leak in `torch._dirichlet_grad()`. This function is used for reparametrized gradients for the `Dirichlet` and `Beta` distributions.

## Questions for reviewiers

- [x] Could a reviewer please confirm that `freeCopyTo()` is being used correctly and doesn't need an additional `decref()`? The author is unfamiliar with PyTorch C++ memory utilities. Help appreciated.

## Tested
- ran locally and confirmed leak is fixed",pytorch
20289,ngimel,pr,2019-05-08T23:14:48Z,define use_cuda in dropout backward to allow peephole optimization toâ€¦,"â€¦ work

",pytorch
20292,syed-ahmed,pr,2019-05-09T00:44:26Z,Move THCTensor_(uniform) to ATen,"# Summary
As a first step for this plan: https://github.com/pytorch/pytorch/issues/19508#issuecomment-485178192, this PR moves `THCTensor_(uniform)` to ATen. Major changes are:
- `uniform_` cuda kernel now utilizes a philox generator.
- the kernel also utilizes TensorIterator
- the kernel uses a grid-stride loop to achieve peak effective bandwidth

# BC breaking change
- Since the engine has changed from `curandStateMTGP32` to `curandStatePhilox4_32_10`, the randoms generated now will be different.
- Here is the diff showing codegen changes: https://gist.github.com/syed-ahmed/4af9ae0d42b6c7dbaa13b9dd0d1dd1e8 (BC breaking change if any)

# Testing
- Philox4_32_10 is known to pass the standard TestU01 Big Crush test (https://www.thesalmons.org/john/random123/papers/random123sc11.pdf) and hence the quality of random numbers generated isn't an issue when compared to the previously used `curandStateMTGP32`.
- I have added a test case in `aten/src/ATen/test/cuda_distributions_test.cu` which verifies that philox offset is incremented properly

# Benchmark
The benchmark was done on a DGX station with 4 V100s.
## Runtime
I modified the script from @jcjohnson 's [multinomial benchmark](https://github.com/jcjohnson/pytorch-multinomial-benchmark) to produce this notebook which shows that there is a general speedup with this PR and a regression hasn't been introduced: https://gist.github.com/syed-ahmed/9d26d4e96308aed274d0f2c7be5218ef

To reproduce the notebook:
- Run https://gist.github.com/syed-ahmed/4208c22c541f1d30ad6a9b1efc1d728f in a container with the current pytorch top of tree with the command: `python uniform_benchmark.py --stats_json before.json`
- Apply this diff to the current pytorch top of tree and run the same script in a container with the command: `python uniform_benchmark.py --stats_json after.json`
- Run the notebook attached above with the `after.json` and `before.json` in the same directory

## Effective Bandwidth
The effected bandwidth was calculated using the script (thanks to @ngimel ): https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
Following are the numbers before and after.
#### Before
```
uniform, size, elements 65536 forward 5.168914794921875e-06 bandwidth (GB/s) 50.71548098597786
uniform, size, elements 131072 forward 5.056858062744141e-06 bandwidth (GB/s) 103.67860705101367
uniform, size, elements 262144 forward 7.164478302001953e-06 bandwidth (GB/s) 146.357621001797
uniform, size, elements 524288 forward 1.1217594146728515e-05 bandwidth (GB/s) 186.9520302275877
uniform, size, elements 1048576 forward 1.923084259033203e-05 bandwidth (GB/s) 218.10297600317384
uniform, size, elements 2097152 forward 3.640890121459961e-05 bandwidth (GB/s) 230.39992200138826
uniform, size, elements 4194304 forward 6.778717041015625e-05 bandwidth (GB/s) 247.49839679819922
uniform, size, elements 8388608 forward 0.00012810707092285157 bandwidth (GB/s) 261.92490202361347
uniform, size, elements 16777216 forward 0.00025241613388061524 bandwidth (GB/s) 265.86598474620627
uniform, size, elements 33554432 forward 0.000497891902923584 bandwidth (GB/s) 269.5720239913193
```
#### After
```
uniform, size, elements 65536 forward 5.550384521484375e-06 bandwidth (GB/s) 47.22988091821306
uniform, size, elements 131072 forward 5.581378936767578e-06 bandwidth (GB/s) 93.93520954942333
uniform, size, elements 262144 forward 6.165504455566406e-06 bandwidth (GB/s) 170.071404141686
uniform, size, elements 524288 forward 6.3276290893554685e-06 bandwidth (GB/s) 331.4277702414469
uniform, size, elements 1048576 forward 8.509159088134765e-06 bandwidth (GB/s) 492.91639239047356
uniform, size, elements 2097152 forward 1.2989044189453124e-05 bandwidth (GB/s) 645.8218077979443
uniform, size, elements 4194304 forward 2.347707748413086e-05 bandwidth (GB/s) 714.6211452997259
uniform, size, elements 8388608 forward 4.4286251068115234e-05 bandwidth (GB/s) 757.6715389250498
uniform, size, elements 16777216 forward 8.672237396240235e-05 bandwidth (GB/s) 773.8356427961071
uniform, size, elements 33554432 forward 0.00016920566558837892 bandwidth (GB/s) 793.2224227438523
```
 ",pytorch
20298,syed-ahmed,pr,2019-05-09T03:35:48Z,Remove support for CUDA 8,1.1.0 stopped support for CUDA 8,pytorch
20302,peterjc123,pr,2019-05-09T05:53:53Z,Switch off USE_DISTRIBUTED on default for MSVC,Fixes https://github.com/pytorch/pytorch/issues/20250,pytorch
20304,peterjc123,pr,2019-05-09T06:14:53Z,Fix overlay_vc_env when called by legacy python,Fixes https://github.com/pytorch/pytorch/issues/20155.,pytorch
20318,peterjc123,pr,2019-05-09T13:08:16Z,Remove the legacy code about selective build,,pytorch
20353,peterjc123,pr,2019-05-10T07:10:01Z,Some essential changes needed before updating the Windows AMI,"1. Add cuda 10.1 build
2. Turn on openmp loop support for VS 2019
3. Remove legacy code about selective builds

Tested through CI.",pytorch
20366,crcrpar,pr,2019-05-10T15:37:17Z,Avoid dynamic dispatch inside the omp loop in AdaptiveAvgPool2d,"This PR changes CPU implementation of `AdaptiveAveragePool2D` by
- move dispatch to outside the OpenMP loop
- support fp16",pytorch
20382,orionr,pr,2019-05-10T20:35:02Z,[tensorboard] Preserve log_dir arg and member for SummaryWriter,"Given that  tensorboardX and our PyTorch 1.1 release had `log_dir` as the argument for SummaryWriter initialization and member variable (which some users access), we need to  preserve this name. However, we might deprecate this in the future and I've added a `get_logdir` method that can be used in the future.

cc @natalialunova, @lanpa ",pytorch
20417,peterjc123,pr,2019-05-13T00:53:16Z,Hotfix for caffe2 windows build,"We don't need to overlay vc env when not using ninja. CMake will deal with it automatically. Overlaying is a no-op when the env is the same with the generator specified but will generate the error ""Cannot find CMAKE_CXX_COMPILER"" when they are different.",pytorch
20482,vishwakftw,pr,2019-05-14T09:36:39Z,Remove checks for CUDA 8 in LU-based tests,"CUDA 8 is no longer supported and removed from CI, so these checks are irrelevant

",pytorch
20484,peterjc123,pr,2019-05-14T10:26:48Z,Portable way of the warning clause,,pytorch
20490,peterjc123,pr,2019-05-14T15:17:01Z,Fix strtod for MSVC,Fixes https://github.com/pytorch/pytorch/issues/20408. Tested locally by @Jonas1312.,pytorch
20491,peterjc123,pr,2019-05-14T16:02:52Z,[WIP] Another hotfix for Caffe2 Windows build,Avoid the in source build that may cause the timeout issue.,pytorch
20526,peterjc123,pr,2019-05-15T09:13:13Z,Fix GetLastError in THAllocator for Windows,,pytorch
20530,peterjc123,pr,2019-05-15T13:12:37Z,Enable simd and loop vectorizer with MSVC,,pytorch
20534,vishwakftw,pr,2019-05-15T15:48:07Z,Remove TH/THC link for single matrix inverse,"- Earlier, we had to use the legacy implementation of `getri` for single matrix inverse from TH and THC
- Now, this has been moved to ATen

Changelog:
- Move single matrix inverse implementation to ATen
- Remove unused code in TH and THC resulting from the change
- Minor modifications made to single matrix CPU function implementations in ATen to avoid redundancy

Test Plan:
- Existing tests should pass to verify if the change is valid",pytorch
20541,ngimel,pr,2019-05-15T17:16:21Z,fix empty dropout,Fix for #20499 ,pytorch
20557,ngimel,pr,2019-05-15T22:25:53Z,Improve performance of advanced indexing backward,"This PR improves performance of advanced indexing backward, partially solving #15245 (performance is still worse than gather, but not by such outrageous margins). Before, using benchmarking harness from #15245, cuda 10/V100:
```
Indexing is faster by at most -270.61607820767887 us on N: 16 D: 256 K: 1
Indexing is slower by at most 11127.466280784833 us on N: 16 D: 4096 K: 4096
```
after:
```
Indexing is faster by at most 23.524456737696028 us on N: 512 D: 4096 K: 4096
Indexing is slower by at most 186.24056029472553 us on N: 16 D: 1024 K: 4096
```
Strategy is to reuse embedding backward kernel, adapting it to handle unindexed dimensions in the beginning by launching additional threadblocks, and also allowing it to handle slices that are bigger than `65K*128`, that is hardly ever a problem for embedding. Still, integer indexing is baked in the kernel, and is important for performance, so for now bigger than 2G element tensors are not supported. 
The main savings come from not having to expand index to all unindexed dimensions, and not sorting expanded index with incoming gradient values, but rather only sorting unexpanded index. 
There are ways to make sorting overhead smaller (thanks @mcarilli for suggestions) but I'll get to it when it becomes a real problem, or rather, when cuda graphs will force us to get rid of thrust::sort calls. 
I've also added tests for indexing backward, before tests for index_put_ and indexing backward were non-existent. 
This PR also fixes #20457 by casting indices to `self` backend.",pytorch
20574,peterjc123,pr,2019-05-16T06:10:19Z,Fix caffe2 build failure on Windows,"Fixes #20568.
Looks like CMake is passing `/MD` when we call `add_library`. We need to fix these with C source files too.",pytorch
20581,vishwakftw,pr,2019-05-16T09:28:29Z,Allow tuples for scale_factor argument in nn.Upsample,"Fixes #20523 .

nn.Upsample was unable to accept tuple inputs for the scale_factor argument due to direct casting to float, which was done in #17732.",pytorch
20620,syed-ahmed,pr,2019-05-17T04:06:30Z,"Move THCTensor_{random, clampedRandom, cappedRandom} to ATen","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* **#20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen**

Differential Revision: [D15454050](https://our.internmc.facebook.com/intern/diff/D15454050)

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
random, size, elements 65536 forward 5.106925964355469e-06 bandwidth (GB/s) 51.331075059570495
random, size, elements 131072 forward 5.497932434082031e-06 bandwidth (GB/s) 95.36093909592368
random, size, elements 262144 forward 7.791519165039062e-06 bandwidth (GB/s) 134.57914660660956
random, size, elements 524288 forward 1.2221336364746093e-05 bandwidth (GB/s) 171.59760090144363
random, size, elements 1048576 forward 2.0668506622314453e-05 bandwidth (GB/s) 202.93212647844044
random, size, elements 2097152 forward 3.9124488830566405e-05 bandwidth (GB/s) 214.40811754315664
random, size, elements 4194304 forward 7.290840148925782e-05 bandwidth (GB/s) 230.1136173239503
random, size, elements 8388608 forward 0.00013821840286254883 bandwidth (GB/s) 242.76385275098409
random, size, elements 16777216 forward 0.0002722597122192383 bandwidth (GB/s) 246.48841157211064
random, size, elements 33554432 forward 0.0005396437644958496 bandwidth (GB/s) 248.71542456418447
```
#### After:
```
random, size, elements 65536 forward 5.841255187988281e-06 bandwidth (GB/s) 44.878025623510204
random, size, elements 131072 forward 5.857944488525391e-06 bandwidth (GB/s) 89.5003360013024
random, size, elements 262144 forward 6.563663482666016e-06 bandwidth (GB/s) 159.75468620065382
random, size, elements 524288 forward 7.276535034179687e-06 bandwidth (GB/s) 288.207504004194
random, size, elements 1048576 forward 1.0349750518798827e-05 bandwidth (GB/s) 405.2565317764571
random, size, elements 2097152 forward 1.6405582427978516e-05 bandwidth (GB/s) 511.3264364021509
random, size, elements 4194304 forward 2.7208328247070314e-05 bandwidth (GB/s) 616.6206114411497
random, size, elements 8388608 forward 4.884481430053711e-05 bandwidth (GB/s) 686.9599665901694
random, size, elements 16777216 forward 9.639024734497071e-05 bandwidth (GB/s) 696.2204771591086
random, size, elements 33554432 forward 0.00017502307891845704 bandwidth (GB/s) 766.8573129291814
```
### Double Type
#### Before:
```
random, size, elements 65536 forward 6.1082839965820315e-06 bandwidth (GB/s) 42.916144721935986
random, size, elements 131072 forward 8.215904235839844e-06 bandwidth (GB/s) 63.81379151340685
random, size, elements 262144 forward 1.3575553894042968e-05 bandwidth (GB/s) 77.240016001124
random, size, elements 524288 forward 2.3760795593261718e-05 bandwidth (GB/s) 88.26101768219948
random, size, elements 1048576 forward 4.4798851013183595e-05 bandwidth (GB/s) 93.62525835240021
random, size, elements 2097152 forward 8.335113525390626e-05 bandwidth (GB/s) 100.64179659276888
random, size, elements 4194304 forward 0.00015572309494018554 bandwidth (GB/s) 107.7374939564633
random, size, elements 8388608 forward 0.0003071308135986328 bandwidth (GB/s) 109.25127181751903
random, size, elements 16777216 forward 0.0006092119216918945 bandwidth (GB/s) 110.15684626398355
random, size, elements 33554432 forward 0.0011054635047912597 bandwidth (GB/s) 121.41307914578674
```
#### After:
```
random, size, elements 65536 forward 5.834102630615234e-06 bandwidth (GB/s) 44.93304567944422
random, size, elements 131072 forward 6.258487701416016e-06 bandwidth (GB/s) 83.77231449721906
random, size, elements 262144 forward 7.848739624023438e-06 bandwidth (GB/s) 133.5980106653706
random, size, elements 524288 forward 1.185894012451172e-05 bandwidth (GB/s) 176.84143591089668
random, size, elements 1048576 forward 2.0167827606201173e-05 bandwidth (GB/s) 207.97004426546874
random, size, elements 2097152 forward 3.463029861450195e-05 bandwidth (GB/s) 242.23319854617557
random, size, elements 4194304 forward 6.528139114379883e-05 bandwidth (GB/s) 256.9984448254775
random, size, elements 8388608 forward 0.00012089729309082031 bandwidth (GB/s) 277.544940355226
random, size, elements 16777216 forward 0.00023464202880859374 bandwidth (GB/s) 286.0053006733214
random, size, elements 33554432 forward 0.00044272661209106447 bandwidth (GB/s) 303.1616449846316
```
",pytorch
20621,syed-ahmed,pr,2019-05-17T04:06:43Z,"Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* **#20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen**
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
normal, size, elements 65536 forward 4.956722259521484e-06 bandwidth (GB/s) 52.88656218258779
normal, size, elements 131072 forward 5.285739898681641e-06 bandwidth (GB/s) 99.18914098114568
normal, size, elements 262144 forward 7.548332214355469e-06 bandwidth (GB/s) 138.91492454529376
normal, size, elements 524288 forward 1.1980533599853516e-05 bandwidth (GB/s) 175.0466273076219
normal, size, elements 1048576 forward 2.091646194458008e-05 bandwidth (GB/s) 200.52645667862762
normal, size, elements 2097152 forward 3.9961338043212894e-05 bandwidth (GB/s) 209.91809610901498
normal, size, elements 4194304 forward 7.39765167236328e-05 bandwidth (GB/s) 226.79110538115253
normal, size, elements 8388608 forward 0.0001377725601196289 bandwidth (GB/s) 243.5494555001696
normal, size, elements 16777216 forward 0.0002710080146789551 bandwidth (GB/s) 247.62686107087774
normal, size, elements 33554432 forward 0.0005375170707702637 bandwidth (GB/s) 249.69947058177252
```
#### After:
```
normal, size, elements 65536 forward 6.198883056640625e-06 bandwidth (GB/s) 42.288908760615385
normal, size, elements 131072 forward 6.756782531738281e-06 bandwidth (GB/s) 77.59432800112916
normal, size, elements 262144 forward 7.560253143310547e-06 bandwidth (GB/s) 138.6958849291706
normal, size, elements 524288 forward 7.550716400146485e-06 bandwidth (GB/s) 277.7421225831386
normal, size, elements 1048576 forward 1.1034011840820313e-05 bandwidth (GB/s) 380.1250225673293
normal, size, elements 2097152 forward 1.802682876586914e-05 bandwidth (GB/s) 465.34019427102237
normal, size, elements 4194304 forward 2.8417110443115234e-05 bandwidth (GB/s) 590.3913430460946
normal, size, elements 8388608 forward 4.8711299896240235e-05 bandwidth (GB/s) 688.8428777608927
normal, size, elements 16777216 forward 9.685993194580078e-05 bandwidth (GB/s) 692.8444265018856
normal, size, elements 33554432 forward 0.00018213510513305663 bandwidth (GB/s) 736.9130069787966
```
### Double Type
#### Before:
```
normal, size, elements 65536 forward 5.8841705322265624e-06 bandwidth (GB/s) 44.55071425348461
normal, size, elements 131072 forward 8.018016815185547e-06 bandwidth (GB/s) 65.38873789925661
normal, size, elements 262144 forward 1.2989044189453124e-05 bandwidth (GB/s) 80.72772597474304
normal, size, elements 524288 forward 2.2075176239013673e-05 bandwidth (GB/s) 95.00046465285668
normal, size, elements 1048576 forward 4.1041374206542965e-05 bandwidth (GB/s) 102.19696784254678
normal, size, elements 2097152 forward 7.57598876953125e-05 bandwidth (GB/s) 110.72624650312186
normal, size, elements 4194304 forward 0.00013725996017456056 bandwidth (GB/s) 122.22949779865557
normal, size, elements 8388608 forward 0.0002614736557006836 bandwidth (GB/s) 128.32815569921402
normal, size, elements 16777216 forward 0.0005080199241638184 bandwidth (GB/s) 132.0988819689674
normal, size, elements 33554432 forward 0.0009479570388793945 bandwidth (GB/s) 141.58629821311564
```
#### After:
```
normal, size, elements 65536 forward 5.991458892822265e-06 bandwidth (GB/s) 43.75294977222444
normal, size, elements 131072 forward 7.293224334716797e-06 bandwidth (GB/s) 71.88699756626349
normal, size, elements 262144 forward 8.094310760498048e-06 bandwidth (GB/s) 129.54481623281296
normal, size, elements 524288 forward 1.2805461883544922e-05 bandwidth (GB/s) 163.7701177100726
normal, size, elements 1048576 forward 2.2592544555664064e-05 bandwidth (GB/s) 185.64991604491345
normal, size, elements 2097152 forward 3.801822662353516e-05 bandwidth (GB/s) 220.6470092112881
normal, size, elements 4194304 forward 6.761550903320313e-05 bandwidth (GB/s) 248.1267425164457
normal, size, elements 8388608 forward 0.00013209104537963867 bandwidth (GB/s) 254.02503177684966
normal, size, elements 16777216 forward 0.0002667689323425293 bandwidth (GB/s) 251.56176699703818
normal, size, elements 33554432 forward 0.0004705166816711426 bandwidth (GB/s) 285.25604559501795
```",pytorch
20622,syed-ahmed,pr,2019-05-17T04:06:55Z,Move THCTensor_(cauchy) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20886 Remove curandStateMTGP32 usage
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* **#20622 Move THCTensor_(cauchy) to ATen**
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

Differential Revision: [D15454052](https://our.internmc.facebook.com/intern/diff/D15454052)

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
cauchy, size, elements 65536 forward 4.980564117431641e-06 bandwidth (GB/s) 52.63339529803734
cauchy, size, elements 131072 forward 6.232261657714844e-06 bandwidth (GB/s) 84.12483762631982
cauchy, size, elements 262144 forward 9.548664093017577e-06 bandwidth (GB/s) 109.81389540833959
cauchy, size, elements 524288 forward 1.59454345703125e-05 bandwidth (GB/s) 131.52052963827754
cauchy, size, elements 1048576 forward 2.86865234375e-05 bandwidth (GB/s) 146.21165262978724
cauchy, size, elements 2097152 forward 5.4748058319091796e-05 bandwidth (GB/s) 153.2220184158516
cauchy, size, elements 4194304 forward 0.00010075807571411133 bandwidth (GB/s) 166.50988897012377
cauchy, size, elements 8388608 forward 0.0001935744285583496 bandwidth (GB/s) 173.34124269355965
cauchy, size, elements 16777216 forward 0.00038077831268310545 bandwidth (GB/s) 176.24129779641603
cauchy, size, elements 33554432 forward 0.0006851387023925781 bandwidth (GB/s) 195.8986224705994
```
#### After:
```
cauchy, size, elements 65536 forward 6.077289581298828e-06 bandwidth (GB/s) 43.13501874366419
cauchy, size, elements 131072 forward 6.2131881713867184e-06 bandwidth (GB/s) 84.38308731972373
cauchy, size, elements 262144 forward 6.46829605102539e-06 bandwidth (GB/s) 162.11008150033175
cauchy, size, elements 524288 forward 6.8783760070800785e-06 bandwidth (GB/s) 304.8905726935182
cauchy, size, elements 1048576 forward 9.505748748779296e-06 bandwidth (GB/s) 441.23867681003264
cauchy, size, elements 2097152 forward 1.5070438385009766e-05 bandwidth (GB/s) 556.6266744001266
cauchy, size, elements 4194304 forward 2.4406909942626954e-05 bandwidth (GB/s) 687.396152951685
cauchy, size, elements 8388608 forward 4.6243667602539064e-05 bandwidth (GB/s) 725.6005792706125
cauchy, size, elements 16777216 forward 9.100198745727539e-05 bandwidth (GB/s) 737.4439380404413
cauchy, size, elements 33554432 forward 0.00017449140548706055 bandwidth (GB/s) 769.1939188944922
```
### Double Type
#### Before:
```
cauchy, size, elements 65536 forward 4.885196685791015e-06 bandwidth (GB/s) 53.660889593753055
cauchy, size, elements 131072 forward 6.229877471923828e-06 bandwidth (GB/s) 84.15703235943361
cauchy, size, elements 262144 forward 9.605884552001953e-06 bandwidth (GB/s) 109.15975455706132
cauchy, size, elements 524288 forward 1.5976428985595704e-05 bandwidth (GB/s) 131.26537863315923
cauchy, size, elements 1048576 forward 2.9621124267578124e-05 bandwidth (GB/s) 141.59840666786866
cauchy, size, elements 2097152 forward 5.5103302001953126e-05 bandwidth (GB/s) 152.23421637604707
cauchy, size, elements 4194304 forward 0.00010124444961547851 bandwidth (GB/s) 165.70998275677383
cauchy, size, elements 8388608 forward 0.0001944279670715332 bandwidth (GB/s) 172.58027487195184
cauchy, size, elements 16777216 forward 0.00034950494766235353 bandwidth (GB/s) 192.01119883668116
cauchy, size, elements 33554432 forward 0.0007002186775207519 bandwidth (GB/s) 191.67973135938277
```
#### After:
```
cauchy, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032
cauchy, size, elements 131072 forward 6.234645843505859e-06 bandwidth (GB/s) 84.09266751632889
cauchy, size, elements 262144 forward 7.433891296386719e-06 bandwidth (GB/s) 141.05344807902503
cauchy, size, elements 524288 forward 1.1401176452636719e-05 bandwidth (GB/s) 183.94171941045587
cauchy, size, elements 1048576 forward 1.960039138793945e-05 bandwidth (GB/s) 213.99082890665372
cauchy, size, elements 2097152 forward 3.434181213378906e-05 bandwidth (GB/s) 244.26806504326578
cauchy, size, elements 4194304 forward 6.517410278320313e-05 bandwidth (GB/s) 257.4215107465028
cauchy, size, elements 8388608 forward 0.0001229524612426758 bandwidth (GB/s) 272.9057365819818
cauchy, size, elements 16777216 forward 0.00023239374160766602 bandwidth (GB/s) 288.77225150621814
cauchy, size, elements 33554432 forward 0.00046050310134887696 bandwidth (GB/s) 291.4589013773367
```",pytorch
20623,syed-ahmed,pr,2019-05-17T04:07:07Z,Move THCTensor_(exponential) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20886 Remove curandStateMTGP32 usage
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* **#20623 Move THCTensor_(exponential) to ATen**
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

Differential Revision: [D15454047](https://our.internmc.facebook.com/intern/diff/D15454047)

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
exponential, size, elements 65536 forward 4.951953887939453e-06 bandwidth (GB/s) 52.937488097063074
exponential, size, elements 131072 forward 5.1164627075195315e-06 bandwidth (GB/s) 102.47079476011184
exponential, size, elements 262144 forward 7.412433624267578e-06 bandwidth (GB/s) 141.46177263119975
exponential, size, elements 524288 forward 1.1911392211914063e-05 bandwidth (GB/s) 176.06271061265014
exponential, size, elements 1048576 forward 2.077341079711914e-05 bandwidth (GB/s) 201.90733437869852
exponential, size, elements 2097152 forward 3.968000411987305e-05 bandwidth (GB/s) 211.4064296631136
exponential, size, elements 4194304 forward 7.367134094238281e-05 bandwidth (GB/s) 227.73056368176054
exponential, size, elements 8388608 forward 0.0001375126838684082 bandwidth (GB/s) 244.00972372926472
exponential, size, elements 16777216 forward 0.0002710747718811035 bandwidth (GB/s) 247.5658783526883
exponential, size, elements 33554432 forward 0.0005277013778686524 bandwidth (GB/s) 254.34409237682056
```
#### After:
```
exponential, size, elements 65536 forward 5.60760498046875e-06 bandwidth (GB/s) 46.74794335782313
exponential, size, elements 131072 forward 7.700920104980468e-06 bandwidth (GB/s) 68.0812153421672
exponential, size, elements 262144 forward 6.551742553710938e-06 bandwidth (GB/s) 160.04536066608443
exponential, size, elements 524288 forward 6.9427490234375e-06 bandwidth (GB/s) 302.0636340043956
exponential, size, elements 1048576 forward 9.472370147705077e-06 bandwidth (GB/s) 442.7935072845709
exponential, size, elements 2097152 forward 1.4712810516357422e-05 bandwidth (GB/s) 570.1567345459731
exponential, size, elements 4194304 forward 2.4566650390625e-05 bandwidth (GB/s) 682.9264768795031
exponential, size, elements 8388608 forward 4.60505485534668e-05 bandwidth (GB/s) 728.6434810009216
exponential, size, elements 16777216 forward 9.00864601135254e-05 bandwidth (GB/s) 744.9384060094111
exponential, size, elements 33554432 forward 0.00017408370971679687 bandwidth (GB/s) 770.9953344764326
```
### Double Type
#### Before:
```
exponential, size, elements 65536 forward 4.985332489013672e-06 bandwidth (GB/s) 52.58305250004783
exponential, size, elements 131072 forward 6.051063537597656e-06 bandwidth (GB/s) 86.64394229913319
exponential, size, elements 262144 forward 9.377002716064453e-06 bandwidth (GB/s) 111.82421843640988
exponential, size, elements 524288 forward 1.549959182739258e-05 bandwidth (GB/s) 135.30369208134132
exponential, size, elements 1048576 forward 2.866983413696289e-05 bandwidth (GB/s) 146.2967654421289
exponential, size, elements 2097152 forward 5.302190780639648e-05 bandwidth (GB/s) 158.2102256793561
exponential, size, elements 4194304 forward 9.615898132324219e-05 bandwidth (GB/s) 174.47372849762968
exponential, size, elements 8388608 forward 0.00018301725387573242 bandwidth (GB/s) 183.34026595537955
exponential, size, elements 16777216 forward 0.0003589057922363281 bandwidth (GB/s) 186.98183604629858
exponential, size, elements 33554432 forward 0.000672616958618164 bandwidth (GB/s) 199.5455604862227
```
#### After:
```
exponential, size, elements 65536 forward 5.755424499511719e-06 bandwidth (GB/s) 45.547291954266775
exponential, size, elements 131072 forward 6.275177001953125e-06 bandwidth (GB/s) 83.54951578844985
exponential, size, elements 262144 forward 7.97271728515625e-06 bandwidth (GB/s) 131.52052963827754
exponential, size, elements 524288 forward 1.2047290802001953e-05 bandwidth (GB/s) 174.07664797561844
exponential, size, elements 1048576 forward 2.0439624786376954e-05 bandwidth (GB/s) 205.20454968407793
exponential, size, elements 2097152 forward 3.5920143127441405e-05 bandwidth (GB/s) 233.53492691379267
exponential, size, elements 4194304 forward 6.896495819091797e-05 bandwidth (GB/s) 243.27160401598564
exponential, size, elements 8388608 forward 0.00012843608856201173 bandwidth (GB/s) 261.2539230653945
exponential, size, elements 16777216 forward 0.0002438235282897949 bandwidth (GB/s) 275.23539041005995
exponential, size, elements 33554432 forward 0.00046614646911621096 bandwidth (GB/s) 287.93037573462635
```",pytorch
20624,syed-ahmed,pr,2019-05-17T04:07:21Z,Move THCTensor_(lognormal) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20886 Remove curandStateMTGP32 usage
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* **#20624 Move THCTensor_(lognormal) to ATen**
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

Differential Revision: [D15454051](https://our.internmc.facebook.com/intern/diff/D15454051)

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
log_normal, size, elements 65536 forward 4.84466552734375e-06 bandwidth (GB/s) 54.1098242015748
log_normal, size, elements 131072 forward 5.0425529479980465e-06 bandwidth (GB/s) 103.97273075895983
log_normal, size, elements 262144 forward 7.326602935791016e-06 bandwidth (GB/s) 143.11898832098927
log_normal, size, elements 524288 forward 1.1749267578125e-05 bandwidth (GB/s) 178.49214736623378
log_normal, size, elements 1048576 forward 2.05230712890625e-05 bandwidth (GB/s) 204.37019103643124
log_normal, size, elements 2097152 forward 3.9284229278564456e-05 bandwidth (GB/s) 213.53627534643442
log_normal, size, elements 4194304 forward 7.281541824340821e-05 bandwidth (GB/s) 230.40746595613766
log_normal, size, elements 8388608 forward 0.00013544559478759766 bandwidth (GB/s) 247.7336531514311
log_normal, size, elements 16777216 forward 0.0002670741081237793 bandwidth (GB/s) 251.27431659866272
log_normal, size, elements 33554432 forward 0.0005250406265258789 bandwidth (GB/s) 255.63303336753222
```
#### After:
```
log_normal, size, elements 65536 forward 5.47647476196289e-06 bandwidth (GB/s) 47.86728897588159
log_normal, size, elements 131072 forward 6.859302520751953e-06 bandwidth (GB/s) 76.43459351936045
log_normal, size, elements 262144 forward 7.7056884765625e-06 bandwidth (GB/s) 136.07817175445544
log_normal, size, elements 524288 forward 8.029937744140625e-06 bandwidth (GB/s) 261.1666574289786
log_normal, size, elements 1048576 forward 1.1892318725585938e-05 bandwidth (GB/s) 352.6901773138733
log_normal, size, elements 2097152 forward 1.9683837890625e-05 bandwidth (GB/s) 426.1672975875969
log_normal, size, elements 4194304 forward 3.241539001464844e-05 bandwidth (GB/s) 517.5694629130921
log_normal, size, elements 8388608 forward 5.803346633911133e-05 bandwidth (GB/s) 578.1910700272298
log_normal, size, elements 16777216 forward 0.00011091709136962891 bandwidth (GB/s) 605.0362768381755
log_normal, size, elements 33554432 forward 0.00021491527557373046 bandwidth (GB/s) 624.5146029834174
```
### Double Type
#### Before:
```
log_normal, size, elements 65536 forward 5.793571472167969e-06 bandwidth (GB/s) 45.247392089547326
log_normal, size, elements 131072 forward 8.199214935302735e-06 bandwidth (GB/s) 63.943682918057576
log_normal, size, elements 262144 forward 1.3582706451416015e-05 bandwidth (GB/s) 77.19934195373004
log_normal, size, elements 524288 forward 2.3326873779296876e-05 bandwidth (GB/s) 89.90283137988553
log_normal, size, elements 1048576 forward 4.379749298095703e-05 bandwidth (GB/s) 95.76584673062604
log_normal, size, elements 2097152 forward 8.105754852294922e-05 bandwidth (GB/s) 103.48953493979646
log_normal, size, elements 4194304 forward 0.0001421213150024414 bandwidth (GB/s) 118.04855590951854
log_normal, size, elements 8388608 forward 0.00027796506881713865 bandwidth (GB/s) 120.71456367804988
log_normal, size, elements 16777216 forward 0.0005494546890258789 bandwidth (GB/s) 122.13721229493271
log_normal, size, elements 33554432 forward 0.0010767412185668946 bandwidth (GB/s) 124.65179718729368
```
#### After:
```
log_normal, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032
log_normal, size, elements 131072 forward 7.789134979248047e-06 bandwidth (GB/s) 67.31017005056627
log_normal, size, elements 262144 forward 9.219646453857422e-06 bandwidth (GB/s) 113.73277763392811
log_normal, size, elements 524288 forward 1.5113353729248047e-05 bandwidth (GB/s) 138.7615242500079
log_normal, size, elements 1048576 forward 2.7089118957519532e-05 bandwidth (GB/s) 154.83353321964444
log_normal, size, elements 2097152 forward 4.64177131652832e-05 bandwidth (GB/s) 180.71997580169503
log_normal, size, elements 4194304 forward 8.719682693481446e-05 bandwidth (GB/s) 192.40626740399748
log_normal, size, elements 8388608 forward 0.0001693272590637207 bandwidth (GB/s) 198.16320293339717
log_normal, size, elements 16777216 forward 0.00033437252044677735 bandwidth (GB/s) 200.70089464986953
log_normal, size, elements 33554432 forward 0.0006206154823303223 bandwidth (GB/s) 216.26551676737367
```",pytorch
20625,syed-ahmed,pr,2019-05-17T04:07:34Z,Move THCTensor_(geometric) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20886 Remove curandStateMTGP32 usage
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* **#20625 Move THCTensor_(geometric) to ATen**
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

Differential Revision: [D15454049](https://our.internmc.facebook.com/intern/diff/D15454049)

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
geometric, size, elements 65536 forward 4.827976226806641e-06 bandwidth (GB/s) 54.296870507456795
geometric, size, elements 131072 forward 5.9986114501953125e-06 bandwidth (GB/s) 87.40156023656598
geometric, size, elements 262144 forward 9.603500366210938e-06 bandwidth (GB/s) 109.18685479404171
geometric, size, elements 524288 forward 1.6007423400878906e-05 bandwidth (GB/s) 131.01121570163838
geometric, size, elements 1048576 forward 2.911090850830078e-05 bandwidth (GB/s) 144.08014778391484
geometric, size, elements 2097152 forward 5.525588989257812e-05 bandwidth (GB/s) 151.81382502947878
geometric, size, elements 4194304 forward 0.00010294198989868164 bandwidth (GB/s) 162.9773818877273
geometric, size, elements 8388608 forward 0.0001985597610473633 bandwidth (GB/s) 168.98908330170744
geometric, size, elements 16777216 forward 0.00038609743118286135 bandwidth (GB/s) 173.8132879941806
geometric, size, elements 33554432 forward 0.0007671475410461426 bandwidth (GB/s) 174.9568639912085
```
#### After:
```
geometric, size, elements 65536 forward 5.98907470703125e-06 bandwidth (GB/s) 43.7703673477707
geometric, size, elements 131072 forward 5.676746368408203e-06 bandwidth (GB/s) 92.3571295905922
geometric, size, elements 262144 forward 6.127357482910156e-06 bandwidth (GB/s) 171.13021443984437
geometric, size, elements 524288 forward 7.076263427734375e-06 bandwidth (GB/s) 296.3643201552561
geometric, size, elements 1048576 forward 1.0535717010498046e-05 bandwidth (GB/s) 398.1033275495814
geometric, size, elements 2097152 forward 1.7604827880859376e-05 bandwidth (GB/s) 476.49474659848323
geometric, size, elements 4194304 forward 2.9888153076171875e-05 bandwidth (GB/s) 561.333313478494
geometric, size, elements 8388608 forward 5.422115325927734e-05 bandwidth (GB/s) 618.8439378916895
geometric, size, elements 16777216 forward 0.00010248422622680665 bandwidth (GB/s) 654.8213951626288
geometric, size, elements 33554432 forward 0.00019872665405273437 bandwidth (GB/s) 675.388657046396
```
### Double Type
#### Before:
```
geometric, size, elements 65536 forward 7.531642913818359e-06 bandwidth (GB/s) 34.80568622272872
geometric, size, elements 131072 forward 7.486343383789062e-06 bandwidth (GB/s) 70.03258775643313
geometric, size, elements 262144 forward 1.2500286102294922e-05 bandwidth (GB/s) 83.8841600439443
geometric, size, elements 524288 forward 2.1970272064208986e-05 bandwidth (GB/s) 95.45407511891482
geometric, size, elements 1048576 forward 4.1151046752929686e-05 bandwidth (GB/s) 101.9246004890846
geometric, size, elements 2097152 forward 7.607698440551757e-05 bandwidth (GB/s) 110.26472809812907
geometric, size, elements 4194304 forward 0.00013311147689819335 bandwidth (GB/s) 126.03883895625013
geometric, size, elements 8388608 forward 0.00026131629943847655 bandwidth (GB/s) 128.40543078293493
geometric, size, elements 16777216 forward 0.0005186843872070312 bandwidth (GB/s) 129.38284948456277
geometric, size, elements 33554432 forward 0.0010293865203857423 bandwidth (GB/s) 130.38613323759532
```
#### After:
```
geometric, size, elements 65536 forward 6.048679351806641e-06 bandwidth (GB/s) 43.33904721229799
geometric, size, elements 131072 forward 7.328987121582031e-06 bandwidth (GB/s) 71.5362152098894
geometric, size, elements 262144 forward 1.009225845336914e-05 bandwidth (GB/s) 103.89904349407041
geometric, size, elements 524288 forward 1.6951560974121092e-05 bandwidth (GB/s) 123.71438849800283
geometric, size, elements 1048576 forward 3.087997436523438e-05 bandwidth (GB/s) 135.82601949054973
geometric, size, elements 2097152 forward 5.675792694091797e-05 bandwidth (GB/s) 147.7962366161136
geometric, size, elements 4194304 forward 0.00010924100875854492 bandwidth (GB/s) 153.57983408119776
geometric, size, elements 8388608 forward 0.0002037382125854492 bandwidth (GB/s) 164.69385675957594
geometric, size, elements 16777216 forward 0.0003897523880004883 bandwidth (GB/s) 172.18332989384
geometric, size, elements 33554432 forward 0.0007770538330078125 bandwidth (GB/s) 172.72642164375063
```",pytorch
20626,syed-ahmed,pr,2019-05-17T04:07:46Z,Speedup bernoulli_scalar_cuda_kernel with grid-stride loop,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #20886 Remove curandStateMTGP32 usage
* **#20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop**
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen
* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen

Differential Revision: [D15454046](https://our.internmc.facebook.com/intern/diff/D15454046)


## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
bernoulli, size, elements 65536 forward 5.810260772705078e-06 bandwidth (GB/s) 45.117424200902754
bernoulli, size, elements 131072 forward 5.700588226318359e-06 bandwidth (GB/s) 91.97085970522794
bernoulli, size, elements 262144 forward 7.650852203369141e-06 bandwidth (GB/s) 137.0534905298847
bernoulli, size, elements 524288 forward 1.1038780212402343e-05 bandwidth (GB/s) 189.98041084682507
bernoulli, size, elements 1048576 forward 1.817464828491211e-05 bandwidth (GB/s) 230.77772588765578
bernoulli, size, elements 2097152 forward 3.152847290039063e-05 bandwidth (GB/s) 266.06451972800966
bernoulli, size, elements 4194304 forward 5.8722496032714846e-05 bandwidth (GB/s) 285.7033868358262
bernoulli, size, elements 8388608 forward 0.0001120924949645996 bandwidth (GB/s) 299.3459286511284
bernoulli, size, elements 16777216 forward 0.0002196049690246582 bandwidth (GB/s) 305.58900510336235
bernoulli, size, elements 33554432 forward 0.0004137754440307617 bandwidth (GB/s) 324.3733525907877
```
#### After:
```
bernoulli, size, elements 65536 forward 5.7387351989746094e-06 bandwidth (GB/s) 45.679751881013715
bernoulli, size, elements 131072 forward 5.600452423095703e-06 bandwidth (GB/s) 93.61529397837378
bernoulli, size, elements 262144 forward 6.201267242431641e-06 bandwidth (GB/s) 169.09060019623223
bernoulli, size, elements 524288 forward 6.272792816162109e-06 bandwidth (GB/s) 334.3250863629039
bernoulli, size, elements 1048576 forward 8.275508880615235e-06 bandwidth (GB/s) 506.83336342310577
bernoulli, size, elements 2097152 forward 1.2857913970947266e-05 bandwidth (GB/s) 652.4081603714445
bernoulli, size, elements 4194304 forward 2.348184585571289e-05 bandwidth (GB/s) 714.4760298270282
bernoulli, size, elements 8388608 forward 4.356622695922851e-05 bandwidth (GB/s) 770.1936647257047
bernoulli, size, elements 16777216 forward 8.656024932861328e-05 bandwidth (GB/s) 775.2850126994326
bernoulli, size, elements 33554432 forward 0.0001675891876220703 bandwidth (GB/s) 800.8734328534002
```
### Double Type
#### Before:
```
bernoulli, size, elements 65536 forward 5.733966827392578e-06 bandwidth (GB/s) 45.717739200665285
bernoulli, size, elements 131072 forward 6.6208839416503905e-06 bandwidth (GB/s) 79.18700956254952
bernoulli, size, elements 262144 forward 1.0859966278076171e-05 bandwidth (GB/s) 96.55425929975851
bernoulli, size, elements 524288 forward 1.7333030700683594e-05 bandwidth (GB/s) 120.99165092445668
bernoulli, size, elements 1048576 forward 3.1557083129882816e-05 bandwidth (GB/s) 132.91165038090057
bernoulli, size, elements 2097152 forward 5.902767181396485e-05 bandwidth (GB/s) 142.11314358523305
bernoulli, size, elements 4194304 forward 0.00011337995529174805 bandwidth (GB/s) 147.9733869785806
bernoulli, size, elements 8388608 forward 0.00022054195404052734 bandwidth (GB/s) 152.14534643070206
bernoulli, size, elements 16777216 forward 0.0004380941390991211 bandwidth (GB/s) 153.18366079491483
bernoulli, size, elements 33554432 forward 0.0008704972267150879 bandwidth (GB/s) 154.1851299245198
```
#### After:
```
bernoulli, size, elements 65536 forward 5.877017974853515e-06 bandwidth (GB/s) 44.60493418969575
bernoulli, size, elements 131072 forward 5.819797515869141e-06 bandwidth (GB/s) 90.08698302138468
bernoulli, size, elements 262144 forward 6.091594696044922e-06 bandwidth (GB/s) 172.1348928025049
bernoulli, size, elements 524288 forward 8.232593536376953e-06 bandwidth (GB/s) 254.73770698546193
bernoulli, size, elements 1048576 forward 1.3000965118408203e-05 bandwidth (GB/s) 322.6148183461581
bernoulli, size, elements 2097152 forward 2.2871494293212892e-05 bandwidth (GB/s) 366.7713133413114
bernoulli, size, elements 4194304 forward 4.316329956054687e-05 bandwidth (GB/s) 388.69169342501107
bernoulli, size, elements 8388608 forward 8.46099853515625e-05 bandwidth (GB/s) 396.5776835981966
bernoulli, size, elements 16777216 forward 0.00016601085662841796 bandwidth (GB/s) 404.2438269577137
bernoulli, size, elements 33554432 forward 0.00031869888305664063 bandwidth (GB/s) 421.14276244936264
```",pytorch
20689,vishwakftw,pr,2019-05-19T17:58:01Z,Enable batched QR decomposition and add a `some` option,"This PR covers two important points with respect to the QR decomposition:
- batching of input matrices (#7500)
- adding `some` as an option in `torch.qr` akin to NumPy's `mode` option (#10538)

Changelog:
- Enable batching for inputs to `torch.qr`
- Move QR decomposition implementation to ATen (CPU and CUDA)
- Remove existing implementations in TH/THC
- Add a `some` option to `torch.qr` that will enable users to switch between complete and reduced decomposition
- Modify doc strings

Test plan:
- Add new tests, remove old ones.

Closes #10538",pytorch
20752,peterjc123,pr,2019-05-21T10:07:47Z,Add support for CMake switches for VS 2019,Appending `arch` to the generator name is not supported for VS starting from VS 2019.,pytorch
20797,vishwakftw,pr,2019-05-22T05:14:11Z,Modify cos to cosh in Vec256,Minor typo fix.,pytorch
20861,orionr,pr,2019-05-23T14:58:49Z,[tensorboard] Some name and variable cleanup,"As a part of https://github.com/pytorch/pytorch/pull/20580 I noticed that we had some unusual variable naming in `summary.py`. This cleans it up and also removes some variables that weren't being used.

I'll wait until we have an `add_custom_scalars` test to land this.

cc @lanpa @natalialunova ",pytorch
20879,rdzhabarov,pr,2019-05-23T20:49:44Z,[docker] Add llvm8 installation step.,"Add ability to build docker container with llvm8.

@ezyang  ",pytorch
20886,syed-ahmed,pr,2019-05-23T22:03:48Z,Remove curandStateMTGP32 usage,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#20886 Remove curandStateMTGP32 usage**
* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #20625 Move THCTensor_(geometric) to ATen
* #20624 Move THCTensor_(lognormal) to ATen
* #20623 Move THCTensor_(exponential) to ATen
* #20622 Move THCTensor_(cauchy) to ATen
* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen

Differential Revision: [D15535503](https://our.internmc.facebook.com/intern/diff/D15535503)

## Summary:
This PR removes curandStateMTGP32 usages since it's not stream-safe.
Main changes are:
- It modifies THCTensor_(getRNGState) and THCTensor_(setRNGState) to not read/write curandStateMTGP anymore.
- It modifies RRelu.cu and cuda multinomial kernels to use curandStatePhilox
- It deletes new_state.clone() from torch.cuda.random.py to get a performance boost.",pytorch
20889,syed-ahmed,pr,2019-05-23T22:51:53Z,Update cuSPARSE namespace collision w/ CUDA 10.1 Update 1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#20889 Update cuSPARSE namespace collision w/ CUDA 10.1 Update 1**

cuSPARSE 10.1.157 and later give us CUSPARSE_VER_* symbols so that we can detect and avoid the conflict.  We should prefer cuSPARSE's own implementation of this function when it's available since it will be aware of all the latest cuSPARSE error codes.

Differential Revision: [D15495545](https://our.internmc.facebook.com/intern/diff/D15495545)",pytorch
20904,vishwakftw,pr,2019-05-24T12:52:17Z,Remove extra workspace queries in matrix inverse computation,"Earlier, the workspace size query and allocation was placed inside the loop.
However, since we have batches of matrices with the same number of rows and columns, the workspace size query and allocation for every matrix in the batch is redundant.

This PR moves the workspace size query and allocation outside the loop, effectively saving (batch_size - 1) number of queries and allocation (and consequently the deallocation).

There is a tremendous speedup in inverse computation as a result of this change.

Changelog:
- Move workspace query and allocation outside the batch loop

Test Plan:
- All existing tests for inverse should pass to verify that the change is correct

",pytorch
20920,ngimel,pr,2019-05-24T18:16:23Z,Fix indexing of large tensors,"Uses `int64_t` OffsetCalculator when necessary, fix for #20888 
cc @colesbury. 
@colesbury, should it be `uint32_t` here https://github.com/pytorch/pytorch/blob/47043220ee3f532ab01a265336ef78d18f6b6645/aten/src/ATen/native/TensorIterator.cpp#L628? Default indexing type is `uint32_t`, not `int32_t`. ",pytorch
20945,peterjc123,pr,2019-05-25T08:08:11Z,Check for incompatible versions between CUDA and MSVC,,pytorch
20968,ssnl,pr,2019-05-26T16:26:38Z,.view(...) now suggests .reshape(...) instead .contiguous().view(...),,pytorch
20976,ssnl,pr,2019-05-27T04:31:29Z,BatchSampler now uses list.clear() instead of creating new objects,[soumith]: PR reverted,pytorch
20977,ssnl,pr,2019-05-27T04:33:15Z,Update cuda pinned memory note to include tensor.to,separate bits of changes from #19228 ,pytorch
21005,peterjc123,pr,2019-05-28T05:41:57Z,Fix build error in c10 on Windows,"Targets https://github.com/pytorch/pytorch/issues/20635#issuecomment-496265510
Reference:
1. https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros?view=vs-2015#microsoft-specific-predefined-macros
2. https://docs.microsoft.com/en-us/cpp/cpp/deprecated-cpp?view=vs-2019",pytorch
21007,peterjc123,pr,2019-05-28T06:09:48Z,Fix incorrect torch version in CMake,Fixes https://github.com/pytorch/pytorch/issues/20525,pytorch
21008,peterjc123,pr,2019-05-28T06:16:17Z,Some minor fixes for the changes in #20945,Fixes after #20945 ,pytorch
21062,peterjc123,pr,2019-05-29T13:25:49Z,"Fix ""cuda: unknown error"" on Windows","Thanks @Jonas1312 for validating this workground. 
Fixes #20635.
However, I don't know exactly why this one is needed.
The following are my guesses:
1. It is a CUDA bug. Static linking against `cudart` is the default now, so they didn't run enough tests for dynamic ones.
2. It is related to UCRT. But (1)according to msdn, shared DLLs should share the same CRT. (2) The CUDA related objects like `CUDevice` passing to `cudart` are stored on the stack, not the heap. (3) If this is the case, it should always fail, not sometimes. https://docs.microsoft.com/en-us/cpp/c-runtime-library/potential-errors-passing-crt-objects-across-dll-boundaries?view=vs-2019
3. It is a bug of our side. However, I was unable to find it.",pytorch
21067,vishwakftw,pr,2019-05-29T17:22:04Z,Make CUDA triu / tril support batches of size > 65535,"In the previous implementation of triu / tril, we passed the batch size in the 2nd dimension of a grid. This is limited to 65535, which means that performing triu / tril on a tensor with batch size > 65535 will throw an error. This PR removes the dependence on the 2nd dimension, and corresponding non-contiguity constraints.

Changelog:
- Compute offset, row and col in the kernel
- Use 1st dimension of grid alone
- Remove unnecessary contiguity checks on tensors as a result of this change.

Test Plan:
- All tests should pass to verify that the change is correct.",pytorch
21069,ssnl,pr,2019-05-29T17:51:47Z,[TESTING] [DataLoader] delay cancel_join_thread until receiver exits,,pytorch
21158,rdzhabarov,pr,2019-05-30T17:41:14Z,[docker] Add llvm installation to the docker image.,LLVM is based on the clang-version.,pytorch
21160,vishwakftw,pr,2019-05-30T18:11:52Z,Add CUDA support for _dirichlet_grad,"Changelog:
- Migrate _dirichlet_grad implementation from TH to ATen
- Add CUDA support for _dirichlet_grad

Closes #11030.
Closes #15773.",pytorch
21190,peterjc123,pr,2019-05-31T06:13:47Z,Improve build docs and process for Windows,"Fixes #21026.
1. Improve build docs for Windows
2. Change `BUILD_SHARED_LIBS=ON` for Caffe2 local builds
3. Change to out-source builds for LibTorch and Caffe2 (transferred to #21452)",pytorch
21191,vishwakftw,pr,2019-05-31T06:19:12Z,Add CUDA support for _dirichlet_grad,"Changelog:
- Migrate _dirichlet_grad implementation from TH to ATen
- Add CUDA support for _dirichlet_grad

Closes #11030.
Closes #15773.",pytorch
21226,ssnl,pr,2019-05-31T21:42:27Z,Fix typo in test_dataloader,,pytorch
21236,ngimel,pr,2019-06-01T00:37:43Z,compare scalar device with common device,"I think there was a typo in #20690 here https://github.com/pytorch/pytorch/pull/20690/files#diff-b47a50873394e38a005b4c1acd151957R130. 
Original conditional was ` common_backend == Backend::CUDA && op.tensor.type().backend() == Backend::CPU)`, now it is `op.device.is_cuda() && op.tensor.device().is_cpu()`. It seems that `op.device` and `op.tensor.device()` should be the same, so this conditional is never true. This leads to spurious h2d copies for operations between cuda tensors and cpu scalars, because cpu scalars are now sent to gpu, instead of being passed to lambdas directly. 
Unfortunately, I don't know how to test this change, because functionally everything was fine after #20690, it was just a performance regression. 

cc @colesbury ",pytorch
21242,peterjc123,pr,2019-06-01T02:32:10Z,[TEST] Debug windows sccache,,pytorch
21248,peterjc123,pr,2019-06-01T07:53:44Z,Fix `sccache not being used on Windows`,Fixes https://github.com/pytorch/pytorch/issues/21167.,pytorch
21256,musikisomorphie,pr,2019-06-01T21:20:23Z,add ssim and ms-ssim loss  #6934,"Hi, @soumith, @veritas9872. I added ssim/ms-ssim loss in the loss.py and functional.py. Some related units tests are also added in the test_nn.py. As @veritas9872 requested, I exposed the parameters to the users. 
Also, I run some experiments for comparing the results between the original matlab implementation and my version.
For ssim the difference is around 1%, for ms-ssim the difference is slightly larger than ssim. ",pytorch
21274,vishwakftw,pr,2019-06-03T07:16:01Z,Add derivative for QR decomposition,"Changelog:
- Implement derivative for QR decomposition for tall and square matrices i.e., num rows >= num cols

Test Plan:
- Add tests for QR decomposition derivative in common_methods_invocations.py

",pytorch
21289,syed-ahmed,pr,2019-06-03T16:54:17Z,Move THCTensor_(cauchy) to ATen,"## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
cauchy, size, elements 65536 forward 4.980564117431641e-06 bandwidth (GB/s) 52.63339529803734
cauchy, size, elements 131072 forward 6.232261657714844e-06 bandwidth (GB/s) 84.12483762631982
cauchy, size, elements 262144 forward 9.548664093017577e-06 bandwidth (GB/s) 109.81389540833959
cauchy, size, elements 524288 forward 1.59454345703125e-05 bandwidth (GB/s) 131.52052963827754
cauchy, size, elements 1048576 forward 2.86865234375e-05 bandwidth (GB/s) 146.21165262978724
cauchy, size, elements 2097152 forward 5.4748058319091796e-05 bandwidth (GB/s) 153.2220184158516
cauchy, size, elements 4194304 forward 0.00010075807571411133 bandwidth (GB/s) 166.50988897012377
cauchy, size, elements 8388608 forward 0.0001935744285583496 bandwidth (GB/s) 173.34124269355965
cauchy, size, elements 16777216 forward 0.00038077831268310545 bandwidth (GB/s) 176.24129779641603
cauchy, size, elements 33554432 forward 0.0006851387023925781 bandwidth (GB/s) 195.8986224705994
```
#### After:
```
cauchy, size, elements 65536 forward 6.077289581298828e-06 bandwidth (GB/s) 43.13501874366419
cauchy, size, elements 131072 forward 6.2131881713867184e-06 bandwidth (GB/s) 84.38308731972373
cauchy, size, elements 262144 forward 6.46829605102539e-06 bandwidth (GB/s) 162.11008150033175
cauchy, size, elements 524288 forward 6.8783760070800785e-06 bandwidth (GB/s) 304.8905726935182
cauchy, size, elements 1048576 forward 9.505748748779296e-06 bandwidth (GB/s) 441.23867681003264
cauchy, size, elements 2097152 forward 1.5070438385009766e-05 bandwidth (GB/s) 556.6266744001266
cauchy, size, elements 4194304 forward 2.4406909942626954e-05 bandwidth (GB/s) 687.396152951685
cauchy, size, elements 8388608 forward 4.6243667602539064e-05 bandwidth (GB/s) 725.6005792706125
cauchy, size, elements 16777216 forward 9.100198745727539e-05 bandwidth (GB/s) 737.4439380404413
cauchy, size, elements 33554432 forward 0.00017449140548706055 bandwidth (GB/s) 769.1939188944922
```
### Double Type
#### Before:
```
cauchy, size, elements 65536 forward 4.885196685791015e-06 bandwidth (GB/s) 53.660889593753055
cauchy, size, elements 131072 forward 6.229877471923828e-06 bandwidth (GB/s) 84.15703235943361
cauchy, size, elements 262144 forward 9.605884552001953e-06 bandwidth (GB/s) 109.15975455706132
cauchy, size, elements 524288 forward 1.5976428985595704e-05 bandwidth (GB/s) 131.26537863315923
cauchy, size, elements 1048576 forward 2.9621124267578124e-05 bandwidth (GB/s) 141.59840666786866
cauchy, size, elements 2097152 forward 5.5103302001953126e-05 bandwidth (GB/s) 152.23421637604707
cauchy, size, elements 4194304 forward 0.00010124444961547851 bandwidth (GB/s) 165.70998275677383
cauchy, size, elements 8388608 forward 0.0001944279670715332 bandwidth (GB/s) 172.58027487195184
cauchy, size, elements 16777216 forward 0.00034950494766235353 bandwidth (GB/s) 192.01119883668116
cauchy, size, elements 33554432 forward 0.0007002186775207519 bandwidth (GB/s) 191.67973135938277
```
#### After:
```
cauchy, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032
cauchy, size, elements 131072 forward 6.234645843505859e-06 bandwidth (GB/s) 84.09266751632889
cauchy, size, elements 262144 forward 7.433891296386719e-06 bandwidth (GB/s) 141.05344807902503
cauchy, size, elements 524288 forward 1.1401176452636719e-05 bandwidth (GB/s) 183.94171941045587
cauchy, size, elements 1048576 forward 1.960039138793945e-05 bandwidth (GB/s) 213.99082890665372
cauchy, size, elements 2097152 forward 3.434181213378906e-05 bandwidth (GB/s) 244.26806504326578
cauchy, size, elements 4194304 forward 6.517410278320313e-05 bandwidth (GB/s) 257.4215107465028
cauchy, size, elements 8388608 forward 0.0001229524612426758 bandwidth (GB/s) 272.9057365819818
cauchy, size, elements 16777216 forward 0.00023239374160766602 bandwidth (GB/s) 288.77225150621814
cauchy, size, elements 33554432 forward 0.00046050310134887696 bandwidth (GB/s) 291.4589013773367
```
Resubmit of https://github.com/pytorch/pytorch/pull/20622",pytorch
21297,syed-ahmed,pr,2019-06-03T19:02:26Z,Move THCTensor_(exponential) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #21301 Remove curandStateMTGP32 usage
* #21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #21299 Move THCTensor_(lognormal) to ATen
* #21298 Move THCTensor_(geometric) to ATen
* **#21297 Move THCTensor_(exponential) to ATen**

Resubmit of https://github.com/pytorch/pytorch/pull/20623


## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
exponential, size, elements 65536 forward 4.951953887939453e-06 bandwidth (GB/s) 52.937488097063074
exponential, size, elements 131072 forward 5.1164627075195315e-06 bandwidth (GB/s) 102.47079476011184
exponential, size, elements 262144 forward 7.412433624267578e-06 bandwidth (GB/s) 141.46177263119975
exponential, size, elements 524288 forward 1.1911392211914063e-05 bandwidth (GB/s) 176.06271061265014
exponential, size, elements 1048576 forward 2.077341079711914e-05 bandwidth (GB/s) 201.90733437869852
exponential, size, elements 2097152 forward 3.968000411987305e-05 bandwidth (GB/s) 211.4064296631136
exponential, size, elements 4194304 forward 7.367134094238281e-05 bandwidth (GB/s) 227.73056368176054
exponential, size, elements 8388608 forward 0.0001375126838684082 bandwidth (GB/s) 244.00972372926472
exponential, size, elements 16777216 forward 0.0002710747718811035 bandwidth (GB/s) 247.5658783526883
exponential, size, elements 33554432 forward 0.0005277013778686524 bandwidth (GB/s) 254.34409237682056
```
#### After:
```
exponential, size, elements 65536 forward 5.60760498046875e-06 bandwidth (GB/s) 46.74794335782313
exponential, size, elements 131072 forward 7.700920104980468e-06 bandwidth (GB/s) 68.0812153421672
exponential, size, elements 262144 forward 6.551742553710938e-06 bandwidth (GB/s) 160.04536066608443
exponential, size, elements 524288 forward 6.9427490234375e-06 bandwidth (GB/s) 302.0636340043956
exponential, size, elements 1048576 forward 9.472370147705077e-06 bandwidth (GB/s) 442.7935072845709
exponential, size, elements 2097152 forward 1.4712810516357422e-05 bandwidth (GB/s) 570.1567345459731
exponential, size, elements 4194304 forward 2.4566650390625e-05 bandwidth (GB/s) 682.9264768795031
exponential, size, elements 8388608 forward 4.60505485534668e-05 bandwidth (GB/s) 728.6434810009216
exponential, size, elements 16777216 forward 9.00864601135254e-05 bandwidth (GB/s) 744.9384060094111
exponential, size, elements 33554432 forward 0.00017408370971679687 bandwidth (GB/s) 770.9953344764326
```
### Double Type
#### Before:
```
exponential, size, elements 65536 forward 4.985332489013672e-06 bandwidth (GB/s) 52.58305250004783
exponential, size, elements 131072 forward 6.051063537597656e-06 bandwidth (GB/s) 86.64394229913319
exponential, size, elements 262144 forward 9.377002716064453e-06 bandwidth (GB/s) 111.82421843640988
exponential, size, elements 524288 forward 1.549959182739258e-05 bandwidth (GB/s) 135.30369208134132
exponential, size, elements 1048576 forward 2.866983413696289e-05 bandwidth (GB/s) 146.2967654421289
exponential, size, elements 2097152 forward 5.302190780639648e-05 bandwidth (GB/s) 158.2102256793561
exponential, size, elements 4194304 forward 9.615898132324219e-05 bandwidth (GB/s) 174.47372849762968
exponential, size, elements 8388608 forward 0.00018301725387573242 bandwidth (GB/s) 183.34026595537955
exponential, size, elements 16777216 forward 0.0003589057922363281 bandwidth (GB/s) 186.98183604629858
exponential, size, elements 33554432 forward 0.000672616958618164 bandwidth (GB/s) 199.5455604862227
```
#### After:
```
exponential, size, elements 65536 forward 5.755424499511719e-06 bandwidth (GB/s) 45.547291954266775
exponential, size, elements 131072 forward 6.275177001953125e-06 bandwidth (GB/s) 83.54951578844985
exponential, size, elements 262144 forward 7.97271728515625e-06 bandwidth (GB/s) 131.52052963827754
exponential, size, elements 524288 forward 1.2047290802001953e-05 bandwidth (GB/s) 174.07664797561844
exponential, size, elements 1048576 forward 2.0439624786376954e-05 bandwidth (GB/s) 205.20454968407793
exponential, size, elements 2097152 forward 3.5920143127441405e-05 bandwidth (GB/s) 233.53492691379267
exponential, size, elements 4194304 forward 6.896495819091797e-05 bandwidth (GB/s) 243.27160401598564
exponential, size, elements 8388608 forward 0.00012843608856201173 bandwidth (GB/s) 261.2539230653945
exponential, size, elements 16777216 forward 0.0002438235282897949 bandwidth (GB/s) 275.23539041005995
exponential, size, elements 33554432 forward 0.00046614646911621096 bandwidth (GB/s) 287.93037573462635
```

Differential Revision: [D15632931](https://our.internmc.facebook.com/intern/diff/D15632931)",pytorch
21298,syed-ahmed,pr,2019-06-03T19:02:40Z,Move THCTensor_(geometric) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #21301 Remove curandStateMTGP32 usage
* #21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #21299 Move THCTensor_(lognormal) to ATen
* **#21298 Move THCTensor_(geometric) to ATen**
* #21297 Move THCTensor_(exponential) to ATen

Resubmit of https://github.com/pytorch/pytorch/pull/20625

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
geometric, size, elements 65536 forward 4.827976226806641e-06 bandwidth (GB/s) 54.296870507456795
geometric, size, elements 131072 forward 5.9986114501953125e-06 bandwidth (GB/s) 87.40156023656598
geometric, size, elements 262144 forward 9.603500366210938e-06 bandwidth (GB/s) 109.18685479404171
geometric, size, elements 524288 forward 1.6007423400878906e-05 bandwidth (GB/s) 131.01121570163838
geometric, size, elements 1048576 forward 2.911090850830078e-05 bandwidth (GB/s) 144.08014778391484
geometric, size, elements 2097152 forward 5.525588989257812e-05 bandwidth (GB/s) 151.81382502947878
geometric, size, elements 4194304 forward 0.00010294198989868164 bandwidth (GB/s) 162.9773818877273
geometric, size, elements 8388608 forward 0.0001985597610473633 bandwidth (GB/s) 168.98908330170744
geometric, size, elements 16777216 forward 0.00038609743118286135 bandwidth (GB/s) 173.8132879941806
geometric, size, elements 33554432 forward 0.0007671475410461426 bandwidth (GB/s) 174.9568639912085
```
#### After:
```
geometric, size, elements 65536 forward 5.98907470703125e-06 bandwidth (GB/s) 43.7703673477707
geometric, size, elements 131072 forward 5.676746368408203e-06 bandwidth (GB/s) 92.3571295905922
geometric, size, elements 262144 forward 6.127357482910156e-06 bandwidth (GB/s) 171.13021443984437
geometric, size, elements 524288 forward 7.076263427734375e-06 bandwidth (GB/s) 296.3643201552561
geometric, size, elements 1048576 forward 1.0535717010498046e-05 bandwidth (GB/s) 398.1033275495814
geometric, size, elements 2097152 forward 1.7604827880859376e-05 bandwidth (GB/s) 476.49474659848323
geometric, size, elements 4194304 forward 2.9888153076171875e-05 bandwidth (GB/s) 561.333313478494
geometric, size, elements 8388608 forward 5.422115325927734e-05 bandwidth (GB/s) 618.8439378916895
geometric, size, elements 16777216 forward 0.00010248422622680665 bandwidth (GB/s) 654.8213951626288
geometric, size, elements 33554432 forward 0.00019872665405273437 bandwidth (GB/s) 675.388657046396
```
### Double Type
#### Before:
```
geometric, size, elements 65536 forward 7.531642913818359e-06 bandwidth (GB/s) 34.80568622272872
geometric, size, elements 131072 forward 7.486343383789062e-06 bandwidth (GB/s) 70.03258775643313
geometric, size, elements 262144 forward 1.2500286102294922e-05 bandwidth (GB/s) 83.8841600439443
geometric, size, elements 524288 forward 2.1970272064208986e-05 bandwidth (GB/s) 95.45407511891482
geometric, size, elements 1048576 forward 4.1151046752929686e-05 bandwidth (GB/s) 101.9246004890846
geometric, size, elements 2097152 forward 7.607698440551757e-05 bandwidth (GB/s) 110.26472809812907
geometric, size, elements 4194304 forward 0.00013311147689819335 bandwidth (GB/s) 126.03883895625013
geometric, size, elements 8388608 forward 0.00026131629943847655 bandwidth (GB/s) 128.40543078293493
geometric, size, elements 16777216 forward 0.0005186843872070312 bandwidth (GB/s) 129.38284948456277
geometric, size, elements 33554432 forward 0.0010293865203857423 bandwidth (GB/s) 130.38613323759532
```
#### After:
```
geometric, size, elements 65536 forward 6.048679351806641e-06 bandwidth (GB/s) 43.33904721229799
geometric, size, elements 131072 forward 7.328987121582031e-06 bandwidth (GB/s) 71.5362152098894
geometric, size, elements 262144 forward 1.009225845336914e-05 bandwidth (GB/s) 103.89904349407041
geometric, size, elements 524288 forward 1.6951560974121092e-05 bandwidth (GB/s) 123.71438849800283
geometric, size, elements 1048576 forward 3.087997436523438e-05 bandwidth (GB/s) 135.82601949054973
geometric, size, elements 2097152 forward 5.675792694091797e-05 bandwidth (GB/s) 147.7962366161136
geometric, size, elements 4194304 forward 0.00010924100875854492 bandwidth (GB/s) 153.57983408119776
geometric, size, elements 8388608 forward 0.0002037382125854492 bandwidth (GB/s) 164.69385675957594
geometric, size, elements 16777216 forward 0.0003897523880004883 bandwidth (GB/s) 172.18332989384
geometric, size, elements 33554432 forward 0.0007770538330078125 bandwidth (GB/s) 172.72642164375063
```

Differential Revision: [D15632932](https://our.internmc.facebook.com/intern/diff/D15632932)",pytorch
21299,syed-ahmed,pr,2019-06-03T19:02:52Z,Move THCTensor_(lognormal) to ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #21301 Remove curandStateMTGP32 usage
* #21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* **#21299 Move THCTensor_(lognormal) to ATen**
* #21298 Move THCTensor_(geometric) to ATen
* #21297 Move THCTensor_(exponential) to ATen

Resubmit of https://github.com/pytorch/pytorch/pull/20624

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
log_normal, size, elements 65536 forward 4.84466552734375e-06 bandwidth (GB/s) 54.1098242015748
log_normal, size, elements 131072 forward 5.0425529479980465e-06 bandwidth (GB/s) 103.97273075895983
log_normal, size, elements 262144 forward 7.326602935791016e-06 bandwidth (GB/s) 143.11898832098927
log_normal, size, elements 524288 forward 1.1749267578125e-05 bandwidth (GB/s) 178.49214736623378
log_normal, size, elements 1048576 forward 2.05230712890625e-05 bandwidth (GB/s) 204.37019103643124
log_normal, size, elements 2097152 forward 3.9284229278564456e-05 bandwidth (GB/s) 213.53627534643442
log_normal, size, elements 4194304 forward 7.281541824340821e-05 bandwidth (GB/s) 230.40746595613766
log_normal, size, elements 8388608 forward 0.00013544559478759766 bandwidth (GB/s) 247.7336531514311
log_normal, size, elements 16777216 forward 0.0002670741081237793 bandwidth (GB/s) 251.27431659866272
log_normal, size, elements 33554432 forward 0.0005250406265258789 bandwidth (GB/s) 255.63303336753222
```
#### After:
```
log_normal, size, elements 65536 forward 5.47647476196289e-06 bandwidth (GB/s) 47.86728897588159
log_normal, size, elements 131072 forward 6.859302520751953e-06 bandwidth (GB/s) 76.43459351936045
log_normal, size, elements 262144 forward 7.7056884765625e-06 bandwidth (GB/s) 136.07817175445544
log_normal, size, elements 524288 forward 8.029937744140625e-06 bandwidth (GB/s) 261.1666574289786
log_normal, size, elements 1048576 forward 1.1892318725585938e-05 bandwidth (GB/s) 352.6901773138733
log_normal, size, elements 2097152 forward 1.9683837890625e-05 bandwidth (GB/s) 426.1672975875969
log_normal, size, elements 4194304 forward 3.241539001464844e-05 bandwidth (GB/s) 517.5694629130921
log_normal, size, elements 8388608 forward 5.803346633911133e-05 bandwidth (GB/s) 578.1910700272298
log_normal, size, elements 16777216 forward 0.00011091709136962891 bandwidth (GB/s) 605.0362768381755
log_normal, size, elements 33554432 forward 0.00021491527557373046 bandwidth (GB/s) 624.5146029834174
```
### Double Type
#### Before:
```
log_normal, size, elements 65536 forward 5.793571472167969e-06 bandwidth (GB/s) 45.247392089547326
log_normal, size, elements 131072 forward 8.199214935302735e-06 bandwidth (GB/s) 63.943682918057576
log_normal, size, elements 262144 forward 1.3582706451416015e-05 bandwidth (GB/s) 77.19934195373004
log_normal, size, elements 524288 forward 2.3326873779296876e-05 bandwidth (GB/s) 89.90283137988553
log_normal, size, elements 1048576 forward 4.379749298095703e-05 bandwidth (GB/s) 95.76584673062604
log_normal, size, elements 2097152 forward 8.105754852294922e-05 bandwidth (GB/s) 103.48953493979646
log_normal, size, elements 4194304 forward 0.0001421213150024414 bandwidth (GB/s) 118.04855590951854
log_normal, size, elements 8388608 forward 0.00027796506881713865 bandwidth (GB/s) 120.71456367804988
log_normal, size, elements 16777216 forward 0.0005494546890258789 bandwidth (GB/s) 122.13721229493271
log_normal, size, elements 33554432 forward 0.0010767412185668946 bandwidth (GB/s) 124.65179718729368
```
#### After:
```
log_normal, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032
log_normal, size, elements 131072 forward 7.789134979248047e-06 bandwidth (GB/s) 67.31017005056627
log_normal, size, elements 262144 forward 9.219646453857422e-06 bandwidth (GB/s) 113.73277763392811
log_normal, size, elements 524288 forward 1.5113353729248047e-05 bandwidth (GB/s) 138.7615242500079
log_normal, size, elements 1048576 forward 2.7089118957519532e-05 bandwidth (GB/s) 154.83353321964444
log_normal, size, elements 2097152 forward 4.64177131652832e-05 bandwidth (GB/s) 180.71997580169503
log_normal, size, elements 4194304 forward 8.719682693481446e-05 bandwidth (GB/s) 192.40626740399748
log_normal, size, elements 8388608 forward 0.0001693272590637207 bandwidth (GB/s) 198.16320293339717
log_normal, size, elements 16777216 forward 0.00033437252044677735 bandwidth (GB/s) 200.70089464986953
log_normal, size, elements 33554432 forward 0.0006206154823303223 bandwidth (GB/s) 216.26551676737367
```

Differential Revision: [D15632930](https://our.internmc.facebook.com/intern/diff/D15632930)",pytorch
21300,syed-ahmed,pr,2019-06-03T19:03:12Z,Speedup bernoulli_scalar_cuda_kernel with grid-stride loop,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #21301 Remove curandStateMTGP32 usage
* **#21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop**
* #21299 Move THCTensor_(lognormal) to ATen
* #21298 Move THCTensor_(geometric) to ATen
* #21297 Move THCTensor_(exponential) to ATen

Resubmit of https://github.com/pytorch/pytorch/pull/20626

## Effective Bandwidth Benchmark
- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68
- on V100
### Float Type
#### Before:
```
bernoulli, size, elements 65536 forward 5.810260772705078e-06 bandwidth (GB/s) 45.117424200902754
bernoulli, size, elements 131072 forward 5.700588226318359e-06 bandwidth (GB/s) 91.97085970522794
bernoulli, size, elements 262144 forward 7.650852203369141e-06 bandwidth (GB/s) 137.0534905298847
bernoulli, size, elements 524288 forward 1.1038780212402343e-05 bandwidth (GB/s) 189.98041084682507
bernoulli, size, elements 1048576 forward 1.817464828491211e-05 bandwidth (GB/s) 230.77772588765578
bernoulli, size, elements 2097152 forward 3.152847290039063e-05 bandwidth (GB/s) 266.06451972800966
bernoulli, size, elements 4194304 forward 5.8722496032714846e-05 bandwidth (GB/s) 285.7033868358262
bernoulli, size, elements 8388608 forward 0.0001120924949645996 bandwidth (GB/s) 299.3459286511284
bernoulli, size, elements 16777216 forward 0.0002196049690246582 bandwidth (GB/s) 305.58900510336235
bernoulli, size, elements 33554432 forward 0.0004137754440307617 bandwidth (GB/s) 324.3733525907877
```
#### After:
```
bernoulli, size, elements 65536 forward 5.7387351989746094e-06 bandwidth (GB/s) 45.679751881013715
bernoulli, size, elements 131072 forward 5.600452423095703e-06 bandwidth (GB/s) 93.61529397837378
bernoulli, size, elements 262144 forward 6.201267242431641e-06 bandwidth (GB/s) 169.09060019623223
bernoulli, size, elements 524288 forward 6.272792816162109e-06 bandwidth (GB/s) 334.3250863629039
bernoulli, size, elements 1048576 forward 8.275508880615235e-06 bandwidth (GB/s) 506.83336342310577
bernoulli, size, elements 2097152 forward 1.2857913970947266e-05 bandwidth (GB/s) 652.4081603714445
bernoulli, size, elements 4194304 forward 2.348184585571289e-05 bandwidth (GB/s) 714.4760298270282
bernoulli, size, elements 8388608 forward 4.356622695922851e-05 bandwidth (GB/s) 770.1936647257047
bernoulli, size, elements 16777216 forward 8.656024932861328e-05 bandwidth (GB/s) 775.2850126994326
bernoulli, size, elements 33554432 forward 0.0001675891876220703 bandwidth (GB/s) 800.8734328534002
```
### Double Type
#### Before:
```
bernoulli, size, elements 65536 forward 5.733966827392578e-06 bandwidth (GB/s) 45.717739200665285
bernoulli, size, elements 131072 forward 6.6208839416503905e-06 bandwidth (GB/s) 79.18700956254952
bernoulli, size, elements 262144 forward 1.0859966278076171e-05 bandwidth (GB/s) 96.55425929975851
bernoulli, size, elements 524288 forward 1.7333030700683594e-05 bandwidth (GB/s) 120.99165092445668
bernoulli, size, elements 1048576 forward 3.1557083129882816e-05 bandwidth (GB/s) 132.91165038090057
bernoulli, size, elements 2097152 forward 5.902767181396485e-05 bandwidth (GB/s) 142.11314358523305
bernoulli, size, elements 4194304 forward 0.00011337995529174805 bandwidth (GB/s) 147.9733869785806
bernoulli, size, elements 8388608 forward 0.00022054195404052734 bandwidth (GB/s) 152.14534643070206
bernoulli, size, elements 16777216 forward 0.0004380941390991211 bandwidth (GB/s) 153.18366079491483
bernoulli, size, elements 33554432 forward 0.0008704972267150879 bandwidth (GB/s) 154.1851299245198
```
#### After:
```
bernoulli, size, elements 65536 forward 5.877017974853515e-06 bandwidth (GB/s) 44.60493418969575
bernoulli, size, elements 131072 forward 5.819797515869141e-06 bandwidth (GB/s) 90.08698302138468
bernoulli, size, elements 262144 forward 6.091594696044922e-06 bandwidth (GB/s) 172.1348928025049
bernoulli, size, elements 524288 forward 8.232593536376953e-06 bandwidth (GB/s) 254.73770698546193
bernoulli, size, elements 1048576 forward 1.3000965118408203e-05 bandwidth (GB/s) 322.6148183461581
bernoulli, size, elements 2097152 forward 2.2871494293212892e-05 bandwidth (GB/s) 366.7713133413114
bernoulli, size, elements 4194304 forward 4.316329956054687e-05 bandwidth (GB/s) 388.69169342501107
bernoulli, size, elements 8388608 forward 8.46099853515625e-05 bandwidth (GB/s) 396.5776835981966
bernoulli, size, elements 16777216 forward 0.00016601085662841796 bandwidth (GB/s) 404.2438269577137
bernoulli, size, elements 33554432 forward 0.00031869888305664063 bandwidth (GB/s) 421.14276244936264
```

Differential Revision: [D15632935](https://our.internmc.facebook.com/intern/diff/D15632935)",pytorch
21301,syed-ahmed,pr,2019-06-03T19:03:24Z,Remove curandStateMTGP32 usage,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#21301 Remove curandStateMTGP32 usage**
* #21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #21299 Move THCTensor_(lognormal) to ATen
* #21298 Move THCTensor_(geometric) to ATen
* #21297 Move THCTensor_(exponential) to ATen

Resubmit of https://github.com/pytorch/pytorch/pull/20886

## Summary:
This PR removes curandStateMTGP32 usages since it's not stream-safe.
Main changes are:
- It modifies THCTensor_(getRNGState) and THCTensor_(setRNGState) to not read/write curandStateMTGP anymore.
- It modifies RRelu.cu and cuda multinomial kernels to use curandStatePhilox
- It deletes new_state.clone() from torch.cuda.random.py to get a performance boost.

Differential Revision: [D15632929](https://our.internmc.facebook.com/intern/diff/D15632929)",pytorch
21324,vishwakftw,pr,2019-06-04T02:49:04Z,[RELAND] Fix bug in multinomial_alias_draw,"An incorrect increment / decrement caused the samples to not be generated from a multinomial distribution

Changelog:
- Remove the incorrect increment / decrement operation

Fixes #21257, fixes #21508

cc: @LeviViana @neerajprad ",pytorch
21335,peterjc123,pr,2019-06-04T07:23:10Z,"Revert ""Fix the caffe2_gpu linkage with torch on Windows""","The original PR (#16071) is not working anymore after `caffe2` and `torch` is unified. What's more, It is making the binary big since the optimizing flag is disabled on a very big project(the `torch` library used to be small, but it now applies on the whole `caffe2` and `caffe2_gpu` library). We need to get it reverted.",pytorch
21364,syed-ahmed,pr,2019-06-04T18:26:45Z,[CPU] Refactor Random Number Generators in ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#21364 [CPU] Refactor Random Number Generators in ATen**
* #21301 Remove curandStateMTGP32 usage
* #21300 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop
* #21299 Move THCTensor_(lognormal) to ATen
* #21298 Move THCTensor_(geometric) to ATen
* #21297 Move THCTensor_(exponential) to ATen

Resubmit of https://github.com/pytorch/pytorch/pull/16604
This PR originated from https://github.com/pytorch/pytorch/pull/13070.

## Summary:
The purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (`THCRNGState, THCState, THCRandom_Init` etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR is the first part of more refactoring effort surrounding RNGs and only handles the PyTorch front-end and CPU backend for now. It does the following:
- Clarifies generator concepts by reviewing Generator, CPUGenerator and CUDAGenerator classes.
- Moves mt19937 from TH to aten as MT19937RNGEngine.h and also moves distributions from THRandom.cpp to DistributionsHelper.h. Adds PhiloxRNGEngine.h engine and adds unit tests for it.
- Fixes hardcoded generator related code in several python files used for code generation, such as `function_wrapper.py` etc.
- Fixes generator front-end python bindings to include device kwarg and default kwarg
- Removes creation of generator from Types.
- Updates documentations and comments and adds documentation for `torch.Generator` api.

Differential Revision: [D15696497](https://our.internmc.facebook.com/intern/diff/D15696497)",pytorch
21386,syed-ahmed,pr,2019-06-04T22:17:35Z,Adds CUDA C++11 and Profiling Notes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#21386 Adds CUDA C++11 and Profiling Notes**

Differential Revision: [D15640102](https://our.internmc.facebook.com/intern/diff/D15640102)",pytorch
21410,peterjc123,pr,2019-06-05T13:06:52Z, Fix Caffe2 CI job for new Windows AMI,,pytorch
21452,peterjc123,pr,2019-06-06T05:17:10Z,[Re-landing] Fix caffe2 windows CI for new Windows AMI,The alternative of #21410.,pytorch
21460,vfdev-5,pr,2019-06-06T12:34:19Z,Addresses bad behavior with overridden optimizer.step by #20124,"This PR addresses the problem described in the comment: https://github.com/pytorch/pytorch/pull/20203#issuecomment-499231276
and previously coded bad behaviour:
- a warning was raised all the times when lr schedulling is initialized

Now the code checks that:
- on the second call of `lr_scheduler.step`, ensure that `optimizer.step` has been already called, otherwise raise a warning (as it was done in #20203 )
- if optimizer's step is overridden -> raise once another warning to aware user about the new pattern:
`opt.step()` -> `lrs.step()` as we can not check this .

Now tests check that 
- at initialization (`lrs = StepLR(...)`)there is no warnings
- if we replace `optimizer.step` by something else (similarly to the [code of nvidia/apex](https://github.com/NVIDIA/apex/blob/master/apex/amp/_process_optimizer.py#L287)) there is another warning raised.

cc @ezyang 

PS. honestly I would say that there is a lot of overhead introduced for simple warnings. I hope all these checks will be removed in future `1.2.0` or other versions...
",pytorch
21555,syed-ahmed,pr,2019-06-08T02:08:06Z,[CUDA] Refactor Random Number Generators in ATen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#21555 [CUDA] Refactor Random Number Generators in ATen**
* #21364 [CPU] Refactor Random Number Generators in ATen

### Summary:
Resubmit of: https://github.com/pytorch/pytorch/pull/13070

### How to migrate your code after https://github.com/pytorch/pytorch/pull/21364 and https://github.com/pytorch/pytorch/pull/21555 refactors?
1. Before `CPUGenerator`/`CUDAGenerator` constructor took `at::Context`. Now `CPUGenerator` is constructed with a `uint64_t seed` and `CUDAGenerator` is constructed with a `Device`.
2. `Generator& copy(const Generator& other)` function is deleted. The analogous function to use is `std::shared_ptr<Generator> clone() const;`
3. `Generator::initialSeed()`, `Generator::manualSeed(uint64_t seed)` and `Generator::manualSeedAll(uint64_t seed)` functions are replaced with the following:
   - For `Generator::initialSeed()`, use `Generator::current_seed()`
   - For `Generator::manualSeed(uint64_t seed)` use `Generator::set_current_seed(uint64_t seed)`
   - For `Generator::manualSeedAll(uint64_t seed)` use `CUDAGenerator::set_current_seed(uint64_t seed)` in a loop
4. Creation of generators through types are deleted: `at::CPU(at::kFloat).generator().release();`.
   - To create a CPUGenerator, use `at::detail::createCPUGenerator()` with `#include <ATen/CPUGenerator.h>`
   - To create a CUDAGenerator, use `at::cuda::detail::createCUDAGenerator()` with `#include <ATen/CUDAGenerator.h>`
   - Both of these APIs, return a shared pointer to the backend specific `Generator`.
5. `THRandom_random()` and `THRandom_random64()` is replaced with `CPUGenerator::random()` and `CPUGenerator::random64()` respectively.
6. `at::check_generator` is replaced with `at::get_generator_or_default`. Also the function now lies in `ATen/Utils.h` header and `ATen/CheckGenerator.h` is deleted.
    Note that, we used to add a generated line using `at::check_generator` for Tensor function definitions with `Generator *` parameter. That mechanism has been
    deleted. It is now the user's responsibility to add `at::get_generator_or_default` if they want a backend specific `Generator`.
7. `ATen/cuda/Array.h` has been moved to `ATen/core/Array.h`
8. For CUDA kernels we used to use a function called `next_philox_seed`. This is now part of the generator class as `CUDAGenerator::philox_engine_inputs`.

### New API introduced after https://github.com/pytorch/pytorch/pull/21364 and https://github.com/pytorch/pytorch/pull/21555 refactors
1. `at::detail::getDefaultCPUGenerator()` in `#include <ATen/CPUGenerator.h>`- returns the default CPUGenerator singleton.
2. `at::cuda::detail::getDefaultCUDAGenerator(Device)` in `#include <ATen/CUDAGenerator.h>` - returns the default CUDAGenerator singleton (on specific device).
3. Helper distribution functions in `ATen/core/DistributionsHelper.h` following `std` distribution style of coding. For instance,
   you can now define
   ```
   auto gen = at::detail::getDefaultCPUGenerator();
   at::uniform_real_distribution<double> uniform(0, 1);
   auto sample = uniform(gen);
   ```
4. `at::detail::getNonDeterministicRandom()` in `#include <ATen/core/Generator.h>`- returns a nondeterministic random number /dev/random or time
5. RNG engines - `ATen/core/MT19937RNGEngine.h` and `ATen/core/PhiloxRNGEngine.h`
6. `torch.seed()` - seeds the default CPU and CUDAGenerator with a nondeterministic seed.
7. `torch.Generator(device='cuda')` - You can now create `torch.Generator()` instances for CUDA. Moreover, you can now pass `torch.Generator(device='cuda')` to CUDA random sampling methods which take Generator as a parameter.
8. `torch.default_cuda_generators` - returns a tuple of default cuda torch.Generators. The size of the tuple is equal to the number of GPUs availabe in the system

Differential Revision: [D15875780](https://our.internmc.facebook.com/intern/diff/D15875780)",pytorch
21580,peterjc123,pr,2019-06-10T02:39:09Z,Fix NVTX path on Windows,,pytorch
21588,vishwakftw,pr,2019-06-10T13:44:18Z,"Port SVD to ATen, enable batching for matrix inputs","Changelog:
- Port SVD TH implementation to ATen/native/BatchLinearAlgebra.cpp
- Port SVD THC implementation to ATen/native/cuda/BatchLinearAlgebra.cu
- Allow batches of matrices as arguments to `torch.svd`
- Remove existing implementations in TH and THC
- Update doc string
- Update derivatives to support batching
- Modify nuclear norm implementation to use at::svd instead of _batch_svd
- Remove _batch_svd as it is redundant

Test Plan:
- Add new test suite for SVD in test_torch.py with port to test_cuda.py
- Add tests in common_methods_invocations.py for derivative testing",pytorch
21590,vishwakftw,pr,2019-06-10T14:45:43Z,Skip triangular_solve CUDA test on non-default stream,,pytorch
21619,ngimel,pr,2019-06-10T22:17:17Z,make range functions respect current stream,"Stream is not respected on range/linspace/logspace functions, which contributes to #21589 (this is not a complete solution for that issue). ",pytorch
21658,ngimel,pr,2019-06-11T22:01:26Z,fix multihead attention for half,"Currently multihead attention for half type is broken 
```
  File ""/home/ngimel/pytorch/torch/nn/functional.py"", line 3279, in multi_head_attention_forward
    attn_output = torch.bmm(attn_output_weights, v)
RuntimeError: Expected object of scalar type Float but got scalar type Half for argument #2 'mat2'
```
because softmax converts half inputs into fp32 inputs. This is unnecessary - all the computations in softmax will be done in fp32 anyway, and the results need to be converted into fp16 for the subsequent batch matrix multiply, so nothing is gained by writing them out in fp32. This PR gets rid of type casting in softmax, so that half works. ",pytorch
21689,vishwakftw,pr,2019-06-12T17:02:16Z,"Allow batch sizes > 65535 for inverse, solve, cholesky_solve and triaâ€¦","â€¦ngular_solve

Changelog:
- Iterate over mini batches of 65535 matrices (maximum)

Test Plan:
- Added slow tests to test the behavior in test_torch and test_cuda

Fixes #21643 and fixes #13276 
",pytorch
21720,ssnl,pr,2019-06-13T02:02:47Z,rebuild_storage_fd retry on EINTR,"Some data loader tests are flaky on py 2 with the following error
```
Jun 12 22:17:31 Traceback (most recent call last):
Jun 12 22:17:31   File ""test_dataloader.py"", line 798, in test_iterable_dataset
Jun 12 22:17:31     fetched = sorted([d.item() for d in dataloader_iter])
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 697, in __next__
Jun 12 22:17:31     idx, data = self._get_data()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 664, in _get_data
Jun 12 22:17:31     success, data = self._try_get_data()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 617, in _try_get_data
Jun 12 22:17:31     data = self.data_queue.get(timeout=timeout)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/queues.py"", line 135, in get
Jun 12 22:17:31     res = self._recv()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/queue.py"", line 22, in recv
Jun 12 22:17:31     return pickle.loads(buf)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 1382, in loads
Jun 12 22:17:31     return Unpickler(file).load()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 858, in load
Jun 12 22:17:31     dispatch[key](self)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 1133, in load_reduce
Jun 12 22:17:31     value = func(*args)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/reductions.py"", line 274, in rebuild_storage_fd
Jun 12 22:17:31     fd = multiprocessing.reduction.rebuild_handle(df)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py"", line 157, in rebuild_handle
Jun 12 22:17:31     new_handle = recv_handle(conn)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py"", line 83, in recv_handle
Jun 12 22:17:31     return _multiprocessing.recvfd(conn.fileno())
Jun 12 22:17:31 OSError: [Errno 4] Interrupted system call
```

Apparently, Python 2.7's `recvfd` calls `recvmsg` without EINTR retry: https://github.com/python/cpython/blob/2.7/Modules/_multiprocessing/multiprocessing.c#L174 
So we should call it with an outer try-catch loop.",pytorch
21723,ssnl,pr,2019-06-13T03:34:02Z,rebuild_storage_fd retry on EINTR,"Some data loader tests are flaky on py 2 with the following error
```
Jun 12 22:17:31 Traceback (most recent call last):
Jun 12 22:17:31   File ""test_dataloader.py"", line 798, in test_iterable_dataset
Jun 12 22:17:31     fetched = sorted([d.item() for d in dataloader_iter])
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 697, in __next__
Jun 12 22:17:31     idx, data = self._get_data()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 664, in _get_data
Jun 12 22:17:31     success, data = self._try_get_data()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 617, in _try_get_data
Jun 12 22:17:31     data = self.data_queue.get(timeout=timeout)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/queues.py"", line 135, in get
Jun 12 22:17:31     res = self._recv()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/queue.py"", line 22, in recv
Jun 12 22:17:31     return pickle.loads(buf)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 1382, in loads
Jun 12 22:17:31     return Unpickler(file).load()
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 858, in load
Jun 12 22:17:31     dispatch[key](self)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/pickle.py"", line 1133, in load_reduce
Jun 12 22:17:31     value = func(*args)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/reductions.py"", line 274, in rebuild_storage_fd
Jun 12 22:17:31     fd = multiprocessing.reduction.rebuild_handle(df)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py"", line 157, in rebuild_handle
Jun 12 22:17:31     new_handle = recv_handle(conn)
Jun 12 22:17:31   File ""/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py"", line 83, in recv_handle
Jun 12 22:17:31     return _multiprocessing.recvfd(conn.fileno())
Jun 12 22:17:31 OSError: [Errno 4] Interrupted system call
```

Apparently, Python 2.7's `recvfd` calls `recvmsg` without EINTR retry: https://github.com/python/cpython/blob/2.7/Modules/_multiprocessing/multiprocessing.c#L174 
So we should call it with an outer try-catch loop.",pytorch
21747,ssnl,pr,2019-06-13T17:51:22Z,[docs] improve torch.load & torch.save doc formatting,,pytorch
21772,peterjc123,pr,2019-06-14T03:47:58Z,Switch to out-source builds for LibTorch,,pytorch
21786,orionr,pr,2019-06-14T16:12:44Z,[tensorboard] Cleanup API and remove 'experimental' warning,"This cleans up the `torch.utils.tensorboard` API to remove all kwargs usage (which isn't clear to the  user) and removes the ""experimental"" warning in prep for our 1.2 release.

We also don't need the additional PyTorch version checks now that we are in the codebase itself.

cc @ezyang @lanpa @natalialunova ",pytorch
21846,vishwakftw,pr,2019-06-16T15:19:22Z,Bag of documentation fixes,"Thanks @henon for raising the issues.

Fixes #21830 
Fixes #21831 
Fixes #21832 
Fixes #21827
Fixes #21822 
Fixes #21820",pytorch
21858,vishwakftw,pr,2019-06-17T15:03:02Z,Port symeig to ATen and enable batching of inputs,"Changelog:
- Port `symeig` from TH/THC to ATen
- Enable batching of matrix inputs for `symeig`
- Modify derivative computation based on batching
- Update docs to reflect the change

Test Plan:
- Added additional tests in `test_torch.py` (with a port to `test_cuda.py`) and `common_methods_invocations.py` to test if both the port and batching work.",pytorch
22029,syed-ahmed,pr,2019-06-20T18:11:19Z,Use at::detail::* instead of detail::* to avoid ambiguity in windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#22029 Use at::detail::* instead of detail::* to avoid ambiguity in windows**

### Summary
Looks like Windows build fails because I used detail::Array in the at::native namespace, which confuses windows as there is a detail namespace in at::native. I used the full at::detail::Array in this patch, hoping it will resolve the issue: https://github.com/pytorch/pytorch/pull/21555#issuecomment-504105620

Differential Revision: [D15965039](https://our.internmc.facebook.com/intern/diff/D15965039)",pytorch
22043,ssnl,pr,2019-06-20T21:45:10Z,Reduce excessive CI printing in TestHub,"https://github.com/pytorch/pytorch/pull/21132 reverted https://github.com/pytorch/pytorch/pull/19606.

Now these tests again print like 40% lines of CI outputs (e.g., https://circleci.com/gh/pytorch/pytorch/2041825?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link)

This PR now uses the functionality introduced in https://github.com/pytorch/vision/issues/862. ",pytorch
22057,ssnl,pr,2019-06-21T03:33:09Z,[dataloader] rename test to be more consistent,,pytorch
22058,ssnl,pr,2019-06-21T03:44:43Z,[TESTING] [DataLoader] Test if the new cancel_join_thread fixes test_proper_exit flakiness,,pytorch
22074,apaszke,pr,2019-06-21T15:06:42Z,Fix minor issues with #21736,cc @mrshenli ,pytorch
22110,apaszke,pr,2019-06-22T10:19:14Z,Make Dropout.__repr__ consistent with other modules,"Fixes #22106.

",pytorch
22115,apaszke,pr,2019-06-23T10:34:03Z,[Easy] Restore default values on premature test exit,"Previously any assert failures would leave the updated setting, making
the test suite semantics dependent on the order in which the tests are run.

The diff is large only due to the indentation change (might be good to review without whitespace changes).

cc @yf225 ",pytorch
22119,apaszke,pr,2019-06-23T14:52:37Z,Improve repr for IncompatibleKeys,Fixes #20128.,pytorch
22126,peterjc123,pr,2019-06-24T04:57:34Z,Add finding thnvrtc_library into torchconfig.cmake,Fixes https://github.com/pytorch/pytorch/pull/21861#issuecomment-504805368,pytorch
22174,supriyar,pr,2019-06-25T00:43:23Z,Adding FC and Relu QNNPACK ops to C10 registry,"Summary:
This is a preliminary change outlining the approach we plan to follow to integrate QNNPACK operators into the pytorch backend. The operators will not be made visibile to the user in the python world, so ultimately we will have a function that calls qnnpack backend based on the environment being run on.

The goal of the project is to integrate QNNPACK library with PyTorch to achieve good performance for quantized mobile models.

Differential Revision: D15806325

",pytorch
22182,peterjc123,pr,2019-06-25T02:49:17Z,Prevent VS from emitting errors when using swap in Optional.h,Fixes https://github.com/pytorch/pytorch/issues/21706,pytorch
22183,syed-ahmed,pr,2019-06-25T03:14:02Z,Fixes bugs in torch.multinomial without replacement,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#22183 Fixes philox_engine_input in torch.multinomial without replacement**

### Summary
Fixes: https://github.com/pytorch/pytorch/issues/22086. For torch.multinomial without replacement, I incremented philox_engine_inputs outside the for loop, whereas it should have been inside. Hence, since the for loop launches multiple kernels, the philox_engine_inputs weren't being updated and the same randoms were being used, which is seen in Issue#22086. In addition, the global thread index inside sampleMultinomialWith* kernels were not correct. They are now correctly following the global index formula for a kernel launch of 1D grid with 2D block.

CC: @LeviViana

Differential Revision: [D15985324](https://our.internmc.facebook.com/intern/diff/D15985324)",pytorch
22208,ssnl,pr,2019-06-25T17:17:08Z,Disable test_proper_exit flaky worker_kill,"I learned from https://github.com/pytorch/pytorch/pull/22058 that `worker_kill` is just flaky, regardless of `hold_iter_reference`. So let's disable it altogether for now.",pytorch
22229,ssnl,pr,2019-06-25T20:44:59Z,pin_memory malloc now uses existing context if available.,"This is achieved by using `cuDevicePrimaryCtxGetState` as a way to check whether a primary context exists on a device. It is not too slow, from this benchmark of a single call to it on CUDA 10.1, Titan Xp, driver 415.27:
```
---------------------------------------------------------------------
Benchmark                              Time           CPU Iterations
---------------------------------------------------------------------
BM_cuDevicePrimaryCtxGetState        301 ns        301 ns    2319746
```

Commits:
 
1. Add `CUDAHooks::getDeviceWithPrimaryContext` which returns a device index with primary context (if exists). 
    Link `c10/cuda` against `libcuda` for device API calls.
2. Use `getDeviceWithPrimaryContext` to check primary context in `pin_memory`. 
    Fix `OptionalDeviceGuard` doc.
3. Refactor `test_cuda_primary_ctx.py` to support multiple tests. 
    Add test for this in that file.

Fixes https://github.com/pytorch/pytorch/issues/21081.",pytorch
22230,ssnl,pr,2019-06-25T20:55:36Z,update IterableDataset doc to be consistent with current behavior,,pytorch
22357,ssnl,pr,2019-06-28T22:46:31Z,Fix typos in gradcheck error message,,pytorch
22361,ssnl,pr,2019-06-29T02:42:51Z,Move thnvrtc and DynamicLibrary to ATen,"Having the NVRTC stub in ATen is necessary to call driver APIs in ATen. This is currently blocking https://github.com/pytorch/pytorch/pull/22229.

`DynamicLibrary` is also moved as it is used in the stub code, and seems general enough.",pytorch
22362,ssnl,pr,2019-06-29T03:18:49Z,Move thnvrtc and DynamicLibrary to ATen,"Having the NVRTC stub in ATen is necessary to call driver APIs in ATen. This is currently blocking https://github.com/pytorch/pytorch/pull/22229.

`DynamicLibrary` is also moved as it is used in the stub code, and seems general enough.",pytorch
22363,ssnl,pr,2019-06-29T03:54:41Z,Comment on why Windows build_pytorch.bat builds twice,"I've noticed that Windows CI seems to build twice, e.g., https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-build/60304/console

This adds a comment explaining why.
",pytorch
22367,vishwakftw,pr,2019-06-29T09:32:43Z,Fix QNNPACK and NNPACK settings,`setup.py` recommends setting `USE_QNNPACK=0` and `USE_NNPACK=0` to disable building QNNPACK and NNPACK respectively. However this wasn't reflected correctly because we were looking for `NO_QNNPACK` and `NO_NNPACK`. This PR fixes it.,pytorch
22379,vishwakftw,pr,2019-06-30T06:24:37Z,Port lu_solve to ATen,"Changelog:
- Port TH implementation to ATen/native/BatchLinearAlgebra.cpp
- Port THC implementation to ATen/native/cuda/BatchLinearAlgebra.cu
- Remove TH/THC implementations
- Update doc strings

Test Plan:
- Existing tests should pass to confirm that the port is successful",pytorch
22411,malmaud,pr,2019-07-01T21:28:40Z,Add type stubs to import 'nn' modules,Forgot to mirror the `nn/ __init__.py` semantics in the new `nn` type stub.,pytorch
22510,supriyar,pr,2019-07-03T21:10:41Z,Add clone() implementation for QTensor,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22510 Add clone() implementation for QTensor**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D16059576/)

Added a new function to implement clone operation on quantized tensors. Also added a test case which can be tested as shown in test plan.

This change is required to be able to call torch.jit.trace on quantized models.
Clone implementation calls copy_ on QTensor internally.

Differential Revision: [D16059576](https://our.internmc.facebook.com/intern/diff/D16059576/)",pytorch
22527,peterjc123,pr,2019-07-04T03:00:53Z,Fix cuda detection script,Fixes https://github.com/pytorch/pytorch/issues/22507,pytorch
22533,vishwakftw,pr,2019-07-04T08:49:34Z,Fix torch.normal with CUDA tensors,"`addcmul_out` overwrote the samples, which led to constant values being output by `torch.normal`. 

Changelog:
- Replace the `addcmul_out` calls with combo of inplace `mul` and `add` and justification for this change.

Test Plan:
- Enable tests for test_normal on all devices

Fixes #22529",pytorch
22545,ssnl,pr,2019-07-05T05:52:17Z,Fix spectral_norm load_state_dict with strict=False,"Fixes https://github.com/pytorch/pytorch/issues/21251

also fixes some missing hook removals.",pytorch
22563,peterjc123,pr,2019-07-06T06:18:09Z,Add some essentials for building c++ extensions on Windows,Fixes https://github.com/pytorch/pytorch/issues/22489.,pytorch
22576,ssnl,pr,2019-07-07T18:07:24Z,Add Module.requires_grad_,addresses https://github.com/pytorch/pytorch/issues/20241,pytorch
22579,peterjc123,pr,2019-07-08T01:36:57Z,Use system locale in collect_env.py,Fixes https://github.com/pytorch/pytorch/issues/22570.,pytorch
22618,vishwakftw,pr,2019-07-09T13:43:04Z,"[RELAND] Update note about tensors on CPU for certain MAGMA functions, eliminaâ€¦","â€¦te argument in macro

Changelog:
- Update note about tensors on CPU for the following MAGMA functions
  - magma_(d/s)getrf_gpu and magma_getrf_nopiv_gpu require tensors on CPU for pivots
  - magma_(d/s)geqrf2_gpu requires tensors on CPU for elementary reflectors
  - magma_(d/s)syevd_gpu requires tensors on CPU for eigenvalues
- Remove dummy tensor in ALLOCATE_ARRAY MACRO

Test Plan:
- All existing tests should pass to verify that the patch is correct

This PR has been proposed to eliminate confusion due to the previous comments, as indicated in #22573

",pytorch
22634,ssnl,pr,2019-07-09T18:30:56Z,Avoid potential extra copy in _lu_with_info_cuda,No need to `clone` if the expanded size matches original size.,pytorch
22675,ssnl,pr,2019-07-10T05:16:33Z,Fix a FIXME in test_nn,"https://github.com/pytorch/pytorch/issues/17262 is already resolved, so this should pass now.

",pytorch
22730,vishwakftw,pr,2019-07-11T03:07:47Z,Fix torch.triu / torch.tril on contiguous tensors with non-default stâ€¦,"â€¦rides

Changelog:
- Fix behavior of `torch.triu` / `torch.tril` on certain unsqueezed tensors that lead to uninitialized values on CPU

Test plan:
- Add tests for these cases in test_triu_tril in test_torch

Fixes #22581 ",pytorch
22841,vishwakftw,pr,2019-07-13T18:11:30Z,Remove deprecated linear algebra functions (and methods),"Changelog:
- Removed the following linear algebra functions in PyTorch in favor of the renamed operations
  - `btrifact` (use `lu` instead)
  - `btrifact_with_info` (use `lu` with `get_infos=True` instead)
  - `btrisolve` (use `lu_solve` instead)
  - `btriunpack` (use `lu_unpack` instead)
  - `gesv` (use `solve` instead)
  - `pstrf` (use `cholesky` instead)
  - `potrf` (use `cholesky` instead)
  - `potri` (use `cholesky_inverse` instead)
  - `potrs` (use `cholesky_solve` instead)
  - `trtrs` (use `triangular_solve` instead)

- Removed dead code after the removal of `pstrf`

Test Plan:
- All existing tests should pass to verify that the removal is clean

Closes #22832 ",pytorch
22909,vishwakftw,pr,2019-07-16T14:19:04Z,Allowing batching for det/logdet/slogdet operations,"Changelog:
- Add batching for det / logdet / slogdet operations
- Update derivative computation to support batched inputs (and consequently batched outputs)
- Update docs

Test Plan:
- Add a `test_det_logdet_slogdet_batched` method in `test_torch.py` to test `torch.det`, `torch.logdet` and `torch.slogdet` on batched inputs. This relies on the correctness of `torch.det` on single matrices (tested by `test_det_logdet_slogdet`). A port of this test is added to `test_cuda.py`
- Add autograd tests for batched inputs",pytorch
22923,supriyar,pr,2019-07-16T20:05:13Z,Add support to print QTensor in cpp,"Summary: Print quantized tensor by first dequantizing it and then printing. Also print the scale, zero_point. size and type of tensor.

Differential Revision: D16286397

",pytorch
22929,ssnl,pr,2019-07-16T21:23:11Z,[doc] Fix F.one_hot doc signature,,pytorch
22950,supriyar,pr,2019-07-16T23:17:24Z,Add support to print QTensor in cpp,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22950 Add support to print QTensor in cpp**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16286397/)

Print quantized tensor by first dequantizing it and then printing. Also print the scale, zero_point. size and type of tensor.

Differential Revision: [D16286397](https://our.internmc.facebook.com/intern/diff/D16286397/)",pytorch
22990,ssnl,pr,2019-07-17T20:29:38Z,Add multiprocessing_context= argument to DataLoader,Fixes https://github.com/pytorch/pytorch/issues/22131,pytorch
22995,ssnl,pr,2019-07-17T21:55:42Z,Slightly improve irfft doc,,pytorch
23000,orionr,pr,2019-07-17T22:32:29Z,[tensorboard] Cleanup API and remove 'experimental' warning,"This fixes ASAN test issues with https://github.com/pytorch/pytorch/pull/21786 seen at https://circleci.com/api/v1.1/project/github/pytorch/pytorch/2212325/output/105/0?file=true and lands it again.

This cleans up the `torch.utils.tensorboard` API to remove all kwargs usage (which isn't clear to the  user) and removes the ""experimental"" warning in prep for our 1.2 release.

We also don't need the additional PyTorch version checks now that we are in the codebase itself.

cc @yf225, @lanpa, @natalialunova
",pytorch
23018,vishwakftw,pr,2019-07-18T14:18:18Z,Modify symmetric eigendecomposition derivative,"The derivative of the symmetric eigendecomposition was previously a triangular matrix.

Changelog:
- Modify the derivative of symeig from a triangular matrix to a symmetric matrix with reason specified as a comment.

Test Plan:
- Existing gradcheck and gradgradchecks are ported to test_autograd to verify that the change is correct. Input to symeig is symmetrized before passing

",pytorch
23023,orionr,pr,2019-07-18T16:47:50Z,[tensorboard] Only import PIL when needed,"Fixes https://github.com/pytorch/pytorch/issues/22389

In most cases we only import `PIL` methods when we need them, but we missed a spot.

cc @lanpa @natalialunova @sanekmelnikov",pytorch
23123,ngimel,pr,2019-07-20T03:51:10Z,fix indexing for more than 65535 elems in non-indexed first dim,"Fixes #22843, also adds test from #23102",pytorch
23130,ssnl,pr,2019-07-20T15:24:19Z,Fix typos in comments,,pytorch
23131,ssnl,pr,2019-07-20T15:29:25Z,Fix lint,,pytorch
23132,ssnl,pr,2019-07-20T15:33:01Z,Fix typo in dataloader.py,,pytorch
23150,peterjc123,pr,2019-07-22T04:00:30Z,Add windows docs for the binaries,,pytorch
23157,vishwakftw,pr,2019-07-22T09:48:06Z,Remove empty THCThreadLocal{.h/.cpp},"These files were removed from the build process and cleaned in https://github.com/pytorch/pytorch/pull/9735.

Closes #22572",pytorch
23328,fritzo,pr,2019-07-24T19:58:18Z,Fix distributions.Categorical.sample bug from .view(),"This modernizes distributions code by replacing a few uses of `.contiguous().view()` with `.reshape()`, fixing a sample bug in the `Categorical` distribution.

The bug is exercised by the following test:
```py
batch_shape = (1, 2, 1, 3, 1)
sample_shape = (4,)
cardinality = 2
logits = torch.randn(batch_shape + (cardinality,))
dist.Categorical(logits=logits).sample(sample_shape)
# RuntimeError: invalid argument 2: view size is not compatible with
#   input tensor's size and stride (at least one dimension spans across
#   two contiguous subspaces). Call .contiguous() before .view().
#   at ../aten/src/TH/generic/THTensor.cpp:203
```
I have verified this works locally, but I have not added this as a regression test because it is unlikely to regress (the code is now simpler).",pytorch
23356,supriyar,pr,2019-07-25T00:51:32Z,Add support to serialize qtensor in JIT.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23356 Add support to serialize qtensor in JIT.**

Adds qtensor specific fields to the proto file so that they get serialized into the model.json

Differential Revision: [D16473237](https://our.internmc.facebook.com/intern/diff/D16473237/)",pytorch
23359,supriyar,pr,2019-07-25T01:09:54Z,[WIP] Add support to serialize quantized module in JIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23359 [WIP] Add support to serialize quantized module in JIT**
* #23356 Add support to serialize qtensor in JIT.

Differential Revision: [D16473914](https://our.internmc.facebook.com/intern/diff/D16473914/)",pytorch
23375,vishwakftw,pr,2019-07-25T13:00:54Z,Remove unused cuBLAS driver functions for getrs,"Changelog:
- Remove getrs driver functions from THCBlas{.h/.cpp}

Test Plan:
- Build to pass to confirm no callsites were missed.

",pytorch
23460,vishwakftw,pr,2019-07-26T22:40:52Z,Rename gels to lstsq,"Changelog:
- Rename `gels` to `lstsq`
- Fix all callsites
- Rename all tests
- Create a tentative alias for `lstsq` under the name `gels` and add a deprecation warning to not promote usage.

Test Plan:
- All tests should pass to confirm that the patch is correct

",pytorch
23472,peterjc123,pr,2019-07-27T07:03:33Z,Add some compiler flags for building cpp extensions on Windows,"(1) Add `COMMON_MSVC_FLAGS` to the flags in the ninja codepath
(2) Add `/EHsc` to `COMMON_MSVC_FLAG`
(3) Remove `-fPIC` and `-std=c++11` from the flags in the windows codepath",pytorch
23477,ilhamfp,pr,2019-07-27T12:49:25Z,Fix torch.lerp typo,This pull request address #22697 issue.,pytorch
23484,ssnl,pr,2019-07-28T05:54:25Z,pin_memory should not copy on already pinned tensors,fixes https://github.com/pytorch/pytorch/issues/21076,pytorch
23583,peterjc123,pr,2019-07-31T01:55:50Z,Update MKL to 2019.4 for Windows,,pytorch
23587,ssnl,pr,2019-07-31T03:54:48Z,Enable len(dataloader) for iterable dataset,"Copy-paste comment from code for reasoning:

```
            # NOTE [ IterableDataset and __len__ ]
            #
            # For `IterableDataset`, `__len__` could be inaccurate when one naively
            # does multi-processing data loading, since the samples will be duplicated.
            # However, no real use case should be actually using that behavior, so
            # it should count as a user error. We should generally trust user
            # code to do the proper thing (e.g., configure each replica differently
            # in `__iter__`), and give us the correct `__len__` if they choose to
            # implement it (this will still throw if the dataset does not implement
            # a `__len__`).
            #
            # To provide a further warning, we track if `__len__` was called on the
            # `DataLoader`, save the returned value in `self._len_called`, and warn
            # if the iterator ends up yielding more than this number of samples.
```

Fixes https://github.com/pytorch/pytorch/issues/30184",pytorch
23588,ilhamfp,pr,2019-07-31T04:43:05Z,Fix typos in .circleci/README.md,Fix typos in .circleci/README.md,pytorch
23591,vishwakftw,pr,2019-07-31T06:26:34Z,Fix regression in torch.qr,"Changelog:
- Use narrow instead of narrow_copy while returning

Test Plan:
- All tests should pass to ensure that the change is correct

Fixes #23580 ",pytorch
23605,vishwakftw,pr,2019-07-31T16:50:50Z,Fix regression in torch.qr,Cherry pick of 02d5c62f34a921ef6a54bc33152e87938267c122,pytorch
23606,vishwakftw,pr,2019-07-31T17:00:04Z,Fix regression in torch.qr (#23591),"Summary:
Changelog:
- Use narrow instead of narrow_copy while returning
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23591

Test Plan:
- All tests should pass to ensure that the change is correct

Fixes https://github.com/pytorch/pytorch/issues/23580

Differential Revision: D16581174

Pulled By: ezyang

fbshipit-source-id: 1b6bf7d338ddd138ea4c6aa6901834dd202ec79c

",pytorch
23615,ssnl,pr,2019-07-31T19:40:14Z,fix typo,,pytorch
23634,vishwakftw,pr,2019-07-31T23:57:15Z,[v1.2.0] Allowing batching for det/logdet/slogdet operations (#22909),"Summary:
Changelog:
- Add batching for det / logdet / slogdet operations
- Update derivative computation to support batched inputs (and consequently batched outputs)
- Update docs
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22909

Test Plan:
- Add a `test_det_logdet_slogdet_batched` method in `test_torch.py` to test `torch.det`, `torch.logdet` and `torch.slogdet` on batched inputs. This relies on the correctness of `torch.det` on single matrices (tested by `test_det_logdet_slogdet`). A port of this test is added to `test_cuda.py`
- Add autograd tests for batched inputs

Differential Revision: D16580988

Pulled By: ezyang

fbshipit-source-id: b76c87212fbe621f42a847e3b809b5e60cfcdb7a

",pytorch
23646,ssnl,pr,2019-08-01T06:04:53Z,Fix pin_memory_thread not exiting quickly,fixes https://github.com/pytorch/pytorch/issues/23642,pytorch
23647,ssnl,pr,2019-08-01T06:08:12Z,[1.2.0 branch] fix pin_memory_thread not exiting quickly,v1.2.0 branch version of https://github.com/pytorch/pytorch/pull/23646,pytorch
23658,supriyar,pr,2019-08-01T16:43:18Z,Add support for using caffe2::ThreadPool in pytorch mobile QNNPACK.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23658 Add support for using caffe2::ThreadPool in pytorch mobile QNNPACK.**

**How things work for caffe2:**
Caffe2 Ops -> NNPACK/QNNPACK -> pthreadpool_compute_1/2/3/4d_tiled -> pthreadpool_compute_1d (caffe2 shim) -> caffe2::ThreadPool

**Before this PR:**
Pytorch Ops -> NNPACK/QNNPACK -> pthreadpool_compute_1/2/3/4d_tiled -> pthreadpool_compute_1d (third_party implementation without mobile optimization)

caffe2::ThreadPool is optimized for mobile. This change leverages this logic for pytorch mobile as a temporary solution improve pytorch mobile perf. It is guarded by the C10_MOBILE macro.
For server side we return nullptr.

**Plan for next steps:**
Implement a mobile version of ""at::parallel_for"" which uses caffe2::ThreadPool internally so all ATen/TH multithreading usage is mobile optimized.
Refactor QNNPACK and/or pthreadpool to explicitly using ""at::parallel_for"" primitive to replace pthreadpool_compute_1d for Pytorch.
After QNNPACK is refactored, we will delete the mobile_threadpool() API.

Differential Revision: [D16594020](https://our.internmc.facebook.com/intern/diff/D16594020/)",pytorch
23671,ssnl,pr,2019-08-01T18:23:36Z,Slightly improve dataloader docs on when auto-batching is disabled,cc @gchanan ,pytorch
23672,ssnl,pr,2019-08-01T18:25:33Z,[v1.2.0] Slightly improve dataloader docs on when auto-batching is disabled,,pytorch
23707,ssnl,pr,2019-08-02T02:20:16Z,fix align_corners doc,,pytorch
23709,ssnl,pr,2019-08-02T02:23:56Z,[1.2.0] fix align_corners doc,https://github.com/pytorch/pytorch/pull/23707,pytorch
23713,ssnl,pr,2019-08-02T02:54:25Z,[1.2.0] Use dst dir for temp file (#23629),"Summary:
Fixes https://github.com/pytorch/pytorch/issues/23607
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23629

Differential Revision: D16594223

Pulled By: soumith

fbshipit-source-id: db0275415111f08fc13ab6be00b76737a20f92df

",pytorch
23715,ssnl,pr,2019-08-02T02:57:18Z,[1.2.0] Fix CTC loss for zero-length targets on GPU (#23298),"Summary:
Fixes: https://github.com/pytorch/pytorch/issues/18215 at last!

Also sprinkle tests...
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23298

Differential Revision: D16582145

Pulled By: soumith

fbshipit-source-id: bc8b1a629de0c2606e70a2218ccd135f4a9cdc5d

",pytorch
23735,vishwakftw,pr,2019-08-02T16:22:34Z,Document empty_strided,"Changelog:
- Add doc string for torch.empty_strided
- Remove empty file named `python` in test/

Fixes #23688 ",pytorch
23740,vishwakftw,pr,2019-08-02T17:23:14Z,[v1.2.0] Document empty_strided,"Changelog:
- Add doc string for torch.empty_strided
- Remove empty file named `python` in test/

",pytorch
23744,ssnl,pr,2019-08-02T19:44:48Z,[data loader] make more iterator attributes private,"1. Prefixed underscores to any `DataLoaderIter` attribute that is not part of the data loader ctor argument list. 
2. Prefixed `DataLoader.dataset_kind` with underscore because it only makes sense with the private enum `_DatasetKind`, and is an implementation detail.
3. Disallow setting `DataLoader.dataset` and `DataLoader.batch_sampler` after initializing a `DataLoader` because they affect other attributes in `__init__`.

These changes should not have major BC breaking effect since the big changes are on the iterator class and most users don't even store it. I GitHub searched `pin_memory_thread` and (while I didn't look through all result pages) results I see are forks of pytorch and blog posts on how data loader works.",pytorch
23761,ssnl,pr,2019-08-03T18:01:54Z,Fix dataloader._shutdown_workers if not all workers are started,"Otherwise you may see errors like
```
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001F99F5CB9D8>
Traceback (most recent call last):
  File ""C:\Users\Divyansh J\Anaconda3\envs\pytorch\lib\site-packages\torch\utils\data\dataloader.py"", line 883, in __del__
    self._shutdown_workers()
  File ""C:\Users\Divyansh J\Anaconda3\envs\pytorch\lib\site-packages\torch\utils\data\dataloader.py"", line 860, in _shutdown_workers
    if self.workers_status[worker_id]:
IndexError: list index out of range
```

e.g. https://discuss.pytorch.org/t/how-to-construct-dataset-with-iterator-for-multi-process-dataloader/49612/5",pytorch
23762,ssnl,pr,2019-08-03T18:12:26Z,[v1.2.0]  Fix dataloader._shutdown_workers if not all workers are started,,pytorch
23775,vishwakftw,pr,2019-08-05T02:54:04Z,Fix docstring for argmax,Fixes #23757 ,pytorch
23855,vishwakftw,pr,2019-08-06T03:40:21Z,[v1.2.0] Fix argmax docstring,,pytorch
23910,ssnl,pr,2019-08-06T22:22:35Z,Document benchmarking practice for CUDA,,pytorch
23953,vishwakftw,pr,2019-08-07T17:44:55Z,Fix regression in triangular_solve when number of batches = 1 for CUDA,"Changelog:
- When number of batches = 1, dispatch to trsm instead of trsm_batched in MAGMA

Test Plan:
- All triangular_solve tests should pass to ensure that the change is valid

",pytorch
23972,supriyar,pr,2019-08-07T20:31:02Z,Add support to serialize quantized module in JIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23972 Add support to serialize quantized module in JIT**

Add support to serialize Linear module and test it.

Differential Revision: [D16695771](https://our.internmc.facebook.com/intern/diff/D16695771/)",pytorch
23997,vishwakftw,pr,2019-08-08T03:23:11Z,[v1.2.0] Fix regression in triangular_solve when number of batches = 1 for CUDA,"Changelog:
- When number of batches = 1, dispatch to trsm instead of trsm_batched in MAGMA

Test Plan:
- All triangular_solve tests should pass to ensure that the change is valid

",pytorch
24042,nehaljwani,pr,2019-08-08T21:59:51Z,tensor_numpy: add missing include header,"This patch fixes the following error:
```
In file included from /path/to/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4:0,
                 from ../torch/csrc/utils/numpy_stub.h:19,
                 from ../torch/csrc/utils/tensor_numpy.cpp:2:
../torch/csrc/utils/tensor_numpy.cpp: In function 'bool torch::utils::is_numpy_scalar(PyObject*)':
../torch/csrc/utils/tensor_numpy.cpp:223:11: error: 'PyInt_Check' was not declared in this scope
   return (PyArray_IsIntegerScalar(obj) ||
           ^
../torch/csrc/utils/tensor_numpy.cpp:225:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^```

",pytorch
24043,nehaljwani,pr,2019-08-08T22:00:58Z,Set HAVE_MKL to true if MKL_FOUND,"On setting `MKLDNN_THREADING=OMP:INTEL`, the CMake configure stage fails with:
```
CMake Error at third_party/ideep/mkl-dnn/cmake/OpenMP.cmake:67 (message):
  Intel OpenMP runtime could not be found.  Please either use OpenMP runtime
  that comes with the compiler (via -DMKLDNN_THREADING={OMP,OMP:COMP}), or
  install Intel MKL / Intel MKL-ML (e.g.  scripts/prepare_mkl.sh)
```",pytorch
24103,supriyar,pr,2019-08-09T18:32:48Z,Add new qnnpack_add and qnnpack_maxpool op to C10 registry,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24103 Add new qnnpack_add and qnnpack_maxpool op to C10 registry**

This change adds a quantized add and maxpool2d operation for pytorch mobile.

These operators follow the structure of qnnpack in terms of create/setup and run calls. The plan to refactor QNNPACK to make it more functional is currently for FC and Conv ops where the cost of create/setup is high.
For ops like add and maxpool the cost of calling create and setup in each operator invocation is negligible.

Once we migrate FC and Conv QNNPACK ops to be functional in nature, we will consider changing these ops as well to make it consistent.

Differential Revision: [D16734190](https://our.internmc.facebook.com/intern/diff/D16734190/)",pytorch
24123,orionr,pr,2019-08-09T21:18:34Z,[tensorboard] added mesh tests,"Summary:
This diff adds mesh tests and fixes add_mesh method:

added tests to test_tensorboard.py
fixed an error occured after updating tensorboard to the latest version (added ""components"" argument to create_summary_metadata): tensorflow/tensorboard@5e5badc#diff-068400aa3e34121b7256539582374597
Differential Revision: D16714759",pytorch
24131,fritzo,pr,2019-08-09T21:40:58Z,Vectorize LowerCholeskyTransform,"Removes older `torch.stack`-based logic in favor of `torch.diagonal()` and `torch.diag_embed()`.

I see 100x speedup in my application, where my batched matrix has shape `(800, 32 ,32)`.
```py
import torch
from torch.distributions import constraints, transform_to
x = torch.randn(800, 32, 32, requires_grad=True)

# Before this PR:
%%timeit
transform_to(constraints.lower_cholesky)(x).sum().backward()
# 579 ms Â± 34.4 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

# After this PR:
%%timeit
transform_to(constraints.lower_cholesky)(x).sum().backward()
# 4.5 ms Â± 241 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```",pytorch
24148,vishwakftw,pr,2019-08-10T16:28:05Z,Enable torch.eye for bool and half,"Changelog:
- Enable torch.eye for bool and float16 dtypes

Test Plan:
- Tests added in test_torch.py for all available devices and dtypes (except torch.bfloat16)

Fixes #24088 ",pytorch
24163,vishwakftw,pr,2019-08-11T18:32:46Z,Allow torch.tril / triu to handle bool and half inputs,"Changelog:
- Enable torch.tril / triu for bool and float16 dtypes

Test Plan:
- Tests added in test_torch.py for all devices and dtypes (except bfloat16)

Fixes #24035 

",pytorch
24267,ssnl,pr,2019-08-13T19:55:48Z,Fix infer np scalar dtype mem leak,Fixes #24200 . I'm a bit worried that the test might be flaky...,pytorch
24295,orionr,pr,2019-08-13T23:16:32Z,Remove hard Caffe2 dependency for TensorBoard,"Fixes https://github.com/pytorch/pytorch/issues/24175 and https://github.com/pytorch/pytorch/issues/15618

We should not be importing caffe2 (and dependencies like future, etc) unless needed within `torch.utils.tensorboard`.",pytorch
24333,vishwakftw,pr,2019-08-14T15:29:42Z,Enable broadcasting of batch dimensions RHS and LHS tensors for lu_solve,"Changelog:
- Enable broadcasting of RHS and LHS tensors for lu_solve. This means that you can now have RHS with size `3 x 2` and LHS with size `4 x 3 x 3` for instance
- Remove deprecated behavior of having 2D tensors for RHS. Now all tensors have to have a last dimension which equals the number of right hand sides
- Modified docs

Test Plan:
- Add tests for new behavior in test_torch.py with a port to test_cuda.py",pytorch
24389,peterjc123,pr,2019-08-15T04:27:25Z,Fix Z7_MSVC_OVERRIDE for C source files,Fixes https://github.com/pytorch/pytorch/issues/24145#issuecomment-521507234,pytorch
24438,vishwakftw,pr,2019-08-15T21:27:23Z,Enable torch.cholesky for batches > 262140,"Changelog:
- Iterate over mini batches of 262140 matrices (maximum)

Test Plan:
- Added slow tests to test the behavior in test_torch and test_cuda

Fixes #24403 ",pytorch
24503,rohan-varma,pr,2019-08-16T18:10:40Z,Add resnext 32x4d shapes to benchmark,"Summary: Adds resnext-1011 32x4d shapes to the qconv benchmarks. (Also ran the code formatter)

Differential Revision: D16845746

",pytorch
24887,vishwakftw,pr,2019-08-20T02:31:49Z,Remove THCHalfAutoNumerics.cuh,"Changelog:
- Replace uses of functions in THCHalfAutoNumerics with those in THCNumerics in THCUNN.

Test Plan:
- All tests should pass to verify that the change is correct

",pytorch
24913,rohan-varma,pr,2019-08-20T16:43:05Z,Add qconv_test to benchmarking tests,"Summary:
Adds the tests defined in `qconv_tests.py` to `benchmark_all_tests.py` so that they are ran by `benchmark_all_tests`.

The next diff will create another `ai_benchmark_test` specifying the qconv operations similar to D16768680. Since AI-PEP integrates with benchmark_all_tests, this should add these qconv benchmarks to AI-PEP.

Differential Revision: D16908445

",pytorch
25076,ssnl,pr,2019-08-23T01:55:50Z,implement bool_tensor.bernoulli_,Fixes https://github.com/pytorch/pytorch/issues/25072,pytorch
25111,ssnl,pr,2019-08-23T18:45:58Z,pin_memory thread now uses 1 thread only,Fixes #25010 ,pytorch
25145,peterjc123,pr,2019-08-24T06:23:17Z,Ensure tests get passed on Windows,"(1) check error codes after every test command
(2) add missing LibTorch tests mentioned in https://discuss.pytorch.org/t/pre-compiled-tests-failing/54166",pytorch
25158,ssnl,pr,2019-08-25T08:00:24Z,Fix possible deadlock in SharedCache inside a forked child proc,"Related: https://github.com/pytorch/pytorch/issues/24927#issuecomment-524608021

`fork` inherits lock state. So if we happen to unfortunately fork when the `SharedCache` lock is held. We could deadlock in the child process when some code tries to acquire it. 

Following pytorch multiprocessing library design, this patch resets the lock to a new object after fork. A similar example from python core lib for `multiprocessing.Queue` is : 

```py
class Queue(object):
    def __init__(self, ...):
        ...
        self._after_fork()
        if sys.platform != 'win32':
            register_after_fork(self, Queue._after_fork)

    def _after_fork(self):
        debug('Queue._after_fork()')
        self._notempty = threading.Condition(threading.Lock())
        self._buffer = collections.deque()
        self._thread = None
        self._jointhread = None
        self._joincancelled = False
        self._closed = False
        self._close = None
        self._send_bytes = self._writer.send_bytes
        self._recv_bytes = self._reader.recv_bytes
        self._poll = self._reader.poll
```

https://github.com/python/cpython/blob/d4d60134b29290049e28df54f23493de4f1824b6/Lib/multiprocessing/queues.py#L54-L78",pytorch
25231,apaszke,pr,2019-08-27T05:36:50Z,Add myself as a CODEOWNER for better discoverability,"Not meant to be a landing blocker or anything like that. This only lets me setup some more effective email filters, hopefully allowing me to discover the current changes earlier and be more responsive.",pytorch
25338,supriyar,pr,2019-08-28T18:42:40Z,[quantization] Rename fbgemm quantized operators to generic `quantized` ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #25626 [quantization] Store bias in PackedConvWeight in fbgemm
* #25428 [quantization] Store bias in PackedLinearWeight struct in fbgemm
* **#25338 [quantization] Rename fbgemm quantized operators to generic `quantized` ops**

As an effort to unify fbgemm and qnnpack at the dispatcher level, we need to have a generic name for the quantized backed ops.
Currently FBGEMM is guarded by the USE_FBGEMM macro and QNNPACK uses USE_QNNPACK.

TBD: Use compile time macro or run_time to switch between fbgemm and qnnpack.

Differential Revision: [D17097735](https://our.internmc.facebook.com/intern/diff/D17097735/)",pytorch
25377,peterjc123,pr,2019-08-29T03:37:38Z,Re-enable libtorch tests on Windows,,pytorch
25398,cloudhan,pr,2019-08-29T16:32:43Z,Fix windows build error when TBB enabled and Windows SDK installed,"Fixed #25320 
See the issue for more infomation.",pytorch
25400,supriyar,pr,2019-08-29T16:47:45Z,Update QNNPACK submodule to 7d2a4e9,"Summary: Bring in fixes for clamp operator and tests

Test Plan: CI

Differential Revision: D17100464

",pytorch
25428,supriyar,pr,2019-08-29T21:29:46Z,[quantization] Store bias in PackedLinearWeight struct in fbgemm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #25626 [quantization] Store bias in PackedConvWeight in fbgemm
* **#25428 [quantization] Store bias in PackedLinearWeight struct in fbgemm**
* #25678 [quantization] Rename FBGEMM quantized operators to generic quantized ops

Added bias as an optional param to the quantized_linear_prepack function.
Bias is quantized during runtime using input scale and weight scale.

Differential Revision: [D17121304](https://our.internmc.facebook.com/intern/diff/D17121304/)",pytorch
25432,supriyar,pr,2019-08-29T22:55:10Z,Skip test_compare_tensor_scalar due to overflow error,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #25428 [quantization] Store bias in PackedLinearWeight struct in fbgemm
* #25338 [quantization] Rename fbgemm quantized operators to generic `quantized` ops
* **#25432 Skip test_compare_tensor_scalar due to overflow error**

Test fails without width argument (it was dropped from hypothesis).
Temporarily skipping until fixed.

Differential Revision: [D17123571](https://our.internmc.facebook.com/intern/diff/D17123571/)",pytorch
25444,peterjc123,pr,2019-08-30T02:06:10Z,Skip useless macros from Windows.h,Applying #25398 to the whole project.,pytorch
25456,yaroslavvb,pr,2019-08-30T05:56:20Z,Fixes #25454,,pytorch
25521,cloudhan,pr,2019-08-31T11:05:34Z,[fix] add onnx export error check,"Fixed #19277
",pytorch
25524,vishwakftw,pr,2019-08-31T20:08:12Z,Eliminate magic numbers in BatchLinearAlgebra.cu,"Changelog:
- We had 65535 as a common magic number for several linalg routines as a batch size limit. This PR explicitly assigns them to a variable to minimize possible errors

Test Plan:
- All existing tests should pass to confirm that the modification is correct

This is a follow-up of the suggestion in #24438.",pytorch
25539,vishwakftw,pr,2019-09-01T15:13:54Z,Fix CUDA distributions test on Windows,"Fixes #25304.

The possible cause for the failure could have been the fact that `at::empty` was creating a tensor with very small values or 0, which led to `cumdist` not summing to a positive number.",pytorch
25556,peterjc123,pr,2019-09-02T16:35:11Z,Add copy logic for LibTorch to avoid issues on Windows,This should work both on VS and Ninja.,pytorch
25578,peterjc123,pr,2019-09-03T12:35:49Z,Enable CPU fused kernel on Windows,,pytorch
25626,supriyar,pr,2019-09-04T02:41:44Z,[quantization] Store bias in PackedConvWeight in fbgemm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25626 [quantization] Store bias in PackedConvWeight in fbgemm**

Add bias as an optional parameter in the packed conv weight struct.

Differential Revision: [D17177723](https://our.internmc.facebook.com/intern/diff/D17177723/)",pytorch
25678,supriyar,pr,2019-09-04T23:37:11Z,[quantization] Rename FBGEMM quantized operators to generic quantized ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #25626 [quantization] Store bias in PackedConvWeight in fbgemm
* #25428 [quantization] Store bias in PackedLinearWeight struct in fbgemm
* **#25678 [quantization] Rename FBGEMM quantized operators to generic quantized ops**

As an effort to unify fbgemm and qnnpack at the dispatcher level, we need to have a generic name for the quantized backed ops.
Currently FBGEMM is guarded by the USE_FBGEMM macro and QNNPACK uses USE_QNNPACK.

Differential Revision: [D17194364](https://our.internmc.facebook.com/intern/diff/D17194364/)",pytorch
25680,supriyar,pr,2019-09-05T01:19:21Z,[quantization] Add Runtime flag for quantized backend.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25680 [quantization] Add Runtime flag for quantized backend.**

Add a runtime flag to choose between FBGEMM and QNNPACK when compiled with both.

The flag can be set by using `torch.backends.quantized.engine = torch.fbgemm/torch.qnnpack` or `ctx::setPreferredQuantizedEngine(at::QEngine)`

Differential Revision: [D17198233](https://our.internmc.facebook.com/intern/diff/D17198233/)",pytorch
25733,vishwakftw,pr,2019-09-05T22:53:08Z,Refactor torch.*solve tests,"Changelog:
- De-duplicate the code in tests for torch.solve, torch.cholesky_solve, torch.triangular_solve
- Skip tests explicitly if requirements aren't met for e.g., if NumPy / SciPy aren't available in the environment
- Add generic helpers for these tests in test/common_utils.py

Test Plan:
- All tests should pass to confirm that the change is not erroneous

Clears one point specified in the discussion in #24333.",pytorch
25741,supriyar,pr,2019-09-05T23:34:22Z,[WIP] Temp commit to add bias() method,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25741 [WIP] Temp commit to add bias() method**
* #25428 [quantization] Store bias in PackedLinearWeight struct in fbgemm
* #25678 [quantization] Rename FBGEMM quantized operators to generic quantized ops

Differential Revision: [D17216486](https://our.internmc.facebook.com/intern/diff/D17216486/)",pytorch
25773,vishwakftw,pr,2019-09-06T14:58:40Z,Fix test_det_logdet_slogdet_batched on PowerPC,"Changelog:
- Simplify generation of singular matrices to just constructing a constant matrix instead of a random singular matrix using random_square_matrix_of_rank, which is susceptible to numerical issues

Test Plan:
- test_det_logdet_slogdet_batched should pass

Fixes #25172

cc: @branfosj @hartb 

Apologies for the delay.",pytorch
25862,supriyar,pr,2019-09-09T17:50:50Z,Add new API for Fully Connected and Convolution Operators in QNNPACK,"Summary: This change adds a new prepack and run function for FC and Convolution operators in QNNPACK. 
The new functions added are `PackBMatrix`, `qnnpackLinear`, `PrePackConvWeights` and `qnnpackConv`

Test Plan: QNNPACK unit tests
fully-connected-test
convolution-test

Reviewers:

Subscribers:

Tasks:

Tags:

",pytorch
25909,vishwakftw,pr,2019-09-10T10:59:16Z,[C++ API] Fix LBFGS on GPU,"Changelog:
- Fixes mismatch of device in LBFGS, and possibly that of data type as well.

Fixes #25854",pytorch
25981,rohan-varma,pr,2019-09-11T06:38:54Z,[pytorch][wip] Add clip_grad_norm_ to c++ api,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25981 [pytorch][wip] Add clip_grad_norm_ to c++ api**

Per https://github.com/pytorch/pytorch/issues/25883, we want to work
towards C++/Python API parity. This diff adds clip_grad_norm_ to the c++ API to
improve parity.

Note: this is a WIP PR.

Differential Revision: [D17312367](https://our.internmc.facebook.com/intern/diff/D17312367/)",pytorch
25982,rohan-varma,pr,2019-09-11T06:48:50Z,Add clip_grad_norm_ to c++ api (#25981),"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25981

Per https://github.com/pytorch/pytorch/issues/25883, we want to work
towards C++/Python API parity. This diff adds clip_grad_norm_ to the c++ API to
improve parity.

Note: this is a WIP PR.
ghstack-source-id: 89882567

Test Plan: Added a unit test.

Differential Revision: D17312367

",pytorch
26005,orionr,pr,2019-09-11T15:33:40Z,Create TensorBoard test classes in all cases,"To give better signal to the user, we will now always create the TensorBoard tests classes and just  disable tests if TensorBoard is not installed.

cc @lanpa @sanekmelnikov @natalialunova @pietern 
[test macos]
",pytorch
26065,ssnl,pr,2019-09-11T22:54:16Z,Fix no auto batching bugs: cannot bulk load; not work with namedtuple,see title,pytorch
26077,rohan-varma,pr,2019-09-12T01:28:44Z,[pytorch] Migrate away from using legacy Variable constructor in test_nn.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26077 [pytorch] Migrate away from using Variable( in test_nn.py**

As per #26071, we would like to get rid of the calls to Variable(
where possible. This diff removes the calls in the test file test_nn.py. The
unit tests should all still pass as expected.

Differential Revision: [D17336484](https://our.internmc.facebook.com/intern/diff/D17336484/)",pytorch
26095,vishwakftw,pr,2019-09-12T14:32:21Z,Enable batching for pinverse,"Changelog:
- Modify existing implementation of pinverse to support batching on inputs

Test Plan:
- Added tests in test_pinverse to test batched implementation

",pytorch
26108,vishwakftw,pr,2019-09-12T18:12:04Z,Skip test_triangular_solve_batched,"cc: @gchanan @zou3519 

I will look into why this is failing spuriously.",pytorch
26113,rohan-varma,pr,2019-09-12T19:32:39Z,"[pytorch][dev setup] remove ""build_deps"" arg from setup.py command in
onnx_c2_setup.sh","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26113 [pytorch][dev setup] remove ""build_deps"" arg from setup.py command in
onnx_c2_setup.sh**


This diff removes the `build_deps` arg passed into `setup.py` in `onnx_c2_setup.sh`. 

After https://github.com/pytorch/pytorch/pull/16914, passing in an
argument such as ""build_deps"" (i.e. `python setup.py build_deps develop`) is
invalid since it gets picked up as an invalid argument in `setup.py`. There is already a flag `RUN_BUILD_DEPS` set to true in `setup.py`, so the script's functionality shouldn't change.

Test plan: Before, this script would execute ""python setup.py build_deps
develop"", which errored. Now it executes ""python setup.py develop"" without an
error. Verified by successfully running the script on devgpu.

Differential Revision: [D17350359](https://our.internmc.facebook.com/intern/diff/D17350359/)",pytorch
26115,vishwakftw,pr,2019-09-12T20:03:53Z,[TEST] Skip testing triangular_solve_batched on non-default CUDA stream,This is for testing purposes.,pytorch
26134,supriyar,pr,2019-09-12T22:32:21Z,[WIP] Integrate forked QNNPACK into mobile PyTorch builds.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26335 Unify Quantization APIs for add, pool and relu
* #26307 Changes to support int8 weight and fp32 bias in QNNPACK
* #26211 Add support to call unpack for pytorch mobile quantized FC and Conv
* #26152 Adding quantized::conv2d function for pytorch mobile in c10
* #26135 Adding quantized::linear function for pytorch mobile in c10
* **#26134 [WIP] Integrate forked QNNPACK into mobile PyTorch builds.**

",pytorch
26135,supriyar,pr,2019-09-12T22:32:31Z,Adding quantized::linear function for pytorch mobile in c10,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26335 Unify Quantization APIs for add, pool and relu
* #26307 Changes to support int8 weight and fp32 bias in QNNPACK
* #26211 Add support to call unpack for pytorch mobile quantized FC and Conv
* #26152 Adding quantized::conv2d function for pytorch mobile in c10
* **#26135 Adding quantized::linear function for pytorch mobile in c10**

Summary:
This change adds the support to call QNNPACK using the refactored API for Linear operators (Fully Connected)
It also has certain cmake changes to enable builing and using pytorch_qnnpack inside aten
I have disabled USE_QNNPACK in CMakeLists.txt. Enabling it results in picking kernels from third_party/QNNPACK during runtime since the function names are the same.

Test Plan:
python test/test_quantized.py TestQNNPackOps.test_qlinear_qnnpack
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17434885](https://our.internmc.facebook.com/intern/diff/D17434885)",pytorch
26140,rohan-varma,pr,2019-09-12T23:17:05Z,[c++ api] Add clip_grad_norm_ to c++ api,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26140 [c++ api] Add clip_grad_norm_ to c++ api**

Per https://github.com/pytorch/pytorch/issues/25883, we want to work towards C++/Python API parity. This diff adds `clip_grad_norm_` to the C++ API to improve parity. 

The implementation and tests are ported over directly from the python api (see https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html)


Differential Revision: [D17312367](https://our.internmc.facebook.com/intern/diff/D17312367/)",pytorch
26152,supriyar,pr,2019-09-13T03:25:29Z,Adding quantized::conv2d function for pytorch mobile in c10,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26335 Unify Quantization APIs for add, pool and relu
* #26307 Changes to support int8 weight and fp32 bias in QNNPACK
* #26211 Add support to call unpack for pytorch mobile quantized FC and Conv
* **#26152 Adding quantized::conv2d function for pytorch mobile in c10**

Summary:
This change adds the support to call QNNPACK using the refactored API for Conv2d operators

Test Plan:
python test/test_quantized.py TestQNNPackOps.test_qconv_qnnpack

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17459892](https://our.internmc.facebook.com/intern/diff/D17459892)",pytorch
26185,vishwakftw,pr,2019-09-13T19:50:01Z,Add derivative of cholesky_solve,"Changelog:
- Add derivative of cholesky_solve. The equations are derived akin to the derivative of solve methods using the technique detailed [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiXrOjIyM7kAhWstlkKHRxqCDgQFjAAegQIAhAC&url=https%3A%2F%2Fpeople.maths.ox.ac.uk%2Fgilesm%2Ffiles%2FNA-08-01.pdf&usg=AOvVaw0BNISOvM_I9KjPrl0xv1R_)

Test Plan:
- Added tests for cholesky_solve in test_autograd.py

Closes half of #4669.",pytorch
26200,yaroslavvb,pr,2019-09-13T21:15:59Z,Add type hint for cuda.set_rng_state,"Fixes #26199

",pytorch
26211,supriyar,pr,2019-09-13T22:22:27Z,Add support to call unpack for pytorch mobile quantized FC and Conv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26335 Unify Quantization APIs for add, pool and relu
* #26307 Changes to support int8 weight and fp32 bias in QNNPACK
* **#26211 Add support to call unpack for pytorch mobile quantized FC and Conv**

Summary:
Currently QNNPACK does not have an unpack function like FBGEMM does.
In order to be able to script quantized models for mobile, we need to save unpacked weights.

This change stores the original weights and bias in the opaque struct and simply returns it when unpack is called

Test Plan:
python test/test_quantized.py TestQNNPackOps.test_qconv_unpack
python test/test_quantized.py TestQNNPackOps.test_qlinear_unpack

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17464430](https://our.internmc.facebook.com/intern/diff/D17464430)",pytorch
26226,ngimel,pr,2019-09-14T01:09:27Z,Improve symbolic_script gradients for addmm,"This PR modifies symbolic script gradients for addmm to return gradient with the same striding as corresponding input (e.g. if an input is column-major, the gradient would also be column-major). 
Notes:

- This approach creates if-blocks in the backward that are not constant-propagated away. This could be avoided by specializing backward, but, per @mruberry, this exposes another bug with gradient accumulator
- The created if-blocks would prevent TreeReduce optimization fusing multiple `mm` operations, which would hurt LSTM performance, so for now column-major optimization is enabled for `addmm` only, and not for `mm` 

Test plan: added a test checking that returned gradients have expected strides. ",pytorch
26254,ngimel,pr,2019-09-15T18:16:53Z,fix cdist gradient computation if first arg is 1xn,"Fixes #26076. @mruberry if #26248 goes in soon, I'll rebase after it, otherwise this should go in because it's a bug fix. 
Side note: cdist backward testing is very light and I suspect is not testing all the code paths, but that's a separate issue.
Test plan: added test for the affected size to test_autograd.py. Streams are tested by existing tests.  ",pytorch
26263,mkuchnik,pr,2019-09-16T03:00:19Z,Fix typo in docs.,,pytorch
26303,vishwakftw,pr,2019-09-16T19:12:46Z,Fix nuclear norm with requires_grad=True,"Changelog:
- Selectively assign compute_uv in the at::svd used internally in the implementation of at::nuclear_norm

Test Plan:
- Add tests in common_method_invocations.py

Refixes: #18275 ",pytorch
26307,supriyar,pr,2019-09-16T20:28:29Z,Changes to support int8 weight and fp32 bias in QNNPACK,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26335 Unify Quantization APIs for add, pool and relu
* **#26307 Changes to support int8 weight and fp32 bias in QNNPACK**

Add support for FP32 bias. Re-quantize bias during time time based on input scale.
If the value of input scale changes in the packed struct we requantize the bias with the updated input scale.

Test Plan:
    python test/test_quantized.py TestQNNPackOps

Differential Revision: [D17504253](https://our.internmc.facebook.com/intern/diff/D17504253)",pytorch
26323,rohan-varma,pr,2019-09-16T22:13:21Z,add known worker ids to dist autograd context,"Summary:
Per https://github.com/pytorch/pytorch/issues/25525 we want to clean
up distributed autograd context on all nodes, in addition to the local one. To
do this, we want to send async RPCs to the other nodes telling them to clean up
the context. The first step for this is for a node's context to know about the
other workers. This diff adds 2 things:
on the context, a set containing the known worker IDs
Passing in workerIDs to this set whenever addSendFunction() is called.

Test Plan: unit tests

Differential Revision: D17406838

",pytorch
26324,rohan-varma,pr,2019-09-16T22:40:50Z,[distributed] add known worker ids to distributed autograd context,"Per https://github.com/pytorch/pytorch/issues/25525 we want to clean up distributed autograd context on all nodes, in addition to the local one. To do this, we want to send async RPCs to the other nodes telling them to clean up the context. 

The first step for this is for a node's context to know about the other workers. This PR does two things:

1) Adds the necessary data structures and getter functions to `DistAutogradContext`
2) Refactors calls to `addSendRpcBackward` to take in the `worker_id` as an additional argument",pytorch
26335,supriyar,pr,2019-09-17T04:15:18Z,"Unify Quantization APIs for add, pool and relu","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26335 Unify Quantization APIs for add, pool and relu**
* #26307 Changes to support int8 weight and fp32 bias in QNNPACK

Summary:
Use the backend engine flag to call QNNPACK for quantized ops.

Test Plan:
python test/test_quantized.py TestQNNPACKOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17504331](https://our.internmc.facebook.com/intern/diff/D17504331)",pytorch
26336,rohan-varma,pr,2019-09-17T04:30:54Z,[pytorch][rpc] add exception handling in enqueueSend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26336 [pytorch][rpc] add exception handling in enqueueSend**

in `ProcessGroupAgent::enqueueSend`, exceptions on the caller side can result in the RPC blocking forever, since the future is never marked as finished. This diff adds exception handling into `enqueueSend` so that the future is marked as completed with an exception, preventing this. See https://github.com/pytorch/pytorch/issues/25516, for more context. 

Note: exceptions on the callee side (such as calling a function with an invalid # of arguments) are already handled in the callback function that is executed on the callee side.

Test plan: Run the following script:

```
#!/usr/bin/env python
import os
import time
from datetime import timedelta
import torch
import torch.distributed as dist
from torch.multiprocessing import Process
import torch.distributed.rpc as rpc

def run(rank, size):
    if rank == 0:
        time.sleep(0.5) # to allow the other process to exit without joining
        ret = rpc.rpc_async(""worker1"", torch.add, args=(torch.ones(2), 2))
        result = ret.wait()

def init_process(rank, size, fn, backend='gloo'):
    """""" Initialize the distributed environment. """"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29509'
    dist.init_process_group(backend, rank=rank, world_size=size, timeout=timedelta(seconds=12))
    rpc.init_model_parallel(""worker{}"".format(rank))
    fn(rank, size)
    if rank == 0: rpc.join_rpc()


if __name__ == ""__main__"":
    size = 2
    processes = []
    for rank in range(size):
        p = Process(target=init_process, args=(rank, size, run))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()
```

Previously, this code would hang until the timeout passed into `init_process_group` was hit, and we'd see an error such as `RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:119] Timed out waiting 1800000ms for send operation to complete`. (note that this is not the original exception, which was lost)

With this patch, the exception is caught and passed up:

```
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib64/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""../dist_examples/run.py"", line 27, in init_process
    fn(rank, size)
  File ""../dist_examples/run.py"", line 17, in run
    result = ret.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:12:918c:face:0:1d:0]:1966
```

The added unit test simulates this behavior.

Differential Revision: [D17416185](https://our.internmc.facebook.com/intern/diff/D17416185/)",pytorch
26364,rohan-varma,pr,2019-09-17T20:11:16Z,[distributed] use timeout in connect function to prevent againstinfinite loop,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26364 [distributed] use timeout in connect function to prevent againstinfinite loop**
infinite loop**
infinite loop**
infinite loop**

Per https://github.com/pytorch/pytorch/issues/25769, we sometimes get
an infinite loop when `TCPStore` calls `tcputil::connect`, and the server
continually returns `ECONNRESET` or `ECONNREFUSED`. If a proper timeout is passed
in, we guard against this by throwing an exception once the timeout has passed.

Testing: Tested with modifying `TCPStore` to connect to an invalid port, thus getting
`ECONNREFUSED`. If a valid timeout is passed in, the function correctly throws an
exception. Steps below:
1) in TCPStore.cpp's constructor, replace the `connect` call with this line:
 `storeSocket_ = tcputil::connect(tcpStoreAddr_, 1, true, std::chrono::milliseconds(3000));`
2) Build the `TCPStoreTest` binary.
3) Run the binary. Expected output:

```
terminate called after throwing an instance of 'std::runtime_error'
  what():  connect() timed out
Aborted (core dumped)
```

We should probably add tests around this, but there are some issues. Namely, the `connect` call in `TCPStore.cpp` does not pass in a `timeout` param at all right now (https://github.com/pytorch/pytorch/blob/929764ac2ad0d7c26e3defa2d4c4d39f4933a7ef/torch/lib/c10d/TCPStore.cpp#L290). We would need to change it to pass this param in, and then call `serverTCPStore.setTimeout()` in the test.

Differential Revision: [D17430164](https://our.internmc.facebook.com/intern/diff/D17430164/)",pytorch
26381,rohan-varma,pr,2019-09-18T01:03:30Z,[rpc][easy] remove extra get_worker_id def in distributed rpc init,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26381 [pytorch][easy] remove extra get_worker_id call in distributed rpc init**

Was looking through this definition and saw that it has 2 identical
definitions of `get_worker_id`. Tested by ensuring that all tests in
`test/test_rpc.py` still pass.

Differential Revision: [D17439495](https://our.internmc.facebook.com/intern/diff/D17439495/)",pytorch
26427,supriyar,pr,2019-09-18T20:53:40Z,Disable QNNPACK tests if pytorch is not built with it.,"Summary: Use the new macro USE_PYTORCH_QNNPACK to enable testing with qnnpack

Test Plan:
test caffe2/test:quantized -- TestQNNPackOps
Summary (total time 4.96s):
  PASS: 0
  FAIL: 0
  SKIP: 4
    caffe2/test:quantized - test_qlinear_qnnpack (test_quantized.TestQNNPackOps)
    caffe2/test:quantized - test_qnnpack_add (test_quantized.TestQNNPackOps)
    caffe2/test:quantized - test_qnnpack_relu (test_quantized.TestQNNPackOps)
    caffe2/test:quantized - test_qnnpack_maxpool2d (test_quantized.TestQNNPackOps)
  FATAL: 0
  TIMEOUT: 0
  OMIT: 0

Reviewed By: ljk53

Differential Revision: D17459791

",pytorch
26451,vishwakftw,pr,2019-09-19T03:07:07Z,Add derivative for cholesky_inverse,"Changelog:

- Add derivative of cholesky_inverse. The equations are derived akin to the derivative of solve methods using the technique detailed [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiXrOjIyM7kAhWstlkKHRxqCDgQFjAAegQIAhAC&url=https%3A%2F%2Fpeople.maths.ox.ac.uk%2Fgilesm%2Ffiles%2FNA-08-01.pdf&usg=AOvVaw0BNISOvM_I9KjPrl0xv1R_)

Test Plan:

- Added tests for cholesky_inverse in test_autograd.py

Closes #4669.",pytorch
26480,vishwakftw,pr,2019-09-19T19:31:48Z,Remove deprecated torch.gels,"Changelog:
- Remove `torch.gels` which was deprecated in v1.2.0

Test Plan:
- No tests were changed and all callsites for `torch.gels` where modified to `torch.lstsq` when `torch.lstsq` was introduced

",pytorch
26514,rohan-varma,pr,2019-09-20T00:37:23Z,test,,pytorch
26554,rohan-varma,pr,2019-09-20T17:55:57Z,[distributed] add timeout parameter to connect function in TCPStore,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26554 [distributed] add timeout parameter to connect function in TCPStore**

Previously, in `TCPStore`'s constructor we did not pass in a timeout to
the `connect` function, which thus used the default timeout (-1, so infinite).
But the timeout variable in `TCPStore.cpp `is configurable by the user and set to
be 300 seconds by default, so we should be passing this into the connect function. (See https://github.com/pytorch/pytorch/issues/26513)

Test plan: force connection errors and verify that the timeout is respected:
1) Without applying this change, force connection error by setting `tcpStorePort_=1` in `TCPStore.cpp`
2) Build the `TCPStoreTest` binary.
3) Run the binary. Observe that `connect()` does not return.
4) Apply this change. Observe that `connect()` does time out.

Differential Revision: [D17486779](https://our.internmc.facebook.com/intern/diff/D17486779/)",pytorch
26570,rohan-varma,pr,2019-09-20T21:40:25Z,[distributed][rpc] separate out rpc to sync and async apis,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26570 [distributed][rpc] separate out rpc to sync and async apis**

Per #24247, this splits out the async and sync rpc implementations to their
own functions. Previously, the user would call `dist.rpc` and pass in an `async` flag if they wanted to run it asynchronously. This change introduces two new methods: `rpc_sync` and `rpc_async` that return the result or a future containing the result, respectively. The common code is moved to `_invoke_rpc`.

This way, we can have stronger type hinting and make the type (async vs sync) of
RPC being done more explicit to our users, as opposed to them having to pass
in an async flag to the function.

Differential Revision: [D17509975](https://our.internmc.facebook.com/intern/diff/D17509975/)",pytorch
26583,rohan-varma,pr,2019-09-21T00:31:47Z,[distributed][nccl] Log NCCL version in NCCL errors.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26583 [distributed][nccl] Log NCCL version in NCCL errors.**

Adds a function that uses `ncclGetVersion` from the NCCL API to retrieve the NCCL version. Converts it into a readable string, and is called in NCCL-related error messages to log the NCCL version. Hopefully this will help with debugging NCCL errors. 

Test plan:
1) Modify `C10D_NCCL_CHECK` in `NCCLUtils.hpp` to always error by setting `ncclResult_t error = ncclSystemError`
2)  force an NCCL error with script `test/simulate_nccl_errors.py`:
Start master node: `python test/simulate_nccl_errors.py localhost 9124 0 2`
Start other node: `python test/simulate_nccl_errors.py localhost 9124 1 2`
On the master node, should see the following error message w/NCCL version:
```
Traceback (most recent call last):
  File ""simulate_nccl_errors.py"", line 29, in <module>
    process_group.allreduce(torch.rand(10).cuda(rank)).wait()
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:375, unhandled system error, NCCL version 2.4.8
```
Differential Revision: [D17473200](https://our.internmc.facebook.com/intern/diff/D17473200/)",pytorch
26586,supriyar,pr,2019-09-21T01:19:41Z,"[reland] Unify Quantization APIs for add, pool and relu","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26586 [reland] Unify Quantization APIs for add, pool and relu**

Summary:

Use the backend engine flag to call QNNPACK for quantized ops.

Test Plan:
python test/test_quantized.py TestQNNPACKOps

Differential Revision: [D17515129](https://our.internmc.facebook.com/intern/diff/D17515129)",pytorch
26598,vishwakftw,pr,2019-09-21T13:57:01Z,Port CUDA implementation of expm1 to ATen,Closes #24562,pytorch
26620,supriyar,pr,2019-09-22T05:18:14Z,Update qengine flag in python to string,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26620 Update qengine flag in python to string**

Summary: This change updates torch.backend.quantized.engine to accept string (""fbgemm""/""qnnpack""/""none"" for now).
set_qengine and get_qengine return an int which represents the at::QEngine enum

Test Plan:
python test/test_torch.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17533582](https://our.internmc.facebook.com/intern/diff/D17533582)",pytorch
26660,ngimel,pr,2019-09-23T20:26:40Z,enable double backward for non-cudnn LSTM and GRU,"An attempt to enable double backward for non-cudnn LSTM and GRU (see #25315, #20449). RNN works already because it does not rely on fused kernels. 
This does not implement double backward function itself, because that is pretty hard to spell out. Instead, it implements backward using differentiable operations, so that double backward can be done automatically. 
The good: seems to work, no effect on performance on the usual case without double backward. because fused lstm backward is used.
The bad: Performance of backward and, especially, double backward, is pretty bad. Scripting would still be a preferred way if we want a performant solution. Performance and/or memory use can be slightly improved if in-place variants can be used for sigmoid_backward and tanh_backward to avoid cat in the end, but I'm not yet sure it's possible, and in any case it is only slight improvement.  
The ugly: I could not figure out a way to reuse workspace that contains the sum of the gates with the applied sigmoid and tanh operations, so that's probably another perf and memory hit. 
cc @soumith, @albanD. If you think this approach is viable, I can extend to GRU and RNN. 
Thanks to @mcarilli whose approach to double backward in weight norm I copied. 

Test plan: added tests to check gradgrad for GRU and LSTM with cudnn disabled. ",pytorch
26728,supriyar,pr,2019-09-24T17:30:47Z,Add threadpool in qlinear and qconv for mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26728 Add threadpool in qlinear and qconv for mobile**

Summary:
Use Caffe2::mobile_threadpool() in linear and conv operators

Perf
Without threadpool - 76ms
With threadpool - 41 ms

Test Plan:
python test/test_quantized.py TestQNNPackOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17553510](https://our.internmc.facebook.com/intern/diff/D17553510)",pytorch
26867,supriyar,pr,2019-09-26T04:48:16Z,Add optimized quantize function for ARM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26867 Add optimized quantize function for ARM**

Summary:
Use caffe2::Int8Quantize for pytorch mobile. Currently this is only implemented for uint8 tensors and runs using NEON intrinsics.
For all other cases it falls back to naive pytorch quantize_val implementation.

Previously, naive implementation of quantize_val is slow on mobile, taking up more than 50% of the execution time.

Results
Before
aten::quantize_per_tensor 42.893 ms
Total model runtime 70.5ms

After
aten::quantize_per_tensor 0.340 ms
Total model runtime 27.5ms

Test Plan:
Tested current python tests work python test/test_quantized.py TestQNNPackOps
Also tested using quantized mobilenetV2 on mobile and compared output

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17638732](https://our.internmc.facebook.com/intern/diff/D17638732)",pytorch
26911,supriyar,pr,2019-09-26T19:08:30Z,Set quantized engine backend for mobile in speed_benchmark_torch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26911 Set quantized engine backend for mobile**

Summary:
Check if QNNPACK is present as a backend (should always be present on mobile).
If it is present then set the backend to QNNPACK

Test Plan:
Test on mobile
./speed_benchmark_torch --model mobilenet_quantized_scripted.pt  --input_dims=""1,3,224,224"" --input_type=float --warmup=5 --iter 20 --print_output True

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17613908](https://our.internmc.facebook.com/intern/diff/D17613908)",pytorch
26933,ssnl,pr,2019-09-26T23:00:29Z,Eliminate outdated comments,,pytorch
26946,ngimel,pr,2019-09-27T02:12:27Z,make repeat respect the current stream,"Kernel launch did not have the stream argument. 

Test plan: should be covered by current tests",pytorch
26982,supriyar,pr,2019-09-27T18:08:57Z,Support qadd_relu on pytorch mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #26992 Unify quantized conv and linear tests
* **#26982 Support qadd_relu on pytorch mobile**

Summary:
Fused add+relu support
Test Plan:
python test/test_quantized.py TestQNNPackOps.test_qnnpack_add

Also,
Add torch.backends.quantized.engine = ""qnnpack""
Ran
python test/test_quantized.py TestQuantizedOps.test_qadd_relu_different_qparams
python test/test_quantized.py TestQuantizedOps.test_qadd_relu_same_qparams
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17635063](https://our.internmc.facebook.com/intern/diff/D17635063)",pytorch
26989,vishwakftw,pr,2019-09-27T20:23:28Z,Remove outdated note in cholesky_solve and triangular_solve doc strings,"We do support inputs with dim > 2 in _out variants

",pytorch
26992,supriyar,pr,2019-09-27T20:50:08Z,Unify quantized conv and linear tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#26992 Unify quantized conv and linear tests**

Summary:
Run the same test for FBGEMM and QNNPACK backends.
Checks that QNNPACK or FBGEMM are supported before running it (using supported_qengines)

Test Plan:
    python test/test_quantized.py TestQuantizedLinear
    python test/test_quantized.py TestQuantizedConv
    python test/test_quantized_models.py
    python test/test_quantized_nn_mods.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17689171](https://our.internmc.facebook.com/intern/diff/D17689171)",pytorch
27000,rohan-varma,pr,2019-09-27T22:25:42Z,[distributed][ez] Use functools.wraps to preserve doc info in rpc,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27000 [distributed][ez] Use functools.wraps to preserve doc info in rpc**

Previously, the function information for our public APIs was being
lost due to the required_initialized decorator. With using
functools.wraps, we no longer lose it and can use introspection such as the
following:
```
from torch import distributed as dist
dist.rpc.__name__
dist.rpc.__doc__
help(dist.rpc)
```
Closes #26999
Differential Revision: [D17638531](https://our.internmc.facebook.com/intern/diff/D17638531/)",pytorch
27018,vishwakftw,pr,2019-09-28T18:15:38Z,[v1.3.0] Remove outdated note in cholesky_solve and triangular_solve doc strings,"We do support inputs with dim > 2 in _out variants

Cherry-pick of #26989 

",pytorch
27025,peterjc123,pr,2019-09-29T02:36:43Z,Fix torch.load for > 2GB file on Windows,Fixes #26998 ,pytorch
27026,ngimel,pr,2019-09-29T03:16:50Z,make cudnn rnn respect current stream,"Make cudnn rnn respect current stream. After this lands, non-default test stream can be reenabled in #26791 

Test plan: default stream functionality is tested in existing tests, stream safety tests will be added in #26791",pytorch
27029,peterjc123,pr,2019-09-29T06:01:47Z,Enable JIT tests on Windows,,pytorch
27031,peterjc123,pr,2019-09-29T07:34:37Z,Fix Windows CI,,pytorch
27044,ngimel,pr,2019-09-30T02:50:07Z,make cudnn rnn respect current stream,Cherry-pick of #27026 to 1.3.0,pytorch
27068,rohan-varma,pr,2019-09-30T15:27:11Z,[pytorch][distributed] add function to get nccl version for error messages,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27068 [pytorch][distributed] add function to get nccl version for error messages**

Same as #26583. Closes https://github.com/pytorch/pytorch/issues/23549.

Adds a function that uses `ncclGetVersion` from the NCCL API to retrieve the NCCL version. Converts it into a readable string, and is called in NCCL-related error messages to log the NCCL version. Hopefully this will help with debugging NCCL errors. 

Test plan:
1) Modify `C10D_NCCL_CHECK` in `NCCLUtils.hpp` to always error by setting `ncclResult_t error = ncclSystemError`
2)  force an NCCL error with script `test/simulate_nccl_errors.py`:
Start master node: `python test/simulate_nccl_errors.py localhost 9124 0 2`
Start other node: `python test/simulate_nccl_errors.py localhost 9124 1 2`
On the master node, should see the following error message w/NCCL version:
```
Traceback (most recent call last):
  File ""simulate_nccl_errors.py"", line 29, in <module>
    process_group.allreduce(torch.rand(10).cuda(rank)).wait()
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:375, unhandled system error, NCCL version 2.4.8
```

Original commit changeset: c5635ce89de1

Differential Revision: [D17639476](https://our.internmc.facebook.com/intern/diff/D17639476/)",pytorch
27069,peterjc123,pr,2019-09-30T15:38:08Z,Fixed seek offset size to 64bit. (#27125 for 1.3.0),"Fixes https://github.com/pytorch/pytorch/issues/27063

",pytorch
27108,ssnl,pr,2019-09-30T22:28:08Z,Automatically select proper tqdm submodule,"Makes notebook UI much nicer

",pytorch
27120,peterjc123,pr,2019-10-01T02:18:09Z,Fix Windows CI for 1.3.0,"
",pytorch
27125,peterjc123,pr,2019-10-01T03:04:31Z,[Reland] Fixed seek offset size to 64bit.,Fixes https://github.com/pytorch/pytorch/issues/26998.,pytorch
27126,rohan-varma,pr,2019-10-01T05:11:22Z,Move imports to after is_available in test_rpc to fix windows builds,"These imports need to go after the check to not break windows since rpc is not supported on windows. I ran the linter and that moved it to before the check in #26570. This wasn't caught by the windows tests in the PR since it was failing due to a JIT-related reason: (https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-test2/49433/console). In the future, we should probably have a warning/a comment/some message about discouraging running linters? Thanks to @peterjc123 for flagging this.",pytorch
27128,ssnl,pr,2019-10-01T05:48:06Z,Fix zero_grad on view grads,"Fixes https://github.com/pytorch/pytorch/issues/25814. This changes behavior in the sense that previously if a backward is run with `create_graph=True` and the `.grad` is stored elsewhere as a reference, this `zero_grad()` call will detach that reference as well. However, I find that behavior unintuitive and unlikely to happen in practice. Hence I made this BC breaking change.

",pytorch
27130,rohan-varma,pr,2019-10-01T09:33:27Z,[wip][distributed] Safeguard against no distributed c10d available,"- WIP, Adds additional safeguards and follows idioms",pytorch
27142,rohan-varma,pr,2019-10-01T16:55:40Z,[pytorch][distributed] add function to get nccl version for error messages,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27142 [pytorch][distributed] add function to get nccl version for error messages**


Adds a function that uses ncclGetVersion from the NCCL API to retrieve the NCCL version. Converts it into a readable string, and is called in NCCL-related error messages to log the NCCL version. Hopefully this will help with debugging NCCL errors.

Differential Revision: [D17639476](https://our.internmc.facebook.com/intern/diff/D17639476/)",pytorch
27146,orionr,pr,2019-10-01T17:31:49Z,Remove note about tb-nightly for mesh,"The mesh plugin is now supported by default TensorBoard install, so removing this comment.

cc @sanekmelnikov @lanpa @natalialunova ",pytorch
27169,rohan-varma,pr,2019-10-01T21:26:14Z,[distributed] clearly distinguish between cpu and gpu-only builds in c10d,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #27170 [pytorch][distributed] add function to get nccl version for error messages&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D17639476/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#27169 [distributed] clearly distinguish between cpu and gpu-only builds in c10d**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D17623948/)



Differential Revision: [D17623948](https://our.internmc.facebook.com/intern/diff/D17623948/)",pytorch
27170,rohan-varma,pr,2019-10-01T21:26:22Z,[pytorch][distributed] add function to get nccl version for error messages,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#27170 [pytorch][distributed] add function to get nccl version for error messages**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D17639476/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #27169 [distributed] clearly distinguish between cpu and gpu-only builds in c10d&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D17623948/)

Adds a function that uses ncclGetVersion from the NCCL API to retrieve the NCCL version. Converts it into a readable string, and is called in NCCL-related error messages to log the NCCL version. Hopefully this will help with debugging NCCL errors.

Differential Revision: [D17639476](https://our.internmc.facebook.com/intern/diff/D17639476/)",pytorch
27180,rohan-varma,pr,2019-10-01T22:26:04Z,skip all rpc and dist autograd spawn tests for <PY36,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27180 skip all rpc and dist autograd spawn tests for <PY36**

skip rpc and distautograd spawns tests for <python 3.6

Differential Revision: [D17697368](https://our.internmc.facebook.com/intern/diff/D17697368/)",pytorch
27188,rohan-varma,pr,2019-10-02T00:26:33Z,[WIP][do not merge] example patch for increasing timeout in TCPStore,- This is a patch to debug a timeout issue in the constructor of TCPStore.,pytorch
27227,orionr,pr,2019-10-02T18:58:33Z,[IGNORE] Remove note about tb-nightly for mesh,"The mesh plugin is now supported by default TensorBoard install, so removing this comment.

cc @sanekmelnikov @lanpa @natalialunova",pytorch
27272,rohan-varma,pr,2019-10-03T01:35:05Z,"[distributed] remove dist.rpc, move dist.rpc calls to dist.rpc_sync/async in tests","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27272 [distributed] remove dist.rpc, move dist.rpc calls to dist.rpc_sync/async in tests**

It looks like some of these calls were recently added as new tests,
lets prefer to rpc_sync and rpc_async so that we don't get the warning in test
logs.

Test plan: `python test/test_rpc_spawn.py`, `python test/test_rpc_fork.py`

Differential Revision: [D17731420](https://our.internmc.facebook.com/intern/diff/D17731420/)",pytorch
27298,supriyar,pr,2019-10-03T17:47:04Z,Avoid calling tensor.numel() in for loops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27298 Avoid calling tensor.numel() in for loops**

Summary:
PR #26908 toggles NonVariableTypeMode in ATen dispatcher, which is where
USE_STATIC_DISPATCH takes place.
This causes an issue with numel() as it gets called through the dispatch mode and probably not getting inlined.
Also the thread local state is expensive to read/write so many times and this kills perf.

PR #27274 is another approach to fix this and has more details.

Test Plan:
Quantized mobilenetV2 perf before this change
Main run finished. Milliseconds per iter: 28.6782. Iters per second: 34.8696

Perf after this change
Main run finished. Milliseconds per iter: 22.2585. Iters per second: 44.9267

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17742565](https://our.internmc.facebook.com/intern/diff/D17742565)",pytorch
27344,orionr,pr,2019-10-03T23:23:12Z,[tensorboard] Add method add_hparams to API doc,"Adds the method `add_hparams` to `torch.utils.tensorboard` API docs. Will want to have this in PyTorch 1.3 release.

cc @sanekmelnikov @lanpa @natalialunova",pytorch
27349,orionr,pr,2019-10-03T23:50:37Z,[tensorboard] Add method add_hparams to API doc,"Adds the method `add_hparams` to `torch.utils.tensorboard` API docs for the PyTorch 1.3 release.

Picked version of https://github.com/pytorch/pytorch/pull/27344 for v1.3.0

cc @sanekmelnikov @lanpa @natalialunova @soumith @gchanan",pytorch
27439,rohan-varma,pr,2019-10-06T06:40:36Z,"[distributed] improve error message on incorrect inputs into gather for
ProcessGroupGloo","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27458 [distributed] improve error message for scatter in processGroupGloo
* **#27439 [distributed] improve error message on incorrect inputs into gather for
ProcessGroupGloo**
ProcessGroupGloo**

When users call dist.gather, they have to pass in a `gather_list` to
the function on the destination worker, and this list needs to have the same
size as the number of processes in the group. When the user initializes this
list incorrectly, the current error message is not very helpful (see https://github.com/pytorch/pytorch/issues/27440). This changes the error message so that the incorrect gather_list size is pointed out and the correct one is given.

Test plan: Modified the unit tests appropriately. Also tested by observing the new error message:

Prev: 
`ValueError: ProcessGroupGloo::gather: requires a single-element output list containing a list with <size> tensors`
New:
`ValueError: ProcessGroupGloo::gather: Incorrect output list size 1. Output list size should be 2, same as size of the process group.`

Differential Revision: [D17781370](https://our.internmc.facebook.com/intern/diff/D17781370/)",pytorch
27457,ngimel,pr,2019-10-07T01:35:44Z,Small fixes to improve TensorIterator overhead for the common case of inputs and outputs of the same type,"Summary:
1) Short-circuits computing common type and type promotion logic for the common case of operands and result of the same type
2) Improves performance of checking memory overlap by returning MemoryOverlap::FULL if tensors are the same, skips the call
from TensorIterator when tensors are the same
3) Changes the default size of DimVector from 5 to 6, thus allowing it not to be resized for a common case of binary operation. `strides`
DimVector is forced to have at least 2*num_tensors elements, which for an operation with 2 inputs and one output is 6
4) If `offset` is 0 (common non-broadcasting case), don't fill `strides` vector with 0-s, because all the values will be subsequently written to.

These changes combined improve the overhead from 1.02 us to .74 us for a simple in-place operation.

Test Plan: should be covered by existing tests

Differential Revision: D17784532

",pytorch
27458,rohan-varma,pr,2019-10-07T05:00:52Z,[distributed] improve error message for scatter in processGroupGloo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27458 [distributed] improve error message for scatter in processGroupGloo**
ProcessGroupGloo

Same as #27439 - improve error message by passing back the
size discrepancy, for `ProcessGroupGloo::scatter`.

Differential Revision: [D17785296](https://our.internmc.facebook.com/intern/diff/D17785296/)",pytorch
27552,supriyar,pr,2019-10-08T18:31:41Z,[mobile][quantization] Add quantized avg_pool2d for pytorch mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27552 [mobile][quantization] Add quantized avg_pool2d for pytorch mobile**

Summary:
Add support to perform avg_pool2d on mobile. Tested using existing avg_pool2d python tests
Uses qnnpack backend, which currently only support 4 dim inputs.

Test Plan:
 python test/test_quantized.py TestQuantizedOps.test_avg_pool2d

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
27596,peterjc123,pr,2019-10-09T07:10:55Z,Fix the arithmetic overflow issue for MSVC,Fixes https://github.com/pytorch/pytorch/issues/27568.,pytorch
27631,supriyar,pr,2019-10-09T20:50:16Z,[mobile][quantization] Add quantized avg_pool2d for pytorch mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27675 [mobile][quantization] Add quantized torch mean implementation
* **#27631 [mobile][quantization] Add quantized avg_pool2d for pytorch mobile**

Summary:
Add support to perform avg_pool2d on mobile. Tested using existing avg_pool2d python tests
Uses qnnpack backend, which currently only support 4 dim inputs.

Test Plan:
 python test/test_quantized.py TestQNNPackOps.test_avg_pool2d

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17973792](https://our.internmc.facebook.com/intern/diff/D17973792)",pytorch
27652,rohan-varma,pr,2019-10-09T23:28:22Z,[distributed][ez] use gloo enum instead of hardcoding string,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27652 [distributed][ez] use gloo enum instead of hardcoding string**

Changes ""gloo"" to dist.backend.GLOO in `rpc_test.py`. Closes #27231

Test plan:
`python test/test_rpc_fork.py -v && python test/test_rpc_spawn.py -v`

Differential Revision: [D17845067](https://our.internmc.facebook.com/intern/diff/D17845067/)",pytorch
27675,supriyar,pr,2019-10-10T04:28:21Z,[mobile][quantization] Add quantized torch mean implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):

* **#27675 [mobile][quantization] Add quantized torch mean implementation**


Summary:
This leverages QNNPACK global average pooling to perform torch.mean on input feature maps
Currently can only support mean along HxW plane in NCHW tensor.

Test Plan:
python test/test_quantized.py TestQuantizedOps.test_mean

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17989336](https://our.internmc.facebook.com/intern/diff/D17989336)",pytorch
27696,vishwakftw,pr,2019-10-10T13:35:04Z,Add Polygamma to the docs,Fixes #25347,pytorch
27774,supriyar,pr,2019-10-11T20:48:38Z,Suppress info messages in qnnpack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27774 Suppress info messages in qnnpack**

Summary:
Printing messages with warning and above severity only

Test Plan:
python test/test_quantized.py TestQNNPackOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17886364](https://our.internmc.facebook.com/intern/diff/D17886364)",pytorch
27775,bryant1410,pr,2019-10-11T20:57:15Z,Add missing argument in distributed.pyi,,pytorch
27812,vishwakftw,pr,2019-10-12T14:01:18Z,Add documentation for torch.lgamma,"Changelog:
- Add doc string in _torch_docs.py, _tensor_docs.py
- Expose in docs/source/torch.rst, docs/source/tensors.rst

Test Plan:
- Remove `lgamma`, `lgamma_` from the blacklist

Fixes #27783 ",pytorch
27883,rohan-varma,pr,2019-10-14T17:58:23Z,[distributed] improve error handling in getNCCLVersion in NCCLUtils,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27883 [distributed] improve error handling in getNCCLVersion in NCCLUtils**

Returns early if NCCL version code returned to us is < 100, to prevent
division errors. This shouldn't actually happen since the nvidia nccl version is way past 0.1.0 but nice to have this safeguard to not rely on assumptions.

Test plan: Follow same process as https://github.com/pytorch/pytorch/pull/27068. Also force version to be < 100 and ensure that ""Unknown NCCL Version"" is returned.

Differential Revision: [D17903234](https://our.internmc.facebook.com/intern/diff/D17903234/)",pytorch
27951,rohan-varma,pr,2019-10-15T00:19:09Z,[distributed] cleanup dist autograd context on other nodes when it is released on one node,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27951 [distributed] cleanup dist autograd context on other nodes when it is released on one node**

Per https://github.com/pytorch/pytorch/issues/25525, we want to clean up the distributed autograd context across the other nodes when a single node is done (here done means exited the context manager `with dist_autograd.context() as context_id: ...`).

This PR does a few things to implement the above:
1) Add classes to encapsulate messages for requesting this context release and the response
2) Handling of this request in `request_callback_impl.cpp`. When we receive this request, we get the context from a given context_id and release it.
3) RPC call in `DistAutogradContainer::releaseContext` to send this command. This currently does not wait for an ack or implement any sort of retrying. We send the RPC to all the workerIds we have come into contact with (implemented in https://github.com/pytorch/pytorch/pull/26324)
4) Relevant unit tests

In follow up PRs, we will add error checking + retries for this call.
Differential Revision: [D17920137](https://our.internmc.facebook.com/intern/diff/D17920137/)",pytorch
28115,ngimel,pr,2019-10-16T17:53:55Z,clean up test_cat_empty,"Remove spurious parts from test_cat_empty
Test plan: no additional tests needed. ",pytorch
28133,vishwakftw,pr,2019-10-16T20:51:45Z,Declare the LAPACK and MAGMA dispatchers instead of defining them with a default error,"This clears a lot of dead code that isn't reachable due to `AT_DISPATCH`.

Test Plan:
- All existing tests should pass to ensure that the change is valid.",pytorch
28188,rohan-varma,pr,2019-10-17T00:19:47Z,[WIP][distributed] add destructor to process group agent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28188 [distributed] add destructor to process group agent**

Implements destructor for process group agent, which for now just
detaches the `listenerThread` if it is `joinable` so that we don't get exceptions when the agent
exits without joining. In the usual case where the user calls `rpc.join_rpc()`, this destructor does not do anything (i.e., it behaves as the same default which was implicitly used previously). This is needed to test
https://github.com/pytorch/pytorch/pull/26336

See more context in the issue https://github.com/pytorch/pytorch/issues/28126 and PR https://github.com/pytorch/pytorch/pull/26336.

Differential Revision: [D17969586](https://our.internmc.facebook.com/intern/diff/D17969586/)",pytorch
28246,supriyar,pr,2019-10-17T20:40:07Z,Fix quantized avg_pool2d test to support non-zero padding,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28246 Fix quantized avg_pool2d test to support non-zero padding**

Summary:
Updated the reference fp32 implementation to use the dequantized input tensor to correctly take padded values into account

Test Plan:
python test/test_quantized.py TestQNNPackOps.test_avg_pool2d

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D17989334](https://our.internmc.facebook.com/intern/diff/D17989334)",pytorch
28338,vishwakftw,pr,2019-10-19T20:14:27Z,Return 0-numel empty tensor from symeig when eigenvectors=False,"Changelog:
- Changes the behavior of returning a zero tensor when eigenvectors=False, matching behavior of torch.eig

Test Plan:
- test_symeig has been modified appropriately for this change

",pytorch
28392,rohan-varma,pr,2019-10-21T22:40:19Z,[distributed] add support for rpc timeouts in process group agent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28392 [distributed] add support for rpc timeouts in process group agent**

Per #25531, we want to clean up futures when we detect that there are
failures/timeouts. As a first step, this diff adds a timeout parameter to process group agent (in miliseconds) that can be set by the user in Python, and also keeps track of timeouts by maintaining a map from timeout --> requests as suggested by @pritamdamania87. The map must stay in sync with the existing `futures_` map, so we write/delete to this map only when the futures_ map is written/deleted.

Note that this diff does not implement the actual process of detecting/handling timed out RPCs. The next step will be to implement a background thread that polls this map periodically and marks futures that have timed out. After this we can also add relevant unit tests. There's still an open question about how we should handle futures that complete/return from the caller after the timeout.

Differential Revision: [D18025163](https://our.internmc.facebook.com/intern/diff/D18025163/)",pytorch
28393,rohan-varma,pr,2019-10-22T01:16:36Z,[distributed] Skip ProcessGroupNCCLTest if CUDA is not available,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28393 [distributed] Skip ProcessGroupNCCLTest if CUDA is not available**

We should skip this test if CUDA is not available and alert the user.
Previously, if this test was ran on cpu it would fail with:
```
terminate called after throwing an instance of 'std::runtime_error'
  what():  cuda runtime error (3) : This binary is linked with CUDA lazy stubs and underlying .so files were not loaded. CUDA functionality is disabled. Set env variable CUDA_LAZY_DEBUG to get messages during startup
```

Open question: should this file eventually be refactored to use `gtest` like we do for most other C++ tests?


Differential Revision: [D18054369](https://our.internmc.facebook.com/intern/diff/D18054369/)",pytorch
28432,rohan-varma,pr,2019-10-22T17:57:14Z,[build] Update googletest,"updates googletest submodule to the latest release, which is 1.10.0. Done by going to `third_party/googletest` and checking out out the commit corresponding to the release, which is seen here: https://github.com/google/googletest/releases.

We want to use this so we can use new features such as `GTEST_SKIP()` to skip tests when writing our unit tests. See https://github.com/pytorch/pytorch/pull/28393 for more context and discussion.",pytorch
28437,supriyar,pr,2019-10-22T19:13:52Z,Run pytorch mobile benchmark in PEP,"Summary:
Add target to build speed_benchmark_torch for PEP.
Added a new argument `--report_pep` to print total runtime information for PEP. Can add per-op stats under this later.

Test Plan: https://our.intern.facebook.com/intern/aibench/details/664440309179004

Reviewed By: hl475

Differential Revision: D18062059

",pytorch
28485,rohan-varma,pr,2019-10-23T02:08:59Z,"[distributed] add test to ensure that dist autograd contexts are cleaned up in
the case of nested rpcs","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28485 [distributed] add test to ensure that dist autograd contexts are cleaned up in
the case of nested rpcs**

Closes #28124. This diff adds a test to ensure that when we have multiple nested RPCs
inside a dist autograd context, the context that is created as a result of a
nested rpc is cleaned up after the node creating the context exits the context
manager. For example, worker 0 might send an rpc to worker 1 that results in an
rpc to worker 2, so worker 2 will have 0's context, even though worker 0 never
directly talked to 2. This test ensures that the context on 2 would also be
cleaned up.

Differential Revision: [D18079212](https://our.internmc.facebook.com/intern/diff/D18079212/)",pytorch
28536,apaszke,pr,2019-10-23T20:00:30Z,Stop using RTLD_GLOBAL for _C,"This should help us resolve a multitude of weird segfaults and crashes
when PyTorch is imported along with other packages. Those would often
happen because libtorch symbols were exposed globally and could be used
as a source of relocations in shared libraries loaded after libtorch.

One complication is that MKL doesn't like being in a local scope, unless
you use the libmkl_rt.so variant, so I adapted our CMake configuration
to either accept that or only allow static linking.

All relocations are also resolved eagerly now, which should let us avoid
some bugs. In the process of writing this patch I found e.g. a missing
TORCH_API declaration in the NCCL bindings.

We also no longer use ctypes to interface with CUDA libraries, but use
pybind to expose the relevant functionality. This has the upside of
no longer forcing us to hardcode the enum values in Python, and also
should let us remove the ungodly hacks we use to load the libraries on
Windows.

Finally, turns out that most of our cuDNN interface in Python is dead
now so I have removed it.

Fixes #3059.",pytorch
28621,vishwakftw,pr,2019-10-24T21:31:53Z,Exclude more files in torch/csrc/distributed when USE_DISTRIBUTED=0,"Changelog:
- Guard inclusion of certain files in torch/csrc/distributed included in caffe2/CMakeLists.txt when USE_DISTRIBUTED=0

Test Plan:
- Builds should be successful
- Tests should pass

",pytorch
28750,vishwakftw,pr,2019-10-26T23:53:12Z,Migrate implementations of triu and tril to a separate file,"Having them in BatchLinearAlgebra.cpp/.cu seemed out of place, since they are more general purpose and this code was interspersed between LAPACK and MAGMA wrappers as well.

Changelog:
- Move tril* / triu* to TriangularOps.cpp/.cu

Test Plan:
- Builds should complete successfully to ensure that the migration is error-free
- Tests should pass to ensure the methods that the front-end is unaffected.",pytorch
28763,bryant1410,pr,2019-10-27T21:01:34Z,Add missing `shuffle` attribute to DistributedSampler typing file,,pytorch
28782,peterjc123,pr,2019-10-28T13:55:23Z,Fix `UNICODE` conflict on Windows,"Fixes https://github.com/pytorch/pytorch/issues/27568.
cc @IlyaOvodov.",pytorch
28811,rohan-varma,pr,2019-10-28T21:40:01Z,[distributed] implement all_gather for arbitrary python objects,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28811 [distributed] implement all_gather for arbitrary python objects**

Per https://github.com/pytorch/pytorch/issues/23232, we'd like to add support for collective communication functions working with Python objects. This PR implements `all_gather` for picklable python objects. Later diffs will implement support for the other primitives, but I wanted to get this out to agree on the high-level approach. This is implemented by pickling the objects into `ByteTensor`s, and then broadcasting them using `dist.all_gather`. We also store the size. This allows us to reconstruct the object.

Differential Revision: [D17889300](https://our.internmc.facebook.com/intern/diff/D17889300/)",pytorch
28813,bryant1410,pr,2019-10-28T22:05:24Z,Add missing ChainDataset to dataset.pyi,,pytorch
28901,ssnl,pr,2019-10-30T19:25:03Z,Improve reshape backward when the op is a view,"Currently, `reshape` does an `as_strided` when the geometry is viewable. However, `as_strided` backward is not very optimized, and can not always detect such cases. Improvements are planned at https://github.com/pytorch/pytorch/pull/8965, and I will finish it some day. But the current situation is that in these cases backward through `reshape` will copy gradient while a simple `view` will not. This is unnecessary.

Notably this affects `flatten` and a whole bunch of other ops implemented on top of `reshape`.

```py
In [15]: x = torch.randn(3, 4, requires_grad=True)

In [16]: y = x.reshape(x.shape)

In [17]: assert y._base is not None

In [18]: gy = torch.randn_like(y)

In [20]: gx = torch.autograd.grad(y, x, gy)[0]

In [21]: gx
Out[21]:
tensor([[ 0.2189,  0.3396, -0.1108,  1.7703],
        [ 1.0737, -0.1222,  1.0765, -1.3363],
        [-1.3798, -0.2950,  0.0800,  0.2501]])

In [22]: gx._base  # not gy
Out[22]:
tensor([ 0.2189,  0.3396, -0.1108,  1.7703,  1.0737, -0.1222,  1.0765, -1.3363,
        -1.3798, -0.2950,  0.0800,  0.2501])

In [23]: gy.zero_()
Out[23]:
tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

In [24]: gx  # not sharing storage with gy
Out[24]:
tensor([[ 0.2189,  0.3396, -0.1108,  1.7703],
        [ 1.0737, -0.1222,  1.0765, -1.3363],
        [-1.3798, -0.2950,  0.0800,  0.2501]])

# but everything is optimized with view, which should be equivalent with reshape in this case
In [25]: y = x.view(x.shape)  

In [26]: assert y._base is not None

In [27]: gy = torch.randn_like(y)

In [28]: gx = torch.autograd.grad(y, x, gy)[0]

In [29]: gx
Out[29]:
tensor([[-2.4463,  1.1446,  0.1501,  0.1212],
        [-1.1125,  1.4661,  0.9092, -0.2153],
        [-0.1937, -0.3381, -1.3883, -0.7329]])

In [30]: gy.zero_()
Out[30]:
tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

In [31]: gx  # sharing storage with gy
Out[31]:
tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

```",pytorch
28934,rohan-varma,pr,2019-10-30T22:36:43Z,[distributed] skip additional flaky rpc tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28934 [distributed] skip additional flaky rpc tests**

These tests are flaky, skip them as we investigate for a root cause

Differential Revision: [D18235766](https://our.internmc.facebook.com/intern/diff/D18235766/)",pytorch
28935,bryant1410,pr,2019-10-30T23:15:09Z,Add missing `default_collate` in dataloader.pyi,,pytorch
28964,vishwakftw,pr,2019-10-31T16:13:50Z,[v1.3.1] Add polygamma and lgamma to the docs,Cherry-pick of ad47788647d6b23b8922ad8c235bebb593b99bee and 82a69a690fa29b683acd7393c4da91270d1ecbfe.,pytorch
29068,rohan-varma,pr,2019-11-01T21:58:35Z,[distributed] allow rpc timeout to be set from python for different rpc backends,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29068 [distributed] allow rpc timeout to be set from python for different rpc backends**
* #28392 [distributed] add support for rpc timeouts in process group agent

RPC timeouts should be set from python for all backends, not just for process group agent

Differential Revision: [D18148449](https://our.internmc.facebook.com/intern/diff/D18148449/)",pytorch
29095,alanhdu,pr,2019-11-03T02:51:38Z,Use pybind11::gil_scoped_* functions instead of AutoGIL/AutoNoGIL,"Given that pybind11 implements these gil functions, I don't think it makes sense for Pytorch to have its own bespoke versions.

Fixes #29065",pytorch
29157,rohan-varma,pr,2019-11-04T22:27:23Z,[rpc] skip additional flaky tests in rpc,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29157 [rpc] skip additional flaky tests in rpc**

As reported, these tests are flaky and time out. Skip them
while we investigate further.See https://github.com/pytorch/pytorch/issues/29156, https://github.com/pytorch/pytorch/issues/29150, https://github.com/pytorch/pytorch/issues/29117,  https://github.com/pytorch/pytorch/issues/29212 for flakiness reports.

This is definitely starting to get whack-a-mole-y, it seems that the case probably is all rpc tests are flaky due to an underlying bug. 

Differential Revision: [D18309204](https://our.internmc.facebook.com/intern/diff/D18309204/)",pytorch
29169,supriyar,pr,2019-11-05T01:17:07Z,Add support for quantized operator conversion from PT to C2 via ONNX,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29465 [WIP] JIT Pass to unpack quantized weights
* **#29169 Add support for quantized operator conversion from PT to C2 via ONNX**

Summary:
This PR adds preliminary support required to be able to run quantized pytorch models on a C2 backend.
For quantized ops we use a custom domain name 'caffe2' to register the ops if they are in the ""quantized"" namespace.

Test Plan:
```
class MyFunction(nn.Module):
    @staticmethod
        def forward(x, y):
          qx = torch.quantize_per_tensor(x, 1.0, 0, torch.quint8)
          qlinear = nnq.Linear(2, 2)
          weight = torch.quantize_per_tensor(y, 1.0, 0, torch.qint8)
          bias = torch.ones(2).to(torch.float)
          out = qlinear(qx)
          qconv = nnq.Conv2d(2, 1, kernel_size=2, stride=1, padding=0)
          out = qconv(out)
          return out.dequantize()

x = torch.ones(2,2,2,2)
y = torch.ones(2,2)
torch.onnx.export(MyFunction(), (x, y), ""output.onnx"", verbose=True, opset_version=9, operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)
```

Produces the following onnx output
```
graph(%0 : Float(2, 2, 2, 2),
      %1 : Float(2, 2)):
  %2 : Double() = onnx::Constant[value={1}](), scope: MyFunction
  %3 : Long() = onnx::Constant[value={0}](), scope: MyFunction
  %4 : QUInt8(2, 2, 2, 2) = _caffe2::Int8Quantize(%0, %2, %3), scope: MyFunction # onnx_symbolic.py:10:0
  %5 : Byte(104) = onnx::Constant[value= 2  2 [ Variable[CPULongType]{2} ]](), scope: MyFunction # /home/supriyar/miniconda3/envs/pytorch/lib/python2.7/site-packages/torch/nn/quantized/modules/linear.py:207:0
  %6 : Double() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedLinear
  %7 : Long() = onnx::Constant[value={0}](), scope: MyFunction/QuantizedLinear
  %8 : QUInt8(2, 2, 2, 2) = _caffe2::Int8FC(%4, %5, %6, %7), scope: MyFunction/QuantizedLinear # /home/supriyar/miniconda3/envs/pytorch/lib/python2.7/site-packages/torch/nn/quantized/modules/linear.py:133:0
  %9 : Byte(128) = onnx::Constant[value= 1  2  2  2 [ Variable[CPULongType]{4} ]](), scope: MyFunction # /home/supriyar/miniconda3/envs/pytorch/lib/python2.7/site-packages/torch/nn/quantized/modules/conv.py:111:0
  %10 : Long() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %11 : Long() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %12 : Tensor = onnx::Unsqueeze[axes=[0]](%10)
  %13 : Tensor = onnx::Unsqueeze[axes=[0]](%11)
  %14 : Tensor = onnx::Concat[axis=0](%12, %13)
  %15 : Long() = onnx::Constant[value={0}](), scope: MyFunction/QuantizedConv2d
  %16 : Long() = onnx::Constant[value={0}](), scope: MyFunction/QuantizedConv2d
  %17 : Tensor = onnx::Unsqueeze[axes=[0]](%15)
  %18 : Tensor = onnx::Unsqueeze[axes=[0]](%16)
  %19 : Tensor = onnx::Concat[axis=0](%17, %18)
  %20 : Long() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %21 : Long() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %22 : Tensor = onnx::Unsqueeze[axes=[0]](%20)
  %23 : Tensor = onnx::Unsqueeze[axes=[0]](%21)
  %24 : Tensor = onnx::Concat[axis=0](%22, %23)
  %25 : Long() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %26 : Double() = onnx::Constant[value={1}](), scope: MyFunction/QuantizedConv2d
  %27 : Long() = onnx::Constant[value={0}](), scope: MyFunction/QuantizedConv2d
  %28 : QUInt8(2, 1, 1, 1) = _caffe2::Int8Conv(%8, %9, %14, %19, %24, %25, %26, %27), scope: MyFunction/QuantizedConv2d # /home/supriyar/miniconda3/envs/pytorch/lib/python2.7/site-packages/torch/nn/quantized/modules/conv.py:133:0
  %29 : Float(2, 1, 1, 1) = _caffe2::Int8Dequantize(%28), scope: MyFunction # onnx_symbolic.py:20:0
  return (%29)
```

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18329753](https://our.internmc.facebook.com/intern/diff/D18329753)",pytorch
29176,supriyar,pr,2019-11-05T02:56:13Z,Temporarily disable qnnpack tests on MACOS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29176 Temporarily disable qnnpack tests on MACOS**

Summary:
Captured in issue  #27326

Test Plan:
python test/test_quantized.py test_qconv

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18336184](https://our.internmc.facebook.com/intern/diff/D18336184)",pytorch
29180,ngimel,pr,2019-11-05T03:19:50Z,enable fast path for TensorIterator for contiguous inputs/no broadcast,"As title. Also, replaces output allocation by `empty` instead of `empty_strided` in the regular path when possible, thus avoiding resizing of outputs and taking additional DeviceGuard for that. 
Test plan: covered by existing tests",pytorch
29184,rohan-varma,pr,2019-11-05T04:37:26Z,[build] update gloo submodule,"Now that https://github.com/facebookincubator/gloo/pull/228 has landed, we can update the gloo submodule to make use of the new APIs that were added, which will help us implement a `shutdown` method in `ProcessGroupAgent`.",pytorch
29248,rohan-varma,pr,2019-11-05T23:45:13Z,[build] update gloo submodule,"Update gloo submodule to use the new APIs introduced in https://github.com/facebookincubator/gloo/pull/232. Done by `cd third_party/gloo && git checkout 7c54124` which is gloo's latest commit.

Next step would be to consume the introduced APIs in `ProcessGroup::Work`. Then we can use this layer to be able to interrupt `ProcessGroupAgent` (only for the gloo backend).
",pytorch
29266,vishwakftw,pr,2019-11-06T02:00:01Z,Enable multinomial for torch.half,"Changelog:
- Re-enable multinomial sampling when the probability tensor has `dtype == torch.half`.

It seems to have been missed in #28481.

Fixes #29211 
",pytorch
29273,peterjc123,pr,2019-11-06T03:15:56Z,Fix virtualenv builds on Windows,Fixes https://github.com/pytorch/pytorch/issues/29058.,pytorch
29276,rohan-varma,pr,2019-11-06T06:00:15Z,"Add docs for RPC, dist autograd, and RRef modules","Closes https://github.com/pytorch/pytorch/issues/28983. Documentation for `torch.distributed.rpc` and `torch.distributed.autograd` modules. Also fixes/tidies up some of the docstrings in rpc/autograd, and moves some functions to be private so they don't show up in the documentation.

Note: Much of the text to describe/explain the RPC/RRef layers are taken from the following RFCs: https://github.com/pytorch/pytorch/issues/23110, https://github.com/pytorch/pytorch/issues/26759",pytorch
29302,vishwakftw,pr,2019-11-06T19:40:58Z,Make PyTorch Python 3.8 compatible,"PEP 590 modifies the `tp_print` offset to `tp_vectorcall_offset` - which requires a Py_ssize_t object.
Passing a nullptr caused compatibility issues for Python 3.8.

Changelog:
- Modify all occurrences of `nullptr  /* tp_print */` to 0  /* tp_vectorcall_offset */
- Minor formatting changes

Test Plan:
- Local fresh build with Python 3.8 completed successfully.

Fixes #28060.
Fixes #29162.

Supersedes https://github.com/pytorch/pytorch/pull/28364",pytorch
29328,supriyar,pr,2019-11-06T21:23:10Z,Disable QNNPACK tests on MacOS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29328 Disable QNNPACK tests on MacOS**

Summary:
Tests are flaky as seen in issue #29326.
Disable until we fix the kernels.

Test Plan:
python test/test_quantized.py TestQNNPackOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18358200](https://our.internmc.facebook.com/intern/diff/D18358200)",pytorch
29465,supriyar,pr,2019-11-08T18:24:13Z,[WIP] JIT Pass to unpack quantized weights,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29465 [WIP] JIT Pass to unpack quantized weights**
* #29169 Add support for quantized operator conversion from PT to C2 via ONNX

Summary:
This change a JIT pass to unpack the quantized weights and insert the unpacked values into the graph.
The pass is called after optimize_graph so it looks for matches using the ""_caffe2::"" pattern for quantized linear and conv nodes.
The actual tensor values are looked up from the params dict

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18408256](https://our.internmc.facebook.com/intern/diff/D18408256)",pytorch
29511,vishwakftw,pr,2019-11-09T22:54:41Z,Expose is_signed for dtype,"Changelog:
- Expose is_signed for torch.dtype by modifying torch/csrc/Dtype.cpp
- Allow half, bfloat16 and bool to also been ""known"" by the isSignedType function

Test Plan:
- Add tests in test/test_torch.py

Closes #29475 

",pytorch
29512,dlibenzi,pr,2019-11-09T23:09:45Z,Fix writeable strings warnings.,,pytorch
29601,rohan-varma,pr,2019-11-11T23:11:14Z,[rpc] poll for timed out futures in process group agent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29601 [rpc] poll for timed out futures in process group agent**

Follow up from https://github.com/pytorch/pytorch/pull/28392. Adds a background thread to `ProcessGroupAgent` that polls for timed out RPCs, and marks them as completed with a timeout exception if they have timed out. Also deletes the futures from the corresponding maps `futures_` and `futureTimeouts`. Unit tests are added to ensure that timed out RPCs are appropriately cleaned up.

The existing `futureTimeouts_` map is reworked so that the key is now the time (in ms) that the RPC would timeout, instead of when the RPC began. The reason for this is that it allows us to order futures which have different timeouts, which is a feature that we'd want to support in the future. A helper function `getRPCEndTime` can be used to retrieve the key into this map. 

Since we'd like to be able to quickly stop checking for timed out RPCs after looking at the first entry in the `futureTimeouts_` map, we cannot key by beginning time (an RPC with a much smaller timeout may later be created) or by the timeout itself (since an RPC with a large timeout would not be at the beginning, even if it is about to time out).

Also adds a `shutdown` variable to process group agent to control the shutting down of this background thread, which can eventually be extended to use for controlling a clean shutdown of process group agent.

Differential Revision: [D18434215](https://our.internmc.facebook.com/intern/diff/D18434215/)",pytorch
29605,rohan-varma,pr,2019-11-11T23:22:56Z,[rpc] refactor and move createException function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29605 [rpc] refactor and move createException function**

Adds a wrapper around the existing `createException` function that
allows passing of an error string, instead of a regular C++ exception. This
allows us to mark futures with exceptions for errors that aren't necessarily c++
exceptions. This function is used by
https://github.com/pytorch/pytorch/pull/29601 and https://github.com/pytorch/pytorch/pull/26336.

Differential Revision: [D18439216](https://our.internmc.facebook.com/intern/diff/D18439216/)",pytorch
29672,ssnl,pr,2019-11-12T22:48:13Z,[WIP] Allow specifying dtype in F.one_hot,Related: https://github.com/pytorch/pytorch/issues/15457,pytorch
29694,supriyar,pr,2019-11-13T00:45:28Z,Add support for quantized operator conversion from PT to C2 via ONNX,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29694 Add support for quantized operator conversion from PT to C2 via ONNX**

Summary:
This PR adds preliminary support required to be able to run quantized pytorch models on a C2 backend.
For quantized ops we use a custom domain name '_caffe2' to register the ops if they are in the ""quantized"" namespace.
The change also adds JIT pass to unpack the quantized weights and insert the unpacked values into the graph.
The actual tensor values are looked up from the params dict.

Test Plan:
Currently tested end-to-end for Linear and Conv operator

python test/onnx/test_pytorch_onnx_caffe2.py TestQuantizedOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18467130](https://our.internmc.facebook.com/intern/diff/D18467130)",pytorch
29709,peterjc123,pr,2019-11-13T05:50:28Z,More checks on MSVC,"The flags `/sdl` and `/permissive-` are switched on automatically when using the VS GUI. Adding those checks will ensure that those annoying errors won't appear when users use the VS GUI to build their project.

More info:
https://docs.microsoft.com/en-us/cpp/build/reference/sdl-enable-additional-security-checks?view=vs-2017
https://docs.microsoft.com/en-us/cpp/build/reference/permissive-standards-conformance?view=vs-2017",pytorch
29748,ssnl,pr,2019-11-13T21:12:13Z,randint accept generator=None,"This PR fixes the inconsistent behavior of `randint`'s `generator=` kwarg. It does not accept `None`, which is inconsistent with how other random functions behave:
```
In [12]: torch.randint(0, 4, size=(2,3), generator=torch.Generator())
Out[12]:
tensor([[2, 0, 1],
        [0, 1, 3]])

In [13]: torch.randint(0, 4, size=(2,3), generator=None)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-a6bc6525a1e1> in <module>
----> 1 torch.randint(0, 4, size=(2,3), generator=None)

TypeError: randint() received an invalid combination of arguments - got (int, int, generator=NoneType, size=tuple), but expected one of:
 * (int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int high, tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
```

Other random functions work fine:
```
In [9]: torch.bernoulli(torch.ones(3))
Out[9]: tensor([1., 1., 1.])

In [10]: torch.bernoulli(torch.ones(3), generator=None)
Out[10]: tensor([1., 1., 1.])
```

This PR also documents the `generator=` kwarg, and fixes https://github.com/pytorch/pytorch/issues/29683 since it's a related easy fix.",pytorch
29762,rohan-varma,pr,2019-11-13T22:44:43Z,[rpc] rename init_model_parallel to init_rpc,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29762 [rpc] rename init_model_parallel to init_rpc**

Rename this API as discussed, since its use cases extend beyond only
model parallelism.

Differential Revision: [D18491743](https://our.internmc.facebook.com/intern/diff/D18491743/)",pytorch
29765,rohan-varma,pr,2019-11-13T23:26:03Z,[rpc] move get_rpc_timeout to pybind,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29765 [rpc] move get_rpc_timeout to pybind**

instead of wrapping this C++ function with python that causes
unnecessary overhead, we can move this to pybind and use the `DefaultRpcAgent`
to get the timeout. This was @pritamdamania87's suggestion in the comments on this PR: https://github.com/pytorch/pytorch/pull/29341/files#r345003483

Differential Revision: [D18493195](https://our.internmc.facebook.com/intern/diff/D18493195/)",pytorch
29769,ssnl,pr,2019-11-14T00:14:06Z,Fix and add more padding mode support for Conv,"Fix #29712 #29668  , add arg checking, doc, and support for reflection and replication padding modes.",pytorch
29780,supriyar,pr,2019-11-14T01:02:21Z,Add test for quantized conv conversion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29780 Add test for quantized conv conversion**
* #29694 Add support for quantized operator conversion from PT to C2 via ONNX

Summary:
Test end-to-end conversion of a single layer quantized conv model from pytorch to C2
Test Plan:
python test/onnx/test_pytorch_onnx_caffe2.py TestQuantizedOps.test_qconv_model

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18496295](https://our.internmc.facebook.com/intern/diff/D18496295)",pytorch
29798,peterjc123,pr,2019-11-14T10:12:46Z,Replace `make` with `cmake --build .` in the docs,Inspired by https://discuss.pytorch.org/t/issues-with-tutorial-installing-c-distributions-of-pytorch/33295/11,pytorch
29857,rohan-varma,pr,2019-11-14T23:22:33Z,[docs] minor updates to rpc docs,"Small fixes to rpc docs:
- mark as experimental and subject to change
- Reference the distributed autograd design document in pytorch notes page.",pytorch
29928,rohan-varma,pr,2019-11-15T22:53:49Z,Add abort API in gloo ProcessGroup Send/Recv Work,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30020 [rpc] Add local shutdown to process group agent
* **#29928 Add abort API in gloo ProcessGroup Send/Recv Work**

Original author: @xush6528 
- Add abort to `c10d::ProcessGroup::Work`.
- Change the return type of `c10d::ProcessGroup::Work::wait()` to boolean to indicate if the work is aborted after waiting.
- Add unit test for the correctness of abort.

Differential Revision: [D5685727](https://our.internmc.facebook.com/intern/diff/D5685727/)",pytorch
29975,ngimel,pr,2019-11-17T00:25:36Z,fix batch norm for empty inputs,"Fix for #29578
Shape check is moved up as much as possible, because backends by and large don't correctly handle empty inputs, so check needs to be done before backend selection. That also automatically takes care of backward, because forward for empty input is automatically differentiable, so no backend-specific backward routines are ever called. 

Test plan: tests for empty inputs are added. ",pytorch
30020,rohan-varma,pr,2019-11-18T18:45:59Z,[rpc] Add shutdown function to process group agent and refactor join,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30020 [rpc] Add local shutdown to process group agent**

After changes in the previous PR (#29928) and the gloo code (https://github.com/facebookincubator/gloo/pull/232), we can now interrupt the listener thread in process group agent, which was blocked on `wait`ing for some work to be received. The `shutdown` method aborts this and joins the all the threads. A condition variable is also added to ensure that we don't abort the thread while `recvWork` is in an invalid state. The new way of shutting down is:

```
rpc.wait_all_workers()
rpc.shutdown()
```

Differential Revision: [D5578006](https://our.internmc.facebook.com/intern/diff/D5578006/)",pytorch
30024,ngimel,pr,2019-11-18T19:39:41Z,[WIP] cache tensor scalar_type in OperandInfo,"As `scalar_type` is an expensive call on tensors, cache its result in OperandInfo. This saves approx 140 ns per op in TensorIterator. 
Test plan: covered by current tests
Differential Revision: [D18576236](https://our.internmc.facebook.com/intern/diff/D18576236/)



",pytorch
30035,ngimel,pr,2019-11-18T21:06:25Z,fix batch norm for empty inputs,"Fix for #29578
Shape check is moved up as much as possible, because backends by and large don't correctly handle empty inputs, so check needs to be done before backend selection. That also automatically takes care of backward, because forward for empty input is automatically differentiable, so no backend-specific backward routines are ever called.

Test plan: tests for empty inputs are added.",pytorch
30050,rohan-varma,pr,2019-11-18T23:01:27Z,[rpc] rename join_rpc to wait_all_workers in public api,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30052 [rpc] polish examples in docstrings and update docs to reflect correct use of
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**
rpc.init_rpc
* **#30050 [rpc] rename join_rpc to wait_all_workers in public api**

Renames this API to wait_all_workers as discussed.

Differential Revision: [D18581466](https://our.internmc.facebook.com/intern/diff/D18581466/)",pytorch
30052,rohan-varma,pr,2019-11-18T23:41:29Z,"[rpc] polish examples in docstrings and update docs to reflect correct use of
rpc.init_rpc","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30052 [rpc] polish examples in docstrings and update docs to reflect correct use of rpc.init_rpc**

Some of the examples provided in `rpc/api.py` were not updated along
with the code changes, this PR updates them. Also removes the
`dist.ProcessGroup` information since `init_rpc` now initializes a default
process group.

Differential Revision: [D18582596](https://our.internmc.facebook.com/intern/diff/D18582596/)",pytorch
30055,ssnl,pr,2019-11-19T00:03:16Z,add doc for F.softplus,,pytorch
30064,supriyar,pr,2019-11-19T02:26:15Z,[WIP] Add more operator support to convert quantized model from pytorch to c2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30064 [WIP] Add more operator support to convert quantized model from pytorch to c2**

Summary:

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_quantized_ts

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18586712](https://our.internmc.facebook.com/intern/diff/D18586712)",pytorch
30065,ngimel,pr,2019-11-19T03:18:33Z,cache tensor scalar_type in OperandInfo,"Caches result of `scalar_type` call in TensorIterator and TensorOptions, because the call is expensive. 
This removes 120 - 150 ns of overhead (from 1.25 us to 1.12 us for out-of-place case, from 0.86 us to 0.73 us for inplace case)

Test plan: covered by existing tests
Differential Revision: D18576236

",pytorch
30078,rohan-varma,pr,2019-11-19T14:09:04Z,[rpc][build] fix windows build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30078 [rpc][build] fix windows build**

Windows tests are broken (https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-test2/56195/consoleFull). Fixes by skipping these tests on windows, since RPC is not supported on windows

Differential Revision: [D18591658](https://our.internmc.facebook.com/intern/diff/D18591658/)",pytorch
30196,ssnl,pr,2019-11-20T23:44:55Z,add missing space to mask index error msg,,pytorch
30197,rohan-varma,pr,2019-11-20T23:46:22Z,[rpc] remove default constructor in futureInfo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30197 [rpc] remove default constructor in futureInfo**

Addresses @pietern's comment on https://github.com/pytorch/pytorch/pull/29601. This default constructor was added because `std::map`'s `operator[]` requires a default constructor. However, instead of using `operator[]`, we can
use `emplace` and remove the constructor, to ensure that the `FutureInfo` struct
doesn't get constructed with garbage values.

Differential Revision: [D18627675](https://our.internmc.facebook.com/intern/diff/D18627675/)",pytorch
30201,rohan-varma,pr,2019-11-21T00:19:05Z,[rpc] default construct rpc agent options based on the backend type,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30208 [rpc] add default arg for init_method
* **#30201 [rpc] default construct rpc agent options based on the backend type**

Provide a default constructor so that users don't have to construct
RPC agent options. Also rename this to RPCBackend Options as suggested.

Differential Revision: [D18628698](https://our.internmc.facebook.com/intern/diff/D18628698/)",pytorch
30202,supriyar,pr,2019-11-21T00:27:57Z,Add output_size argument to caffe2 Int8ResizeNearest,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30202 Add output_size argument to caffe2 Int8ResizeNearest**

Summary:
Pytorch Upsample operator has output_size as an argument.
For quantized tensor inputs we cannot get the input_size to calculate the width and height scale factor.
Instead we pass the output_size directly to caffe2 to calculate the scale factors.

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_upsample

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18631478](https://our.internmc.facebook.com/intern/diff/D18631478)",pytorch
30208,rohan-varma,pr,2019-11-21T01:07:34Z,[rpc] add default arg for init_method,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30208 [rpc] add default arg for init_method**

Adds default arg for init_method so users don't have to pass this in,
and moves it to `RpcBackendOptions` struct.

Differential Revision: [D18630074](https://our.internmc.facebook.com/intern/diff/D18630074/)",pytorch
30273,supriyar,pr,2019-11-21T22:45:20Z,[pytorch][mobile] Add arguments to benchmark to run pytext models. Output results in ms.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30273 [pytorch][mobile] Add arguments to benchmark to run pytext models. Output results in ms.**

Pytext models expect input of the form `1xlength` and another input specifying the length.
Add the `pytext_len` argument to specify this.

Differential Revision: [D18646028](https://our.internmc.facebook.com/intern/diff/D18646028/)",pytorch
30330,rohan-varma,pr,2019-11-22T19:43:15Z,[resubmit][rpc] Add local shutdown to process group agent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30330 [resubmit][rpc] Add local shutdown to process group agent**
[test all]
Resubmit of https://github.com/pytorch/pytorch/pull/30020, which was reverted.
This is now possible due to previous changes made in `gloo` and `ProcessGroupGloo`. We `abort` the listener thread that is waiting for a message, and join all other threads. The destructor calls this same `localShutdown` method, but we ensure this is not called multiple times.


Differential Revision: [D18661775](https://our.internmc.facebook.com/intern/diff/D18661775/)",pytorch
30377,ssnl,pr,2019-11-24T04:49:32Z,format tri[lu]_indices doc better,,pytorch
30381,rohan-varma,pr,2019-11-24T08:14:15Z,[docs] minor doc tweak to use mp.spawn in example,"Per @pietern's comment in https://github.com/pytorch/pytorch/issues/30022, we can make this example launcher a bit simpler by using `torch.multiprocessing`.",pytorch
30389,vishwakftw,pr,2019-11-24T19:17:50Z,Specify ordering on singular values and eigenvalues output from torchâ€¦,"â€¦.svd/symeig respectively

Changelog:
- Adds a note to docstrings of the both functions specifying the ordering

Fixes #30301 

",pytorch
30392,peterjc123,pr,2019-11-25T02:32:30Z,Update docs for cpp_extension on Windows,Targets https://github.com/pytorch/pytorch/issues/30379.,pytorch
30396,rohan-varma,pr,2019-11-25T05:44:10Z,[rpc][easy] remove unnecessary comment block from tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30396 [rpc][easy] remove unnecessary comment block from tests**

This comment block was added in #28376 but the barrier has since gone away, so the comment does not seem necessary.

Differential Revision: [D18682506](https://our.internmc.facebook.com/intern/diff/D18682506/)",pytorch
30397,rohan-varma,pr,2019-11-25T06:09:52Z,[rpc] fix default rpc backend options when no options are passed in,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30397 [rpc] fix default rpc backend options when no options are passed in**

If rpc_backend_options are not passed in and defaulted to `None`, this currently results in a NameError since the name `rpc` is not defined here. We need to just use `backend_registry`. This was not caught before since all the tests pass in these options, so this code path was never executed. A test for these default options is thus also added.

(I would prefer to merge https://github.com/pytorch/pytorch/pull/30208/ where it is also fixed over this, but that PR is currently blocked by broken/flaky tests, so putting this fix up as well.)

Differential Revision: [D18682606](https://our.internmc.facebook.com/intern/diff/D18682606/)",pytorch
30448,bryant1410,pr,2019-11-26T03:55:20Z,Fix missing closing quotes in docs,,pytorch
30452,ngimel,pr,2019-11-26T06:46:25Z,"don't use size()/stride() functions in TensorImpl, use size_[d]/stride_[d] instead","Summary: This improved multi-d microbenchmark by ~100 ns, empty_tensor_restride used to be 13% of iteration time, now about 5%

Test Plan: Covered by existing tests

Differential Revision: D18704233

",pytorch
30490,supriyar,pr,2019-11-26T22:48:07Z,[pytorch] [caffe2] Add support for converting quantized AvgPool2d and Reshape operations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30498 [pytorch][caffe2] Add support for quantized slice conversion
* **#30490 [pytorch] [caffe2] Add support for converting quantized AvgPool2d and Reshape operations**

Summary:
Add symbolic mapping to Int8AvgPool2d and Int8Reshape op in C2

Test Plan:

python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18740520](https://our.internmc.facebook.com/intern/diff/D18740520)",pytorch
30498,supriyar,pr,2019-11-27T00:39:36Z,[pytorch][caffe2] Add support for quantized slice conversion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30679 [onnx][quantization] Add JIT pass to insert permutes for conv ops
* **#30498 [pytorch][caffe2] Add support for quantized slice conversion**

Summary:
Updated Int8SliceOp to accept dim, start and end index similar to Pytorch.

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_slice

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18740519](https://our.internmc.facebook.com/intern/diff/D18740519)",pytorch
30513,peterjc123,pr,2019-11-27T15:26:59Z,Update magma to 2.5.1 for Windows and switch CUDA in CI to 9.2,,pytorch
30545,rohan-varma,pr,2019-11-28T01:07:25Z,[v1.4.0 patch] add default arg for init_method (#30208),"Summary:
Note: This PR has been merged into master at 5c6705e after the 1.4 branch cut (see original PR: https://github.com/pytorch/pytorch/pull/30208). This PR is to merge it into the 1.4 branch.

---- Original Commit Description Follows ---

Pull Request resolved: https://github.com/pytorch/pytorch/pull/30208
Adds default arg for init_method so users don't have to pass this in,
and moves it to `RpcBackendOptions` struct. Removes `init_method` arg from rpc.init_rpc. Also fixes some docs.
ghstack-source-id: 94500475

Test Plan: Unit tests pass.

Reviewed By: mrshenli

Differential Revision: D18630074

fbshipit-source-id: 04b7dd7ec96f4c4da311b71d250233f1f262135a

",pytorch
30554,ngimel,pr,2019-11-28T01:50:13Z,improve .view() performance,"Improve .view() performance by not calling set_ and instead restriding returned alias. This improves performance of .view() operation from ~500ns to ~360 ns

Test plan: covered by existing tests",pytorch
30575,vishwakftw,pr,2019-11-30T00:11:48Z,Specify ordering on singular values and eigenvalues output from torchâ€¦,"â€¦â€¦ (#30389)

Summary:
â€¦.svd/symeig respectively

Changelog:
- Adds a note to docstrings of the both functions specifying the ordering

Fixes https://github.com/pytorch/pytorch/issues/30301
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30389

Differential Revision: D18707608

Pulled By: zou3519

fbshipit-source-id: b0f73631578f39a24fae9af4997c6491de8be9a8

",pytorch
30608,ssnl,pr,2019-12-02T15:20:17Z,Fix capitalization inconsistency in optim.rst,,pytorch
30614,ssnl,pr,2019-12-02T17:09:12Z,fix ref to nonexistent torch.repeat,,pytorch
30620,rohan-varma,pr,2019-12-02T19:17:13Z,Change test_invalid_names test to only test constructor of WorkerInfo,"This tests seems to only test that we throw exceptions in the `WorkerInfo` constructor when invalid names are passed in, so I don't think we need to complicate by initializing RPC, and exposing ourselves to potential flakiness. ",pytorch
30640,rohan-varma,pr,2019-12-03T01:18:21Z,[rpc] modify test_local_shutdown_with_rpc to not be flaky,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30640 [rpc] modify test_local_shutdown_with_rpc to not be flaky**

This test would get very occasional flakes, with an error saying the
RPC timed out. This happened because one worker would still be waiting for the
return value of an RPC, but another worker had already performed its local
shutdown, so it would not have sent the response. This didn't show up in
initial testing since the flakiness is very rare (< 1/100 test runs). We can add a dist.barrier() that all processes call into to alleviate the issue.

Differential Revision: [D18775731](https://our.internmc.facebook.com/intern/diff/D18775731/)",pytorch
30679,supriyar,pr,2019-12-03T19:46:10Z,[onnx][quantization] Add JIT pass to insert permutes for conv ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #30799 Add quantized concat conversion
* **#30679 [onnx][quantization] Add JIT pass to insert permutes for conv ops**

Summary:
Caffe2 expects quantized ops to be in NHWC format while pytorch inputs are in NCHW.
Add a jit pass to insert permutes to convert from nchw2nhwc before each conv op and add nhwc2nchw permute after the conv op.
Using graph rewriter to find consecutive redundant permutes and remove them from the graph

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18790518](https://our.internmc.facebook.com/intern/diff/D18790518)",pytorch
30700,rohan-varma,pr,2019-12-03T23:32:03Z,[wip] record rpc functions to expose them in autograd profiler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30700 [wip] record rpc functions to expose them in autograd profiler**
(Not yet ready for review)
Adds RECORD_FUNCTION macro calls to RPC code, so that the functions
can be recorded in the autograd profiler. This will allow users to see things
such as the CPU time spent in these functions when they use the profiler.

Differential Revision: [D18800095](https://our.internmc.facebook.com/intern/diff/D18800095/)",pytorch
30721,mpjlu,pr,2019-12-04T09:20:39Z,"Fix confusing ""does not have GPU support"" warning message","Many people who use caffe2 are confused about ""does not have GPU support"" warning message.
https://github.com/facebookresearch/video-nonlocal-net/issues/6
facebookarchive/caffe2#346
facebookarchive/caffe2#1634
facebookarchive/caffe2#197

Many none GPU reasons can cause this warning message. It is better to give the error info.
![image](https://user-images.githubusercontent.com/13826327/70129721-41175e00-16ba-11ea-85df-a4b1a1690149.png)
",pytorch
30752,rohan-varma,pr,2019-12-04T20:21:41Z,[v1.4.0 patch] Add local shutdown to process group agent (#30330),"Summary: Note: This PR has been merged into master after the 1.4.0 branch cut at 1350b99 (see original PR: https://github.com/pytorch/pytorch/pull/30330. This PR is to cherry pick it into 1.4.

---- Original Commit Description Follows ---
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30330

This is now possible due to previous changes made in `gloo` and `ProcessGroupGloo`. We `abort` the listener thread that is waiting for a message, and join all other threads. The API is changed so that the previous `wait_all_workers` does not destroy the agent, and this is now done in a new `shutdown` method. All callsites are updated appropriately.

ghstack-source-id: 94673884
ghstack-source-id: 94673884

Test Plan: Unit tests pass.

Reviewed By: mrshenli

Differential Revision: D18661775

fbshipit-source-id: 5aaa7c14603e18253394224994f6cd43234301c2

",pytorch
30771,ngimel,pr,2019-12-04T23:47:39Z,"fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_â€¦","â€¦overflow_large to working state

",pytorch
30793,ngimel,pr,2019-12-05T03:54:34Z,"fix AvgPool2d for 2^31-1 sized inputs, and get test_cuda_kernel_loop_â€¦","â€¦overflow_large to working state

cherry picking for 1.4.0",pytorch
30797,ngimel,pr,2019-12-05T06:17:22Z,reenable cuda_kernel_loop_overflow_large test,"Fix #30771 has landed, original issue #26838 is now closed 

cc @peterjc123 ",pytorch
30799,supriyar,pr,2019-12-05T06:20:44Z, Add quantized concat conversion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30799 Add quantized concat conversion**
* #30679 [onnx][quantization] Add JIT pass to insert permutes for conv ops

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18827396](https://our.internmc.facebook.com/intern/diff/D18827396)",pytorch
30809,peterjc123,pr,2019-12-05T07:58:42Z,Fix `os.register_at_fork` not defined on Windows,"According to https://docs.python.org/3.8/library/os.html#os.register_at_fork, this function is only available in Unix platforms.",pytorch
30810,Yangqing,pr,2019-12-05T08:35:17Z,Intrusive_ptr implementation slower than shared_ptr,"It was a random coding exercise so I wasn't putting much effort into it; but, I was like ""hey is the current intrusive_ptr implementation optimized enough?"" so I compared it with shared_ptr (using std::shared_from_this).

My benchmark result shows that intrusive_ptr is actually slower. On my macbook the speed is:

```
---------------------------------------------------------------
Benchmark                        Time           CPU Iterations
---------------------------------------------------------------
BM_IntrusivePtrCtorDtor         14 ns         14 ns   52541902
BM_SharedPtrCtorDtor            10 ns         10 ns   71898849
BM_IntrusivePtrArray         14285 ns      14112 ns      49775
BM_SharedPtrArray            13821 ns      13384 ns      51602
```

Wanted to share the results so someone could probably take a look if interested.
",pytorch
30817,dlibenzi,pr,2019-12-05T15:30:34Z,Enable equality operator for bfloat16 CPU scalar types.,"See https://github.com/pytorch/xla/issues/1330 for reference.

@mruberry @ailzhang FYI
",pytorch
30828,ssnl,pr,2019-12-05T17:52:01Z,[1.4.0] Enable len(dataloader) for iterable dataset ,Companion PR of https://github.com/pytorch/pytorch/pull/23587 for v.1.4.0,pytorch
30837,rohan-varma,pr,2019-12-05T19:18:48Z,[rpc] modify test_local_shutdown_with_rpc to not be flaky,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30837 [rpc] modify test_local_shutdown_with_rpc to not be flaky**

This test would get very occasional flakes, with an error saying the
RPC timed out. This happened because one worker would still be waiting for the
return value of an RPC, but another worker had already performed its local
shutdown, so it would not have sent the response. This didn't show up in
initial testing since the flakiness is very rare (< 1/100 test runs). This diff
fixes the issue by not erroring if these RPCs timeout. The reason this is okay
is because with a local shutdown, we should not expect for all outstanding RPCs
to be completed, since workers are free to shut down without completing/waiting
on outstanding work.

Differential Revision: [D18775731](https://our.internmc.facebook.com/intern/diff/D18775731/)",pytorch
30842,rohan-varma,pr,2019-12-05T20:25:48Z,[rpc] add an option to record time spent waiting for GIL,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30842 [rpc] profile time spent blocked on GIL acquisition**

We'd like to profile the time spent on GIL acqusiition to debug
performance issues.

Differential Revision: [D18837590](https://our.internmc.facebook.com/intern/diff/D18837590/)",pytorch
30846,vfdev-5,pr,2019-12-05T22:04:54Z,Minor change of docstring example of WeightedRandomSampler,"Previous example
```python
>>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))
        [0, 0, 0, 1, 0]
```
may seem misleading according to provided weights.

",pytorch
30887,supriyar,pr,2019-12-06T18:09:59Z,[onnx][quantization] Add quantized concat conversion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30887 [onnx][quantization] Add quantized concat conversion**

Summary:
Support to convert quantized concat from pytorch to caffe2

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_cat
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D18855676](https://our.internmc.facebook.com/intern/diff/D18855676)",pytorch
30914,rohan-varma,pr,2019-12-07T00:21:32Z,"[rpc] add the worker IDs outside of addSendRpcBackward to ensure they are
recorded in all cases","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30914 [rpc] add the worker IDs outside of addSendRpcBackward to ensure they are recorded in all cases**

When tensors don't require grad, we don't call `addSendRpcBackward`, where we record known workerIDs to clean up the dist autograd context later. But since  https://github.com/pytorch/pytorch/pull/29781, we always include the autograd context ID in RPCs, even if tensors do not require grad. So, it could be possible that we don't release the contexts on some nodes.

This can contribute to OOMs since the contexts will not be cleaned up in this case, which can be checking by running the unit test without this patch. We can fix this issue by moving the `addKnownWorkerIds`  call to the `getMessageWithAutograd` function.

Differential Revision: [D18869191](https://our.internmc.facebook.com/intern/diff/D18869191/)",pytorch
30959,peterjc123,pr,2019-12-09T07:52:07Z,Fix is_fundamental template for MSVC,Fixes https://github.com/pytorch/pytorch/issues/30932,pytorch
30971,peterjc123,pr,2019-12-09T18:00:32Z,Fix conflicts in CMAKE_GENERATOR and generator," ...specified in -G

https://cmake.org/cmake/help/latest/variable/CMAKE_GENERATOR.html
According to the document, the generator could be determined through two methods:
1. Specify in `-G`
2. Read from `CMAKE_GENERATOR`

We should avoid conflicts in these two methods. This fixes https://github.com/pytorch/pytorch/issues/30910.

",pytorch
31023,rohan-varma,pr,2019-12-10T01:45:30Z,[rpc] catch exceptions in ProcessGroupAgent::enqueueSend and report them.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31023 [rpc] catch exceptions in ProcessGroupAgent::enqueueSend and report them.**

Resubmit of https://github.com/pytorch/pytorch/pull/26336, since that PR was a few months old and many things have changed in PG agent. 

Adds support to catch exceptions in ProcessGroupAgent::enqueueSend and
report them in the future by marking the future as completed with an exception
indicating the error. An example of when this could happen is if the receiving
side aborts when the sender is sending the message, previously, we would hang
until the timeout is hit, and the original exception would be lost.

Also adds a destructor that tries to do a non-graceful `shutdown()`, which is needed, otherwise, the tests would complain about the threads not being terminated properly. This destructor is now possible since we have implemented a shutdown that does not rely on sending messages to the other workers telling them to shutdown.

A slight modification is made to `dist_utils.dist_init`: the `rpc.shutdown()` call at the end should be called with `graceful=clean_shutdown`, so that we can force a non-graceful shutdown in tests. This is used in the added unit test and does not affect the other callsites.

Differential Revision: [D18901981](https://our.internmc.facebook.com/intern/diff/D18901981/)",pytorch
31043,marload,pr,2019-12-10T12:00:40Z,Modify code to improve readability,"In the long string, formalstring thinks it is good to have a name.

When using dict, literal is better for readability and faster than dict constructor.

I always appreciate your efforts in creating the world's best frameworks.",pytorch
31077,rohan-varma,pr,2019-12-10T21:45:16Z,Document WorkerInfo and RpcBackendOptions structures in RPC docs.,"We mention `WorkerInfo` and `RpcBackendOptions` in a couple of different locations in our docs, and these are public classes that the user may use, so we should add the class to the documentation. 
<img width=""978"" alt=""Screen Shot 2019-12-10 at 1 42 22 PM"" src=""https://user-images.githubusercontent.com/8039770/70571759-47db2080-1b53-11ea-9d61-c83985a29dd9.png"">
",pytorch
31169,peterjc123,pr,2019-12-12T06:02:45Z,pinning hypothesis for windows,,pytorch
31199,rohan-varma,pr,2019-12-12T18:17:24Z,Fix error message in incorrect rref.localValue() call,"Closes https://github.com/pytorch/pytorch/issues/31198, see the issue for more details. We throw an error when `local_value()` is called on a non-owned rref, but the incorrect node name is printed in the error message. This PR fixes that and adds a relevant unit test.",pytorch
31208,vishwakftw,pr,2019-12-12T21:04:10Z,Fix unflatten when dim is a negative integer,"Changelog:
- Wrap dim to be a positive integer when dim is negative

Test plan:
- Updated tests in test_namedtensor.py

Fixes #31184 
",pytorch
31223,ngimel,pr,2019-12-13T00:39:49Z,fix view call on discontiguous tensor in to_sparse_backward,Fixes #30820,pytorch
31255,rohan-varma,pr,2019-12-13T17:27:24Z,[flaky-tests] fix ProcessGroupGlooTest,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31255 [flaky-tests] fix ProcessGroupGlooTest**

This test had 2 issues. A timeout would occasionally happen due to a timeout of 50ms, and CUDA could would get compiled and run on CPU, leading to errors. This PR fixes those issues.

Differential Revision: [D19028231](https://our.internmc.facebook.com/intern/diff/D19028231/)",pytorch
31270,rohan-varma,pr,2019-12-13T22:39:36Z,Kill MessageType::SHUTDOWN related logic in pg agent,"https://github.com/pytorch/pytorch/pull/30330 got rid of the need to send a `MessageType::SHUTDOWN` message, so we can now remove the logic/utils for this type of message. 

I think we can also delete the enum entry in the `enum MessageType`, but we may want to keep it in case the logic in https://github.com/pytorch/pytorch/pull/30710 is ever moved to C++.

Test plan: All existing unit tests pass",pytorch
31275,vishwakftw,pr,2019-12-14T01:11:10Z,Port cholesky_inverse from TH/THC to ATen,"Changelog:
- Port existing implementation of `cholesky_inverse` from TH/THC to ATen
- Implement batching of inputs
- Update doc string for `cholesky_inverse`

Test Plan:
- Add tests for batched `cholesky_inverse`

Closes #24685, closes #24543",pytorch
31280,ngimel,pr,2019-12-14T02:48:34Z,check devices for all input tensors in index_put,Fix for #30960,pytorch
31352,marload,pr,2019-12-17T02:45:21Z,Some modifications to improve readability,"In the long string, formalstring thinks it is good to have a name.

When using dict, literal is better for readability and faster than dict constructor.

I always appreciate your efforts in creating the world's best frameworks.",pytorch
31380,rohan-varma,pr,2019-12-17T20:33:00Z,[profiler] add enabled API to autograd profiler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31380 [profiler] add enabled API to autograd profiler**

For being able to profile async RPCs, we attach a `RecordFunction` object to the future that is created during the RPC to persist it across the lifetime of the RPC (this is implemented in the next PR:#31381  ). Since we'd only like to do this when profiling is enabled, this PR adds an enabled API to the autograd profiler.

Differential Revision: [D19050391](https://our.internmc.facebook.com/intern/diff/D19050391/)",pytorch
31381,rohan-varma,pr,2019-12-17T20:33:10Z,"[rpc][profiler] add support for profiling rpc.(a)sync, rpc.remote calls.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31381 [rpc][profiler] add support for profiling rpc.(a)sync, rpc.remote calls.**

This PR adds support for being able to profile both sync/async RPCs and preliminary support for `rpc.remote()`, so that users can use the autograd profiler and be able to view metrics such as RPC latency and number of calls in the profiler output.

The way this is implemented is by using the existing `RecordFunction` class provided by the autograd profiler. We create a `RecordFunction` instance when sending an RPC, if autograd profiling is enabled. We also invoke the starting callbacks on this `RecordFunction` instance, this does things such as start the CPU timer.  This instance is then persisted across the lifetime of the RPC by adding a callback that captures it onto to the `Future` created by the RPC. When the RPC is finished (i.e. when `future->markComplete()` is called), the future's callbacks are invoked so we run the `RecordFunction` instance's end callbacks, which among other things, stops the timer so that we get the correct RPC latency.

The `RecordFunction` and relevant callbacks in `profiler.cpp` are modified slightly to support running end callbacks from a different thread (which is needed since futures are marked as completed by a different thread than the main RPC thread). By default, the autograd profiler uses a `thread_local` list of `Events` and `thread_id`. However, since we'd like to run the `RecordFunction`'s callbacks from a different thread, we would like to access the list of `Events` created by the original thread. This is done by attaching the `thread_id` for the event to the `RecordFunction`, and then looking up the event with that thread in `all_event_lists` (see the changes in `profiler.cpp`). To ensure that the original behavior does not change in the profiler, this described behavior is only run when a user calls `setThreadId()` on the `RecordFunction` object.

## Example Profiler Output:
This output can be obtained by running the added tests: `test_profiler_with_*` and inspecting the print output. The tested modes are rpc_sync, rpc_async, and remote, both with builtin and UDFs.

With UDF:
```
-----------------------------------------------  ---------------  ---------------  ---------------  ---------------  --
-------------  ---------------
Name                                             Self CPU total %  Self CPU total   CPU total %      CPU total        C
PU time avg     Number of Calls
-----------------------------------------------  ---------------  ---------------  ---------------  ---------------  --
-------------  ---------------
rpc_async.my_sleep_func(worker0 -> worker1)      99.80%           1.014s           99.80%           1.014s           1.
014s           1
select                                           0.14%            1.417ms          0.14%            1.417ms          8$
.573us         16
fill_                                            0.05%            483.584us        0.05%            483.584us        30
.224us         16
to                                               0.01%            103.701us        0.01%            103.701us        25
.925us         4
-----------------------------------------------  ---------------  ---------------  ---------------  ---------------  --
-------------  ---------------
Self CPU time total: 1.016s
```
With builtin:
```
--------------------------------------------  ---------------  ---------------  ---------------  ---------------  -----
----------  ---------------
Name                                          Self CPU total %  Self CPU total   CPU total %      CPU total        CPU
time avg     Number of Calls
--------------------------------------------  ---------------  ---------------  ---------------  ---------------  -----
----------  ---------------
rpc_remote.aten::add(worker0 -> worker1)      98.48%           66.025ms         98.48%           66.025ms         66.02
5ms         1
fill_                                         0.35%            236.334us        0.35%            236.334us        23.63
3us         10
select                                        0.86%            578.335us        0.86%            578.335us        64.25
9us         9
to                                            0.07%            46.437us         0.07%            46.437us         15.47
9us         3
empty                                         0.14%            90.748us         0.14%            90.748us         45.37
4us         2
set_                                          0.10%            66.270us         0.10%            66.270us         66.27
0us         1
--------------------------------------------  ---------------  ---------------  ---------------  ---------------  -----
----------  ---------------
Self CPU time total: 67.043ms
```

If the RPC/remote call is to a method in a class, the key will look something like `rpc_sync.MyClass.my_class_method(worker0 -> worker1)`

Differential Revision: [D19053322](https://our.internmc.facebook.com/intern/diff/D19053322/)",pytorch
31432,vishwakftw,pr,2019-12-18T19:42:43Z,[v1.4.0] Fix unflatten when dim is a negative integer (#31208),"Summary:
Changelog:
- Wrap dim to be a positive integer when dim is negative
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31208

Test Plan:
- Updated tests in test_namedtensor.py

Fixes https://github.com/pytorch/pytorch/issues/31184

Differential Revision: D19036569

Pulled By: zou3519

fbshipit-source-id: 86e01e20988dee7c4b6c73232f66282d687f9a2c

",pytorch
31533,rohan-varma,pr,2019-12-20T23:08:44Z,[flaky-tests][rpc] fix test_process_group_debug_info flaky test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31533 [flaky-tests][rpc] fix test_process_group_debug_info flaky test**

Fixes `test_process_group_debug_info` which has been flaky and has been disabled (see
https://github.com/pytorch/pytorch/issues/31112).

The fix is to add a `dist.barrier()` after checking the number of idle threads. Without this barrier, another process could have made more progress and sent a request to the local worker, and if we began processing this then the assert would fail.

Test plan: Ran the test 1000 times and it passed every time.

Differential Revision: [D19203366](https://our.internmc.facebook.com/intern/diff/D19203366/)",pytorch
31539,rohan-varma,pr,2019-12-21T00:41:47Z,[rpc] add num_pending_users to debug info,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #31381 [rpc][profiler] add support for profiling rpc.(a)sync, rpc.remote calls.
* **#31539 [rpc] add num_pending_users to debug info**

Adding this metric primarily because it is needed to unblock unit
tests for https://github.com/pytorch/pytorch/pull/31381. It also may be useful
to look at this metric to see the number of pending RRef forks that currently
exist.

Differential Revision: [D19204158](https://our.internmc.facebook.com/intern/diff/D19204158/)",pytorch
31562,peterjc123,pr,2019-12-23T06:50:02Z,[DEBUG] Debug VS 2019 unknown error in CUDA,,pytorch
31580,rohan-varma,pr,2019-12-23T22:21:23Z,[rpc] enable test_backward_node_failure for process group agent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31580 [rpc] enable test_backward_node_failure for process group agent**
* #31023 [rpc] catch exceptions in ProcessGroupAgent::enqueueSend and report them.

Follow up to #31023. As per the title, this PR enables these 2 tests that were previously
disabled for process group agent, since after https://github.com/pytorch/pytorch/pull/31023, we do report exceptions on node failures for PG agent. These tests should now pass, and the exception message is determined via the type of RPC Agent being tested.

Differential Revision: [D19219515](https://our.internmc.facebook.com/intern/diff/D19219515/)",pytorch
31588,rohan-varma,pr,2019-12-24T01:07:36Z,[rpc] fix test_backward_node_failure flakiness,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31588 [rpc] fix test_backward_node_failure flakiness**

Per title. This test can sometimes fail with a different error regex
than the one that is currently tested, so add this error regex to make the test
pass consistently.

Differential Revision: [D19222275](https://our.internmc.facebook.com/intern/diff/D19222275/)",pytorch
31592,peterjc123,pr,2019-12-24T06:15:35Z,"Revert ""Use [[deprecated]] instead of C10_DEPRECATED (#30918)""","This reverts commit f0243ea7129cea3b6e1042057503a4da15fadec2.
This fixes https://github.com/pytorch/pytorch/issues/31418 and https://github.com/pytorch/pytorch/issues/31585, and restores VS 2017 build into a working state.

",pytorch
31667,vishwakftw,pr,2019-12-27T22:31:52Z,Expose torch.poisson in documentation,"Changelog:
- Add doc string for torch.poisson briefing current behavior
- Check for non-positive entries in the tensor passed as input to torch.poisson

Closes #31646
",pytorch
31692,ngimel,pr,2019-12-30T00:49:24Z,add additional types to indexing operations dispatch,"- Fixes #31672 
- Adds Bfloat16 dispatch to the indexing operations that were missing it
    - index_put on cuda does not have bfloat16 dispatch, because I'm not sure bfloat16 math ops work on cuda

Note: `index_put_` with `accum=True` is enabled for `bool`, which does not make much sense, but I'm not the one who started it, so this behavior is preserved. 
",pytorch
31704,peterjc123,pr,2019-12-30T13:44:12Z,Fix nvcc math functions for MSVC 2019,Fixes https://github.com/pytorch/pytorch/issues/31108.,pytorch
31763,vishwakftw,pr,2020-01-02T01:00:53Z,Delete ATen/stub,This folder contained an empty CombinedStub file which isn't explicitly used anywhere.,pytorch
31784,ssnl,pr,2020-01-02T18:03:09Z,Fix and add more padding mode support for Conv,"Fix #29712 #29668 , add arg checking, doc, and support for reflection and replication padding modes.",pytorch
31798,ssnl,pr,2020-01-02T20:43:10Z,clarify when to use `as_tuple` in `torch.nonzero`,,pytorch
31801,supriyar,pr,2020-01-02T23:32:53Z,Fix segfault in caffe2 slice test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31801 Fix segfault in caffe2 slice test**

Summary: Try to fix issue #30764

Test Plan: python test/onnx/test_utility_funs.py TestUtilityFuns

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D19315046](https://our.internmc.facebook.com/intern/diff/D19315046)",pytorch
31813,rohan-varma,pr,2020-01-03T00:20:40Z,"[rpc] use unordered_set instead of vector for futureTimeouts key in
process_group_agent","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31813 [rpc] use unordered_set instead of vector for futureTimeouts key in
process_group_agent**

Closes https://github.com/pytorch/pytorch/issues/31804. We were using
an `std::vector` for the key for a map that keeps track of futures to mark them
if they timeout, but we can instead use an `unordered_set`. This results in a
faster lookup in the code block where we remove futureIDs from this set when
they complete successfully. Previously we were finding them via a linear
`std::find`. Switching it to a constant time find will help performance in the
case where a large number of futures are scheduled to time out at the same
time, or if there is no timeout enforced.

To benchmark a rough perf improvement, I created 50k futures with the same
timeout. Before this PR, the lookup `std::find(futuresAtTime.begin(),
futuresAtTime.end(), id)` took ~200us, now with a `unordered_set` lookup, it takes 1us.

Differential Revision: [D19269798](https://our.internmc.facebook.com/intern/diff/D19269798/)",pytorch
31816,peterjc123,pr,2020-01-03T02:54:09Z,Fix nvcc math functions for MSVC 2019 (v.1.4.0),"Fixes https://github.com/pytorch/pytorch/issues/31108.

",pytorch
31850,ngimel,pr,2020-01-03T23:28:02Z,Adding elementwise kernel also operating on index (#28175),"Summary:
This PR add `gpu_kernel_with_index` as an addition to element-wise kernel template. It allows kernel to not only operate on input tensor value, but also each values index(view as 1d, so from 0 to numel) within the lambda.
Direct use case here is to replace thrust::tabulate used in range/arange/linspace. Benifits are:
- thrust::tabulate causes additional unneccessary synchronization on cpu.
- Now it works with tensor iterator, output no longer needs to be contiguous and a memcpy is saved

It can also potentially be reused to add new function to pytorch later, if we see use case both value and index is needed.(for example unify tril/triu into tensor iterator element-wise? add other pattern?)

Known issues:
https://github.com/pytorch/pytorch/pull/23586 is needed to enable non-contiguous case work properly, since overlapping needs to be checked. Currently non-contiguous tensor falls into TOO_HARD. I could write proper check in this file but I figured using exist method is better. jjsjann123
It does not work beyond 32bit indexing. But thrust was erroring on those case too. We could split tensor in caller to enable this. Index changes after split, so it is easier for caller to pass different lambda, and harder for the template to handle it in general.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28175

Differential Revision: D18708649

Pulled By: ngimel

fbshipit-source-id: 382081c96f266ae7b61095fc1f2af41c6b210fa9

",pytorch
31933,supriyar,pr,2020-01-08T01:10:46Z,Add missing args to upsample_nearest2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31933 Add missing args to upsample_nearest2d**

Summary:
PR #28324 introduced new arguments to upsample_nearest operator, but the corresponding call in symbolic_caffe2 was not updated.
Also added a unit test which will catch such issues in the future for non-quantized ops.

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2.py TestCaffe2Backend_opset9.test_upsample_w_scale

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
31954,ssnl,pr,2020-01-08T18:39:14Z,Fix tensordot allowing negative dims,fixes #31926 ,pytorch
31969,rohan-varma,pr,2020-01-08T23:18:56Z,[easy][rpc] add missing braces for format in rpc _to_worker_info,This was missing and resulted in the incorrect `name` passed into `_to_worker_info` not being printed out in the error message.,pytorch
32024,gaurav1086,pr,2020-01-10T02:01:08Z,Some Performance Fixes.,"aten/src/ATen/core/ivalue.cpp: 77 and 91 : Function parameter 'start' and 'finish' should be passed by reference instead of value.

aten/src/ATen/native/Convolution.cpp:472: Removed Unused variable: output_ss

aten/src/ATen/native/Normalization.cpp:460 : Variable 'use_cudnn' declared/defined together

aten/src/ATen/native/RNN.cpp:281: Reduced scope of the variable: result

aten/src/ATen/native/RNN.cpp:1241: Pass hx by reference instead of value.

aten/src/TH/THMemoryFile.cpp:580: initialize posEol, i = 0

aten/src/TH/vector/VSX.cpp: Memory leak: y multiple locations

aten/src/THC/THCCachingHostAllocator.cpp:276: Unsigned variable 'size' can't be negative so it is unnecessary to test it.",pytorch
32029,gaurav1086,pr,2020-01-10T04:43:53Z,Memory leaks and some performance fixes,Some performance fixes and memory leaks,pytorch
32050,ssnl,pr,2020-01-10T17:42:43Z,[1.4.0] Fix tensordot allowing negative dims (#31954),"Summary:
fixes https://github.com/pytorch/pytorch/issues/31926
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31954

Differential Revision: D19331847

Pulled By: zou3519

fbshipit-source-id: e30dd9517917c056a52be7d16f23247fe28f4e28

",pytorch
32051,ssnl,pr,2020-01-10T17:43:55Z,[v1.4.0] clarify when to use `as_tuple` in `torch.nonzero`,"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31798

Differential Revision: D19272332

Pulled By: zou3519

fbshipit-source-id: 954d086a7b9f1a719e0dac303a4253bf7ec8e9f4

",pytorch
32052,ssnl,pr,2020-01-10T17:46:50Z,[v1.4.0] Fix and add more padding mode support for Conv (#31784),"Summary:
Fix https://github.com/pytorch/pytorch/issues/29712 #29668 , add arg checking, doc, and support for reflection and replication padding modes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31784

Differential Revision: D19301974

Pulled By: ezyang

fbshipit-source-id: a0ed4815c0c22e416b16e256bba04324e376b2f8

",pytorch
32055,ssnl,pr,2020-01-10T18:10:32Z,Make torch.backends.mkldnn usable without import,,pytorch
32067,vishwakftw,pr,2020-01-10T20:32:57Z,Fix typographical error in torch.triu docstring,"below --> above

Fixes #32032",pytorch
32080,gaurav1086,pr,2020-01-11T04:12:56Z,Memory leaks and performance fixes,"Memory leaks and some performance fixes

aten/src/ATen/core/ivalue.cpp: 77 and 91 : Function parameter 'start' and 'finish' should be passed by reference instead of value.

aten/src/ATen/native/Convolution.cpp:472: Removed Unused variable: output_ss

aten/src/ATen/native/Normalization.cpp:460 : Variable 'use_cudnn' declared/defined together

aten/src/TH/THMemoryFile.cpp:580: initialize posEol, i = 0

aten/src/TH/vector/VSX.cpp: Memory leak: y multiple locations

aten/src/THC/THCCachingHostAllocator.cpp:276: Unsigned variable 'size' can't be negative so it is unnecessary to test it.	",pytorch
32090,gaurav1086,pr,2020-01-12T23:59:07Z,[caffe2] Use log1p for precision,"For small magnitude values of x, log1p(x) may be more accurate than log (1 + x)

>>> np.log1p(1e-99)
1e-99
>>> np.log(1 + 1e-99)
0.0",pytorch
32122,vishwakftw,pr,2020-01-13T21:19:24Z,[v1.4.0] Fix typographical error in torch.triu docstring (#32067),"Summary:
below --> above

Fixes https://github.com/pytorch/pytorch/issues/32032
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32067

Differential Revision: D19355788

Pulled By: zou3519

fbshipit-source-id: dc7a2538a78cd11e72d47ad923ef50599a5a87e2

",pytorch
32127,supriyar,pr,2020-01-13T21:29:36Z,QNNPACK: Add support for dynamic requantization.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32128 [Temp] Add operator support for dynamic quant on mobile
* **#32127 QNNPACK: Add support for dynamic requantization.**

",pytorch
32128,supriyar,pr,2020-01-13T21:29:44Z,[Temp] Add operator support for dynamic quant on mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32128 [Temp] Add operator support for dynamic quant on mobile**
* #32364 QNNPACK: Add support for dynamic requantization.

Summary:

Test Plan:
python test/test_quantized.py TestDynamicQuantizedLinear.test_qlinear_qnnpack

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
32133,rohan-varma,pr,2020-01-13T22:06:53Z,move ProcessGroupGlooTest to gtest,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32138 use gtest asserts in ProcessGroupGlooTest instead of other checks
* #32134 [flaky-tests] fix testSend and testRecv in ProcessGroupGlooTest
* **#32133 move ProcessGroupGlooTest to gtest**

We should do this to better debug individual tests in this file.

Differential Revision: [D19375479](https://our.internmc.facebook.com/intern/diff/D19375479/)",pytorch
32134,rohan-varma,pr,2020-01-13T22:07:01Z,[flaky-tests] fix testSend and testRecv in ProcessGroupGlooTest,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32242 [flaky-tests] skip testExceptions in ProcessGroupGloo if built with TSAN
* #32138 use gtest asserts in ProcessGroupGlooTest instead of other checks
* **#32134 [flaky-tests] fix testSend and testRecv in ProcessGroupGlooTest**

These tests weren't written in the most correct way and were often
flaky. It was tricky to identify these tests as flaky until we moved this file
to use gtest.

The gist of the issue is that the test previously would not coordinate sends
and recvs properly. For example, we created a single thread to test an
abortRecv and a successful recv. A separate sender thread was used to send 2
messages. What could go wrong here is that the first send could successfully
complete, resulting in the receiving end processing the message before it gets
the abort signal. In this case we would have an error in the test.

Differential Revision: [D19379395](https://our.internmc.facebook.com/intern/diff/D19379395/)",pytorch
32138,rohan-varma,pr,2020-01-13T22:41:27Z,use gtest expect in ProcessGroupGlooTest instead of other checks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32242 [flaky-tests] skip testExceptions in ProcessGroupGloo if built with TSAN
* **#32138 use gtest asserts in ProcessGroupGlooTest instead of other checks**
* #32134 [flaky-tests] fix testSend and testRecv in ProcessGroupGlooTest

I personally prefer `throw std::runtime_error(""BOOM"")`, but we should
probably have asserts here now that it is gtest. Also ensures that the correct
exceptions are thrown by the `testSignal` tests.

Differential Revision: [D19382905](https://our.internmc.facebook.com/intern/diff/D19382905/)",pytorch
32144,gaurav1086,pr,2020-01-14T00:25:21Z,Use expm1(x) for precision,"For small magnitude values of x, expm1(x) may be more accurate than exp(x) - 1

>>> np.exp(1e-99) - 1
0.0
>>> np.expm1(1e-99)
1e-99
 

",pytorch
32152,rohan-varma,pr,2020-01-14T02:00:55Z,[WIP] port ProcessGroupAgentTest to OSS,"Summary:
Brings in the ProcessGroupAgentTest that was previously written in
internal FB repo to OSS. This will allow these ProcessGroupAgent tests to run
in our OSS CI.

Test Plan: CI, ProcessGroupAgentTest builds and passes.

Differential Revision: D19387056

",pytorch
32159,ngimel,pr,2020-01-14T04:48:33Z,make clip_grad_norm return python float,"Differential Revision: D19388419

",pytorch
32171,rohan-varma,pr,2020-01-14T18:12:30Z,[do not review] testing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32171 [do not review] testing**

Differential Revision: [D19394583](https://our.internmc.facebook.com/intern/diff/D19394583/)",pytorch
32172,rohan-varma,pr,2020-01-14T18:24:22Z,[do not review] foo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32176 [do not review][test]
* #32175 [do not review][testing]
* #32173 [do not review][test]
* **#32172 [do not review] foo**

Differential Revision: [D19394824](https://our.internmc.facebook.com/intern/diff/D19394824/)Foo Foo FOO foo 

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19394824/)!",pytorch
32173,rohan-varma,pr,2020-01-14T18:38:06Z,[do not review][test],"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32176 [do not review][test]
* #32175 [do not review][testing]
* **#32173 [do not review][test]**
* #32172 [do not review] foo

foo

Differential Revision: [D19395296](https://our.internmc.facebook.com/intern/diff/D19395296/)Foo Foo FOO foo 

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19395296/)!",pytorch
32175,rohan-varma,pr,2020-01-14T18:42:32Z,[do not review][testing],"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32176 [do not review][test]
* **#32175 [do not review][testing]**
* #32173 [do not review][test]
* #32172 [do not review] foo

foobar

Differential Revision: [D19395403](https://our.internmc.facebook.com/intern/diff/D19395403/)Foo Foo FOO foo 

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19395403/)!",pytorch
32176,rohan-varma,pr,2020-01-14T18:50:18Z,[do not review][test],"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32176 [do not review][test]**
* #32175 [do not review][testing]
* #32173 [do not review][test]
* #32172 [do not review] foo

foobar

Differential Revision: [D19395535](https://our.internmc.facebook.com/intern/diff/D19395535/)Foo Foo FOO foo 

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19395535/)!",pytorch
32177,gokceneraslan,pr,2020-01-14T18:57:44Z,Add _from_natural_params and product_along_axis function to ExponentialFamily class,"For implementing models like IOMM (http://mlg.eng.cam.ac.uk/zoubin/papers/HelGha07over.pdf) or Product of Experts (https://www.cs.toronto.edu/~hinton/absps/icann-99.pdf), we need the product of PDFs. Interestingly, the product of exponential family distributions is also an exponential family distribution where the natural parameters are just summed up (see IOMM paper for a derivation).

With the help of @fritzo, I implemented two small methods, `_from_natural_params` and `product_along_axis` to ExponentialFamily class and relevant exponential family classes and added tests. 

For a quick test, you can check `Poisson(torch.tensor([3., 10., 2.])).product_along_axis(keepdim=False).rate` is 60.

",pytorch
32180,rohan-varma,pr,2020-01-14T19:54:27Z,[do not review][test] in summary,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32180 [do not review][test] in summary**

foo

Differential Revision: [D19396925](https://our.internmc.facebook.com/intern/diff/D19396925/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19396925/)!",pytorch
32181,rohan-varma,pr,2020-01-14T19:59:31Z,[do not review][test] in summary,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32182 [do not review][test] in test plan
* **#32181 [do not review][test] in summary**

foo

Differential Revision: [D19396925](https://our.internmc.facebook.com/intern/diff/D19396925/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19396925/)!",pytorch
32182,rohan-varma,pr,2020-01-14T19:59:38Z,[do not review][test] in test plan,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32184 [do not review][test only] foo
* #32183 [do not review][test] in test plan
* **#32182 [do not review][test] in test plan**

foo

Differential Revision: [D19397084](https://our.internmc.facebook.com/intern/diff/D19397084/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19397084/)!",pytorch
32183,rohan-varma,pr,2020-01-14T20:03:31Z,[do not review][test] in test plan,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32184 [do not review][test only] foo
* **#32183 [do not review][test] in test plan**
* #32182 [do not review][test] in test plan

foo

Differential Revision: [D19397084](https://our.internmc.facebook.com/intern/diff/D19397084/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19397084/)!",pytorch
32184,rohan-varma,pr,2020-01-14T20:03:37Z,[do not review][test only] foo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32184 [do not review][test only] foo**
* #32183 [do not review][test] in test plan
* #32182 [do not review][test] in test plan

foo

Differential Revision: [D19397217](https://our.internmc.facebook.com/intern/diff/D19397217/)",pytorch
32201,gaurav1086,pr,2020-01-15T01:54:34Z,[caffe2] Logical condition reduction,x || ( !x  &&  y )  <=>  to x || y,pytorch
32215,peterjc123,pr,2020-01-15T08:53:08Z,Fix dll load logic for Python 3.8 on Windows,Fixes https://github.com/pytorch/pytorch/issues/31181 and https://github.com/pytorch/pytorch/pull/31162#discussion_r362495611.,pytorch
32217,peterjc123,pr,2020-01-15T09:03:49Z,[test] Win py3.8 fix test,,pytorch
32236,supriyar,pr,2020-01-15T20:10:37Z,[WIP] Changes to convert and test FBNet v3,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32236 [WIP] Changes to convert and test FBNet v3**

",pytorch
32242,rohan-varma,pr,2020-01-15T21:01:34Z,[flaky-tests] skip testExceptions in ProcessGroupGloo if built with TSAN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32242 [flaky-tests] skip testExceptions in ProcessGroupGloo if built with TSAN**
* #32138 use gtest asserts in ProcessGroupGlooTest instead of other checks
* #32134 [flaky-tests] fix testSend and testRecv in ProcessGroupGlooTest

TSAN and fork don't play well together, so skip this test if we're
building under TSAN. It will still run in other modes.

Differential Revision: [D19416113](https://our.internmc.facebook.com/intern/diff/D19416113/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19416113/)!",pytorch
32249,gaurav1086,pr,2020-01-15T23:19:42Z,[jit]Corrected logical boolean expression,Changed bitwise & to logical && in the boolean expression.,pytorch
32263,rohan-varma,pr,2020-01-16T02:34:23Z,[WIP] Port ProcessGroupAgent test to OSS,,pytorch
32307,ssnl,pr,2020-01-16T18:23:19Z,"scatter_add uses src, not other","using `other` kwarg gives `TypeError: scatter_add_() missing 1 required positional arguments: ""src""`",pytorch
32336,rohan-varma,pr,2020-01-17T02:18:58Z,[docs] Change fut.wait() to torch.jit._wait(fut) in jit overview docs,It looks like the jit Future does not have a `wait()` anymore and this throws an error when trying to run this code.,pytorch
32339,peterjc123,pr,2020-01-17T04:24:53Z,Fix dll load logic for Python 3.8 on Windows (v1.4.0),https://github.com/pytorch/pytorch/pull/32215 for v1.4.0.,pytorch
32360,ssnl,pr,2020-01-17T17:58:15Z,F.normalize uses clamp_min_ inplace,We don't care about autograd when `out!=None` anyways,pytorch
32364,supriyar,pr,2020-01-17T19:08:57Z,QNNPACK: Add support for dynamic requantization.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32128 [Temp] Add operator support for dynamic quant on mobile
* **#32364 QNNPACK: Add support for dynamic requantization.**

",pytorch
32396,gaurav1086,pr,2020-01-18T18:09:35Z,[caffe2] Redundant condition,"Optimize expression: 'A || (!A && B)' <=> 'A || B'

A: relErr <= maxRelErr
!A : relErr > maxRelErr
B: absErr <= absErrForRelErrFailure",pytorch
32431,gaurav1086,pr,2020-01-21T04:33:09Z,[aten]Avoid possible memory leak for sizes/idx,Avoid possible memory leak,pytorch
32446,rohan-varma,pr,2020-01-21T19:19:19Z,[do not review][testing],"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32446 [do not review][testing]**

foo

Differential Revision: [D19502305](https://our.internmc.facebook.com/intern/diff/D19502305/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19502305/)!",pytorch
32476,rohan-varma,pr,2020-01-22T01:03:43Z,"[rpc] make handling of FORWARD_AUTOGRAD_REQ in request_callback_impl
nonblocking.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32476 [rpc] make handling of FORWARD_AUTOGRAD_REQ in request_callback_impl
nonblocking.**
nonblocking.**
nonblocking.**

This makes the handling of FORWARD_AUTOGRAD_REQ in request_callback
nonblocking. Processing this message requires unwrapping the message with
autograd information, processing the original message, and sending back the
message with autograd information wrapped. This makes the processing the
original message nonblocking by grabbing a future to it and marking the parent
future as completed when this one completes.

Differential Revision: [D19509501](https://our.internmc.facebook.com/intern/diff/D19509501/)",pytorch
32478,gaurav1086,pr,2020-01-22T01:32:41Z,[aten] fix vector memory leak,free(y) missing.,pytorch
32479,supriyar,pr,2020-01-22T02:21:15Z,Add operator support for dynamic quant on mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32479 Add operator support for dynamic quant on mobile**

Summary:
Run dynamic quantization on mobile (similar to FBGEMM). Currently only implemented on linear operator

Test Plan:
python test/test_quantized.py TestDynamicQuantizedLinear.test_qlinear

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D19542980](https://our.internmc.facebook.com/intern/diff/D19542980)",pytorch
32562,rohan-varma,pr,2020-01-23T23:19:29Z,"[WIP, not yet ready for review] fix record_function scope with RPC in autograd profiler",Fixes the issue described by https://github.com/pytorch/pytorch/issues/32517,pytorch
32566,rohan-varma,pr,2020-01-24T00:53:12Z,"[rpc] fix assertion errors in record_function scope when running profiler with
rpc","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32566 [rpc] fix assertion errors in record_function scope when running profiler with
rpc**

rpc

Fix for GH issue https://github.com/pytorch/pytorch/issues/32517. The
issue is that the `record_function` decorator uses nested RecordFunction
scopes that are handled by appropriately setting `parent_` and a thread_local
pointer to the current RecordFunction. This was not properly set with RPC
profiling, since `RecordFunction::end()` is called from a different thread so
the original thread_local object is invalid.

This fixes the issue by saving a pointer to the `thread_local` pointer, and
then properly setting the pointer to the parent_ RecordFunction.

There are still some issues with the profiler output with RPC and the
`record_function` decorator. Namely if we do
```
with record_function(""foo""):
   rpc.rpc_sync(...)
```

we'd expect the sum of rpc_sync and foo() to be 100% of CPU time, like it is
for other ops, but this is not currently the case. We should fix this.

Differential Revision: [D19549107](https://our.internmc.facebook.com/intern/diff/D19549107/)",pytorch
32588,rohan-varma,pr,2020-01-24T19:04:25Z,[rpc][easy] remove redundant test in rpc_test.py,"Both `test_wait_all_workers` and `test_wait_all_workers_and_shutdown` test the same pattern of initialize RPC, call `_wait_all_workers`, and `rpc.shutdown(graceful=False)`.

`test_wait_all_workers` seems to be more thorough since it tests one worker driving and the others waiting on it as well.

We shouldn't have duplicate test so removing this `test_wait_all_workers_and_shutdown`.",pytorch
32612,ngimel,pr,2020-01-25T05:29:18Z,enable empty batch for all flavors of convolution,"Enables convolution with an empty batch or number of channels for all flavors of convolution (grouped convolution, convTranspose). Would make #31658 unnecessary. 
There's a philosophical question of whether we need to return zero gradients for weights if weights did not materially participate in the computation, or is it ok to leave weight gradients as None. I vote for the latter, the former is not easy to achieve without modifying code in all the backends, which is what #31658 does, and #26214 did before it, but it also does not add much value. If we decide that returning zero gradients for weights is important, I still think it should be done in the backend-independent manner, by having backend-independent empty_conv_forward function with its defined backward. A strike against #26214 approach is that it enables (meaningless) double backward through weights, ok, but any further backwards will break because returned gradients for double-backward are `zeros_like`, disconnected from the graph, so composability ends at double backward.
I'm returning a view of empty input, I'm open to the idea that it should be a clone, I also don't see how it matters if the input is empty anyway ;-)",pytorch
32620,peterjc123,pr,2020-01-25T16:17:34Z,Make cuda search process of cpp extension quiet,Fixes https://discuss.pytorch.org/t/error-with-cpp-extentions/67559.,pytorch
32656,rohan-varma,pr,2020-01-27T19:47:22Z,"[rpc][flaky-tests] fix for test_handle_send_exceptions and
backward_node_failure by adding more error regexes","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32659 apply linter to rpc test files
* **#32656 [rpc][flaky-tests] fix for test_handle_send_exceptions and
backward_node_failure by adding more error regexes**
backward_node_failure by adding more error regexes**
backward_node_failure by adding more error regexes**
backward_node_failure by adding more error regexes**

backward_node_failure by adding more error regexes

Fixes these flaky tests.

Differential Revision: [D19584453](https://our.internmc.facebook.com/intern/diff/D19584453/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19584453/)!",pytorch
32659,rohan-varma,pr,2020-01-27T19:59:11Z,apply linter to rpc test files,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32659 apply linter to rpc test files**

Applies linter to RPC test files so that we can use linter shortcuts
without getting unnecessary changes to the whole file.

Differential Revision: [D19584742](https://our.internmc.facebook.com/intern/diff/D19584742/)",pytorch
32677,ngimel,pr,2020-01-28T01:19:42Z,replaces .at with [] in getSlot,per title. cc @qizzzh ,pytorch
32688,peterjc123,pr,2020-01-28T05:51:23Z,Test vc 14.16 fix,,pytorch
32709,ngimel,pr,2020-01-28T18:26:15Z,enable empty batch for all flavor of convolutions,"resubmitting #32612 after a merge gone wrong. Enables convolution with an empty batch or number of channels for all flavors of convolution (grouped convolution, convTranspose). Would make #31658 unnecessary. Also returns zero gradients for the parameters, that's necessary for correct DDP operation. ",pytorch
32726,rohan-varma,pr,2020-01-28T21:57:59Z,"[rpc] don't crash callee when function does not exist on it, instead return Exception","Closes https://github.com/pytorch/pytorch/issues/27368. 
Previously, if a function `'func` did not exist on worker A but existed in B, and the user ran `rpc.rpc_sync(A,  func)`, A would crash with a segmentation fault since it is not able to find the function. B would eventually timeout since RPCs by default time out in 60s.

At the root this comes from an unhandled exception when trying to deserialize the `PythonUDF` to run.

This PR makes it so that we can recover from this error, and A reports back a `RemoteException` to B indicating that the function was not found. Now, A will no longer crash and B can handle the exception appropriately and with more information.",pytorch
32757,supriyar,pr,2020-01-29T02:15:19Z,Add support for Dynamic LSTM quantization on Mobile,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32757 Add support for Dynamic LSTM quantization on Mobile**

Summary:
This PR updates the main quantize_dynamic API to use QNNPACK backend for mobile

Test Plan:
python test/test_quantization.py PostTrainingDynamicQuantTest.test_quantized_rnn

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D19632220](https://our.internmc.facebook.com/intern/diff/D19632220)",pytorch
32797,gaurav1086,pr,2020-01-29T23:22:55Z,[torch] fd error check,,pytorch
32820,ngimel,pr,2020-01-30T06:10:34Z,make tests for empty inputs check zero parameter grads,"Make batch norm with empty inputs return zero parameter gradients. Now batch norm, group norm and convolutions now return zero grads for parameters, so make tests check that. Fixes some bullet points in #12013 (interpolate is not fixed by this PR, is being fixed in other PRs) ",pytorch
32823,peterjc123,pr,2020-01-30T09:50:54Z,Make python version a parameterizable option for Windows CI.,,pytorch
32896,orionr,pr,2020-02-01T01:56:10Z,Refine caffe2/caffe2/python for eventual python3 only,"Summary: Cleanup caffe2/caffe2/python for python3 support.

Test Plan: Wait for tests to run.

Differential Revision: D19674482

",pytorch
32902,rohan-varma,pr,2020-02-01T05:22:05Z,[test only][no review] test autograd profiler stuff,,pytorch
32936,rohan-varma,pr,2020-02-03T18:55:18Z,"[rpc] throw correct Exception on local client based on the RemoteException
type.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32936 [rpc] throw correct Exception on local client based on the RemoteException
type.**
type.**
type.**

Closes https://github.com/pytorch/pytorch/issues/32732. Currently if a UDF run in RPC throws an exception such as ValueError or TypeError, we wrap this in a RemoteException on the callee side. When raising this on the caller side, we currently raise a vanilla Exception. This diff changes it so that the correct exception is thrown. Tested by changing the current rpc tests to assert on the right type of error rather than just the base `Exception`.

Differential Revision: [D19700434](https://our.internmc.facebook.com/intern/diff/D19700434/)",pytorch
32939,ngimel,pr,2020-02-03T20:48:09Z,properly update _flat_weights in RNN modules,"Should fix #32346 hopefully. Now when _flat_weights list is updated, `None` elements are appended to it if some weights are missing, subsequent `setattr` calls for the missing weights should repair _flat_weights and make it suitable to use in the backend. ",pytorch
32957,rohan-varma,pr,2020-02-04T00:50:42Z,handle errors in ProcessGroupAgent::listenLoop().,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32957 handle errors in ProcessGroupAgent::listenLoop().**

Closes https://github.com/pytorch/pytorch/issues/29703. If there is a
gloo timeout and `recvWork->wait()` times out in `listenLoop()`,
processGroupagent crashes since there is an unhandled exception in a thread.
This catches the exception and exits the listen loop. In a follow up diff, we
will enhance these error conditions so that if users attempt to send RPCs
again, they are notified that the RPC agent was in a bad state and it was
shutdown

Differential Revision: [D19678979](https://our.internmc.facebook.com/intern/diff/D19678979/)",pytorch
32965,gaurav1086,pr,2020-02-04T03:02:23Z,[toDLPack] return a unique ptr to avoid memory leak,No need to return shared/raw ptr since the owner(src) already has its own copy. Return a unique_ptr to avoid a memory leak.,pytorch
32970,peterjc123,pr,2020-02-04T10:55:10Z,Fix for MKL detection script on Windows,"Fixes https://github.com/pytorch/pytorch/issues/32914.
1. Use `DEFINED ENV{MKLProductDir}` instead of `$ENV{MKLProductDir}`
2. Cache `INTEL_COMPILER_DIR` and `INTEL_MKL_DIR`",pytorch
32987,ngimel,pr,2020-02-04T22:13:59Z,fix lint for #32939,per title,pytorch
32989,ngimel,pr,2020-02-04T23:09:29Z,Properly update _flat_weights in RNN models,"Resubmitting #32939
Should fix #32346 hopefully. Now when _flat_weights list is updated, None elements are appended to it if some weights are missing, subsequent setattr calls for the missing weights should repair _flat_weights and make it suitable to use in the backend.",pytorch
32999,gaurav1086,pr,2020-02-05T02:00:41Z,[caffe2] simplify relative error expr,simplify relative error expr,pytorch
33030,rohan-varma,pr,2020-02-05T23:22:17Z, [profiler] add RecordFunctionAsync struct,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33030 [profiler] add RecordFunctionAsync struct**
getEventList.record()
* #33605 [rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction
* **#33030 [profiler] add RecordFunctionAsync struct**
We previously had an issue (described in #32517, #32884) where async RPCs would not be profiled correctly if there were nested scopes during profiling (nested scopes can be created by `with torch.autograd.profiler.record_function(""foo"")`...)

The cause of this was that the logic for handling nested scopes was not correct when we invoked `RecordFunction::end()` from a separate thread.

As discussed in this PR, the proposal is to add a new `RecordFunctionAsync` that exposes three different APIs:

`before` --> starts the profiling, and also calls into `RecordFunction::setThreadId`
`exitScope` --> resets the thread local state, needed for this to work with nested profiling
`end` --> invokes the end callbacks (different from `RecordFunction::end()`, which invokes the end callbacks and does the logic of `exitScope` in one function.

The next PR will change the RPC layer to use this and remove the unneeded changes in `RecordFunction`.

Differential Revision: [D19739387](https://our.internmc.facebook.com/intern/diff/D19739387/)",pytorch
33078,peterjc123,pr,2020-02-07T04:12:52Z,[Testing Only] Test #33076 with vc 14.16,@EscapeZero ,pytorch
33080,supriyar,pr,2020-02-07T05:07:29Z,[quant] Add a quantized batch_norm operator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33109 [quant] Add Quantized BatchNorm2d module
* **#33080 [quant] Add a quantized batch_norm operator**

Summary:
Quantized batch norm for cases where batch norm cannot be fused with conv.
AVX2 implementation is from Caffe2.

Test Plan:
python test/test_quantized.py TestQuantizedOps.test_batch_norm

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D19861927](https://our.internmc.facebook.com/intern/diff/D19861927)",pytorch
33084,peterjc123,pr,2020-02-07T09:35:16Z,Build ahead-of-time C++ extensions with ninja on windows,,pytorch
33108,ngimel,pr,2020-02-07T22:21:20Z,fix gather regression by not materializing loop vars in the error mesâ€¦,"â€¦sage

Per title, fixes regression reported in #32425. cc @nikitaved ",pytorch
33109,supriyar,pr,2020-02-07T22:24:24Z,[quant] Add Quantized BatchNorm2d module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33109 [quant] Add Quantized BatchNorm2d module**
* #33080 [quant] Add a quantized batch_norm operator

Summary:

Test Plan:
python test/test_quantized_nn_mods.py ModuleAPITest.test_batch_norm

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D19861926](https://our.internmc.facebook.com/intern/diff/D19861926)",pytorch
33117,peterjc123,pr,2020-02-08T04:15:48Z,Refactor and add VS 14.16 and 2019 CI for Windows,Changes according to https://github.com/pytorch/pytorch/issues/18319.,pytorch
33120,peterjc123,pr,2020-02-08T10:06:20Z,Correct /MP usage in MSVC,"## Several flags
`/MP[M]`: It is a flag for the compiler `cl`. It leads to object-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.
`/maxcpucount:[M]`: It is a flag for the generator `msbuild`. It leads to project-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.
`/p:CL_MPCount=[M]`: It is a flag for the generator `msbuild`. It leads the generator to pass `/MP[M]` to the compiler.
`/j[M]`: It is a flag for the generator `ninja`. It leads to object-level multiprocessing. By default, it spawns M processes where M is the number of cores on the PC.

## Reason for the change
1. Object-level multiprocessing is preferred over project-level multiprocessing.
2. ~For ninja, we don't need to set `/MP` otherwise M * M processes will be spawned.~ Actually, it is not correct because in ninja configs, there are only one source file in the command. Therefore, the `/MP` switch should be useless.
3. For msbuild, if it is called through Python configuration scripts, then `/p:CL_MPCount=[M]` will be added, otherwise, we add `/MP` to `CMAKE_CXX_FLAGS`.
4. ~It may be a possible fix for https://github.com/pytorch/pytorch/issues/28271, https://github.com/pytorch/pytorch/issues/27463 and https://github.com/pytorch/pytorch/issues/25393. Because `/MP` is also passed to `nvcc`.~ It is probably not true. Because `/MP` should not be effective given there is only one source file per command.

## Reference
1. https://docs.microsoft.com/en-us/cpp/build/reference/mp-build-with-multiple-processes?view=vs-2019
2. https://github.com/Microsoft/checkedc-clang/wiki/Parallel-builds-of-clang-on-Windows
3. https://blog.kitware.com/cmake-building-with-all-your-cores/",pytorch
33153,rohan-varma,pr,2020-02-10T20:13:17Z,Add interface to collect RPC-based training metrics.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33153 Add interface to collect RPC-based training metrics.**

Adds the initial interface `RpcMetricsHandler.h` for tracking metrics during distributed model parallel/RPC-based training. Metric Handlers should implement this interface in order to log metrics and stream them to some metrics storage.

Differential Revision: [D19615364](https://our.internmc.facebook.com/intern/diff/D19615364/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19615364/)!",pytorch
33174,rohan-varma,pr,2020-02-11T03:11:56Z,[rpc][easy] move unnecessary python call directly to pybind,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33174 [rpc][easy] move unnecessary python call directly to pybind**

Closes https://github.com/pytorch/pytorch/issues/32780. It looks like
this is the only callsite where we do `_get_current_rpc_agent().foo()`, and we
can do this directly in the pybind layer to save some overhead.

Differential Revision: [D19828786](https://our.internmc.facebook.com/intern/diff/D19828786/)",pytorch
33193,rohan-varma,pr,2020-02-11T20:40:22Z,Add option to log subprocess output to files in DDP launcher.,"Closes https://github.com/pytorch/pytorch/issues/7134. This request is to add an option to log the subprocess output (each subprocess is training a network with DDP) to a file instead of the default stdout.

The reason for this is that if we have N processes all writing to stdout, it'll be hard to decipher the output, and it would be cleaner to log these to separate files.

To support this, we add an optional argument `--logdir` set the subprocess stdout to be the a file of the format ""node_rank_{}_local_rank_{}"" in the logging directory. With this enabled, none of the training processes output to the parent process stdout, and instead write to the aformentioned file. If a user accidently passes in something that's not a directory, we fallback to ignoring this argument.

Tested by taking a training script at https://gist.github.com/rohan-varma/2ff1d6051440d2c18e96fe57904b55d9 and running `python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=""29500"" --logdir test_logdir train.py`. This results in a directory `test_logdir` with files ""node_0_local_rank_0"" and ""node_0_local_rank_1"" being created with the training process stdout. ",pytorch
33225,rohan-varma,pr,2020-02-12T01:38:30Z,[profiler] remove redundant assert in record_function_ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33225 [profiler] remove redundant assert in record_function_ops**

This removes a redundant assert statement in `record_function_ops`. In
the else branch in question, we are guaranteed to have `current == &rec`, so
this assert will never fire.

Although, maybe we should add an assert failure when `current == &rec` since it
seems that `current` should always be profiler::record_function_exit.

Differential Revision: [D19849145](https://our.internmc.facebook.com/intern/diff/D19849145/)",pytorch
33325,rohan-varma,pr,2020-02-14T01:07:50Z,[distributed] pass in timeout to TCP store when initializing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33325 [distributed] pass in timeout to TCP store when initializing**

Closes https://github.com/pytorch/pytorch/issues/32924. There was a bug where for TCPStore, we would not respect the timeout passed into `init_process_group` while constructing the TCPStore. Instead, we'd set the timeout after the rendezvous created the store, meaning that we used the default timeout of 300s while connecting to the server. This diff passes the timeout passed into `init_process_group` to rendezvous so that it can be passed into the constructor for TCPStore, so that we can use the right timeout at construction time.

Question: Should we make this change for FileStore as well? Currently the FileStore constructor does not take in a timeout at all.

Differential Revision: [D19871946](https://our.internmc.facebook.com/intern/diff/D19871946/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19871946/)!",pytorch
33344,lezcano,pr,2020-02-14T14:32:42Z,Parametrization Functionality,"Provides the implementation for feature request issue #28937.

Adds the `Parametrization` functionality and implements `Pruning` on top of it [UPDATE: it doesn't implement Pruning].
It adds the `auto` mode, on which the parametrization is just computed once per forwards pass. The previous implementation computed the pruning on every forward, which is not optimal when pruning RNNs for example.

It implements a caching mechanism for parameters. This is implemented through the mechanism proposed at the end of the discussion #7313. In particular, it assumes that the user will not manually change the updated parameters between the call to `backwards()` and the `optimizer.step()`. If they do so, they would need to manually call the `.invalidate()` function provided in the implementation. This could be made into a function that gets a model and invalidates all the parameters in it. It might be the case that this function has to be called in the `.cuda()` and `.to` and related functions.

As described in #7313, this could be used, to implement in a cleaner way the `weight_norm` and `spectral_norm` functions. It also allows, as described in #28937, for the implementation of constrained optimization on manifolds (i.e. orthogonal constraints, positive definite matrices, invertible matrices, weights on the sphere or the hyperbolic space...)

TODO (when implementation is validated):
- More thorough test
- Documentation

Resolves  https://github.com/pytorch/pytorch/issues/28937

@albanD 
",pytorch
33403,peterjc123,pr,2020-02-16T16:02:35Z,Fix avx-512 detection logic for jit fuser with MSVC 2019,Fixes https://github.com/pytorch/pytorch/issues/33401 and #33447.,pytorch
33404,peterjc123,pr,2020-02-16T16:27:37Z,"Revert ""Disable flaky test TestCppExtensionAOT.test_cuda_extension inâ€¦","â€¦ Windows CI (#33282)""

This reverts commit 5b922918d023126ad1f468c68577c9b599ad202d.

Fixes #33270.",pytorch
33432,vishwakftw,pr,2020-02-18T05:45:04Z,Check for consistent devices in at::where,"Changelog:
- Add a check to ensure that all inputs to `where` lie on the same device

Test Plan:
- Added test_where_invalid_device

Fixes #33422 
",pytorch
33434,rohan-varma,pr,2020-02-18T06:49:34Z,[reland][distributed] pass in timeout to TCP store when initializing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33434 [reland][distributed] pass in timeout to TCP store when initializing**

Reland of https://github.com/pytorch/pytorch/pull/33325, since the
unit test was flaky and failed on land.

To ensure that the test is not flaky, I bumped the timeout so the rendezvous
does not timeout (timing out the rendezvous in 1s led to the flakiness). I also
generalized our mechanism for retrying on errors to include retrying on errors
due to timeout in rendezvous.

Ran the test 500 times and it all passed, built on MacOS and verified that it passes there too.

Differential Revision: [D19935390](https://our.internmc.facebook.com/intern/diff/D19935390/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19935390/)!",pytorch
33483,peterjc123,pr,2020-02-19T05:29:09Z,Fix isnan for integral types in MSVC,Fixes https://github.com/pytorch/pytorch/pull/32537#discussion_r381077989.,pytorch
33512,rohan-varma,pr,2020-02-19T19:39:29Z,Skip test_ignore_output tests in c10d if not built with gloo.,,pytorch
33513,rohan-varma,pr,2020-02-19T19:42:36Z,[distributed] skip use_ignore_output tests in c10d if not built with gloo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33513 [distributed] skip use_ignore_output tests in c10d if not built with gloo**

These tests require gloo so like the other tests, they should be
skipped if not building with gloo. Otherwise they crash on Mac if not built
with gloo currently.

verified that it does not crash anymore with this PR.

Differential Revision: [D19976908](https://our.internmc.facebook.com/intern/diff/D19976908/)",pytorch
33589,ngimel,pr,2020-02-20T23:33:14Z,improve EmbeddingBag performance on cuda,"This PR improves performance of EmbeddingBag on cuda by removing 5 kernel launches (2 of those are synchronizing memcopies). 
- 2 memcopies are checking values of offsets[0] and offsets[-1] to be in expected range (0 for the former, less than number of indices for the latter). It seems strange to check only those 2 values, if users are providing invalid offsets, invalid values can be anywhere in the array, not only the first and last element. After this PR, the checks are skipped on cuda, the first value is forced to 0, if the last value is larger than expected, cuda kernel will assert. It is less nice than ValueError, but then again, the kernel could have asserted if other offset values were invalid. On the cpu, the checks are moved inside the cpu implementation from functional.py, and will throw RuntimeError instead of ValueError.
- 3 or 4 initializations (depending on the mode) of the output tensors with .zeros() are unnecessary, because every element of those tensors is written to, so their data can be uninitialized on the start.  ",pytorch
33605,rohan-varma,pr,2020-02-21T03:44:50Z,"[rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction","Stack from [ghstack](https://github.com/ezyang/ghstack):
Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33829 [profiler] improve error handling in RecordFunction/RecordFunctionAsync for profiler
* #33719 [profiler][rpc] fix a race condition in the profiler when multiple threads call
getEventList.record()
* **#33605 [rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction**
* #33030 [profiler] add RecordFunctionAsync struct
getEventList.record()
* **#33605 [rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction**
* #33030 [profiler] add RecordFunctionAsync struct

functions from the general RecordFunction

This follows up on the previous diff and makes it so that the RPC
layer uses `RecordFunctionAsync`. It also removes the now-unneeded functionality
from `RecordFunction`. Now, we are able to profile RPCs with the `record_function`s
scope.

Differential Revision: [D20024417](https://our.internmc.facebook.com/intern/diff/D20024417/)",pytorch
33623,ngimel,pr,2020-02-21T18:20:44Z,improve roll performance,Fixes #33544,pytorch
33626,supriyar,pr,2020-02-21T19:16:06Z,[quant] Regsiter fake_quant and observer attributes as buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33626 [quant] Regsiter fake_quant and observer attributes as buffers**

Summary: For DDP we require the attributes to be registered as buffers. By doing this the value is broadcast from one device to the rest.

Test Plan:
Tested on actual model on GPU

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20038839](https://our.internmc.facebook.com/intern/diff/D20038839)",pytorch
33682,supriyar,pr,2020-02-24T01:45:47Z,[quant] Make FakeQuant use REGISTER_DISPATCH,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33682 [quant] Make FakeQuant use REGISTER_DISPATCH**

Summary:
Previously, there were two API's for CPU and CUDA. This change keeps one top level API, i.e `fake_quantize_per_tensor_affine` and `fake_quantize_per_channel_affine` and uses the device type to dispatch to different backends (CPU and CUDA).
CPU kernel implementation is in QuantizedOpKernels.cpp
CUDA kernel implementation is in fake_quantize_core.cu

Test Plan:
python test/test_fake_quant.py

Benchmark Results for CPU
FakeQuantize tensor of size (2, 256, 128, 128)

Before:
per tensor quant ms 9.905877113342285
per channel quant ms 74.93825674057007

After:
per tensor quant ms 6.028120517730713
per channel quant ms 44.91588592529297

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20072656](https://our.internmc.facebook.com/intern/diff/D20072656)",pytorch
33690,peterjc123,pr,2020-02-24T09:21:34Z,Update MKL to 2020.0.166 for Windows,,pytorch
33693,peterjc123,pr,2020-02-24T14:36:39Z,[TEST ONLY] Debug windows nvcc flaky build,,pytorch
33700,peterjc123,pr,2020-02-24T18:09:02Z,Don't activate vc env again for cuda with ninja on Windows,"Possibly get rid of #28271, #27463 and #25393.",pytorch
33719,rohan-varma,pr,2020-02-24T20:06:55Z,"[profiler][rpc] fix a race condition in the profiler when multiple threads call
getEventList.record()","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33719 [profiler][rpc] fix a race condition in the profiler when multiple threads call
getEventList.record()**
getEventList.record()**
getEventList.record()**

We were seeing a strange error where gathering profiler events (specifically `parse_cpu_trace` in `profiler.py`) would fail with the error:
`IndexError: pop from empty list`.

It turned out that this was because for one particular `Event`, there was a pop recorded but not a push. Instead of the `push` event being completely missing, it was overwritten by a completely different event.

After a bunch of debugging, and trying several hypotheses, it turns out that this was a race condition in `RangeEventList::record`. What happened was that different threads would call into `RangeEventList::record` on the same event list instance, and one record would stomp over the data written by the other one. Somehow the data written was a valid `Event` so the error did not manifest itself until the profiler realized a `pop` was missing a matching `push` in the python code.

I fixed this by adding a lock to serialize writes to `RangeEventList::record`.

To benchmark the performance we have added a simple python script that runs the profiler over a bunch of iterations of a tight loop that uses the profiler. Here are the numbers before/after the change with 1000 iterations:

before: `Iters: 1000 Profiler Latency: 0.38266039061546325 s`
after: `Iters: 1000 Profiler Latency: 0.3876141834259033 s`

Differential Revision: [D20071125](https://our.internmc.facebook.com/intern/diff/D20071125/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D20071125/)!",pytorch
33721,ssnl,pr,2020-02-24T20:39:12Z,Fix potential hang when exiting main process,"The following script reproduces the hang
```py
import multiprocessing, logging
logger = multiprocessing.log_to_stderr()
logger.setLevel(multiprocessing.SUBDEBUG)

import torch


class Dataset:
    def __len__(self):
        return 23425

    def __getitem__(self, idx):
        return torch.randn(3, 128, 128), idx % 100


ds = Dataset()
trdl = torch.utils.data.DataLoader(ds, batch_size=64, num_workers=300, pin_memory=True, shuffle=True)

for e in range(1000):
    for ii, (x, y) in enumerate(trdl):
        print(f'tr {e: 5d} {ii: 5d} avg y={y.mean(dtype=torch.double).item()}')
        if ii % 2 == 0:
            print(""=""*200 + ""BEFORE ERROR"" + ""=""*200)
            1/0
```

The process will hang at joining the putting thread of `data_queue` in **main process**. The root cause is that too many things are put in the queue from the **worker processes**, and the `put` at https://github.com/pytorch/pytorch/blob/062ac6b472af43c9cf83d285e661e24244551f85/torch/utils/data/dataloader.py#L928 is blocked at background thread. The `pin_memory_thread` exits from the set `pin_memory_thread_done_event`, without getting the `(None, None)`. Hence, the main process needs the same treatment as the workers did at
https://github.com/pytorch/pytorch/blob/062ac6b472af43c9cf83d285e661e24244551f85/torch/utils/data/_utils/worker.py#L198 .

After the patch, the script finishes correctly.",pytorch
33745,peterjc123,pr,2020-02-25T05:05:42Z,Use shim executable sccache-cl as the compiler instead of sccache cl,CMake only views the first item of `CC` and `CXX` as executable. So calling `sccache.exe` directly won't work. Using a shim executable resolves this problem.,pytorch
33755,peterjc123,pr,2020-02-25T11:40:46Z,Extend cuda install timeout for Windows jobs,,pytorch
33795,duncanriach,pr,2020-02-26T00:54:48Z,Enhance reproducibility documentation,Improves explanation of non-determinism when running on GPUs. Adds info about `torch.nn.BCELoss` operating non-deterministically on GPUs.,pytorch
33802,gaurav1086,pr,2020-02-26T05:12:56Z,[caffe2][core] Redundant file ptr repositioning,File already opened in append mode. No need for reposition fseek to the eof.,pytorch
33829,rohan-varma,pr,2020-02-26T19:59:30Z,[profiler] improve error handling in RecordFunction/RecordFunctionAsync for profiler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33829 [profiler] improve error handling in RecordFunction/RecordFunctionAsync for profiler**
* #33719 [profiler][rpc] fix a race condition in the profiler when multiple threads call
getEventList.record()
* #33605 [rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction
* #33030 [profiler] add RecordFunctionAsync struct
getEventList.record()
* #33605 [rpc][profiler] integrate RecordFunctionAsync with RPC, and remove unneeded functions from the general RecordFunction
* #33030 [profiler] add RecordFunctionAsync struct

The current RecordFunction implementation doesn't have great error
handling especially when considering developers have to call functions in the
right order to use it correctly (i.e. calling before(), then end(), etc).

This diff adds some error handling, namely, it ensures that the control flow
for RecordFunction/RecordFunctionAsync is as expected. Without these errors,
things would fail in strange ways and give harder to debug error messages, with
this error handling, incorrect usages would be a lot easier to debug.

Differential Revision: [D20123883](https://our.internmc.facebook.com/intern/diff/D20123883/)",pytorch
33840,rohan-varma,pr,2020-02-26T22:33:01Z,[rpc] add support for per-RPC timeouts,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33840 [rpc] add support for per-RPC timeouts**

This PR adds support for per-RPC timeouts. Previously, we just had a single, global timeout that applied to all RPCs but this limits the user's freedom to configure timeouts per their use case.

This is done by adding a `timeout` argument to public-facing RPC APIs `rpc_sync`, `rpc_async`, `_rpc_sync_torchscript`, `_rpc_async_torchscript`, and then that timeout is passed down and eventually reaches `RpcAgent::send()` where the timeout is actually applied.

The `timeout` argument is by default zero, which means that no timeout on the RPC will be enforced.

In this diff we have also removed the `rpc_timeout` from the constructor of the RPC agents and with that we are able to remove all the python scaffolding to pass this timeout to the RPC agent. This is done because it is not needed anymore, the timeouts are handled on a per-RPC basis.

Unit tests are added in python and on the (internal) C++ side.

Note: timeouts are currently not supported for rpc.remote()/rrefs, since they do not directly expose the future to the user, so there needs to be additional work done to track the corresponding futures, mark them as timed out, and notify the user through some hook. This is being tracked in https://github.com/pytorch/pytorch/issues/33803.

Differential Revision: [D20102374](https://our.internmc.facebook.com/intern/diff/D20102374/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D20102374/)!",pytorch
33852,supriyar,pr,2020-02-27T02:53:44Z,[quant] Run weight_post_process for QAT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34232 [quant] Fix histogram observer to work with QAT on GPU
* **#33852 [quant] Run weight_post_process for QAT**

Summary:
This fixes an issue for QAT models. During eval if we call `prepare_qat` and `convert` before calling `load_state_dict` it throws an error because the weight info (num channels) is not updated in the observer module.
It is not an issue for per-tensor case

Fixes issue #33830

Test Plan:
python test/test_quantization.py EagerModePostTrainingQuantTest.test_eval_after_train
python test/test_quantization.py EagerModeQuantizationAwareTrainingTest.test_eval_after_train

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20212996](https://our.internmc.facebook.com/intern/diff/D20212996)",pytorch
33856,peterjc123,pr,2020-02-27T03:59:07Z,Improve dll loading logic on Windows,"The way it works on the Anaconda distribution of Python 3.8 is a bit different. Loading DLLs explicitly  (e.g. `ctype.CDLL`) relies on paths appended by `os.add_dll_directory`. But if you try to load DLLs implicitly (e.g. `from torch._C import *`), it will rely on `PATH`.

Fixes https://github.com/pytorch/vision/issues/1916.",pytorch
33857,peterjc123,pr,2020-02-27T04:31:02Z,Add CUDA_ALWAYS_ASSERT for MSVC,Fixes https://github.com/pytorch/pytorch/pull/32719#discussion_r379918384.,pytorch
33907,ngimel,pr,2020-02-27T22:25:12Z,fix handling of replica parameters in DataParallel,"In DataParallel, replica parameters are not leaves (because they are computed via broadcast from master parameters), and should be treated as such. Fixes #33552",pytorch
33925,silvasean,pr,2020-02-28T01:54:54Z,Fix typo,,pytorch
33926,rohan-varma,pr,2020-02-28T02:07:31Z,"[gloo] dont hold locks in calls to buffer in ProcessGroupGloo:RecvWork::wait() and
SendWork::wait()","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33926 [gloo] dont hold locks in calls to buffer in ProcessGroupGloo:RecvWork::wait() and
SendWork::wait()**
SendWork::wait()**

Closes https://github.com/pytorch/pytorch/issues/30164
The UnboundBuffer calls here are already protected by a mutex. We only
need to hold the lock while writing the shared structures completed_ and
exception_.

Differential Revision: [D20154546](https://our.internmc.facebook.com/intern/diff/D20154546/)",pytorch
33945,supriyar,pr,2020-02-28T17:59:03Z,[quant][onnx] Add support to convert max_pool2d quantized pytorch op to C2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34629 [quant][onnx] Support conversion of quantized sigmoid operator from pytorch to caffe2
* **#33945 [quant][onnx] Add support to convert max_pool2d quantized pytorch op to C2**

Summary:
Add mapping for this operator in symbolics

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_max_pool2d

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20433681](https://our.internmc.facebook.com/intern/diff/D20433681)",pytorch
33977,orionr,pr,2020-02-28T22:54:00Z,[caffe2] Remove python2 from operator_test,"Summary: Removing python2 from operator_test so we can retire python2 support for PyTorch.

Test Plan: waitforsandcastle

Differential Revision: D20129500

",pytorch
33987,rohan-varma,pr,2020-02-28T23:40:47Z,[profiler] fix chrome tracing for profiler run with cuda,"Summary:
There was an error in
https://github.com/pytorch/pytorch/pull/30724/files that resulted in
`export_chrome_trace` generating invalid JSON. This only came up when the
profiler is run with `use_cuda=True` from what it looks like. In the future, we
should have tests that ensure we generate valid JSON because we no longer use
the json library.

See https://github.com/pytorch/pytorch/issues/33961 for details on the issue and repro.
Test Plan: Add UT to validate JSON.

Differential Revision: D20171428

",pytorch
34118,supriyar,pr,2020-03-03T05:32:46Z,[quant] Speed up per-channel min-max observer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34118 [quant] Speed up per-channel min-max observer**

Summary: Previously calc_per_channel_qparams was using for loops and python primitives, which called `item` many times causing slowdown during training.
    These changes uses torch primitives on the tensor to speed up the operation over 60x

    Perf results on MobileNetV2 during training using autograd profiler

    FP32 forward call -
    Self CPU time total: 47.222ms
    CUDA time total: 124.001ms

    before change
    FakeQuant Model -
    Self CPU time total: 19.107s
    CUDA time total: 27.177s

    after change
    FakeQuant Model -
    Self CPU time total: 404.667ms
    CUDA time total: 446.344ms

Test Plan:
    python test/test_quantization.py


Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20287841](https://our.internmc.facebook.com/intern/diff/D20287841)",pytorch
34123,peterjc123,pr,2020-03-03T08:57:01Z,[test] try debug build on Windows,,pytorch
34131,apaszke,pr,2020-03-03T14:15:02Z,Retain the order of parameters while generating ConcreteModuleTypes,"`ConcreteModuleTypeBuilder` used to keep parameters together with all others attributes in an `unordered_map` often leading to reordering them while building up the type. Parameter order is semantically meaningful, so we need to preserve it.",pytorch
34132,apaszke,pr,2020-03-03T15:34:17Z,Throw in aten::slice when it is used with a negative step,"Right now an expression like `lst[::-1]` throws this:
```
RuntimeError: vector::_M_range_check: __n (which is 18446744073709551615) >= this->size() (which is 5)
The above operation failed in interpreter.
Traceback (most recent call last):
  File ""abcd.py"", line 99, in reverse
def reverse(lst):
    # type: (List[Tensor]) -> List[Tensor]
    return lst[::-1]
           ~~~~~~~~ <--- HERE
```

Also, having a step equal to 0 would cause the code to enter an infinite loop.

Ideally we would implement support for this, but I'm not sure what the best strategy is. Turns out that semantics for slices are highly non-trivial (as seen in [the implementation of slice helpers](https://github.com/python/cpython/blob/bed4817d52d7b5a383b1b61269c1337b61acc493/Objects/sliceobject.c#L197-L282)). It seems like our only option are to either (1) use Python APIs to unpack slices which is bad because it introduces Python dependencies in libtorch (2) copy the source of those methods which I really don't like, but I'm not sure what other option there is.",pytorch
34133,apaszke,pr,2020-03-03T15:38:23Z,Throw a proper error when parsing local variable annotations without assignments,"Currently, putting `outputs: List[Tensor]` instead of `outputs: List[Tensor] = []` in your JITed code results in:
```
Traceback (most recent call last):
  File ""custom_lstms.py"", line 453, in <module>
    test_script_stacked_bidir_rnn(5, 2, 3, 7, 4)
  File ""custom_lstms.py"", line 404, in test_script_stacked_bidir_rnn
    rnn = script_lstm(input_size, hidden_size, num_layers, bidirectional=True)
  File ""custom_lstms.py"", line 62, in script_lstm
    other_layer_args=[LSTMCell, hidden_size * dirs, hidden_size]))
  File ""/home/apaszke/pytorch/torch/jit/__init__.py"", line 1267, in script
    return torch.jit._recursive.create_script_module(obj, torch.jit._recursive.infer_methods_to_compile)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 305, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 348, in create_script_module_impl
    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
  File ""/home/apaszke/pytorch/torch/jit/__init__.py"", line 1612, in _construct
    init_fn(script_module)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 340, in init_fn
    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 348, in create_script_module_impl
    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
  File ""/home/apaszke/pytorch/torch/jit/__init__.py"", line 1612, in _construct
    init_fn(script_module)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 340, in init_fn
    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 348, in create_script_module_impl
    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
  File ""/home/apaszke/pytorch/torch/jit/__init__.py"", line 1612, in _construct
    init_fn(script_module)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 340, in init_fn
    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 348, in create_script_module_impl
    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
  File ""/home/apaszke/pytorch/torch/jit/__init__.py"", line 1612, in _construct
    init_fn(script_module)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 340, in init_fn
    scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 317, in create_script_module_impl
    stubs = stubs_fn(nn_module)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 511, in infer_methods_to_compile
    stubs.append(make_stub_from_method(nn_module, method))
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 41, in make_stub_from_method
    return make_stub(func)
  File ""/home/apaszke/pytorch/torch/jit/_recursive.py"", line 34, in make_stub
    ast = torch.jit.get_jit_def(func, self_name=""RecursiveScriptModule"")
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 173, in get_jit_def
    return build_def(ctx, py_ast.body[0], type_line, self_name)
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 206, in build_def
    build_stmts(ctx, body))
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 129, in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 129, in <listcomp>
    stmts = [build_stmt(ctx, s) for s in stmts]
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 181, in __call__
    return method(ctx, node)
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 294, in build_AnnAssign
    rhs = build_expr(ctx, stmt.value)
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 180, in __call__
    raise UnsupportedNodeError(ctx, node)
  File ""/home/apaszke/pytorch/torch/jit/frontend.py"", line 116, in __init__
    source_range = ctx.make_range(offending_node.lineno,
AttributeError: 'NoneType' object has no attribute 'lineno'
```

This patch makes the error message more reasonable:
```
torch.jit.frontend.UnsupportedNodeError: annotated assignments without assigned value aren't supported:
  File ""custom_lstms.py"", line 221
        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]
        inputs = reverse(input.unbind(0))
        outputs: List[Tensor]
        ~ <--- HERE
        for i in range(len(inputs)):
            out, state = self.cell(inputs[i], state)
```
",pytorch
34150,rohan-varma,pr,2020-03-03T20:43:53Z,[distributed] quicker exit in the case of failed tests in distributed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34413 [rpc] improve general error handling in process group agent and fix flaky tests
* **#34150 [distributed] quicker exit in the case of failed tests in distributed**

In the distributed setting we commonly have tests in which there are errors where one process
exits but the other do not (since they are for example waiting for work from
the process that exited). Currently, when this situation happens we do not
handle this well, and wait for process 0 to timeout. This results in wasted
time waiting for test errors and a less helpful `""Process 0 timed out...""` error
message when the error was actually something else. See https://github.com/pytorch/pytorch/issues/34149 for more details.

This diff fixes the issue by checking for exited subprocesses and terminating
the test when we see a subprocess that has exited uncleanly. We still enforce
timeouts and return when all processes have exited cleantly in the happy path.

Tested by ensuring that if we have an uncaught error in one rank, we quickly terminate the test, previously, we would wait for timeout.

Differential Revision: [D20231032](https://our.internmc.facebook.com/intern/diff/D20231032/)",pytorch
34174,rohan-varma,pr,2020-03-03T23:30:44Z,"[test all] Back out ""Revert D20171428: [profiler] fix chrome tracing for profiler run with cuda""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34174 [test all] Back out ""Revert D20171428: [profiler] fix chrome tracing for profiler run with cuda""**

Reland of https://github.com/pytorch/pytorch/pull/33987, which was reverted due to failed python 3.5 test.

The py3.5 test failed in the call to `json.load()` complaining that str is expected instead of bytes (for some reason this didn't show up in the other python versions we test).

To fix, create the tempfile with mode ""w+"" to ensure that it does not open in binary mode. Adding [test all] should ensure that the 3.5 CI runs here and verifies that it is indeed fixed.

Original Summary:
Summary:
There was an error in
https://github.com/pytorch/pytorch/pull/30724/files that resulted in
export_chrome_trace generating invalid JSON. This only came up when the
profiler is run with use_cuda=True from what it looks like. In the future, we
should have tests that ensure we generate valid JSON because we no longer use
the json library.

Differential Revision: [D20237040](https://our.internmc.facebook.com/intern/diff/D20237040/)",pytorch
34178,rohan-varma,pr,2020-03-04T00:35:59Z,[TESTING] record fn changes,,pytorch
34205,peterjc123,pr,2020-03-04T09:21:17Z,Update MAGMA to 2.5.2 for Windows,,pytorch
34232,supriyar,pr,2020-03-04T18:29:41Z,[quant] Fix histogram observer to work with QAT on GPU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34232 [quant] Fix histogram observer to work with QAT on GPU**
* #33852 [quant] Run weight_post_process for QAT

Summary:
By default `torch.zeros` creates the tensor on GPU. Need to specify the device argument to get it to work correctly on GPU during QAT.

Test Plan:
1. Tested by running QAT on GPU

2. python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20286351](https://our.internmc.facebook.com/intern/diff/D20286351)",pytorch
34239,ssnl,pr,2020-03-04T18:50:26Z,Fix cpp_ext build dir create permission,Fixes https://github.com/pytorch/pytorch/issues/34238,pytorch
34288,peterjc123,pr,2020-03-05T04:45:18Z,Ensure torch_cuda is linked against on Windows,Fixes https://github.com/pytorch/pytorch/issues/31611.,pytorch
34356,vishwakftw,pr,2020-03-06T04:55:35Z,Remove **kwargs from torch.meshgrid,"Changelog:
- Remove **kwargs from torch.meshgrid as they serve no purpose

Closes #34206",pytorch
34357,vishwakftw,pr,2020-03-06T05:02:46Z,Remove hotpatches that circumvent MAGMA bug,"Changelog:
- The magma implementation of small singular square batch matrices had a bug that resulted in nan values in the LU factorization result. This has been fixed in MAGMA 2.5.2. This PR removes the existing patch that was a temporary workaround for this bug.

Test Plan:
- Existing tests for det and lu should pass",pytorch
34413,rohan-varma,pr,2020-03-07T04:33:12Z,[rpc] improve general error handling in process group agent and fix flaky tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34413 [rpc] improve general error handling in process group agent and fix flaky tests**

In this diff we have made various improvements to ProcessGroupAgent in order to accomodate edge and error cases such as a ""non-clean"" shutdown (shutdowns in which we abort RPC as quickly as possible, and don't wait for all pending work across all RPC agents to be completed):

1. Catch and log exceptions in `enqueueRecv`. This prevents us from calling `std::terminate()` in a different thread and logs an error message indicating the issue. With this we no longer have crashes caused by exceptions in this thread during non-graceful shutdown.

2. Provide cleaner error messages everywhere (and use `c10::str` where possible). One example is in `agent::send()`.

3. Add the ability to abort pending sends that cause blocking waits in `handleSend`. The reason we need to abort this is since during a non-graceful shutdown, we could become blocked waiting for these since there is no guarantee the remote end is still active and this would result in a long wait and eventual timeout. We abort these by adding them to a map, and go through this map during `shutdown()`.

4. Fix flaky tests: `test_handle_send_exceptions` and `test_backward_node_failure` and `test_backward_node_failure_python_udf`. These tests were flaky since they dealt with non-graceful shutdown of workers which has chances for a bunch of edge cases explained above.

We have also refactored `createExceptionResponse`, `enqueueRecv`, and some test functions for the above reasons in this diff.

For testing:
Ensured that the tests are no longer flaky with 500 tests runs. Also added a unit test in the internal `ProcessGroupAgentTest.cpp`.

Differential Revision: [D20269074](https://our.internmc.facebook.com/intern/diff/D20269074/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D20269074/)!",pytorch
34448,peterjc123,pr,2020-03-08T04:19:20Z,Fix the missing ';' in Conv.cpp,"Fixes https://github.com/pytorch/pytorch/issues/34415.
BTW, isn't this tested on CI? Maybe we need to introduce some tests with legacy versions of cuDNN.",pytorch
34499,rohan-varma,pr,2020-03-09T19:10:51Z,[profiler] use swap in RangeEventList::consolidate to reduce time the lock is held.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35055 [profiler] Allow record_function ctx manager to profile futures
* **#34499 [profiler] use swap in RangeEventList::consolidate to reduce time the lock is held.**
getEventList.record()

`RangeEventList::consolidate` currently iterates through `blocks`, which
we lock access to and accumulates them into `result`. Instead of doing
this, we can swap with an empty `forward_list` in constant time, and then
unlock, and use this local list in order to populate `result`. This was suggested by @mrshenli 

Differential Revision: [D20346423](https://our.internmc.facebook.com/intern/diff/D20346423/)",pytorch
34511,rohan-varma,pr,2020-03-09T22:55:17Z,"[rpc][profiler] add a test case to verify record_function context manager works
with RPC-based profiling","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34511 [rpc][profiler] add a test case to verify record_function context manager works
with RPC-based profiling**

With https://github.com/pytorch/pytorch/pull/34122/files, issues
with using record_function context manager and profiling RPCs were fixed. This
adds a test case to verify that we can use RPC with the `record_function`
decorator.

Differential Revision: [D20352242](https://our.internmc.facebook.com/intern/diff/D20352242/)",pytorch
34629,supriyar,pr,2020-03-11T23:33:27Z,[quant][onnx] Support conversion of quantized sigmoid operator from pytorch to caffe2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34629 [quant][onnx] Support conversion of quantized sigmoid operator from pytorch to caffe2**
* #33945 [quant][onnx] Add support to convert max_pool2d quantized pytorch op to C2

Summary:
Add support for sigmoid in the conversion flow through onnx

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_quantized_sigmoid
python test/onnx/test_pytorch_onnx_caffe2_quantized.py TestQuantizedOps.test_small_model

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20433680](https://our.internmc.facebook.com/intern/diff/D20433680)",pytorch
34639,peterjc123,pr,2020-03-12T04:40:56Z,Attempt to resolve inconsistent dll linkage warnings on MSVC,"Continue the work in https://github.com/pytorch/pytorch/pull/19242.
Remove the template declarations that implies different dll linkage.",pytorch
34650,rohan-varma,pr,2020-03-12T16:36:35Z, per-RPC timeouts for rpc_sync and rpc_async,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34650 per-RPC timeouts for rpc_sync and rpc_async**
timedelta
* **#34650 per-RPC timeouts for rpc_sync and rpc_async**

This PR adds an optional argument into `rpc_sync` and `rpc_async` as well as `RpcAgent::send()` that allows the user to specify a timeout for an RPC to override the default set timeout. If the user does not specify this argument, then the currently set default RPC timeout given in the RPC constructor or by `rpc.set_rpc_timeout()` is used. Otherwise, we use the passed in timeout.

This diff does not address:
1) timeout support when called rpc.rpc_async is called as a JIT operator. For this to work, we would need to change the logic in `register_distributed_ops` to pass in this timeout to `rpcTorchscript`. 
2) Per-RPC timeouts for internal messages or `rpc.remote()`. A follow-up diff will address the latter with the approach of raising the timeout error at the earliest next possible time to the user, such as when the next time the RRef is forked or `to_here` is called

Added unit tests to confirm the current behavior

Differential Revision: [D20376953](https://our.internmc.facebook.com/intern/diff/D20376953/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D20376953/)!",pytorch
34657,rohan-varma,pr,2020-03-12T17:38:37Z,[docs][1.5] Update distributed autograd note,"- Update API calls `backward` and `optim.step` now that we require `context_id`
- Add notes to clarify purpose of distributed autograd context (this was a source of confusion in some feedback)
- Add note that details why optimizer requires context_id
- Clearly specify that we don't have SMART mode yet

",pytorch
34670,rohan-varma,pr,2020-03-12T19:31:08Z,[docs][1.5] update RPC docs to reflect correct use of dist_autograd backwards and dist_optim step(),"- Clarify that `torch.distributed.autograd.backwards()` does not use the current thread local autograd context, instead it looks it up based on the context_id passed in
- Clarify the same for `torch.distributeed.optimizer.optim.step()`",pytorch
34777,peterjc123,pr,2020-03-15T04:22:56Z,Use randomtemp to resolve intermittent cuda build errors ,"Fixes #25393.
Core logic of randomtemp: https://github.com/peterjc123/randomtemp/blob/master/randomtemp/randomtemp.cpp",pytorch
34802,peterjc123,pr,2020-03-16T14:09:33Z,[TEST ONLY] [DON'T REVIEW] Use docker image for Windows CI,,pytorch
34828,rohan-varma,pr,2020-03-16T19:38:08Z,[rpc] fix test_debug_info for python 3.5,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34828 [rpc] fix test_debug_info for python 3.5**

python 3.5 does not ensure ordering of dictionary keys, this was added
in python 3.6+. Fixing this so the test is no longer flaky in 3.5. Tested by
500 stresstests with python 3.5

Differential Revision: [D20474996](https://our.internmc.facebook.com/intern/diff/D20474996/)",pytorch
34844,supriyar,pr,2020-03-16T22:13:42Z,[quant][mobile] Not use qnnpack max_pool2d if ceil_mode is true,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34844 [quant][mobile] Not use qnnpack max_pool2d if ceil_mode is true**

Summary:
QNNPACK max_pool2d operator does not support ceil_mode so this can cause crashes in the kernel when it is set to true.
We default to the server implementation when ceil_mode is set to true

Test Plan:
python test/test_quantized.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20478701](https://our.internmc.facebook.com/intern/diff/D20478701)",pytorch
34858,peterjc123,pr,2020-03-17T03:23:16Z,Print cuda install logs for Windows CI,Related to #34821.,pytorch
34900,neerajprad,pr,2020-03-17T18:54:32Z,Reduce memory overhead of categorical.sample,"Fixes #34714 (using the discussed solution). Thanks to @jjabo for flagging and suggesting this.

Instead of expanding `probs` to prepend `sample_shape`, it is better  to use the `num_samples` argument to `torch.multinomial` instead, which is faster and consumes lesser memory. 

Existing tests should cover this. I have profiled this on different inputs and the change results in faster `.sample` (e.g. 100X faster on the example in the issue), or at worst is similar to what we have now with the default `sample_shape` argument.

cc. @fritzo, @alicanb, @ezyang ",pytorch
34912,supriyar,pr,2020-03-17T20:44:14Z,[quant][onnx] Add aten::max_pool2d to jit pass,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34912 [quant][onnx] Add aten::max_pool2d to jit pass**

Summary: max_pool2d quantized op actually shows up as aten::max_pool2d

Test Plan:
python test/test_pytorch_onnx_caffe2_quantized.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20497780](https://our.internmc.facebook.com/intern/diff/D20497780)",pytorch
34940,peterjc123,pr,2020-03-18T03:11:52Z,Install CUDA manually on Windows CI to avoid flakiness,Fixes https://github.com/pytorch/pytorch/issues/34821.,pytorch
34943,rohan-varma,pr,2020-03-18T04:48:29Z,[rpc] various fixes for ProcessGroupAgent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34943 [rpc] various fixes for ProcessGroupAgent**

Various small changes to address @jjlilley and @mrshenli's comments on
https://github.com/pytorch/pytorch/pull/34413:
1) Continue trying even if one `agent->send()` fails when cleaning up dist
autograd ctx
2) Use RAII for lock in process group agent `handleSend`
3) Return bool instead of int in `ProcessGroupAgent::handleRecv` to determine
if the count should be incremented
4) Move recvCounts increment in timed out future processing to be within the
block that ensures the future already doesn't have an error.

Differential Revision: [D20506065](https://our.internmc.facebook.com/intern/diff/D20506065/)",pytorch
34945,peterjc123,pr,2020-03-18T06:57:05Z,[TEST ONLY] Csarofeen/cuda fuser,,pytorch
35017,vfdev-5,pr,2020-03-19T02:08:45Z,Docs fix: Added missing indent,,pytorch
35028,rohan-varma,pr,2020-03-19T07:07:40Z,[profiler] remove unused _push_range and _pop_range,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35028 [profiler] remove unused _push_range and _pop_range**

removes these methods that are not used anywhere in the codebase. With this we can also remove public declaration of TORCH_API popRange and TORCH_API pushRange since those were the only use cases.

Differential Revision: [D20531148](https://our.internmc.facebook.com/intern/diff/D20531148/)",pytorch
35042,orionr,pr,2020-03-19T16:56:57Z,[pytorch] Remove python2 support from tests and torch.jit,"Summary: Removing python2 tests and some compat code in torch.jit. Check if dependent projects and external tests have any issues after these changes.

Test Plan: waitforsandcastle

Differential Revision: D18942633

",pytorch
35055,rohan-varma,pr,2020-03-19T19:40:08Z,[profiler] Extend record_function ctx manager to profile futures,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35055 [profiler] Extend record_function ctx manager to profile futures**

This is the first step to improving the way RPCs are profiled as suggested by Ilia. For now, since RPC can return two different types of futures, we have to implement two different code paths, one for the python eager mode future and one for the jit future (to be done in the next diff).

This diff implements the python eager part. We have defined a method `_call_end_callbacks_on_future` that takes in a future and schedules a `RecordFunction` to be completed as a callback on the future. We also implement the usage of this new API in `rpc/api.py` and add tests. The old tests for RPC profiling should still apply, and they pass as expected. It is implemented for rpc_sync, rpc_async, as well as remote.

The way we make this work for `remote` (since rpc.remote directly does not return the future) is that we save the future corresponding to the RPC that creates the rref on the remote node. it is saved in `rref_impl.h`. This future can then be accessed through `py_rref`, and eventually from python, and we can use that to attach the profiling callbacks.

We have also included support for Script functions called in eager-mode RPC. This means that if users input script functions into python eager mode RPCs, they will be recorded as expected. We have also implemented a JIT operator that is responsible for scheduling the end callbacks in this case (see record_function_ops.cpp). Note that profiling when RPC is invoked in a JIT function is not supported, this will be done in later diffs.

As part of this, we have removed the code for the way RPCs are currently profiled (by passing `RecordFunction` shared_ptrs to C++), and delete the incorrect use of `RecordFunction`.

Differential Revision: [D20452003](https://our.internmc.facebook.com/intern/diff/D20452003/)",pytorch
35084,peterjc123,pr,2020-03-20T03:46:28Z,test clang-cl build,,pytorch
35109,rohan-varma,pr,2020-03-20T18:51:55Z,Refactored rpc docs,Reorganize as per @jlin27 's comments. Screenshots added in comments.,pytorch
35121,supriyar,pr,2020-03-20T21:13:08Z,[quant][graphmode] Add observers for dynamic quant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35265 [quant][graph] Add a new observer type for dynamic quantization
* #35235 [quant][graph] Add _choose_qparams function for graph mode
* **#35121 [quant][graphmode] Add observers for dynamic quant**

Summary:
For dynamic quantization we insert observers at the input to mimic the quatization of activations that happens in the operator
Observer for weight is inserted similar to static quant

Test Plan:
python test/test_quantize_script.py

Sample output for single layer FC

.graph(%self : __torch__.___torch_mangle_4.M,
      %x.2 : Tensor):
  %_observer_1 : __torch__.torch.quantization.observer.MinMaxObserver = prim::GetAttr[name=""_observer_1""](%self)
  %x.1 : Tensor = prim::CallMethod[name=""forward""](%_observer_1, %x.2)
  %2 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=""fc""](%self)
  %3 : Tensor = prim::CallMethod[name=""forward""](%2, %x.1) # test/test_quantize_script.py:19:23
  return (%3)

graph(%self : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear,
      %input.1 : Tensor):
 %2 : Function = prim::Constant[name=""linear""]()
 %3 : Tensor = prim::GetAttr[name=""weight""](%self)
 %_observer_0 : __torch__.torch.quantization.observer.MinMaxObserver = prim::GetAttr[name=""_observer_0""](%self)
 %7 : Tensor = prim::CallMethod[name=""forward""](%_observer_0, %3)
 %4 : Tensor = prim::GetAttr[name=""bias""](%self)
 %5 : Tensor = prim::CallFunction(%2, %input.1, %7, %4) # /home/supriyar/miniconda3/envs/pytorch_py3/lib/python3.7/site-packages/torch/nn/modules/linear.py:87:15
 return (%5)

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20599144](https://our.internmc.facebook.com/intern/diff/D20599144)",pytorch
35136,vishwakftw,pr,2020-03-20T23:34:23Z,Temporary fix for determinant bug on CPU,"Changelog:
- Make diagonal contiguous

Temporarily Fixes #34061 ",pytorch
35145,peterjc123,pr,2020-03-21T02:54:03Z,[TEST] Clang-cl build without annotations,,pytorch
35177,peterjc123,pr,2020-03-22T07:18:56Z,Load torch_global_deps for Windows,Fixes https://discuss.pytorch.org/t/torch-cat-runtimeerror-error-in-loadlibrarya/71188/8.,pytorch
35235,supriyar,pr,2020-03-23T20:54:06Z,[quant][graph] Add _choose_qparams function for graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35366 [quant][graph] Add pass to insert quant dequant for dynamic quantization
* #35325 [quant][graph] Update dynamic quant tests to use new qconfig
* #35265 [quant][graph] Add a new observer type for dynamic quantization
* **#35235 [quant][graph] Add _choose_qparams function for graph mode**

Summary: For dynamic quantization in graph mode, we need an operator that returns the qparams of the tensor
similar to the linear_dynamic quantized op

Test Plan:
python test/test_quantized_tensor.py TestQuantizedTensor.test_choose_qparams

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20608793](https://our.internmc.facebook.com/intern/diff/D20608793)",pytorch
35261,rohan-varma,pr,2020-03-24T01:03:49Z,"[dist autograd] profile the amount of time spent executing
dist_autograd.backward()","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35261 [dist autograd] profile the amount of time spent executing
dist_autograd.backward()**

Uses the `RECORD_FUNCTION` macro to profile the amount of time in
`dist_autograd.backward` and ensure that it shows up in the profiler output. Since
`dist_autograd.backward()` is blocking, we can avoid stuffing the RecordFunction
into a callback. This does not support profiling the RPCs that are created when
gradients are forwarded over to other nodes; this can be added in a follow up
diff.

Example profiler output:
```
> Name                                               Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
> -------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
> torch::distributed::autograd::backward             96.36%           30.086ms         96.58%           30.158ms         30.158ms         1
> ones_like                                          0.10%            30.203us         0.23%            71.264us         71.264us         1
> empty_like                                         0.06%            19.786us         0.10%            32.257us         32.257us         1
> empty                                              0.35%            108.691us        0.35%            108.691us        21.738us         5
> fill_                                              0.43%            133.097us        0.43%            133.097us        7.829us          17
> torch::autograd::GraphRoot                         0.06%            20.135us         0.06%            20.135us         20.135us         1
> SumBackward0                                       0.04%            12.744us         0.25%            79.130us         79.130us         1
> expand                                             0.21%            66.386us         0.21%            66.386us         66.386us         1
> torch::distributed::autograd::RecvRpcBackward      0.49%            154.228us        0.49%            154.228us        77.114us         2
> select                                             1.10%            342.145us        1.10%            342.145us        21.384us         16
> to                                                 0.07%            22.096us         0.07%            22.096us         5.524us          4
> set_                                               0.09%            29.306us         0.09%            29.306us         14.653us         2
> torch::distributed::autograd::SendRpcBackward      0.02%            6.413us          0.02%            6.413us          3.207us          2
> AddBackward0                                       0.01%            2.628us          0.01%            2.628us          2.628us          1
> clone                                              0.61%            189.775us        0.61%            189.775us        94.888us         2
> -------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
> Self CPU time total: 31.224ms
```

One issue is that the profile may show output such as the execution of a `SendRpcBackward` that is not part of this request, but instead in response to another node. Created https://github.com/pytorch/pytorch/issues/35266 to track that issue.
Differential Revision: [D20611653](https://our.internmc.facebook.com/intern/diff/D20611653/)",pytorch
35265,supriyar,pr,2020-03-24T01:15:32Z,[quant][graph] Add a new observer type for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35366 [quant][graph] Add pass to insert quant dequant for dynamic quantization
* #35325 [quant][graph] Update dynamic quant tests to use new qconfig
* **#35265 [quant][graph] Add a new observer type for dynamic quantization**
* #35235 [quant][graph] Add _choose_qparams function for graph mode

Summary:
In graph mode we need to observer the activation tensor for dynamic quantization. This observer should behave the same way as the quantization functions called in the dynamic operator.
Currently for qlinear_dynamic we call quant_utils::ChooseQuantizationParams which has its own logic for calculating scale and zero_point.
We mimic those calculations in the new observer.

Test Plan:
python test/test_quantization.py ObserverTest

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20630988](https://our.internmc.facebook.com/intern/diff/D20630988)",pytorch
35275,peterjc123,pr,2020-03-24T03:51:34Z,Load torch_global_deps for Windows (v1.5.0),"(cherry picked from commit 1d9b7add90df49b94c7d6dd2e185fd19fd9d7c75)

",pytorch
35311,rohan-varma,pr,2020-03-24T18:29:54Z,[rpc][easy] remove code duplication on ProcessGroupAgent::enqueueSend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35311 [rpc][easy] remove code duplication on ProcessGroupAgent::enqueueSend**

This must have snuck in since a couple PRs updated this same area and
the merge conflict was not resolved properly.

Also contains a slight improvement to the message to include the node id of the failure

Differential Revision: [D20602683](https://our.internmc.facebook.com/intern/diff/D20602683/)",pytorch
35325,supriyar,pr,2020-03-24T21:46:41Z,[quant][graph] Update dynamic quant tests to use new qconfig,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35366 [quant][graph] Add pass to insert quant dequant for dynamic quantization
* **#35325 [quant][graph] Update dynamic quant tests to use new qconfig**
* #35265 [quant][graph] Add a new observer type for dynamic quantization
* #35235 [quant][graph] Add _choose_qparams function for graph mode

Summary:
default_dynamic_qconfig now holds activation observer

Test Plan:
python test/test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20640487](https://our.internmc.facebook.com/intern/diff/D20640487)",pytorch
35344,ngimel,pr,2020-03-25T00:56:30Z,fix complex conversions on cuda,Fixes #35225. ,pytorch
35355,peterjc123,pr,2020-03-25T03:53:03Z,"Revert ""Load torch_global_deps for Windows (#35177)""","This reverts commit d7a7bcb0428273fa54a836b52e750608ebe7e4de.

The previous commit is not useful because torch_global_deps doesn't include any external dependencies.",pytorch
35362,peterjc123,pr,2020-03-25T04:41:02Z,Load all DLLs in the lib directory for Windows,Fixes #35358.,pytorch
35364,peterjc123,pr,2020-03-25T05:13:59Z,Fix some incorrect annotations found by clang-cl,"Fixes incorrect usages of symbol annotations including:
1. Exporting or importing a function/class in an anonymous namespace.
2. Exporting or importing a function/class implementation in a header file. However, by removing the symbol annotations, they are now local symbols. If they need to be remain global, I can move the implementations to the source file.",pytorch
35365,peterjc123,pr,2020-03-25T05:35:43Z,Fix openmp detection with clang-cl,,pytorch
35366,supriyar,pr,2020-03-25T05:46:22Z,[quant][graph] Add pass to insert quant dequant for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35366 [quant][graph] Add pass to insert quant dequant for dynamic quantization**
* #35325 [quant][graph] Update dynamic quant tests to use new qconfig
* #35265 [quant][graph] Add a new observer type for dynamic quantization
* #35235 [quant][graph] Add _choose_qparams function for graph mode

Summary:
Add _choose_qparams_per_tensor which returns scale and zero_point similar to the dynamic quantization in the operator

Test Plan:
python test/test_quantize_script.py
Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
35368,peterjc123,pr,2020-03-25T06:20:16Z,Load all DLLs in the lib directory for Windows (v1.5.0),https://github.com/pytorch/pytorch/pull/35362 for release v1.5.,pytorch
35375,peterjc123,pr,2020-03-25T11:40:53Z,Update randomtemp on Windows,"Introduce max retry times to the flaky CUDA build command.
Changes: https://github.com/peterjc123/randomtemp/compare/v0.2...v0.3
Targets https://github.com/pytorch/pytorch/issues/25393#issuecomment-603776413.",pytorch
35393,rohan-varma,pr,2020-03-25T18:11:35Z,[rpc] create error string in listenLoop outside of lock,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35393 [rpc] create error string in listenLoop outside of lock**
ungraceful shutdown
* **#35393 [rpc] create error string in listenLoop outside of lock**

This error string was being created inside the lock scope, but we don't need to
hold the lock for this.

Differential Revision: [D20632225](https://our.internmc.facebook.com/intern/diff/D20632225/)",pytorch
35394,rohan-varma,pr,2020-03-25T18:11:43Z,"[rpc] call threadPool.waitWorkComplete after listenerThread.join() to fix
ungraceful shutdown","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36084 [rpc] allow ability to abort second call to RecvWork::wait() in ProcessGroupAgent::listenLoop
* **#35394 [rpc] call threadPool.waitWorkComplete after listenerThread.join() to fix
ungraceful shutdown**
ungraceful shutdown**
ungraceful shutdown**
ungraceful shutdown**
ungraceful shutdown**
* #35393 [rpc] create error string in listenLoop outside of lock

This is one of the causes of flakiness seen in `dist_autograd_node_failure` (the other is a `std::terminate` in RPC retries which is being fixed by @osalpekar).

The root issue is that since we call `threadPool.waitWorkComplete()` after `listenerThread.join()`, it is possible that in some ungraceful shutdown situations, `listenerThread` enqueues more `RecvWork` into the thread pool. Since `listenerThread` is only responsible for enqueueing the `RecvWork` and not waiting on it, it can exit, and shutdown will continue. As part of shutdown we then call  `_cleanup_python_rpc_handler` which sets `pyRunFunction_` to `None`. Although after this, we could still be processing work in the RPC threadpools. This is why we would see errors such as `NoneType not callable` in `request_callback.cpp`. 

The fix here is to wait for all locally enqueued work to be completed before shutting down the python part.

Test plan: run `dist_autograd_node_failure` tests. Although, completely resolving the flakiness is also dependent on fixing the `std::terminate()` issue mentioned above.

Differential Revision: [D20632405](https://our.internmc.facebook.com/intern/diff/D20632405/)",pytorch
35395,rohan-varma,pr,2020-03-25T18:11:53Z,[rpc] Add a debug only check to debug python cleanup races,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35395 [rpc] Add a debug only check to debug python cleanup races**
ungraceful shutdown
* #35393 [rpc] create error string in listenLoop outside of lock

This adds a debug-only check when we are running Python UDFs over RPC. It ensures that `pyRunFunction_` is not None, if it is, this is indicative of a bug/race in our shutdown code, for example, this was the primary issue that the previous diff in this stack resolves. It's useful to have this check to prevent against regressions and allow easier debugging. Added it only in debug builds so it does not affect production builds performance.

Differential Revision: [D20632634](https://our.internmc.facebook.com/intern/diff/D20632634/)",pytorch
35448,supriyar,pr,2020-03-26T02:22:50Z,[quant][graph] Add pass to insert quant dequant for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35586 [quant][graph] Add quant fusion for dynamic quantization
* **#35448 [quant][graph] Add pass to insert quant dequant for dynamic quantization**

Summary:
Add _choose_qparams_per_tensor which returns scale and zero_point similar to the dynamic quantization in the operator

Test Plan:
python test/test_quantize_script.py
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20755679](https://our.internmc.facebook.com/intern/diff/D20755679)",pytorch
35451,supriyar,pr,2020-03-26T03:52:37Z,[quant][graph] Update dynamic quant tests to use new qconfig,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35451 [quant][graph] Update dynamic quant tests to use new qconfig**
* #35455 [quant][graph] Add a new observer type for dynamic quantization

Summary:
default_dynamic_qconfig now holds activation observer

Test Plan:
python test/test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20664585](https://our.internmc.facebook.com/intern/diff/D20664585)",pytorch
35453,supriyar,pr,2020-03-26T04:09:56Z,[quant][graph] Add a new observer type for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35451 [quant][graph] Update dynamic quant tests to use new qconfig
* **#35453 [quant][graph] Add a new observer type for dynamic quantization**

Summary:
In graph mode we need to observer the activation tensor for dynamic quantization. This observer should behave the same way as the quantization functions called in the dynamic operator.
Currently for qlinear_dynamic we call quant_utils::ChooseQuantizationParams which has its own logic for calculating scale and zero_point.
We mimic those calculations in the new observer.

Test Plan:
python test/test_quantization.py ObserverTest

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
35455,supriyar,pr,2020-03-26T04:44:17Z,[quant][graph] Add a new observer type for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35451 [quant][graph] Update dynamic quant tests to use new qconfig
* **#35455 [quant][graph] Add a new observer type for dynamic quantization**

Summary:
In graph mode we need to observer the activation tensor for dynamic quantization. This observer should behave the same way as the quantization functions called in the dynamic operator.
Currently for qlinear_dynamic we call quant_utils::ChooseQuantizationParams which has its own logic for calculating scale and zero_point.
We mimic those calculations in the new observer.

Test Plan:
python test/test_quantization.py ObserverTest

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20664586](https://our.internmc.facebook.com/intern/diff/D20664586)",pytorch
35521,peterjc123,pr,2020-03-27T04:28:26Z,Formatting cmake (to lowercase without space for if/elseif/else/endif),"Running commands:
```bash
shopt -s globstar

sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i caffe2/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i torch/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i c10/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i cmake/**/*.cmake
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i cmake/**/*.cmake.in
```
We may further convert all the commands into lowercase according to the following issue: https://gitlab.kitware.com/cmake/cmake/commit/77543bde41b0e52c3959016698b529835945d62d.",pytorch
35525,peterjc123,pr,2020-03-27T08:22:36Z,Add cmakelint to CI,,pytorch
35539,orionr,pr,2020-03-27T15:39:33Z,Remove python2 support from setup.py,As a followup to https://github.com/pytorch/pytorch/pull/35042 this removes python2 from setup.py and adds Python 3.8 to the list of supported versions. We're already testing this in CircleCI.,pytorch
35568,rohan-varma,pr,2020-03-27T20:44:50Z,[JIT] Improve the error message when registering a custom class twice,"I hit this exception when including the registration code with `torch::class_` in a header file, which was included in multiple cpp files and thus called this twice. It could be helpful to improve the error msg here to indicate what exactly happened.",pytorch
35586,supriyar,pr,2020-03-28T00:39:37Z,[quant][graph] Add quant fusion for dynamic quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35586 [quant][graph] Add quant fusion for dynamic quantization**
* #35448 [quant][graph] Add pass to insert quant dequant for dynamic quantization

Summary:
This pass fuses the choose_qparams-quant-dequant sequence
Fusion for weight tensor is the same as static quant.

Test Plan:
python test/test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20755680](https://our.internmc.facebook.com/intern/diff/D20755680)",pytorch
35588,peterjc123,pr,2020-03-28T02:57:34Z,[WIP] [reland][pytorch][PR] Fix some incorrect annotationâ€¦,"â€¦s found by clang-cl""

This reverts commit a9b540d109aa72e6ba8748019ef1c3ba0d8fac2b.

",pytorch
35589,peterjc123,pr,2020-03-28T03:08:30Z,CMake script cleanup - mixed case for function names,"Running the following code.
```bash
cmake --help-command-list |
grep -v ""cmake version"" |
while read c; do
    echo 's/\b'""$(echo $c | tr '[:lower:]' '[:upper:]')""'\(\s*\)(/'""$c""'\1(/g'
done >convert.sed &&
git ls-files -z -- bootstrap '*.cmake' '*.cmake.in' '*CMakeLists.txt' |
egrep -z -v '^(cmake/Modules/|cmake/Modules_CUDA_fix/)' |
xargs -0 sed -i -f convert.sed &&
rm convert.sed
```
cmake-lint is too sensitive about mixed case so I didn't switch the check on.",pytorch
35591,ngimel,pr,2020-03-28T04:04:24Z,don't replace TensorImpl for inplace min/max dim,"Test Plan: buck test mode/dev //caffe2/test:cuda -- 'test_dim_reduction_cpu \(test_torch\.TestTorchDeviceTypeCPU\)'

Differential Revision: D20718321

",pytorch
35596,rohan-varma,pr,2020-03-28T05:08:01Z,[WIP] Register RecordFunction as a custom class.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35596 [WIP] Register RecordFunction as a custom class.**
* #35055 [profiler] Extend record_function ctx manager to profile futures

Registers `RecordFunction` as a torchscript custom class. Using this, we can get rid of the `at::cpp_custom_type_hack` used by `RecordFunction`. Currently, the implementation is not ideal in that it returns a `Tensor` back to the caller and then converts this `Tensor` back .

This will also be needed in order to create an operator that attaches a RecordFunction to the future returned by `invoke_rpc_torchscript`.

Resolves https://github.com/pytorch/pytorch/issues/35026

Differential Revision: [D20709120](https://our.internmc.facebook.com/intern/diff/D20709120/)",pytorch
35653,peterjc123,pr,2020-03-30T03:54:12Z,Fix AVX detection with clang-cl,Defining macros `/D__F16C__` or sth similar won't work on clang-cl.,pytorch
35658,ngimel,pr,2020-03-30T05:38:43Z,Moves torch cpu math tests to device-generic framework,"Per title. Also, replaces reference computation with `math.xx` functions and torch.apply_  with numpy/scipy as appropriate. ",pytorch
35677,orionr,pr,2020-03-30T15:38:22Z,"Remove python2 and 3.5 from requirements.txt, README and docs",Some more cleanup now that we no longer support python2 or 3.5 on master and eventually PyTorch 1.6 release.,pytorch
35700,orionr,pr,2020-03-30T20:09:49Z,Move libtorch to py3 and cleanup other CircleCI config,"This moves libtorch to Python 3.6 and cleans up other CircleCI config for the removal of python2.

Going to see if all tests pass on this and will also land before https://github.com/pytorch/pytorch/pull/35677",pytorch
35808,rohan-varma,pr,2020-04-01T15:52:05Z,[1.5 release] Refactored rpc docs,"This is a PR to merge #35109 into the 1.5 release. It refactors the RPC documentation as per requests from doc engineering. #35109 is landed into master.

Original Summary:
Reorganize as per jlin27 's comments. Screenshots added in comments.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35109

Differential Revision: D20788774

Pulled By: rohan-varma

fbshipit-source-id: 7d64be70ef76ed6ff303d05d39c338293c234766

",pytorch
35809,rohan-varma,pr,2020-04-01T16:21:52Z,[Docs] Fix typo in RPC docs,It's also fixed in the cherry pick PR https://github.com/pytorch/pytorch/pull/35808,pytorch
35846,supriyar,pr,2020-04-01T23:04:05Z,[WIP] Add graph mode support for quantized lstm op,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35846 [WIP] Add graph mode support for quantized lstm op**
* #35586 [quant][graph] Add quant fusion for dynamic quantization
* #35448 [quant][graph] Add pass to insert quant dequant for dynamic quantization

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
35893,supriyar,pr,2020-04-02T19:13:15Z,[quant][graphmode] Add new tensorlist observer for LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35916 [quant][graphmode] Add quantize_per_tensor.tensors
* #35894 [quant][graphmode] Insert Observers for dynamic LSTM
* **#35893 [quant][graphmode] Add new tensorlist observer for LSTM**

Summary:
LSTM operator inputs have tensor list for activations and weights.
In graph mode we need a new observer to work with tensor list

Test Plan:
python test/quantization/test_quantization.py ObserverTest
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20830389](https://our.internmc.facebook.com/intern/diff/D20830389)",pytorch
35894,supriyar,pr,2020-04-02T19:13:22Z,[quant][graphmode] Insert Observers for dynamic LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35916 [quant][graphmode] Add quantize_per_tensor.tensors
* **#35894 [quant][graphmode] Insert Observers for dynamic LSTM**
* #35893 [quant][graphmode] Add new tensorlist observer for LSTM

Summary: Insert new TensorListObserver only for weight input of dynamic LSTM
This is because we are currently not observing the activation inputs in graph mode.
Activation tensors are dynamically quantized within the aten::qlinear_dynamic op

Test Plan:
python test/quantization/test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20830387](https://our.internmc.facebook.com/intern/diff/D20830387)",pytorch
35908,rohan-varma,pr,2020-04-02T22:21:02Z,[profiler] Allow record_function ctx manager to profile futures,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35908 [profiler] Allow record_function ctx manager to profile futures**

This is the first step to improving the way RPCs are profiled as suggested by Ilia. For now, since RPC can return two different types of futures, we have to implement two different code paths, one for the python eager mode future and one for the jit future.

This diff implements the python eager part. We have defined a method `_call_end_callbacks_on_future` that takes in a future and schedules a `RecordFunction` to be completed as a callback on the future.

Once https://github.com/pytorch/pytorch/pull/35039 lands, we can implement the JIT codepath by registering an operator that takes a `Future(t)` as well.

These code paths will be merged once the futures are merged.

Differential Revision: [D20452003](https://our.internmc.facebook.com/intern/diff/D20452003/)",pytorch
35909,rohan-varma,pr,2020-04-02T22:22:50Z,[profiler] Allow record_function ctx manager to profile futures,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

https://github.com/pytorch/pytorch/pull/35055

This is the first step to improving the way RPCs are profiled as suggested by Ilia. For now, since RPC can return two different types of futures, we have to implement two different code paths, one for the python eager mode future and one for the jit future.

This diff implements the python eager part. We have defined a method `_call_end_callbacks_on_future` that takes in a future and schedules a `RecordFunction` to be completed as a callback on the future.

Once https://github.com/pytorch/pytorch/pull/35039 lands, we can implement the JIT codepath by registering an operator that takes a `Future(t)` as well.

These code paths will be merged once the futures are merged.

Differential Revision: [D20452003](https://our.internmc.facebook.com/intern/diff/D20452003/)",pytorch
35916,supriyar,pr,2020-04-02T23:57:44Z,[quant][graphmode] Add quantize_per_tensor.tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35916 [quant][graphmode] Add quantize_per_tensor.tensors**
* #35894 [quant][graphmode] Insert Observers for dynamic LSTM
* #35893 [quant][graphmode] Add new tensorlist observer for LSTM

Summary:
quantize_per_tensor can now accept list of tensors.
Needed for operators like LSTM and cat

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20830388](https://our.internmc.facebook.com/intern/diff/D20830388)",pytorch
35973,vishwakftw,pr,2020-04-03T20:43:28Z,[RELAND] Remove patches that circumvent MAGMA bug,"Changelog:
- The magma implementation of small singular square batch matrices had a bug that resulted in nan values in the LU factorization result. This has been fixed in MAGMA 2.5.2. This PR removes the existing patch that was a temporary workaround for this bug.

Test Plan:
- Existing tests for det and lu should pass

This is a re-submit of #34357",pytorch
36016,vishwakftw,pr,2020-04-04T04:11:47Z,Allowing casting str to int in JIT,"Changelog:
- Allow int(str) in TorchScript

Test Plan:
- Added tests in test_jit.py

Closes #35948 ",pytorch
36039,peterjc123,pr,2020-04-05T08:14:27Z,Enable backtrace with MSVC,Make it possible to report the C++ exceptions in console.,pytorch
36076,supriyar,pr,2020-04-06T17:24:20Z,[quant][graphmode] Insert quant-dequant for LSTM weights,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36076 [quant][graphmode] Insert quant-dequant for LSTM weights**

Summary:
Use TensorList observer that reutrns a list of tensors for scale and zero point values.
Conver these to Tensor scale anc zero point as expected by the quantize_per_tensor API

Test Plan:
python test/quantization/test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
36084,rohan-varma,pr,2020-04-06T18:53:57Z,[rpc] allow ability to abort second call to RecvWork::wait() in ProcessGroupAgent::listenLoop,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36084 [rpc] allow ability to abort second call to RecvWork::wait() in ProcessGroupAgent::listenLoop**
ungraceful shutdown
ungraceful shutdown
ungraceful shutdown
ungraceful shutdown

https://github.com/pytorch/pytorch/pull/30330 added support to abort the call to a `RecvWork` created by `recvAnysource` but there is an additional call to `pg_->recv()` to actually get the tensor sent over the wire (the previous call is the preamble for the tensor). This adds support to be able to abort this call as well in `::shutdown()`, which can be used to avoid hangs during ungraceful shutdown.

Added an internal test case in `ProcessGroupAgentTest` to ensure that an appropriate error message is raised when this happens.

Differential Revision: [D20632764](https://our.internmc.facebook.com/intern/diff/D20632764/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D20632764/)!",pytorch
36088,crcrpar,pr,2020-04-06T19:25:30Z,Fix exception message of `torch.optim.AdamW`.,PyTorch does not implement `SparseAdamW`.,pytorch
36092,zheng-xq,pr,2020-04-06T19:50:23Z,Add trivial reduce for Cuda,"Detect non-read-only loads, and not to use __ldg.

",pytorch
36139,peterjc123,pr,2020-04-07T14:04:41Z,[Test] Windows nightly builds on CircleCI,,pytorch
36151,orionr,pr,2020-04-07T17:11:47Z,[pytorch] Add error when PyTorch used with Python 2,"Summary: Python 2 has reached end-of-life and is no longer supported by PyTorch. To avoid confusing behavior when trying to use PyTorch with Python 2, detect this case early and fail with a clear message.  This commit covers `import torch` only and not C++  for now.

Test Plan: waitforsandcastle

Differential Revision: D20894381

",pytorch
36173,supriyar,pr,2020-04-07T21:48:56Z,[quant] Enable fusion for conv modules with bias,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36173 [quant] Enable fusion for conv modules with bias**

Summary:
Previously we were ignoring the conv bias during training if it existed
This PR adds the bias from the conv op during the conv+bn fusion process

Test Plan:
python test/quantization/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20921613](https://our.internmc.facebook.com/intern/diff/D20921613)",pytorch
36205,peterjc123,pr,2020-04-08T04:45:20Z,"Revert ""Revert D20885968: [pytorch][PR] Enable backtrace with MSVC""","This reverts commit 8afa001d898914a48d6b9e3d944a99607d2819c1 and made a few improvements including the following items.
1. return `std::string` for `get_module_base_name`
2. eliminate `module should always be true` warning
3. do `SymInitialize` and `SymCleanup` once to save time

",pytorch
36208,peterjc123,pr,2020-04-08T05:08:44Z,Don't build deps for `python setup.py egg_info`,Fixes https://github.com/pytorch/pytorch/issues/36207.,pytorch
36234,peterjc123,pr,2020-04-08T15:57:02Z,Bumping VS version in CI to 14.12,,pytorch
36269,zheng-xq,pr,2020-04-08T21:47:21Z,Add trivial reduce for Cuda,"Detect non-read-only loads, and not to use __ldg.

Resubmitting https://github.com/pytorch/pytorch/pull/36092",pytorch
36275,rohan-varma,pr,2020-04-08T22:35:46Z,[rpc] Allow profiling in RPC to work with torchscript function invocations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36275 [rpc] Allow profiling in RPC to work with torchscript function**

Calling a TorchScript function from within RPC was added after initial
support for the profiler with RPC, hence, we were not recording torchscript
funtions invoked under RPC correctly. This diff passes the `RecordFunction` to
the `_invoke_torchscript..` calls similar to what is done for builtin and UDFs.

However, this is only a temporary solution. We will be removing the use of
`RecordFunction` as a standalone in the RPC code in
https://github.com/pytorch/pytorch/pull/35055. This diff is to unblock
recording of torchscript functions in the meantime.

Test plan:
Added tests for calling a script function with builtin, sync, and
asyc. The output looks like below:

```
------  ---------------  ---------------  ---------------  ---------------  ---------------
> Name                                                                                                        Self CPU
total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
> ----------------------------------------------------------------------------------------------------------  ---------
------  ---------------  ---------------  ---------------  ---------------  ---------------
> rpc_sync#__torch__.torch.testing._internal.distributed.rpc.rpc_test.my_script_func(worker1 -> worker2)      99.92%
        1.056s           99.92%           1.056s           1.056s           1
> select                                                                                                      0.04%
        383.661us        0.04%            383.661us        95.915us         4
> fill_                                                                                                       0.02%
        210.966us        0.02%            210.966us        52.741us         4
> to                                                                                                          0.00%
        26.276us         0.00%            26.276us         26.276us         1
> empty                                                                                                       0.02%
        159.802us        0.02%            159.802us        79.901us         2
> set_                                                                                                        0.01%
        93.818us         0.01%            93.818us         93.818us         1
> ----------------------------------------------------------------------------------------------------------  ---------
------  ---------------  ---------------  ---------------  ---------------  ---------------
> Self CPU time total: 1.057s
```

Note that we use `torch.jit._qualified_name` to get the name of the script fn.

Differential Revision: [D20930453](https://our.internmc.facebook.com/intern/diff/D20930453/)",pytorch
36293,zheng-xq,pr,2020-04-09T02:00:19Z,Add trivial reduce for Cuda,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36563 Add sync-point insertions and block/thread local memory allocations
* #36480 Make trivial thread-idx for degenerate statements without thread-idx.
* #36306 Add the one-block multi-thread global reduction support.
* **#36293 Add trivial reduce for Cuda**

Detect non-read-only loads, and not to use __ldg.
Resubmiting #36092

Differential Revision: [D20935933](https://our.internmc.facebook.com/intern/diff/D20935933)",pytorch
36306,zheng-xq,pr,2020-04-09T10:30:09Z,Add the one-block multi-thread global reduction support.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36563 Add sync-point insertions and block/thread local memory allocations
* #36480 Make trivial thread-idx for degenerate statements without thread-idx.
* **#36306 Add the one-block multi-thread global reduction support.**
* #36293 Add trivial reduce for Cuda

Missing __syncthreads between sections.

Differential Revision: [D20957254](https://our.internmc.facebook.com/intern/diff/D20957254)",pytorch
36310,marload,pr,2020-04-09T11:45:38Z,Some code refactoring,"I have performed some code refactoring that can help pytorch.
I hope you like it.
Thx :)",pytorch
36325,supriyar,pr,2020-04-09T18:22:19Z,"[quant][onnx] Mark upsample_nearest2d, sigmoid and reshape as no scale","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36325 [quant][onnx] Mark upsample_nearest2d, sigmoid and reshape as no scale**

Summary:
return the scale of the input tensor

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D20947338](https://our.internmc.facebook.com/intern/diff/D20947338)",pytorch
36376,musikisomorphie,pr,2020-04-10T08:37:30Z,double backwards for nn.Fold and nn.Unfold (issue #33452),"After lots of experiments, I didn't manage to directly print the gradients of Fold/Unfold_backward (let me know if I am wrong). 
Thus, in my testing codes, I compare the gradients of  Fold/Unfold_backward implicitly by comparing the gradients of its following operation.  ",pytorch
36379,musikisomorphie,pr,2020-04-10T09:27:42Z,PR for double backwards of nn.Fold and nn.Unfold  (issue #33452),"@soumith @ezyang @albanD  After lots of experiments, I didn't manage to directly print the gradients of Fold/Unfold_backward (let me know if I am wrong).
Thus, in my testing codes, I compare the gradients of Fold/Unfold_backward implicitly by comparing the gradients of its following operation.",pytorch
36401,rohan-varma,pr,2020-04-10T18:57:19Z,order includes in torch/csrc/autograd/init.cpp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36401 order includes in torch/csrc/autograd/init.cpp**

As title, as requested by @mrshenli in https://github.com/pytorch/pytorch/pull/35055/files

Differential Revision: [D20968111](https://our.internmc.facebook.com/intern/diff/D20968111/)",pytorch
36465,ngimel,pr,2020-04-12T23:35:42Z,move test_abs to device-generic tests,"Per title. test_abs used to be marked as slow_test and run on cpu only. Conceptually similar tests are done in TestTorchMathOps, so it's a matter of adding `abs` test there. 2 remaining checks (correct abs for large-valued long tensors, and correct abs for signed zeros) are factored into separate tests. ",pytorch
36474,ngimel,pr,2020-04-13T04:51:19Z,always use resize_ in max/min with out= kwarg,"Addresses the comment https://github.com/pytorch/pytorch/pull/35591#discussion_r405720398, and makes behavior with `out` kwarg consistent with other functions that `resize_` their passed out. 
The behavior when passing non-contiguous slice with not-exactly-matching dimensions is not very intuitive though:
```
import torch
out=torch.zeros(4,4)
out_ind = torch.zeros(4,4, dtype=torch.long)
src=torch.randn(4,4)
out_col = out[:,1]
out_ind_col = out_ind[:,1]
torch.max(src, 1, out=(out_col, out_ind_col)) #sets second column, as expected
print(out)
out.zero_()
torch.max(src, 1, out=(out_col, out_ind_col), keepdim=True) #sets 4 consequtive element, starting from the first element of second column
print(out)
```
This is common for all the functions that start by resizing their out.  
Test Plan: buck test mode/dev //caffe2/test:cuda -- 'test_dim_reduction_cpu \(test_torch\.TestTorchDeviceTypeCPU\)'

Differential Revision: D20989387

",pytorch
36480,zheng-xq,pr,2020-04-13T11:33:01Z,Make trivial thread-idx for degenerate statements without thread-idx.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36563 Add sync-point insertions and block/thread local memory allocations
* **#36480 Make trivial thread-idx for degenerate statements without thread-idx.**

Differential Revision: [D20992505](https://our.internmc.facebook.com/intern/diff/D20992505)",pytorch
36494,supriyar,pr,2020-04-13T17:19:23Z,[quant] Update qbatch_norm name to qbatch_norm2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* #36549 [quant][graph] Add quantized batch_norm2d support to graph mode
* #36548 [quant][graph] Add valueQuantizable function
* **#36494 [quant] Update qbatch_norm name to qbatch_norm2d**

Summary:
Make name consistent with op. Since we have batch_norm2d and batch_norm3d ops

Test Plan:
python test/quantization/test_quantized.py test_batch_norm2d
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21008831](https://our.internmc.facebook.com/intern/diff/D21008831)",pytorch
36518,supriyar,pr,2020-04-13T21:32:46Z,[quant][graph] Add quantized batch_norm2d op for graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36518 [quant][graph] Add quantized batch_norm2d op for graph mode**
* #36494 [quant] Update qbatch_norm name to qbatch_norm2d

Summary:

Test Plan:
test_quantize_script.py test_qbatch_norm

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
36542,rohan-varma,pr,2020-04-14T00:13:40Z,[CI] fix test_distributed for python 3.8+,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36542 [CI] fix test_distributed for python 3.8+**

Python 3.8 set the default multiprocessing start mode to spawn for mac (https://docs.python.org/3/library/multiprocessing.html), but we
need fork in these tests, otherwise there are some pickling issues (ex: when attempting to pickle the tempfiles). See more details in https://github.com/pytorch/pytorch/issues/36515
Test: Ensure that these tests succeed when run with python 3.8

Differential Revision: [D21007753](https://our.internmc.facebook.com/intern/diff/D21007753/)",pytorch
36548,supriyar,pr,2020-04-14T00:52:30Z,[quant][graph] Add valueQuantizable function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36622 [quant][graph] Graph mode quantization support for sigmoid
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* #36549 [quant][graph] Add quantized batch_norm2d support to graph mode
* **#36548 [quant][graph] Add valueQuantizable function**

Summary:
Refactor to be able to observe based on inputs to ops

Test Plan:
test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21048963](https://our.internmc.facebook.com/intern/diff/D21048963)",pytorch
36549,supriyar,pr,2020-04-14T00:52:38Z,[quant][graph] Add quantized batch_norm2d support to graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36622 [quant][graph] Graph mode quantization support for sigmoid
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* **#36549 [quant][graph] Add quantized batch_norm2d support to graph mode**
* #36548 [quant][graph] Add valueQuantizable function

Summary:

Test Plan:
python test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21048965](https://our.internmc.facebook.com/intern/diff/D21048965)",pytorch
36552,supriyar,pr,2020-04-14T01:52:37Z,[quant][graph] Add quantized batch_norm2d_relu to graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36622 [quant][graph] Graph mode quantization support for sigmoid
* **#36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode**
* #36692 [quant][graph] Add quantized batch_norm2d support to graph mode
* #36691 [quant][graph] Add useQuantizable function

Summary:
Do the fusion for inplace and non-inplace relu
Tested for functional relu as well.
Functional batch_norm is not a usual use-case (since it expects the weight, bias, mean, var) so that is not tested.

Test Plan:
test_quantize_script.py test_batch_norm2d_relu

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21075253](https://our.internmc.facebook.com/intern/diff/D21075253)",pytorch
36563,zheng-xq,pr,2020-04-14T07:23:18Z,Add sync-point insertions and block/thread local memory allocations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36563 Add sync-point insertions and block/thread local memory allocations**

Differential Revision: [D21014238](https://our.internmc.facebook.com/intern/diff/D21014238)",pytorch
36565,peterjc123,pr,2020-04-14T07:44:04Z,[TEST] release/1.5 build,,pytorch
36580,peterjc123,pr,2020-04-14T13:42:12Z,[TEST] Win nightly builds with cleanup,,pytorch
36622,supriyar,pr,2020-04-14T22:20:09Z,[quant][graph] Graph mode quantization support for sigmoid,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36622 [quant][graph] Graph mode quantization support for sigmoid**
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* #36692 [quant][graph] Add quantized batch_norm2d support to graph mode
* #36691 [quant][graph] Add useQuantizable function

Summary:

Test Plan:
python test/quantization/test_quantize_script.py test_swap_dequantize_all_ops

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21075255](https://our.internmc.facebook.com/intern/diff/D21075255)",pytorch
36623,ngimel,pr,2020-04-14T22:20:47Z,Preserve references to buffers when doing module.apply_,"Currently, if one moves a module between devices or casts to a different datatype, by default the references to parameters will be preserved, but references to buffers won't be. This PR makes handling of parameters and buffers consistent. ",pytorch
36630,supriyar,pr,2020-04-14T23:21:54Z,Update qbatch_norm2d opbenchmark test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36630 Update qbatch_norm2d opbenchmark test**

Summary:

Test Plan:
OMP_NUM_THREADS=1 python -m pt.qbatchnorm_test

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21030508](https://our.internmc.facebook.com/intern/diff/D21030508)",pytorch
36641,ngimel,pr,2020-04-15T04:37:19Z,[DO NOT MERGE] indexing bfloat16 with masks,,pytorch
36657,peterjc123,pr,2020-04-15T15:49:54Z,Migrate release CI jobs to CircleCI for Windows,"It should work for both tagged builds and nightly builds now.
Corresponding test pr: https://github.com/pytorch/pytorch/pull/36580",pytorch
36658,peterjc123,pr,2020-04-15T16:13:12Z,Migrate release CI jobs to CircleCI for Windows (v1.5 Release),#36657 for v1.5 release,pytorch
36691,supriyar,pr,2020-04-15T23:41:39Z,[quant][graph] Add useQuantizable function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36622 [quant][graph] Graph mode quantization support for sigmoid
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* #36692 [quant][graph] Add quantized batch_norm2d support to graph mode
* **#36691 [quant][graph] Add useQuantizable function**

Summary: Enables to selectively insert observers at the inputs of aten/call functionc

Test Plan:
test_quantize_script.py
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21055597](https://our.internmc.facebook.com/intern/diff/D21055597)",pytorch
36692,supriyar,pr,2020-04-15T23:41:47Z,[quant][graph] Add quantized batch_norm2d support to graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36622 [quant][graph] Graph mode quantization support for sigmoid
* #36552 [quant][graph] Add quantized batch_norm2d_relu to graph mode
* **#36692 [quant][graph] Add quantized batch_norm2d support to graph mode**
* #36691 [quant][graph] Add useQuantizable function

Summary:

Test Plan:
python test_quantize_script.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21055596](https://our.internmc.facebook.com/intern/diff/D21055596)",pytorch
36693,crcrpar,pr,2020-04-15T23:46:30Z,Add Sphinx to requirements for docs build,"Current `requirements.txt` lacks dependencies for documentation build and https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#building-documentation doesn't mention that we need Sphinx and contrib to build.

The reason the version of sphinx is specified is that `javasphinx` uses a module that the latest stable `Sphinx` doesn't have. I found 1.8 works by chance.",pytorch
36707,peterjc123,pr,2020-04-16T02:50:37Z,[WIP] Switch back to the original sccache bucket,We will revert this change when we find out the reason why it doesn't work currently.,pytorch
36719,peterjc123,pr,2020-04-16T09:25:17Z,test new sccache on Windows,,pytorch
36728,ssnl,pr,2020-04-16T17:30:07Z,[doc] improve tensor.view doc,"fix inaccurate formula. advertise `reshape` better.

",pytorch
36740,supriyar,pr,2020-04-16T19:19:32Z,Enable jit trace check_trace for quantized inputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36740 Enable jit trace check_trace for quantized inputs**

Summary:
Issue #23986

Test Plan:
python test/quantization/test_quantized_nn_mods.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21077551](https://our.internmc.facebook.com/intern/diff/D21077551)",pytorch
36817,supriyar,pr,2020-04-17T18:40:46Z,[quant][graph] Update quantize_dynamic_script API to take sample model args,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36844 [quant][graph] Add quantized::mul_relu and quantized::mul_scalar_relu ops
* #36818 [quant][graph] Support for quantized::mul and quantized::mul_scalar
* **#36817 [quant][graph] Update quantize_dynamic_script API to take sample model args**

Summary:
For dynamic quant we need to run the observers on the weights to calculate the qparams before calling convert on the model.
The API requires the user to provide dummy inputs that will be fed to the model after the prepare step to run the observers

Test Plan:
test_quanitze_script.py
test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21134439](https://our.internmc.facebook.com/intern/diff/D21134439)",pytorch
36818,supriyar,pr,2020-04-17T18:40:52Z,[quant][graph] Support for quantized::mul and quantized::mul_scalar,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36844 [quant][graph] Add quantized::mul_relu and quantized::mul_scalar_relu ops
* **#36818 [quant][graph] Support for quantized::mul and quantized::mul_scalar**
* #36817 [quant][graph] Update quantize_dynamic_script API to take sample model args

Summary:

Test Plan:
python test_quantize_script.py test_quantized_mul
python test_quantize_script.py test_quantized_mul_scalar

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21134438](https://our.internmc.facebook.com/intern/diff/D21134438)",pytorch
36844,supriyar,pr,2020-04-17T23:03:23Z,[quant][graph] Add quantized::mul_relu and quantized::mul_scalar_relu ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36844 [quant][graph] Add quantized::mul_relu and quantized::mul_scalar_relu ops**
* #36818 [quant][graph] Support for quantized::mul and quantized::mul_scalar
* #36817 [quant][graph] Update quantize_dynamic_script API to take sample model args

Summary:

Test Plan:
python test/quantization/test_quantize_script.py TestQuantizeScriptPTSQOps.test_quantized_mul_relu
python test/quantization/test_quantize_script.py TestQuantizeScriptPTSQOps.test_quantized_mul_scalar_relu
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21134440](https://our.internmc.facebook.com/intern/diff/D21134440)",pytorch
36857,rohan-varma,pr,2020-04-18T02:08:40Z,[rpc] Remove redundant call to createExceptionResponse,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36857 [rpc] Remove redundant call to createExceptionResponse**

This was a redundant call as we immediately took the msg and converted
it back to a string

Differential Revision: [D21104235](https://our.internmc.facebook.com/intern/diff/D21104235/)",pytorch
36858,crcrpar,pr,2020-04-18T02:10:46Z,[takeover] BTRS algorithm for fast/efficient binomial sampling,"The original PR is https://github.com/pytorch/pytorch/pull/31278.

CC: @ezyang @jamestwebber @fritzo @zasdfgbnm 

---

<!-- # This PR - CPU
In [1]: import torch; import torch.distributions as dist

In [2]: counts = torch.randint(10, 1000, [1000,1000])
   ...: p = 0.5 * torch.ones(1000, 1000)

In [3]: %timeit dist.binomial.Binomial(total_count=counts, probs=p).sample()
94.8 ms Â± 911 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
-->
```
# This PR - GPU
In [1]: import torch; import torch.distributions as dist

In [2]: counts = torch.randint(10, 1000, [1000,1000]).cuda(); p = 0.5 * torch.ones(1000, 1000).cuda()

In [3]:  %timeit dist.binomial.Binomial(total_count=counts, probs=p).sample()
737 Âµs Â± 216 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

# master (commit: 806f22b167c74897cf67c0828b528fa3e4e6d6de) - GPU
In [5]: counts = torch.randint(10, 1000, [1000,1000]).cuda(); p = 0.5 * torch.ones(1000, 1000).cuda()

In [6]: %timeit dist.binomial.Binomial(total_count=counts, probs=p).sample()
46.3 ms Â± 76.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```",pytorch
36859,ngimel,pr,2020-04-18T03:45:21Z,reenable bfloat16 masked_select,"Try reenabling bfloat16 masked_select, see it windows tests pass. ",pytorch
36882,fritzo,pr,2020-04-19T03:32:01Z,Add a .with_cache() method to distributions.Transform objects,"This resolves an issue observed by @stefanwebb where the composition of multiple transforms is cached only if all components are cached.

This PR adds a new method `.with_cache()` so that e.g. you can compose a normalizing flow (that needs to be cached) with a `SigmoidTransform` (that wasn't already cached) by calling `.with_cache()` on the latter. This issue also comes up when composing non-cached constraint transforms as returned by `transform_to()` and `biject_to()`: after this PR you can call `transform_to(constraints.positive).with_cache()` to get a cached `ExpTransform`.

## Tested
- [x] added a unit test",pytorch
36886,peterjc123,pr,2020-04-19T03:42:23Z,Use lld-link over link.exe,"Link time for `torch_cpu.dll` on a 16-core machine
link.exe ~7m
lld-link.exe (/threads) ~1m
lld-link.exe (/threads:no) ?",pytorch
36928,marload,pr,2020-04-20T15:13:24Z,Early Return Pattern 'if return else return' -> 'if return return',"I think 'if return' is better than 'if return return' in the early return pattern. Also, I think this method is better to match the uniformity of the code. Thank you.",pytorch
36970,rohan-varma,pr,2020-04-21T00:35:38Z,make test_distributed gloo test use MultiProcessTestCase,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36970 make test_distributed gloo test use MultiProcessTestCase**

We would like to move all distributed testing to use the existing
multiprocessing tooling defined in common_distributed.py. With this change, we
make `TestDistBackend` inherit from `MultiProcessTestCase` and enable fork mode
multiprocessing. In the next step, we can enable spawn mode for these tests
which will give us TSAN coverage.

Differential Revision: [D21146947](https://our.internmc.facebook.com/intern/diff/D21146947/)",pytorch
37014,supriyar,pr,2020-04-21T18:17:36Z,[quant][graph] Add check for qconfig_dict key,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37125 [quant][graph] Add JIT passes for dynamic quant multi uses of quant node
* #37093 [quant][graph] Run dynamic quantization for specific ops
* #37045 [graph][quant] Enable accessing child/grandchild modules in forward
* **#37014 [quant][graph] Add check for qconfig_dict key**

Summary:
User should only pass name as key in dict.

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21283696](https://our.internmc.facebook.com/intern/diff/D21283696)",pytorch
37027,rohan-varma,pr,2020-04-21T21:22:46Z,[rpc] Make RPC timeout APIs consistent. ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37027 [rpc] Make RPC timeout APIs consistent.**

We now have timeout parameters in rpc_sync and async accepting float types, so we should change the types in _set_rpc_timeout, _get_rpc_timeout, and `RpcBackendOptions` to be consistent with this. Also modified the associated documentation.

Added tests to ensure that we can initialize `RpcBackendOptions` with a float for timeout.

Differential Revision: [D21125171](https://our.internmc.facebook.com/intern/diff/D21125171/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21125171/)!",pytorch
37045,supriyar,pr,2020-04-22T01:31:31Z,[graph][quant] Enable accessing child/grandchild modules in forward,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37125 [quant][graph] Add JIT passes for dynamic quant multi uses of quant node
* #37093 [quant][graph] Run dynamic quantization for specific ops
* **#37045 [graph][quant] Enable accessing child/grandchild modules in forward**
* #37014 [quant][graph] Add check for qconfig_dict key

Summary:
Fixes to get the correct path for child modules

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21283698](https://our.internmc.facebook.com/intern/diff/D21283698)",pytorch
37052,rohan-varma,pr,2020-04-22T04:15:20Z,remove record_function_enter and record_function_exit from header,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37052 remove record_function_enter and record_function_exit from header**

These only need to be in the cpp as they are not referenced anywhere
else. These functions should only be used from the python operators
torch.opts.profiler.record_function_{enter, exit}.

Differential Revision: [D21171987](https://our.internmc.facebook.com/intern/diff/D21171987/)",pytorch
37070,peterjc123,pr,2020-04-22T15:34:59Z,[TEST] Use lld link clang 10 on Windows,,pytorch
37093,supriyar,pr,2020-04-22T18:35:04Z,[quant][graph] Run dynamic quantization for specific ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37125 [quant][graph] Add JIT passes for dynamic quant multi uses of quant node
* **#37093 [quant][graph] Run dynamic quantization for specific ops**
* #37045 [graph][quant] Enable accessing child/grandchild modules in forward
* #37014 [quant][graph] Add check for qconfig_dict key

Summary:
Specify which ops should/can be dynamically quantized. Similar to static quantization

Test Plan:
python test_quantize_script.py test_dynamic_multi_op

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21283695](https://our.internmc.facebook.com/intern/diff/D21283695)",pytorch
37125,supriyar,pr,2020-04-23T02:47:31Z,[quant][graph] Add JIT passes for dynamic quant multi uses of quant node,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37125 [quant][graph] Add JIT passes for dynamic quant multi uses of quant node**
* #37093 [quant][graph] Run dynamic quantization for specific ops
* #37045 [graph][quant] Enable accessing child/grandchild modules in forward
* #37014 [quant][graph] Add check for qconfig_dict key

Summary:
For dynamic quant we need to replicate the choose_qparams and quantize function in addition to replicating dequant.
RemoveRedundantQuantizeOps pass checks for the choose_qparams - quant - dequant pattern in the graph and removes it if the node following it cannot be quantized using dynamic quantization.

Test Plan:
python test_quantize_script.py test_dynamic_quant_multi_uses

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21283697](https://our.internmc.facebook.com/intern/diff/D21283697)",pytorch
37144,mpjlu,pr,2020-04-23T13:42:25Z,To fix caffe2 model with Copy OP cannot export to onnx model,To fix caffe2 model with Copy OP cannot export to onnx model,pytorch
37235,peterjc123,pr,2020-04-24T14:31:51Z,Replacing EHa with EHsc,"We should not rely on the async exceptions. Catching C++ only exception is more sensible and may get a boost in both space (1163 MB -> 1073 MB, 0.92x) and performance(51m -> 49m, 0.96x).",pytorch
37263,supriyar,pr,2020-04-24T22:02:43Z,[quant] Enable QNNPACK tests for TestSerialization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37351 [quant] Enable qnnpack tests for test_quantize and test_numeric_suite
* **#37263 [quant] Enable QNNPACK tests for TestSerialization**

Summary:

Test Plan:
python test/test_quantization.py TestSerialization
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21286402](https://our.internmc.facebook.com/intern/diff/D21286402)",pytorch
37296,peterjc123,pr,2020-04-25T11:53:39Z,[TEST] Ci all/eha to ehsc,,pytorch
37302,peterjc123,pr,2020-04-25T15:15:23Z,Treat cross-execution-space-call as errors for NVCC on Windows,"On Windows, when you call those unsupported functions like `std::pow`, `std::isnan` or `std::isinf` in the device function and compile, a warning is thrown:
```
kernel.cu
kernel.cu(39): warning: calling a __host__ function from a __host__ __device__ function is not allowed

kernel.cu(42): warning: calling a __host__ function from a __host__ __device__ function is not allowed

kernel.cu(39): warning: calling a __host__ function(""isnan<double> "") from a __host__ __device__ function(""test_"") is not allowed

kernel.cu(42): warning: calling a __host__ function(""isinf<double> "") from a __host__ __device__ function(""test_"") is not allowed
```
However, those calls will lead to runtime errors, see https://github.com/pytorch/pytorch/pull/36749#issuecomment-619239788 and https://github.com/pytorch/pytorch/issues/31108.  So we should treat them as errors.
Previously, the situation is worse because the warnings are turned off by passing in `-w`.",pytorch
37312,crcrpar,pr,2020-04-26T04:33:13Z,Use `gpu_kernel` in Affine Quantizer,"Removes `CUDA_tensor_apply2` from Affine Quantizer.

cc: @zasdfgbnm 

# Profiling

## This PR

### quint8

```==4458==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  4.8703ms        20  243.52us  207.60us  312.66us  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  751.95us        10  75.194us  74.372us  79.044us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE0_clEvEUlfN3c106quint8EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_
      API calls:  100.00%  162.48us        10  16.247us  13.383us  35.997us  cudaLaunchKernel
```

### qint8

```==14289==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  4.8143ms        20  240.71us  155.68us  327.78us  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  748.85us        10  74.884us  73.892us  78.565us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE_clEvEUlfN3c105qint8EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_
      API calls:  100.00%  166.61us        10  16.661us  13.387us  39.237us  cudaLaunchKernel
```

### qint32

```
==17303==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  19.011ms        20  950.55us  308.07us  1.0331ms  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  1.1440ms        10  114.40us  113.42us  117.74us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE1_clEvEUlfN3c106qint32EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_
      API calls:  100.00%  163.78us        10  16.378us  13.747us  35.668us  cudaLaunchKernel
```

## Original

commit: b428f454e13f6e8055124ea19c32b554017137d0

### quint8

```
==4361==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  5.6212ms        20  281.06us  230.17us  352.82us  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  780.85us        10  78.084us  77.633us  78.561us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE0_clEvEUlRfRN3c106quint8EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_
      API calls:  100.00%  166.07us        10  16.606us  13.535us  36.578us  cudaLaunchKernel
```

### qint8

```
==12583==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  5.5765ms        20  278.82us  226.51us  351.23us  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  783.28us        10  78.328us  77.826us  80.386us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE_clEvEUlRfRN3c105qint8EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_
      API calls:  100.00%  161.05us        10  16.104us  13.363us  34.284us  cudaLaunchKernel
```

### qint32

```
==17267==       Range ""quantize_per_tensor, seq = 0""
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  19.815ms        20  990.77us  381.03us  1.0717ms  quantize_per_tensor, seq = 0
 GPU activities:  100.00%  1.1778ms        10  117.78us  117.51us  118.44us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE1_clEvEUlRfRN3c106qint32EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_
      API calls:  100.00%  172.26us        10  17.226us  14.094us  37.952us  cudaLaunchKernel
```


## 

# Environment

```shell
Collecting environment information...
PyTorch version: 1.6.0a0+010771e
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: TITAN V
Nvidia driver version: 440.33.01
cuDNN version: /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.6.0a0+010771e
[conda] blas                      1.0                         mkl
[conda] magma-cuda102             2.5.2                         1    pytorch
[conda] mkl                       2020.0                      166
[conda] mkl-include               2020.0                      166
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.0.15           py37ha843d7b_0
[conda] mkl_random                1.1.0            py37hd6b4f25_0
[conda] torch                     1.6.0a0+010771e           dev_0    <develop>
```",pytorch
37319,peterjc123,pr,2020-04-26T13:35:16Z,Generate environment restore script for Windows build jobs,for better debugging purposes,pytorch
37340,peterjc123,pr,2020-04-27T09:35:41Z,[TESTING ONLY] CUDA separate compilation on Windows,,pytorch
37351,supriyar,pr,2020-04-27T17:53:52Z,[quant] Enable qnnpack tests for test_quantize and test_numeric_suite,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37351 [quant] Enable qnnpack tests for test_quantize and test_numeric_suite**

Summary:

Test Plan:
python test/test_quantization.py PostTrainingStaticQuant

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21293704](https://our.internmc.facebook.com/intern/diff/D21293704)",pytorch
37376,ssnl,pr,2020-04-27T22:19:22Z,Add better device idx parse checks,Fixes https://github.com/pytorch/pytorch/issues/32079,pytorch
37391,rohan-varma,pr,2020-04-28T04:08:41Z,Allow customizing retryable message types in Faulty agent tests,"Summary:
It doesn't seem like we could customize the retryable message types by
passing faulty_messages into dist_utils, as the `FaultyRpcAgentTestFixture`
overrode the `rpc_backend_options` function and provided the default list of
retryable message types. Needed to fix this as part of adding timeout injection
support as mentioned in https://github.com/pytorch/pytorch/issues/36272


Differential Revision: D21270127

",pytorch
37400,peterjc123,pr,2020-04-28T06:22:31Z,Eliminate warnings for cpp extensions on Windows,Improve the readability of the logs.,pytorch
37414,peterjc123,pr,2020-04-28T15:10:07Z,Retry anaconda upload,,pytorch
37450,rohan-varma,pr,2020-04-28T19:48:47Z,Allow customizing retryable message types in Faulty agent tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37450 Allow customizing retryable message types in Faulty agent tests**

It doesn't seem like we could customize the retryable message types by
passing faulty_messages into dist_utils, as the `FaultyRpcAgentTestFixture`
overrode the `rpc_backend_options` function and provided the default list of
retryable message types. Needed to fix this as part of adding timeout injection
support as mentioned in https://github.com/pytorch/pytorch/issues/36272

Differential Revision: [D21270127](https://our.internmc.facebook.com/intern/diff/D21270127/)",pytorch
37485,rohan-varma,pr,2020-04-29T04:57:22Z,Add timeout injection to faulty agent for testing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37485 Add timeout injection to faulty agent for testing**

Adds arbitrary timeout injection to faulty RPC agent. This is to better test scenarios that need information about how long-running RPCs, such as properly testing RPC timeouts and the profiler in all scenarios.

This is done by overriding ProcessGroupAgent's `enqueueSend()` function to inject the timeout. Determining which messages to timeout is done similar to the existing `faulty_messages` by having the user specify a mapping of message to timeout.

Added unit tests that verify RPC timeouts work with builtin + TorchScript functions, which was not tested before.

Differential Revision: [D21296537](https://our.internmc.facebook.com/intern/diff/D21296537/)",pytorch
37486,peterjc123,pr,2020-04-29T05:15:35Z,Unify the path for environment restore script,,pytorch
37526,supriyar,pr,2020-04-29T20:07:38Z, [quant] Release qnnpack original weights for conv/linear,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37526 [quant] Release qnnpack original weights for conv/linear**

Summary:
QNNPACK currently does not support an unpack function. So we store the original weights in the packed structure which is directly returned to the user when unpack is called.
However for memory constrained environments (like mobile), storing these extra weights in memory is expensive. We need to release these weights after packing on mobile to free up the memory. As a side-effect user cannot call unpack on mobile once the model is run.

The change is gated by C10_MOBILE which is enabled for mobile builds.

The change saves 36MB on device for Speech Model.

Test Plan:
python test/test_quantization.py
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21309597](https://our.internmc.facebook.com/intern/diff/D21309597)",pytorch
37562,supriyar,pr,2020-04-30T03:46:05Z,[quant] Check qengine for TestNormalization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37562 [quant] Check qengine for TestNormalization**

Summary:
The model has a LinearLayer which needs fbgemm. Fixes failing windows test.

Test Plan:
python test/test_quantization.py TestPostTrainingStatic

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21321032](https://our.internmc.facebook.com/intern/diff/D21321032)",pytorch
37595,supriyar,pr,2020-04-30T18:13:36Z,[quant] Release qnnpack original weights for conv/linear,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37595 [quant] Release qnnpack original weights for conv/linear**

Summary:
QNNPACK currently does not support an unpack function. So we store the original weights in the packed structure which is directly returned to the user when unpack is called.
However for memory constrained environments (like mobile), storing these extra weights in memory is expensive. We need to release these weights after packing on mobile to free up the memory. As a side-effect user cannot call unpack on mobile once the model is run.

The change is gated by C10_MOBILE which is enabled for mobile builds.

The change saves 36MB on device for Speech Model.

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21365495](https://our.internmc.facebook.com/intern/diff/D21365495)",pytorch
37604,rohan-varma,pr,2020-04-30T20:00:26Z,[DistributedSampler] Only create torch.generator and seed when shuffling,We don't need to create `torch.Generator()` and seed it if we are not shuffling.,pytorch
37627,rohan-varma,pr,2020-04-30T23:23:17Z,[MultiProcessTestCase] Improve the error message when a process terminates,"When a subprocess terminates with an exception in a distributed test, log the process number as well",pytorch
37635,supriyar,pr,2020-05-01T00:31:57Z,[quant][graph] Fix bug in replaceConvolutionWithConv2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37779 [quant][mobile] Return for conv with empty batch
* #37637 [quant][graph] Fix bug in replicateDequant
* **#37635 [quant][graph] Fix bug in replaceConvolutionWithConv2d**

Summary:
replaceConvolutionWithConv2d incorrectly assumes that the size of padding is 2. For Conv1d it is 1, in which case we cannot replace with aten::conv2d

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21354930](https://our.internmc.facebook.com/intern/diff/D21354930)",pytorch
37637,supriyar,pr,2020-05-01T02:17:11Z,[quant][graph] Fix bug in replicateDequant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37779 [quant][mobile] Return for conv with empty batch
* **#37637 [quant][graph] Fix bug in replicateDequant**
* #37635 [quant][graph] Fix bug in replaceConvolutionWithConv2d

Summary:
Insert dequant op at specific offset, rather than for all inputs of user

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21354931](https://our.internmc.facebook.com/intern/diff/D21354931)",pytorch
37646,cloudhan,pr,2020-05-01T04:48:48Z,Migrate clamp from the TH to Aten (CUDA),"Fixed #24544

Reference #24507",pytorch
37714,bharatr21,pr,2020-05-02T09:58:51Z,Update README to include few (missing?) links,Update of README,pytorch
37715,bharatr21,pr,2020-05-02T10:16:25Z,DOC: Add documentation for Tensor.is_nonzero,Fixes #37438 by adding documentation for `Tensor.is_nonzero`,pytorch
37763,peterjc123,pr,2020-05-04T12:19:40Z,Using LoadLibraryEx and LOAD_LIBRARY_SEARCH_* flag for loading DLLs oâ€¦,"â€¦n Windows

Without this PR, the OS try to find the DLL in the following directories.
- The directory from which the application loaded.
- The system directory. Use the GetSystemDirectory function to get the path of this directory.
- The 16-bit system directory. There is no function that obtains the path of this directory, but it is searched.
- The Windows directory. Use the GetWindowsDirectory function to get the path of this directory.
- The current directory.
- The directories that are listed in the PATH environment variable. Note that this does not include the per-application path specified by the App Paths registry key. The App Paths key is not used when computing the DLL search path.

If we use  LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags, the directories are searched in the following order.

- The directory that contains the DLL (LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR). This directory is searched only for dependencies of the DLL to be loaded.
- The application directory (LOAD_LIBRARY_SEARCH_APPLICATION_DIR).
- Paths explicitly added to the application search path with the AddDllDirectory function (LOAD_LIBRARY_SEARCH_USER_DIRS) or the SetDllDirectory function. If more than one path has been added, the order in which the paths are searched is unspecified.
- The System32 directory (LOAD_LIBRARY_SEARCH_SYSTEM32).

Advantages:
1. The directory that contains the DLL comes first and it's desirable for us, because the dependencies in `lib` should always be preferred.
2. The system directory is considered in the last place. According to some of the bug reports, the DLL load failure are caused by loading the conflicting ones in systemroot.

Neural:
1. The directories in `PATH` are not considered. Similar things happen as described in the previous point. So it may be beneficial for normal users. However, it may cause failures if there are some new dependencies if built from source. (Resolved by making the fallback to `LoadLibraryW` if error code is `126`)

Disadvantages:
1. LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags is only available for Win7/2008 R2 + KB2533623 and up. (Resolved by making the fallback to `LoadLibraryW` if it is not supported)
2. Failure during the call of `LoadLibraryEx` will lead to the OS to pop up a modal dialog, which can block the process if user is using a CLI-only interface. This can be switched off by calling `SetErrorMode`. (Resolved by calling `SetErrorMode`)

Test plan:
Test some common cases (in a new repo maybe) including
1. Python 3.6/3.7/3.8, conda python, conda install
2. Python 3.6/3.7/3.8, conda python, pip install
3. Python 3.6/3.7/3.8, official python, pip install
Plus some corner cases like
1. Conflicting DLLs in systemroot or `PATH`
2. Remove some local dependencies and use global ones

References:
1. https://docs.microsoft.com/en-us/windows/win32/api/errhandlingapi/nf-errhandlingapi-seterrormode
2. https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-loadlibraryexa
3. https://docs.microsoft.com/en-us/windows/win32/dlls/dynamic-link-library-search-order#standard-search-order-for-desktop-applications

What do you think, @malfet @ezyang ?",pytorch
37779,supriyar,pr,2020-05-04T18:28:50Z,[quant][mobile] Return for conv with empty batch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37779 [quant][mobile] Return for conv with empty batch**
* #37637 [quant][graph] Fix bug in replicateDequant
* #37635 [quant][graph] Fix bug in replaceConvolutionWithConv2d

Summary:
We should just return empty output

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21385789](https://our.internmc.facebook.com/intern/diff/D21385789)",pytorch
37807,rohan-varma,pr,2020-05-05T03:04:23Z,[WIP][DDP] Synchronize event copies that can occur on different streams.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37858 [not for land] backward callback stream issue
* **#37807 [WIP][DDP] Synchronize event copies that can occur on different streams.**

Differential Revision: [D21396403](https://our.internmc.facebook.com/intern/diff/D21396403/)",pytorch
37811,peterjc123,pr,2020-05-05T04:54:11Z,Delay loading the cuda library on Windows,"so we can import torch compiled with cuda on a CPU-only machine.
need tests",pytorch
37843,supriyar,pr,2020-05-05T17:46:02Z,[quant][tests] Enable tests to run on all qengine backends,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37890 [quant][tests] Update test_quantize_script to test multiple backends
* **#37843 [quant][tests] Enable tests to run on all qengine backends**

Summary:
Refactor tests to use supported_qengines

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21409626](https://our.internmc.facebook.com/intern/diff/D21409626)",pytorch
37845,bharatr21,pr,2020-05-05T18:12:00Z,DOC: Add documentation for Tensor.is_nonzero,Fixes #37438 by adding documentation for `Tensor.is_nonzero`,pytorch
37858,rohan-varma,pr,2020-05-05T19:23:49Z,[not for land] backward callback stream issue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37858 [not for land] backward callback stream issue**
* #37807 [WIP][DDP] Synchronize event copies that can occur on different streams.

backward callback stream issue

Differential Revision: [D21409295](https://our.internmc.facebook.com/intern/diff/D21409295/)",pytorch
37881,ssnl,pr,2020-05-05T21:22:44Z,Fix conv non zero padding being applied in wrong dim,Turns out F.pad takes in dims in reverse order. Fixes https://github.com/pytorch/pytorch/issues/37844,pytorch
37884,rohan-varma,pr,2020-05-05T21:43:19Z,Support rpc_async call with timeout in JIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37884 Support rpc_async call with timeout in JIT**

Adds support to use rpc_timeout param in rpc_async call from jit for
parity with eager mode. Done by:
1) Add timeout as an input in ir_emitter.cpp if it is specified
2) Parse float IValue from inputs in `prim::rpc_async` operator. Give the default if needed.

Added UTs in `jit/rpc_test_faulty`.

Differential Revision: [D21268895](https://our.internmc.facebook.com/intern/diff/D21268895/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21268895/)!",pytorch
37890,supriyar,pr,2020-05-05T23:15:13Z,[quant][tests] Update test_quantize_script to test multiple backends,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37890 [quant][tests] Update test_quantize_script to test multiple backends**
* #37843 [quant][tests] Enable tests to run on all qengine backends

Summary:

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
37904,peterjc123,pr,2020-05-06T02:18:39Z,Add test jobs on CPU agents for CUDA builds on Windows,Targets https://github.com/pytorch/pytorch/pull/37811#issuecomment-624367089.,pytorch
37911,peterjc123,pr,2020-05-06T04:11:46Z,[test only] test rebuild on Windows CI,,pytorch
37917,peterjc123,pr,2020-05-06T10:04:20Z,Fix rebuild with Ninja on Windows,It is currently broken due to a ninja bug.,pytorch
37926,peterjc123,pr,2020-05-06T14:34:21Z,Create a desktop shortcut for restoring pytorch environment on CircleCI,,pytorch
37929,peterjc123,pr,2020-05-06T15:18:26Z,Test new sccache,,pytorch
37939,supriyar,pr,2020-05-06T16:47:36Z,[quant][test] Fix macos/windows CI test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37939 [quant][test] Fix macos/windows CI test**

Summary:
macos and windows test were skipped for quantization tests in CI target determinator

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
37943,supriyar,pr,2020-05-06T18:19:35Z,Reland [quant][tests] Enable tests to run on all qengine backends,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37943 Reland [quant][tests] Enable tests to run on all qengine backends**

Summary:
Refactor tests to use supported_qengines

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21435514](https://our.internmc.facebook.com/intern/diff/D21435514)",pytorch
37967,crcrpar,pr,2020-05-06T22:10:07Z,Implement Stochastic Rounding,"This PR introduces Stochastic Rounding (SR). We've had success with SR training networks to convergence using only FP16 weights. We've recently confirmed with this implementation that ResNet50 and Transformer can be trained in FP16 without significant loss. We have also previously trained a wider set of networks maintaining convergence behavior. Runs were compared with deprecated nvidia/apex's AMP using FP32 ""master weights"".
Using SR prevents the need to have an FP32 copy of the weights. This can significantly decrease the memory footprint of the network.

## Recent results:
* ResNet50 and SGD-SR converged to 76 of top-1
Trained with native AMP features (GradScaler and autocast)
Parameters are in FP16 by calling model.half()
* FP16 Transformer and Adam-SR at scale reached BLEU score of 25.0 with the same number of epochs as with non-SR Adam.

<!--
This PR introduces Stochastic Rounding (SR). With SR, I've experimentally confirmed ResNet50 and Transformer can be trained in FP16 without significant loss. 

### results
- FP16 ResNet50 and SGD-SR converged to 76 of top-1
    - trained with native AMP features (`GradScaler` and `autocast`)
    - parameters are in FP16 by calling `model.half()`
- FP16 Transformer and Adam-SR at scale reached BLEU score of 25.0 with the same epochs as Transformer and Adam
    - script used deprecated nvidia/apex's AMP style so master weights are used. Usually master weights are FP32 and copied to FP16 model parameters after each update to avoid the loss of numerical precision. To see stochastic rounding power, I hacked the script so that master weights can be maintained in FP16.
-->

## How Stochastic Rounding Works
It rounds FP32 values to FP16 stochastically though we can implement rounding algorithms to dtypes other than FP16. Assume `x` is an FP32, `y` and `z` are FP16 and 0 < `y` < `x` < `z`. Usually, `x` is rounded to `y` if (1) round_to_zero or (2) round_to_nearest and abs(x - y) < abs(x - z). However, SR rounds `x` to `y` (`z`) with the probability of abs(x - z) / abs(y - z) (abs(x - y) / abs(y - z)).

## Design of `round_stochastically` kernel
Current implementation only supports the cast from FP32/64 to FP16, however, we can support cast to other dtypes like bfloat16.

## Design of Optimizer kernels
The kernels load params, gradients, and some momentums in `scalar_t` and cast them into FP32 (`float`) before do some update math in 32 bits. After that, they write updates values in `scalar_t` and SR is applied in this writing out phase. These custom optimizers, even if they will not be able to be in `torch.optim`, works better than `torch.optim` optimizers in low precision training.",pytorch
37969,crcrpar,pr,2020-05-06T22:20:44Z,Implement `gpu_kernel_multiple_outputs`,"This PR introduces a variant of `gpu_kernel` for functions that return multiple values with `thrust::tuple`.
With this I simplified `prelu_cuda_backward_share_weights_kernel`.

### Why using `thrust::tuple`?
Because `std::tuple` does not support `operator=` on device code which makes the implementation complicated.",pytorch
37990,rohan-varma,pr,2020-05-07T02:46:01Z,"Remove dead code in ddp.{h, cpp}","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37990 Remove dead code in ddp.{h, cpp}**

The code in `ddp.{h, cpp}` and the corresponding pybind implementations are no longer used. The pybinded calls were all private APIs and only ran in unittests, so we should remove these unused APIs.

https://github.com/pytorch/pytorch/pull/20234 from a year ago also mentioned that we should delete `_dist_broadcast_coalesced`

Verified that all tests pass with cuda by running `test_c10d` on a gpu-enabled machine.

Differential Revision: [D21443879](https://our.internmc.facebook.com/intern/diff/D21443879/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21443879/)!",pytorch
37991,cloudhan,pr,2020-05-07T03:01:36Z,Port atan from TH to ATen,"Fixed #24538
Related #24507",pytorch
37992,rohan-varma,pr,2020-05-07T03:24:43Z,Make cpp ProcessGroupGloo have same timeout as python,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37992 Make cpp ProcessGroupGloo have same timeout as python**

Previously this was 10k ms, but we default to 30min in Python. We should make them the same for clarity.

Differential Revision: [D21444339](https://our.internmc.facebook.com/intern/diff/D21444339/)",pytorch
37993,peterjc123,pr,2020-05-07T03:26:14Z,Set SCCACHE_IDLE_TIMEOUT to INFINITE(0) on Windows,,pytorch
38025,cloudhan,pr,2020-05-07T17:03:33Z,add dtype checking for gather and scatter,"Fixed #37996

in the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype.
https://github.com/pytorch/pytorch/blob/2b41b9bceb01851df83d40c1280b3d3b09e1395b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L106
add a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.",pytorch
38045,rohan-varma,pr,2020-05-07T20:24:35Z,Skip RPC profiling tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38045 Skip RPC profiling tests**

We are working on fixing these (e.g.
https://github.com/pytorch/pytorch/pull/37311) but a few PRs still need to land
before these tests are fixed. Disable them for now to avoid noise

Differential Revision: [D21461340](https://our.internmc.facebook.com/intern/diff/D21461340/)",pytorch
38046,ngimel,pr,2020-05-07T20:30:27Z,fix multinomial kernels to properly advance random states,"Before, multinomial kernels did not advance random states enough, which lead to the same sequence being generated over and over with a shift of 4. This PR fixes that. 
Fixes #37403",pytorch
38061,crcrpar,pr,2020-05-07T21:56:48Z,Port `masked_fill` CUDA kernel to ATen,"Related issue: https://github.com/pytorch/pytorch/issues/24303

This PR changes CPU implementation of https://github.com/pytorch/pytorch/pull/33330 though I don't see terrible regression as follows.

## Profiling

### CPU
Benchmark is executed with https://github.com/mingfeima/op_bench-py on the machine with 12 Intel(R) Core(TM) i7-7800X CPU @ 3.50GHz.

| input size | contiguous | this pr [ms] | v1.5 [ms] | speedup      | 
|------------|------------|--------------|-----------|--------------| 
| 128 1000   | TRUE       | 0.469        | 0.466     | 0.9936034115 | 
| 256 1000   | TRUE       | 0.94         | 0.932     | 0.9914893617 | 
| 512 1000   | TRUE       | 1.87         | 1.875     | 1.002673797  | 
| 1024 1000  | TRUE       | 3.735        | 3.748     | 1.003480589  | 
| 128 1000   | FALSE      | 0.467        | 0.469     | 1.004282655  | 
| 256 1000   | FALSE      | 0.937        | 0.94      | 1.003201708  | 
| 512 1000   | FALSE      | 1.873        | 1.878     | 1.002669514  | 
| 1024 1000  | FALSE      | 3.748        | 3.761     | 1.003468517  | 

### GPU
I see some regression in GPU kernel so investigating now.

```
# This PR
torch.version = 1.6.0a0+b30e60d, torch.path = /home/mkozuki/ghq/github.com/crcrpar/pyt-1/torch
Size: 65536, Forward: 1.1904239654541016e-05, Effective BW (GB/s): 16.5157965317845
Size: 131072, Forward: 1.2037754058837891e-05, Effective BW (GB/s): 32.66522958336304
Size: 262144, Forward: 1.218557357788086e-05, Effective BW (GB/s): 64.53795506413618
Size: 524288, Forward: 1.201152801513672e-05, Effective BW (GB/s): 130.9462041813418
Size: 1048576, Forward: 3.201484680175781e-05, Effective BW (GB/s): 98.25841177622878
Size: 2097152, Forward: 6.691217422485351e-05, Effective BW (GB/s): 94.02558014118655
Size: 4194304, Forward: 0.00012657642364501953, Effective BW (GB/s): 99.40960281267283
Size: 8388608, Forward: 0.0002461743354797363, Effective BW (GB/s): 102.22765078641397
Size: 16777216, Forward: 0.0004850029945373535, Effective BW (GB/s): 103.77595307016936
Size: 33554432, Forward: 0.0009639811515808106, Effective BW (GB/s): 104.42454796437107
```

```
# v1.5
torch.version = 1.5.0, torch.path = /home/mkozuki/.pyenv/versions/anaconda3-2020.02/envs/torch15/lib/python3.7/site-packages/torch
Size: 65536, Forward: 1.4140605926513672e-05, Effective BW (GB/s): 13.903788919777442
Size: 131072, Forward: 1.4185905456542969e-05, Effective BW (GB/s): 27.718780532168072
Size: 262144, Forward: 1.4300346374511718e-05, Effective BW (GB/s): 54.99391269303102
Size: 524288, Forward: 1.4100074768066406e-05, Effective BW (GB/s): 111.55004678146771
Size: 1048576, Forward: 2.8598308563232423e-05, Effective BW (GB/s): 109.99699485879117
Size: 2097152, Forward: 5.365610122680664e-05, Effective BW (GB/s): 117.25518358864254
Size: 4194304, Forward: 0.00010152578353881836, Effective BW (GB/s): 123.93809297900103
Size: 8388608, Forward: 0.00019715070724487304, Effective BW (GB/s): 127.6476475873989
Size: 16777216, Forward: 0.00038855552673339844, Effective BW (GB/s): 129.53527755258082
Size: 33554432, Forward: 0.0007711434364318847, Effective BW (GB/s): 130.53770705197672
```",pytorch
38078,rohan-varma,pr,2020-05-07T23:15:24Z,dedupe test skipping in common_distributed and test_distributed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38078 dedupe test skipping in common_distributed and test_distributed**

`common_distributed` and `test_distributed` have some error codes that overlap but are for different reasons, for example, code 75 in `test_distributed` is ""no cuda available"" but in common_distributed it is ""need at least 2 CUDA devices"".

This is an issue because the tests in `test_distributed` now use the utils in `common_distributed`, so we could get the wrong reason for skipping tests.

It is also the source of test failures in https://github.com/pytorch/pytorch/pull/37990.

This diff makes it so that the test skipping logic is deduped and put into `common_distributed.py`, where it can be reused and then imported into `test_distributed`

Differential Revision: [D21466768](https://our.internmc.facebook.com/intern/diff/D21466768/)",pytorch
38096,crcrpar,pr,2020-05-08T01:51:31Z,Update `custom_fwd` signature,"With the reference of https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/#python-2-no-longer-supported, this PR updates the signature of `custom_fwd`.

cc: @mcarilli",pytorch
38112,cloudhan,pr,2020-05-08T06:18:16Z,fix compilation error with gcc 5.5,Fixed #38111,pytorch
38128,peterjc123,pr,2020-05-08T17:05:49Z,Fix debug build failure,Fixes https://github.com/pytorch/pytorch/issues/38024 by reverting https://github.com/pytorch/pytorch/pull/37302 for the debug configuration.,pytorch
38146,rohan-varma,pr,2020-05-08T19:23:20Z,Use handlExceptionGILHeld in future.wait pybind,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38146 Use handlExceptionGILHeld in future.wait pybind**

This code already grabs the GIL, but
PythonRpcHandler::handleException() does too. This would result in the gil
being unnecessarily acquired twice

Differential Revision: [D21480846](https://our.internmc.facebook.com/intern/diff/D21480846/)",pytorch
38147,rohan-varma,pr,2020-05-08T19:27:09Z,Use py::pickle in RRef pickling pybind code,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38147 Use py::pickle in RRef pickling pybind code**

We're seeing many warnings of the form:
```
/home/rvarm1/pytorch/torch/distributed/rpc/__init__.py:14: FutureWarning:
pybind11-bound class 'torch.distributed.rpc.R
Ref' is using an old-style placement-new '__setstate__' which has been
deprecated. See the upgrade guide in pybind11's
docs. This message is only visible when compiled in debug mode.
```
in test logs, it turns out this is because pybind recommends using `py::pickle`
instead of manually defining getstate and setstate (see https://github.com/pybind/pybind11/blob/master/docs/upgrade.rst#id5). Changing to use pybind's
recommendation will silence these warnings. We maintain the same functionality which is to throw in all cases if this is called

Note that return types need to be added to the function to satisfy the contract
pybind expects, but they don't return anything since we TORCH_CHECK(false) in
all cases.

Differential Revision: [D21446260](https://our.internmc.facebook.com/intern/diff/D21446260/)",pytorch
38153,supriyar,pr,2020-05-08T21:08:57Z,[quant] Disable qnnpack test when TSAN is enabled,"Summary: Test fails when opt-tsan is enabled

Test Plan: buck test mode/opt-tsan //caffe2/test:quantization -- 'test_single_linear_dynamic \(quantization\.test_quantize\.TestGraphModePostTrainingStatic\)' --run-disabled

Reviewed By: vkuzo

Differential Revision: D21482799

",pytorch
38178,peterjc123,pr,2020-05-09T02:54:41Z,Use thrust::host_vector instead of std::vector,Fixes #38024.,pytorch
38213,bharatr21,pr,2020-05-11T06:13:00Z,DOC: Add missing args for index_add,Fix #37752 by updating `index_add`documentation as suggested by @danpovey ,pytorch
38247,supriyar,pr,2020-05-11T17:28:12Z,[quant] Implement unsqueeze/squeeze for per-channel qtensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38407 [quant] Return default qconfig when backend is 'none'
* #38341 [quant][graphmode] Add quantized::conv1d to graphmode
* #38332 [quant] Add quantized::conv1d op benchmarck
* #38283 [quant] Add support for Quantized Conv1d and ConvRELU1d
* #38248 [quant] Add support for quantized::conv1d operator
* **#38247 [quant] Implement unsqueeze/squeeze for per-channel qtensor**

Summary:
Per-channel quantized tensor axis value is shifted based on the unsqueeze/squeeze dim

Test Plan:
python test/test_quantization.py TestQuantizedTensor.test_qtensor_unsqueze

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21550293](https://our.internmc.facebook.com/intern/diff/D21550293)",pytorch
38248,supriyar,pr,2020-05-11T17:28:18Z,[quant] Add support for quantized::conv1d operator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'
* #38341 [quant][graphmode] Add quantized::conv1d to graphmode
* #38332 [quant] Add quantized::conv1d op benchmarck
* #38283 [quant] Add support for Quantized Conv1d and ConvRELU1d
* **#38248 [quant] Add support for quantized::conv1d operator**
* #38247 [quant] Implement unsqueeze/squeeze for per-channel qtensor

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21553661](https://our.internmc.facebook.com/intern/diff/D21553661)",pytorch
38252,supriyar,pr,2020-05-11T18:01:48Z,[quant][mobile] Ensure qconv doesn't assert with empty batch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38284 [quant][mobile] Don't release bias tensor
* **#38252 [quant][mobile] Ensure qconv doesn't assert with empty batch**

Summary:
Return empty batch output if input has empty batch on mobile.

Test Plan:
python test/test_quantization.py TestQNNPackOps.test_qconv_empty_batch

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21515998](https://our.internmc.facebook.com/intern/diff/D21515998)",pytorch
38255,rohan-varma,pr,2020-05-11T18:06:05Z,Remove unnecessary RPC profiling code after future merge,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38255 Remove unnecessary RPC profiling code after future merge**

Now that the futures are consolidated after
https://github.com/pytorch/pytorch/pull/35154, there is no
`torch.distributed.rpc.Future` and we do not need a special path. All futures
can now be profiled through the use of the jit operator defined in
record_function_ops.cpp

As a result, we also get rid of the record_function_ops.h file.
RPC profiling tests are currently disabled, although I re-enabled them locally
to ensure that they still work with this change.

Differential Revision: [D21506091](https://our.internmc.facebook.com/intern/diff/D21506091/)",pytorch
38283,supriyar,pr,2020-05-11T23:50:56Z,[quant] Add support for Quantized Conv1d and ConvRELU1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'
* #38341 [quant][graphmode] Add quantized::conv1d to graphmode
* #38332 [quant] Add quantized::conv1d op benchmarck
* **#38283 [quant] Add support for Quantized Conv1d and ConvRELU1d**
* #38248 [quant] Add support for quantized::conv1d operator
* #38247 [quant] Implement unsqueeze/squeeze for per-channel qtensor

Summary:
Adds support for the modules and tests

Test Plan:
python test/test_quantization.py TestStaticQuantizedModule.test_conv1d_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21553665](https://our.internmc.facebook.com/intern/diff/D21553665)",pytorch
38284,supriyar,pr,2020-05-12T00:05:31Z,[quant][mobile] Don't release bias tensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38284 [quant][mobile] Don't release bias tensor**
* #38252 [quant][mobile] Ensure qconv doesn't assert with empty batch

Summary:
Bias is used to calculate out channels

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21515997](https://our.internmc.facebook.com/intern/diff/D21515997)",pytorch
38302,peterjc123,pr,2020-05-12T04:31:21Z,Using LoadLibraryEX [Reland],"This reverts commit 1ab4f35499aa933677152aca6a1ba2cbe86639f8.

Without this PR, the OS try to find the DLL in the following directories.
- The directory from which the application loaded.
- The system directory. Use the GetSystemDirectory function to get the path of this directory.
- The 16-bit system directory. There is no function that obtains the path of this directory, but it is searched.
- The Windows directory. Use the GetWindowsDirectory function to get the path of this directory.
- The current directory.
- The directories that are listed in the PATH environment variable. Note that this does not include the per-application path specified by the App Paths registry key. The App Paths key is not used when computing the DLL search path.

If we use  LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags, the directories are searched in the following order.

- The directory that contains the DLL (LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR). This directory is searched only for dependencies of the DLL to be loaded.
- The application directory (LOAD_LIBRARY_SEARCH_APPLICATION_DIR).
- Paths explicitly added to the application search path with the AddDllDirectory function (LOAD_LIBRARY_SEARCH_USER_DIRS) or the SetDllDirectory function. If more than one path has been added, the order in which the paths are searched is unspecified.
- The System32 directory (LOAD_LIBRARY_SEARCH_SYSTEM32).

Advantages:
1. The directory that contains the DLL comes first and it's desirable for us, because the dependencies in `lib` should always be preferred.
2. The system directory is considered in the last place. According to some of the bug reports, the DLL load failure are caused by loading the conflicting ones in systemroot.

Neural:
1. The directories in `PATH` are not considered. Similar things happen as described in the previous point. So it may be beneficial for normal users. However, it may cause failures if there are some new dependencies if built from source. (Resolved by making the fallback to `LoadLibraryW` if error code is `126`)

Disadvantages:
1. LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags is only available for Win7/2008 R2 + KB2533623 and up. (Resolved by making the fallback to `LoadLibraryW` if it is not supported)
2. Failure during the call of `LoadLibraryEx` will lead to the OS to pop up a modal dialog, which can block the process if user is using a CLI-only interface. This can be switched off by calling `SetErrorMode`. (Resolved by calling `SetErrorMode`)

Test plan:
Test some common cases (in a new repo maybe) including
1. Python 3.6/3.7/3.8, conda python, conda install
2. Python 3.6/3.7/3.8, conda python, pip install
3. Python 3.6/3.7/3.8, official python, pip install
Plus some corner cases like
1. Conflicting DLLs in systemroot or `PATH`
2. Remove some local dependencies and use global ones

References:
1. https://docs.microsoft.com/en-us/windows/win32/api/errhandlingapi/nf-errhandlingapi-seterrormode
2. https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-loadlibraryexa
3. https://docs.microsoft.com/en-us/windows/win32/dlls/dynamic-link-library-search-order#standard-search-order-for-desktop-applications
",pytorch
38332,supriyar,pr,2020-05-12T17:39:31Z,[quant] Add quantized::conv1d op benchmarck,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'
* #38341 [quant][graphmode] Add quantized::conv1d to graphmode
* **#38332 [quant] Add quantized::conv1d op benchmarck**
* #38283 [quant] Add support for Quantized Conv1d and ConvRELU1d
* #38248 [quant] Add support for quantized::conv1d operator
* #38247 [quant] Implement unsqueeze/squeeze for per-channel qtensor

Summary:

Test Plan:

python -m pt.qconv_test --test QConv1d_N1_IC128_OC256_L64_G1_kernel3_stride1_pad0
Forward Execution Time (us) : 147.844

python -m pt.conv_test --test Conv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu
Forward Execution Time (us) : 470.750

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21553662](https://our.internmc.facebook.com/intern/diff/D21553662)",pytorch
38341,supriyar,pr,2020-05-12T18:41:03Z,[quant][graphmode] Add quantized::conv1d to graphmode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'
* **#38341 [quant][graphmode] Add quantized::conv1d to graphmode**
* #38332 [quant] Add quantized::conv1d op benchmarck
* #38283 [quant] Add support for Quantized Conv1d and ConvRELU1d
* #38248 [quant] Add support for quantized::conv1d operator
* #38247 [quant] Implement unsqueeze/squeeze for per-channel qtensor

Summary:

Test Plan:
python test/test_quantization.py TestQuantizeScriptPTSQOps.test_quantized_conv1d

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21554256](https://our.internmc.facebook.com/intern/diff/D21554256)",pytorch
38352,rohan-varma,pr,2020-05-12T20:11:37Z,Use Future's then() API to fix RPC profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38352 Use Future's then() API to fix RPC profiling**

Fixes the RPC profiling by using the `then()` API added in https://github.com/pytorch/pytorch/pull/37311. Instead of adding a regular callback, we return a new future that completes when the profiling callback is finished. This is transparent to the user as the future still completes with the value of the original future (i.e. the RPC's return value)

To make this work for RRef, we add a `_set_profiling_future` to set the profiling future, and `_get_profiling_future` to retrieve this future and wait on it in the tests.

Re-enabled profiling tests and stress tested them 1000 times to verify the fix

Differential Revision: [D21506940](https://our.internmc.facebook.com/intern/diff/D21506940/)",pytorch
38373,cloudhan,pr,2020-05-13T02:44:22Z,Migrate erfc from TH to ATen (CUDA),"Fixed #24559
Reference #24507",pytorch
38384,cloudhan,pr,2020-05-13T06:49:24Z,Implement logaddexp,"Resolve #38377
Related #38349 

This op should be disambiguated with `logsumexp` which do a reduction on a tensor over a specific axis.",pytorch
38390,peterjc123,pr,2020-05-13T11:14:29Z,[WIP] [DON'T MERGE] Reproduce sccache failure,,pytorch
38403,ssnl,pr,2020-05-13T16:16:38Z,Relax sampler check in BatchSampler,"Since the check was added in https://github.com/pytorch/pytorch/pull/6249, one can not pass an iterable as a sampler to the data loader anymore, which was a very handy feature (e.g., https://github.com/pytorch/pytorch/issues/1337). I think the check should be removed for two-fold reasons:
1. It is too strict. There is no reason that it should not be a general iterable.
2. It is inconsistent. In `DataLoader` (the main place where people use samplers), you can pass a general iterable as `batch_sampler` but not `sampler` due to this check.",pytorch
38407,supriyar,pr,2020-05-13T16:51:22Z,[quant] Return default qconfig when backend is 'none',"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38452 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* **#38407 [quant] Return default qconfig when backend is 'none'**

Summary:
We can still run some quantized tests even when fbgemm/qnnpack isn't enabled
Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21554257](https://our.internmc.facebook.com/intern/diff/D21554257)",pytorch
38423,ssnl,pr,2020-05-13T20:20:21Z,Improve syncbn doc format,,pytorch
38438,supriyar,pr,2020-05-13T21:37:52Z,[quant] Fusion support for conv1d + ReLU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38452 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules
* #38449 [quant] Support for functional quantized::conv1d
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* **#38438 [quant] Fusion support for conv1d + ReLU**
* #38407 [quant] Return default qconfig when backend is 'none'

Summary:
Fusion for PTQ flow in eager mode. Graph mode to follow

Test Plan:
python test/test_quantization.py TestFusion

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21575920](https://our.internmc.facebook.com/intern/diff/D21575920)",pytorch
38441,supriyar,pr,2020-05-13T21:55:56Z,[quant][graphmode] Add support for quantized conv1d + relu fusion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38452 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules
* #38449 [quant] Support for functional quantized::conv1d
* **#38441 [quant][graphmode] Add support for quantized conv1d + relu fusion**
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'

Summary:

Test Plan:
python test/test_quantization.py test_quantized_conv1d_relu

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21575919](https://our.internmc.facebook.com/intern/diff/D21575919)",pytorch
38449,supriyar,pr,2020-05-13T23:34:41Z,[quant] Support for functional quantized::conv1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38452 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules
* **#38449 [quant] Support for functional quantized::conv1d**
* #38441 [quant][graphmode] Add support for quantized conv1d + relu fusion
* #38438 [quant] Fusion support for conv1d + ReLU
* #38407 [quant] Return default qconfig when backend is 'none'

Summary:
Also update docs to reflect conv1d op support

Test Plan:
python test/test_quantization.py TestQuantizedFunctional.test_conv1d_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21575921](https://our.internmc.facebook.com/intern/diff/D21575921)",pytorch
38452,supriyar,pr,2020-05-14T01:08:28Z,[quant] Support for fused ConvBn1d and ConvBnRelu1d modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38452 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules**

Summary:

Test Plan:
python test/test_quantization.py TestFused

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21632878](https://our.internmc.facebook.com/intern/diff/D21632878)",pytorch
38455,bharatr21,pr,2020-05-14T02:39:31Z,DOC: Correct upsample doc to match interpolation,Fix #38334 and correct the docs of `torch.nn.functional.upsample`,pytorch
38508,supriyar,pr,2020-05-14T20:49:00Z,[quant] Support empty batch input for quantized ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38508 [quant] Support empty batch input for quantized ops**

Summary:

Test Plan:
python test/test_quantization.py TestQuantizedOps.test_empty_batch

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21581937](https://our.internmc.facebook.com/intern/diff/D21581937)",pytorch
38534,ngimel,pr,2020-05-15T03:27:11Z,"Revert ""Vectorize non-persistent Softmax kernels (#36485)""","This reverts commit c879c6fb98ec38197c86c703e1011c8b94f14c59.
(it produces incorrect results)
",pytorch
38577,rohan-varma,pr,2020-05-15T22:09:13Z,Larger timeout for operations against ProcessGroup for RPC,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38577 larger timeout for operations against ProcessGroup for RPC**

We don't want to limit a timeout to 30 min since there could be no
operations within that time frame. Bump to 2^31 -1. We should bump this to the max value allowed by std::chrono::milliseconds, but in the gloo code this line would then overflow: https://github.com/facebookincubator/gloo/blob/master/gloo/transport/tcp/pair.cc#L896. Filed a issue in gloo to fix this: https://github.com/facebookincubator/gloo/issues/256

closes https://github.com/pytorch/pytorch/issues/33107
closes https://github.com/pytorch/pytorch/issues/38531

Differential Revision: [D21602425](https://our.internmc.facebook.com/intern/diff/D21602425/)",pytorch
38584,supriyar,pr,2020-05-15T23:24:20Z,[quant] Remove TensorListObserver,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38584 [quant] Remove TensorListObserver**

Summary:
All observers will support tensor lists in future PR

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21623464](https://our.internmc.facebook.com/intern/diff/D21623464)",pytorch
38590,rohan-varma,pr,2020-05-16T01:33:21Z,Implement timeout support for RRefs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38590 Implement timeout support for RRefs**

This PR implements timeout semantics for RRef for parity with rpc_sync and rpc_async. How it works:

- Timeout parameter is added to rpc.remote. If the rpc.remote call times out, note that the error won't be raised to the user in that call, as it is not blocking (similar to rpc_async). Instead, the timeout error will be raised the next time the RRef is used (either by pickling or to_here call).
- Error handling semantics are added to RRef to deal with the timeout errors. Previously, if there was an error creating the OwnerRRef, the callback on the local user would throw an error in a callback, resulting in an `std::terminate`. Instead of this, the error is now caught and surfaced to the user the next time the RRef is used. As part of this, we have added an `RPCErrorType` enum and defined RRef error handlers to handle the `RPCErrorrTypes` (currently just timeout and unknown)
- A timeout parameter is added to `to_here()` which gives the user control over the max amount of time it can block for.
- Before blocking, `to_here` checks if the RRef creation on the remote node has timed out, and if so, throws with that information.
- `ctx.prepareChildForFork()` which is called when the RRef is pickled (i.e. used as an arg over RPC) checks if the `rpc.remote()` call had timed out, and if so, raises that error to the user.
- Tests are added, primarily via delay injection.

Differential Revision: [D21588165](https://our.internmc.facebook.com/intern/diff/D21588165/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21588165/)!",pytorch
38615,ngimel,pr,2020-05-16T21:49:29Z,fix clip_grad_norm to work with parameters on the different devices,"Per title. 
We move all the individual gradient norms to a single device before stacking (no-op if all the gradients are already on a single device), `clip_coef` is copied to the device of gradient, which may be suboptimal as there could be multiple copies, but no worse than when we were synchronizing for each parameter. In a simple case of all gradients on a single device, there should be no synchronization. 
Also, we no longer error out if parameter list is empty or none of the parameters have gradients, and return 0 total_norm instead. 
Fixes #38605",pytorch
38625,ssnl,pr,2020-05-17T15:13:41Z,"Do not use ""buffer"" in reentrant autograd err msg","`buffer` is also used to refer to `nn.Module`'s buffer. Wording is changed to reduce confusion between the two.

",pytorch
38637,peterjc123,pr,2020-05-18T09:14:18Z,Make find_first_set works on x86 MSVC,"Fixes https://github.com/pytorch/pytorch/issues/38322#issuecomment-630031072.
Tested locally.",pytorch
38706,peterjc123,pr,2020-05-19T04:31:20Z,Fix find_first_set for x86 MSVC (Updated),,pytorch
38736,supriyar,pr,2020-05-19T20:38:13Z,[quant] Onnx export of quantized models with new API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38736 [quant] Onnx export of quantized models with new API**

Summary:
qconv2d and qlinear APIs were changed recently so updating the scale code accordingly

Test Plan:
python test/onnx/test_pytorch_onnx_caffe2_quantized.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21647724](https://our.internmc.facebook.com/intern/diff/D21647724)",pytorch
38748,rohan-varma,pr,2020-05-20T00:37:22Z,Allow profiler to be enabled remotely with RPC,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38748 Allow profiler to be enabled remotely with RPC**

RFC: https://github.com/pytorch/pytorch/issues/39675

This diff contains the message scaffolding and profiler changes in order to be able to remotely run the profiler across different nodes and aggregate the results on a single node.

As discussed, we have implemented this by creating new message types, that similar to autograd messages, wrap the profiling information with the original message, and send this new message over the wire. On the receiving end, this wrapped message is detected, we fetch the original message from it, and process the original message with the profiler enabled. When sending a response with profiling information, we serialize the profiled `Events` and send them back over RPC. When such a message is received, the events profiled on the remote node are stored (added back to the local profiler).

Changes in this PR:
- New message types (run_with_profiling_req, run_with_profiling_resp) to send profiling info over the wire. Message parsing logic is added to handle these wrapped types.
- Handling of sending profiler data over the wire, in particular, the attributes of the `ProfilerConfig` and the serialized profiled `Event`s
- The logic for wrapping RPC messages is deduped with that in `rpc_with_autograd`, and the common payload wrapping/unwrapping logic is moved to helper functions in `rpc/utils.cpp`
- Changes in `autograd/utils.cpp` to detect if we have enabled the profiler and are sending an RPC, if so, uses the above new message types
- Changes in request_callback to parse and turn on the profiler in a thread-local fashion
- Serialization and deserialization of profiling `Events`, and support to add the remote events to the thread-local profiler
- Introduction of the concept of `node_id`, which as discussed with @ilia-cher , will be used along with the `Event`s handle attribute to distinguish between events. When there are events from different nodes, this node information is rendered in the profile output (e.g. when printing tables), otherwise, it is not, since it is irrelevant.
- Some changes to profiler.cpp to add useful helper methods/guards
- toHere() is now profiled for RRefs
- Unittests

### Example use case
```
# Assume that RPC is initialized across worker_1 and worker_2
# On worker_1
with torch.autograd.profiler.profile(profile_memory=True) as p:
    rpc.rpc_sync(worker_2, torch.add, args=(torch.tensor(1), torch.tensor(1))
print(p.key_averages().table()) # should show that torch.add ran on remote node, in addition to the local profiling output
p.export_chrome_trace(""/tmp/dist_trace.json"") # should show torch.add in trace, and indicate that it ran on a remote node
```

### Example outputs
The printed table will look like:
```
  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                                                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Node ID
----------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
rpc_async#udf_with_torch_ops(worker1 -> worker2)      0.00%            0.000us          0                4.549ms          4.549ms          1                1
ones                                                  8.31%            29.587us         20.72%           73.806us         36.903us         2                2
empty                                                 16.42%           58.487us         16.42%           58.487us         9.748us          6                2
fill_                                                 5.77%            20.557us         6.78%            24.136us         12.068us         2                2
is_complex                                            1.00%            3.579us          1.00%            3.579us          1.789us          2                2
add                                                   18.33%           65.315us         22.42%           79.883us         79.883us         1                2
mul                                                   13.40%           47.737us         15.98%           56.943us         56.943us         1                2
relu                                                  7.34%            26.154us         15.34%           54.637us         54.637us         1                2
threshold                                             5.77%            20.561us         8.00%            28.483us         28.483us         1                2
sigmoid                                               11.06%           39.392us         25.54%           90.965us         90.965us         1                2
sigmoid_out                                           10.46%           37.260us         12.59%           44.865us         44.865us         1                2
resize_                                               2.13%            7.605us          2.13%            7.605us          7.605us          1                2
----------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
```

The trace would look like: 
![dist_trace_correct_offset](https://user-images.githubusercontent.com/8039770/84076181-3059e480-a98a-11ea-8df5-75c36d66c38a.png)
(Note that the events on Node 2 are offset by an approximate time that is described below).

Result of `test_stress_light_rpc` before and after this diff:

```
> Rank 2 finished testing 1000 times in 0.3938124179840088 seconds.
> Rank 0 finished testing 1000 times in 0.37873053550720215 seconds.
> Rank 3 finished testing 1000 times in 0.3726627826690674 seconds.
> Rank 1 finished testing 1000 times in 0.38887619972229004 seconds.
```

After
```
Rank 1 finished testing 1000 times in 0.3706204891204834 seconds.
Rank 2 finished testing 1000 times in 0.37796807289123535 seconds.
Rank 3 finished testing 1000 times in 0.39801454544067383 seconds.
Rank 0 finished testing 1000 times in 0.40166211128234863 seconds.
```
Differential Revision: [D19510010](https://our.internmc.facebook.com/intern/diff/D19510010/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19510010/)!",pytorch
38749,supriyar,pr,2020-05-20T00:40:12Z,[quant] Support for fused ConvBn1d and ConvBnRelu1d modules (#38452),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38749 [quant] Support for fused ConvBn1d and ConvBnRelu1d modules (#38452)**

Summary:

Test Plan:
python test/test_quantization.py TestFused

Differential Revision: [D21654659](https://our.internmc.facebook.com/intern/diff/D21654659)",pytorch
38804,rohan-varma,pr,2020-05-20T19:40:11Z,Remove unneeded const from process group agent header,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38804 Remove unneeded const from process group agent header**

This is only needed in the process group agent definition, and
removing it from the header file prevents other translation units that include
it from having this constant.

Differential Revision: [D21668514](https://our.internmc.facebook.com/intern/diff/D21668514/)",pytorch
38844,rohan-varma,pr,2020-05-21T00:45:30Z,Log incorrect device in ProcessGroupGloo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38844 Log incorrect device in ProcessGroupGloo**

Enhances error message in ProcessGroupGloo to log the unsupported
device. Been seeing a few issues with this and this will provide more debug
information.

Differential Revision: [D21676881](https://our.internmc.facebook.com/intern/diff/D21676881/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D21676881/)!",pytorch
38860,peterjc123,pr,2020-05-21T06:30:25Z,Fix cpp extension build failure if path contains space,,pytorch
38862,peterjc123,pr,2020-05-21T08:34:13Z,Better handling for msvc env when compiling cpp extensions,"Fixes https://github.com/pytorch/pytorch/issues/38861#issuecomment-631934636.
1. Error out if msvc env is activated but `DISTUTILS_USE_SDK` is not set.
2. Attempt to activate msvc env before running ninja build

",pytorch
38888,benoitsteiner,pr,2020-05-21T20:13:57Z,Made sure torchscript compiles in optimized mode,"Test Plan: ran the build

Differential Revision: D21045046

",pytorch
38890,ssnl,pr,2020-05-21T21:00:23Z,Fix broken reference in sync bn doc,,pytorch
38891,supriyar,pr,2020-05-21T21:08:35Z,[quant] Support save/load state_dict for quantized dynamic RNNs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38891 [quant] Support save/load state_dict for quantized dynamic RNNs**

Summary:
Previously dynamic LSTM modules weren't able to save/load from state_dict since CellParams used in RNNs isn't serializable from python
This change uses a class similar to LinearPackedParams to save/load the weight tensors

Test Plan:
python test/test_quantization.py TestSerialization

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
38903,peterjc123,pr,2020-05-22T02:51:12Z,Use old circleci windows image,,pytorch
38907,peterjc123,pr,2020-05-22T03:51:41Z,Use old circleci windows image (with full testing),,pytorch
38908,peterjc123,pr,2020-05-22T04:12:16Z,Test whether CUDA image is broken or not,,pytorch
38909,peterjc123,pr,2020-05-22T04:18:42Z,Use old circleci windows image for both CPU and CUDA,,pytorch
38943,ngimel,pr,2020-05-22T23:14:10Z,restore proper cuda assert behavior with DNDEBUG,"Per title. #32719 essentially disabled asserts in cuda kernels in release build. Asserts in cuda kernels are typically used to prevent invalid reads/writes, so without asserts invalid read/writes are silent errors in most cases (sometimes they would still cause ""illegal memory access"" errors, but because of caching allocator this usually won't happen). 
We don't need 2 macros, CUDA_ALWAYS_ASSERT and CUDA_KERNEL_ASSERT because all current asserts in cuda kernels are important to prevent illegal memory accesses, and they should never be disabled. 
This PR removes macro CUDA_ALWAYS_ASSERT and instead makes CUDA_KERNEL_ASSERT (that is commonly used in the kernels) an asserttion both in release and debug builds. 
Fixes #38771",pytorch
38945,ngimel,pr,2020-05-23T01:44:13Z,improve accuracy of logsoftmax computation on cuda,"Fixes #38839. Previously, if magnitude of input values was large, when computing `max+log(sum)` the `log(sum)` value was essentially ignored, now the result is computed as
`x-max-log(sum)` which has a better chance of preserving accuracy. ",pytorch
38949,peterjc123,pr,2020-05-23T07:28:16Z,Test migrating CUDA >= 10.1 binary jobs to use VS 2019,,pytorch
38950,peterjc123,pr,2020-05-23T07:47:33Z,Disable some unsupported module for 32-bit build,Fixes https://github.com/pytorch/pytorch/issues/38322#issuecomment-632976523 and https://github.com/pytorch/pytorch/issues/38322#issuecomment-628698852.,pytorch
38956,peterjc123,pr,2020-05-23T14:23:46Z,Test migrate changes with VS 2017,,pytorch
38957,peterjc123,pr,2020-05-23T14:28:06Z,Expose VC_YEAR in Windows binary test jobs,To make it configurable  in https://github.com/pytorch/builder/pull/445.,pytorch
38959,peterjc123,pr,2020-05-23T15:47:00Z,Migrate Windows build jobs to VS 2019 for CUDA >= 10.1,"This PR relies on https://github.com/pytorch/pytorch/pull/38957 and https://github.com/pytorch/builder/pull/445.
Tested with pytorch/pytorch#38949 and pytorch/pytorch#38956.
Will need a rebase after the dependent commits go in",pytorch
39041,supriyar,pr,2020-05-26T23:07:36Z,[quant] Add reduce_range argument for qlinear_dynamic,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39187 [quant] Enable per-channel quantization for quantized nn.LSTM
* #39186 [quant] Make reduce_range default for RNNs
* #39125 [quant] Dynamic Linear module to use reduce_range
* **#39041 [quant] Add reduce_range argument for qlinear_dynamic**

Summary:
reduce_range option restricts the activation tensor to 7 bits instead of 8.
This is necessary to enable per channel quant for RNNs and LSTMs

Test Plan:
python test/test_quantization.py TestDynamicQuantizedLinear

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21769691](https://our.internmc.facebook.com/intern/diff/D21769691)",pytorch
39047,ngimel,pr,2020-05-27T03:32:27Z,fix asserts in cuda code,"Gets rid of some in-kernel asserts where they can be replaced with static_asserts
Replaces bare in-kernel `assert` in one case with `CUDA_KERNEL_ASSERT` where necessary
replaces host code `assert`s with `TORCH_INTERNAL_ASSERT`
Another group of asserts is in fractional max pooling kernels which should be fixed regardless #39044, the problems there are not just asserts. 
I've audited remaining cases of in-kernel asserts, and they are more like `TORCH_INTERNAL_ASSERT`, so they should not happen with invalid user data. I think it's ok to leave them as is. 
",pytorch
39054,yaochengji,pr,2020-05-27T07:11:26Z,fix pybind11 dependency,"```
./torch/csrc/autograd/python_anomaly_mode.h:3:10: fatal error: pybind11/pybind11.h: No such file or directory
```
I tried to build _C.so but got the error above. This PR is my fix.",pytorch
39056,peterjc123,pr,2020-05-27T08:29:10Z,"Revert ""Use old circleci windows image for both CPU and CUDA (#38909)""","This reverts commit f3f3097a4c54d426cf4a54ba80a9a6f66cdafa17.

",pytorch
39057,peterjc123,pr,2020-05-27T09:11:43Z,Fix Windows binary jobs after migrating to the new circleci image,"We cannot remove the VS2019 installation anymore because `VS 2019 BuildTools` is installed in the new image. So alternatively for getting more disk space, we could just remove the cached packages. But be careful that we need to preserve `_Instances`, otherwise, vswhere won't be able to detect installed vs builds.",pytorch
39105,supriyar,pr,2020-05-27T22:03:13Z,[quant] Add save/load state_dict to quantized dynamic RNNs,"Summary:
Previously dynamic LSTM modules weren't able to save/load from state_dict since PackedParameter used in RNNs isn't serializable from python


Test Plan:
python test/test_quantization.py TestSerialization

Reviewers:

Subscribers:

Tasks:

Tags:

",pytorch
39125,supriyar,pr,2020-05-28T03:51:45Z,[quant] Dynamic Linear module to use reduce_range,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39187 [quant] Enable per-channel quantization for quantized nn.LSTM
* #39186 [quant] Make reduce_range default for RNNs
* **#39125 [quant] Dynamic Linear module to use reduce_range**
* #39041 [quant] Add reduce_range argument for qlinear_dynamic

Summary:
switch to setting reduce_range to true for version > 3.
Models serialized with older state_dict will have version <=3 so will be run with reduce_range=false

Verified with backward compatibility tests (works with no changes to these tests)

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21769689](https://our.internmc.facebook.com/intern/diff/D21769689)",pytorch
39126,rohan-varma,pr,2020-05-28T03:56:40Z,Remove variable shadowing in TensorPipe lambda,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39126 Remove variable shadowing in TensorPipe lambda**

`futureResponseMessage` is shadowed in the `pipeWrite` lambda which
creates some confusion, since it is used in the initial error handling but then a future of the same name is created when marking the future as completed. This change removes this by getting rid of the `futureResponseMessage` capture, instead capturing the message id. This change also makes it so that we don't need to copy it into the lambda.


Differential Revision: [D22127398](https://our.internmc.facebook.com/intern/diff/D22127398/)",pytorch
39136,peterjc123,pr,2020-05-28T09:56:30Z,Make collect_env more robust on Windows,Fixes https://github.com/pytorch/pytorch/issues/39133.,pytorch
39144,hgaiser,pr,2020-05-28T14:30:21Z,Add option to allow loading state dict with mismatching shapes.,"This PR adds an option to override the choice of loading weights or not and optionally to produce an error. This can be very useful if you want to do transfer learning, transferring weights from model A to model B where A and B have a different output (number of classes, for example).

Example usage:

```python
import torch

# Base model.
a = torch.nn.Sequential(
    torch.nn.Conv2d(7, 5, 1),
    torch.nn.Conv2d(5, 3, 1)
)

# Model with an additional layer, requires `strict=False`.
b = torch.nn.Sequential(
    torch.nn.Conv2d(7, 5, 1),
    torch.nn.Conv2d(5, 3, 1),
    torch.nn.Conv2d(3, 3, 1)
)

# Model with an additional layer and one of the layers has a different shape of weights,
# requires `strict=False` and the optional callback introduced in this PR: `tensor_check_fn`.
c = torch.nn.Sequential(
    torch.nn.Conv2d(7, 5, 1),
    torch.nn.Conv2d(3, 3, 1),
    torch.nn.Conv2d(3, 3, 1)
)

# This line works without this PR.
b.load_state_dict(a.state_dict(), strict=False)

# The next part doesn't work without this PR, because c has a layer that is of different shape.
def tensor_check_fn(param, input_param, error_msgs):
	if param.shape != input_param.shape:
		return False
	return True

c.load_state_dict(a.state_dict(), strict=False, tensor_check_fn=tensor_check_fn)
```

This change should be backwards compatible, unless some custom layer implements the `_load_from_state_dict` method, because an argument has been added there.",pytorch
39149,ssnl,pr,2020-05-28T15:01:40Z,[1.5.1] Change len(DataLoader) for IterableDataset (#38925),"Summary:
Fix https://github.com/pytorch/pytorch/issues/36176

One-liner change to ensure that ```len(loader) == (len(dataset) // batch_size)``` for IterableDataset.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38925

Differential Revision: D21731587

Pulled By: ezyang

fbshipit-source-id: 59a086165a004c0c1c8a1ee0776b1444bd26de23

",pytorch
39180,ngimel,pr,2020-05-28T17:42:30Z,Restore thrust path for 1d tensors cumulative ops,"Restores thrust path for computing prefix sums for tensors with a single non-degenerate dimension. Benchmark on P100  before:
```
import time
import torch

l = 4000
t=1000
for _ in range(6):
    for dtype in (torch.half, torch.float, torch.double):
        a = torch.randn(l, device=""cuda"", dtype=dtype)
        print(f'torch.cumsum(a) a.numel() == {l} for {t} times {dtype}')
        # dry run
        torch.cumsum(a, 0)
        torch.cuda.synchronize()
        # Iterate
        start = time.time()
        for _ in range(t):
            torch.cumsum(a, 0)
        # Final Synchronize Before Teardown
        torch.cuda.synchronize()
        end = time.time()
        elapsed = end - start
        bw = t * l * 2 * a.element_size() * 1e-9/elapsed
        print(f'Time {elapsed} bandwidth {bw}')
    l *= 2
```
```
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float16
Time 0.29149866104125977 bandwidth 0.05488875984145705
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float32
Time 0.24511313438415527 bandwidth 0.130551959528402
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float64
Time 0.25238871574401855 bandwidth 0.25357710550304885
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float16
Time 0.5812790393829346 bandwidth 0.05505101307965633
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float32
Time 0.4885847568511963 bandwidth 0.13099057861007293
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float64
Time 0.5031211376190186 bandwidth 0.2544118909528429
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float16
Time 1.1607651710510254 bandwidth 0.05513604439220951
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float32
Time 0.9755356311798096 bandwidth 0.13120996907637011
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float64
Time 1.0045702457427979 bandwidth 0.25483533987283175
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float16
Time 2.3198938369750977 bandwidth 0.055174938594129294
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float32
Time 1.949366569519043 bandwidth 0.13132471029456586
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float64
Time 2.00749135017395 bandwidth 0.2550446854755488
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float16
Time 4.63812518119812 bandwidth 0.055194715536735495
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float32
Time 3.897014856338501 bandwidth 0.13138261435345344
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float64
Time 4.013219356536865 bandwidth 0.2551567479938705
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float16
Time 9.274584770202637 bandwidth 0.05520462777427539
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float32
Time 7.792156934738159 bandwidth 0.1314141910354645
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float64
Time 8.02474856376648 bandwidth 0.2552104883693396
```
after:
```
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float16
Time 0.033731937408447266 bandwidth 0.47432792864109924
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float32
Time 0.031197071075439453 bandwidth 1.025737317539167
torch.cumsum(a) a.numel() == 4000 for 1000 times torch.float64
Time 0.03245425224304199 bandwidth 1.972006611667389
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float16
Time 0.034340858459472656 bandwidth 0.931834596906329
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float32
Time 0.031183481216430664 bandwidth 2.0523686741645197
torch.cumsum(a) a.numel() == 8000 for 1000 times torch.float64
Time 0.031975507736206055 bandwidth 4.003063878015136
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float16
Time 0.032624006271362305 bandwidth 1.9617455767895642
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float32
Time 0.03129267692565918 bandwidth 4.0904138787514
torch.cumsum(a) a.numel() == 16000 for 1000 times torch.float64
Time 0.03260397911071777 bandwidth 7.851802356107085
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float16
Time 0.032918691635131836 bandwidth 3.888368390176069
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float32
Time 0.030851364135742188 bandwidth 8.29785026275116
torch.cumsum(a) a.numel() == 32000 for 1000 times torch.float64
Time 0.037447452545166016 bandwidth 13.6724921243299
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float16
Time 0.03391098976135254 bandwidth 7.549175114073387
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float32
Time 0.03214144706726074 bandwidth 15.929587704267457
torch.cumsum(a) a.numel() == 64000 for 1000 times torch.float64
Time 0.034329891204833984 bandwidth 29.828233182859922
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float16
Time 0.03589606285095215 bandwidth 14.263402705915954
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float32
Time 0.033178091049194336 bandwidth 30.863740728231736
torch.cumsum(a) a.numel() == 128000 for 1000 times torch.float64
Time 0.03487515449523926 bandwidth 58.72375419238841
```",pytorch
39186,supriyar,pr,2020-05-28T18:41:28Z,[quant] Make reduce_range default for RNNs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39187 [quant] Enable per-channel quantization for quantized nn.LSTM
* **#39186 [quant] Make reduce_range default for RNNs**
* #39125 [quant] Dynamic Linear module to use reduce_range
* #39041 [quant] Add reduce_range argument for qlinear_dynamic

Summary:
We always set reduce_range to true for RNNs since it improves accuracy and can enable use of per-channel quant
Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21770192](https://our.internmc.facebook.com/intern/diff/D21770192)",pytorch
39187,supriyar,pr,2020-05-28T18:41:39Z,[quant] Enable per-channel quantization for quantized nn.LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39187 [quant] Enable per-channel quantization for quantized nn.LSTM**
* #39186 [quant] Make reduce_range default for RNNs
* #39125 [quant] Dynamic Linear module to use reduce_range
* #39041 [quant] Add reduce_range argument for qlinear_dynamic

Summary:

Test Plan:
python test/test_quantization.py TestPostTrainingDynamic.test_per_channel_lstm_quantize

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21770193](https://our.internmc.facebook.com/intern/diff/D21770193)",pytorch
39218,peterjc123,pr,2020-05-29T04:01:14Z,Implement CUDA_KERNEL_ASSERT for MSVC,Tested locally on CPU/GPU + Debug/Release.,pytorch
39220,peterjc123,pr,2020-05-29T04:22:11Z,Followup for cuda assert cleanups,,pytorch
39249,peterjc123,pr,2020-05-29T15:34:58Z,Fix windows upload jobs,Fixes https://github.com/pytorch/pytorch/issues/39247.,pytorch
39264,ssnl,pr,2020-05-29T18:39:46Z,[doc] [distributed] fix typo,,pytorch
39288,peterjc123,pr,2020-05-30T02:17:02Z,[v1.5.1] Implement CUDA_KERNEL_ASSERT for MSVC (#39218),"Summary:
Tested locally on CPU/GPU + Debug/Release.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39218

Differential Revision: D21786500

Pulled By: malfet

fbshipit-source-id: 7e871003d3509436952932b5ff3599e36bb8f205

",pytorch
39290,peterjc123,pr,2020-05-30T02:31:53Z,[v1.5.1] Delay loading the cuda library on Windows (#37811),"Summary:
so we can import torch compiled with cuda on a CPU-only machine.
need tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37811

Differential Revision: D21417082

Pulled By: ezyang

fbshipit-source-id: 7a521b651bca7cbe38269915bd1d1b1bb756b45b

",pytorch
39291,peterjc123,pr,2020-05-30T02:35:12Z,[v1.5.1] Using LoadLibraryEX [Reland] (#38302),"Summary:
This reverts commit 1ab4f35499aa933677152aca6a1ba2cbe86639f8.

Without this PR, the OS try to find the DLL in the following directories.
- The directory from which the application loaded.
- The system directory. Use the GetSystemDirectory function to get the path of this directory.
- The 16-bit system directory. There is no function that obtains the path of this directory, but it is searched.
- The Windows directory. Use the GetWindowsDirectory function to get the path of this directory.
- The current directory.
- The directories that are listed in the PATH environment variable. Note that this does not include the per-application path specified by the App Paths registry key. The App Paths key is not used when computing the DLL search path.

If we use  LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags, the directories are searched in the following order.

- The directory that contains the DLL (LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR). This directory is searched only for dependencies of the DLL to be loaded.
- The application directory (LOAD_LIBRARY_SEARCH_APPLICATION_DIR).
- Paths explicitly added to the application search path with the AddDllDirectory function (LOAD_LIBRARY_SEARCH_USER_DIRS) or the SetDllDirectory function. If more than one path has been added, the order in which the paths are searched is unspecified.
- The System32 directory (LOAD_LIBRARY_SEARCH_SYSTEM32).

Advantages:
1. The directory that contains the DLL comes first and it's desirable for us, because the dependencies in `lib` should always be preferred.
2. The system directory is considered in the last place. According to some of the bug reports, the DLL load failure are caused by loading the conflicting ones in systemroot.

Neural:
1. The directories in `PATH` are not considered. Similar things happen as described in the previous point. So it may be beneficial for normal users. However, it may cause failures if there are some new dependencies if built from source. (Resolved by making the fallback to `LoadLibraryW` if error code is `126`)

Disadvantages:
1. LoadLibraryEx with LOAD_LIBRARY_SEARCH_* flags is only available for Win7/2008 R2 + KB2533623 and up. (Resolved by making the fallback to `LoadLibraryW` if it is not supported)
2. Failure during the call of `LoadLibraryEx` will lead to the OS to pop up a modal dialog, which can block the process if user is using a CLI-only interface. This can be switched off by calling `SetErrorMode`. (Resolved by calling `SetErrorMode`)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38302

Test Plan:
Test some common cases (in a new repo maybe) including
1. Python 3.6/3.7/3.8, conda python, conda install
2. Python 3.6/3.7/3.8, conda python, pip install
3. Python 3.6/3.7/3.8, official python, pip install
Plus some corner cases like
1. Conflicting DLLs in systemroot or `PATH`
2. Remove some local dependencies and use global ones

References:
1. https://docs.microsoft.com/en-us/windows/win32/api/errhandlingapi/nf-errhandlingapi-seterrormode
2. https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-loadlibraryexa
3. https://docs.microsoft.com/en-us/windows/win32/dlls/dynamic-link-library-search-order#standard-search-order-for-desktop-applications

Differential Revision: D21524090

Pulled By: malfet

fbshipit-source-id: 0cf5e260c91759b0af8c7aa0950a488e3b653ef5

",pytorch
39313,supriyar,pr,2020-05-31T22:23:54Z,[quant] Make reduce_range default for RNNs and enable per-channel quant,"Summary:
We always set reduce_range to true for RNNs since it improves accuracy and can enable use of per-channel quant

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

",pytorch
39334,peterjc123,pr,2020-06-01T16:22:45Z,Fix return types of Windows API functions in __init__.py,Fixes https://github.com/pytorch/pytorch/issues/39327.,pytorch
39344,rohan-varma,pr,2020-06-01T18:40:06Z,[v1.5.1 cherry pick] fix the device inconsistency for import convert_sync_batchnorm,"Original PR: https://github.com/pytorch/pytorch/pull/38729
This fixes the device inconsistency reported in #37930

",pytorch
39376,peterjc123,pr,2020-06-02T00:51:16Z,More fixes about using Windows API through ctypes,Representation of `NULL` using `c_void_p` is `None` in ctypes.,pytorch
39412,supriyar,pr,2020-06-02T21:50:54Z,[quant][graphmode] Dynamic Quant : Do not depend on input shapes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39458 [quant][graphmode] Dynamic quant: Insert observers for module output
* **#39412 [quant][graphmode] Dynamic Quant : Do not depend on input shapes**

Summary:
This PR introduces changes to enable running the weight observer standalone in the graph
It extracts the nodes from the graph that correspond to the observed weight value and adds all the related nodes to a new subgraph
The subgraph is then executed using GraphFunction

Test Plan:
python test/test_quantization.py TestGraphMostPostTrainingStatic
python test/test_quantization.py TestQuantizeDynamicScript

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21872940](https://our.internmc.facebook.com/intern/diff/D21872940)",pytorch
39458,supriyar,pr,2020-06-03T18:20:17Z,[quant][graphmode] Dynamic quant: Insert observers for module output,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39458 [quant][graphmode] Dynamic quant: Insert observers for module output**
* #39412 [quant][graphmode] Dynamic Quant : Do not depend on input shapes

Summary:
Previously if we had a CallMethod followed by a CallFunction, we didn't check for observers at output of CallMethod since it was handled separately.
This change makes it default to check the outputs of all nodes to identify values that need observers

Test Plan:
python test/test_quantization.py test_dynamic_shared_weights

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21872939](https://our.internmc.facebook.com/intern/diff/D21872939)",pytorch
39513,peterjc123,pr,2020-06-04T10:05:50Z,Fixes caffe2 loading issues on Windows,"Addresses https://github.com/pytorch/pytorch/issues/27840#issuecomment-638715422.
Contains a bunch of fixes (https://github.com/pytorch/pytorch/pull/39376 + https://github.com/pytorch/pytorch/pull/39334 + https://github.com/pytorch/pytorch/pull/38302 + https://github.com/pytorch/pytorch/pull/35362)",pytorch
39515,peterjc123,pr,2020-06-04T10:46:23Z,Fix caffe2 import issues on Windows (v1.5.1),"Like https://github.com/pytorch/pytorch/pull/35362 and https://github.com/pytorch/pytorch/pull/33856, but for caffe2. I think this is safe because those two PRs are already in v1.5.0.
Corresponding PR in master: https://github.com/pytorch/pytorch/pull/39513.",pytorch
39528,zheng-xq,pr,2020-06-04T17:42:50Z,Fast tanh for the LLVM backend.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39528 Fast tanh for the LLVM backend.**

Differential Revision: [D21927791](https://our.internmc.facebook.com/intern/diff/D21927791)",pytorch
39530,rohan-varma,pr,2020-06-04T18:44:18Z,Cleanup rref_impl,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39602 Remove hacky double registration of to_here op in reg_distributed_ops
* #39531 Enable RRef timeout for tensorpipe
* **#39530 Cleanup rref_impl**

Some cleanups for consistency and code reuse. Uses torch_check instead
of explicitly throwing runtime error. Calls RRefContext::handleError() for
default error handler fallback.

Differential Revision: [D21881244](https://our.internmc.facebook.com/intern/diff/D21881244/)",pytorch
39531,rohan-varma,pr,2020-06-04T18:44:27Z,Enable RRef timeout for tensorpipe,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39531 Enable RRef timeout for tensorpipe**
* #39530 Cleanup rref_impl

Enables RRef timeout support in TP agent by having TP agent mark
timeout errors with `makeRPCError` API. Also does some refactoring so TP agent
can print out the timeout for each future that has timed out.

Differential Revision: [D21881475](https://our.internmc.facebook.com/intern/diff/D21881475/)",pytorch
39602,rohan-varma,pr,2020-06-05T21:27:57Z,Remove hacky double registration of to_here op in reg_distributed_ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39602 Remove hacky double registration of to_here op in reg_distributed_ops**

This was added as a part of
https://github.com/pytorch/pytorch/pull/38590 but we can use default arguments
here. We use fmt:;format to bind the default value to the rpc timeout at
runtime.

Differential Revision: [D21912719](https://our.internmc.facebook.com/intern/diff/D21912719/)",pytorch
39604,supriyar,pr,2020-06-05T21:37:17Z,[quant] Add reduce_range params for quantized_lstm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39666 [quant] Enable per-channel quantization for LSTM Modules
* **#39604 [quant] Add reduce_range params for quantized_lstm**

Summary:
This change preserves BC for older models that are saved with reduce_range set to false.
Newer models will use the version information in RNN module to toggle reduce_range parameter

Internally this is implemented using a new CellParams type that calls the linear functions with reduce_range option set to true.
New models serialized will use the CellParams struct for the `__getstate__` and `__setstate__` calls. Older models using QuantizedCellParamsDynamic will continue to use their original serialization/de-serialization methods

tested using LSTM BC test and test_quantized_rnn

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21977600](https://our.internmc.facebook.com/intern/diff/D21977600)",pytorch
39622,peterjc123,pr,2020-06-06T08:25:40Z,Fix dll load failure in virtual environments on Windows,Fixes https://github.com/pytorch/pytorch/issues/39620.,pytorch
39627,peterjc123,pr,2020-06-06T11:20:33Z,Fix circleci postnightly jobs,Fixes https://github.com/pytorch/pytorch/issues/39626.,pytorch
39628,ssnl,pr,2020-06-06T15:46:38Z,Improve DistributedSampler docs and add seed option,,pytorch
39635,peterjc123,pr,2020-06-07T09:20:44Z,[TEST ONLY] Test sccache built in 2020/06/07,,pytorch
39638,peterjc123,pr,2020-06-07T12:40:20Z,Fix smoke test jobs,Fixes https://github.com/pytorch/pytorch/issues/39626.,pytorch
39646,ssnl,pr,2020-06-08T03:20:57Z,update convert_sync_batchnorm docs,fix some inaccuracies,pytorch
39653,peterjc123,pr,2020-06-08T09:58:03Z,[TEST ONLY] Test MSVC ASAN build,,pytorch
39659,peterjc123,pr,2020-06-08T14:40:08Z,[TEST ONLY] Test sccache built in 2020/06/08,,pytorch
39666,supriyar,pr,2020-06-08T16:52:52Z,[quant] Enable per-channel quantization for LSTM Modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39666 [quant] Enable per-channel quantization for LSTM Modules**
* #39604 [quant] Add reduce_range params for quantized_lstm

Summary:

Test Plan:
python test/test_quantization.py TestPostTrainingDynamic.test_per_channel_lstm_quantize

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21977601](https://our.internmc.facebook.com/intern/diff/D21977601)",pytorch
39681,ssnl,pr,2020-06-08T22:37:35Z,Add out= variants for cuda.comm.broadcast/gather/scatter,Partially fixes https://github.com/pytorch/pytorch/issues/38911,pytorch
39687,supriyar,pr,2020-06-09T00:34:57Z,[quant][graphmode] Test weight observer for dynamic quant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39687 [quant][graphmode] Test weight observer for dynamic quant**

Summary:
Run the observer on the weight values and compare it with the calculated attributes in the graph

Test Plan:
python test/test_quantization.py test_dynamic_weight_observer

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D21961907](https://our.internmc.facebook.com/intern/diff/D21961907)",pytorch
39688,ssnl,pr,2020-06-09T00:45:24Z,Avoid initializing unnecessary tensors in nccl.reduce,"While working on https://github.com/pytorch/pytorch/issues/38911, I realized that `nccl.reduce` only needs a single output tensor, while our current implementation requires a list of output tensors. This, along with a TODO I fixed in reduce_add, should have some speed up for data parallel.",pytorch
39696,ngimel,pr,2020-06-09T03:27:10Z,always resize_ min/max outputs,"Resubmit of #36474. Addresses the comment in #35591, and makes behavior with `out` kwarg consistent with other functions that `resize_` their passed out.",pytorch
39703,peterjc123,pr,2020-06-09T04:56:59Z,Cleanup debug info switches with MSVC,"Switch off `/Z7` so that we don't generate debug info in Release and MinSizeRel builds, so that we will probably get smaller static libraries and object files and faster build time",pytorch
39709,ssnl,pr,2020-06-09T05:59:00Z,[WIP] Move comm.reduce to c++ and add out=,on top of #39681 #39688 ,pytorch
39710,ssnl,pr,2020-06-09T06:00:26Z,[WIP] Move comm.reduce to c++ and add out= ,on top of #39681 #39688,pytorch
39712,peterjc123,pr,2020-06-09T06:30:40Z,Cleanup cuda install scripts for Windows jobs,"1. Separate script
2. Don't print result in stdout
3. Don't collect logs if installation succeeds",pytorch
39715,peterjc123,pr,2020-06-09T08:03:59Z,Clone submodules in parallel,,pytorch
39717,zheng-xq,pr,2020-06-09T08:30:03Z,[TensorExpr] Fast sigmoid for LLVM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39717 [TensorExpr] Fast sigmoid for LLVM**

Differential Revision: [D21949849](https://our.internmc.facebook.com/intern/diff/D21949849)",pytorch
39729,ssnl,pr,2020-06-09T15:18:25Z,Fix error message in autograd,,pytorch
39733,ssnl,pr,2020-06-09T16:04:22Z,Fix Gather::apply accessing moved tensors,Somehow this gets uncovered when I make changes in https://github.com/pytorch/pytorch/pull/39710,pytorch
39841,peterjc123,pr,2020-06-11T02:21:08Z,Add runtime check for MSVC redist,Fixes https://github.com/pytorch/pytorch/issues/39734.,pytorch
39848,peterjc123,pr,2020-06-11T08:41:02Z,[TEST ONLY] static libtorch builds for Windows,,pytorch
39854,vfdev-5,pr,2020-06-11T13:48:23Z,Fix typo in warning message,Fix typo,pytorch
39857,ssnl,pr,2020-06-11T14:43:13Z,clarify utils::take_tensors comment fine_grained option,,pytorch
39869,ssnl,pr,2020-06-11T18:28:54Z,Kill DataLoader worker when we can't join,"There still are occasional reports of DataLoader workers not exiting (e.g., https://github.com/pytorch/pytorch/issues/39570). Before we figure out why, we should just kill them if the join timesout to prevent hanging.",pytorch
39873,ngimel,pr,2020-06-11T20:03:45Z,fix multinomial for empty batch,Per title,pytorch
39874,supriyar,pr,2020-06-11T20:07:37Z,[quant] Enable reduce_range for graphmode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39874 [quant] Enable reduce_range for graphmode**

Summary:
When fbgemm backend is set we make sure reduce_range is set to true to avoid overflow in the operator
Also adds test for per-channel quant with graph mode and compare numerics with eager mode

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22011205](https://our.internmc.facebook.com/intern/diff/D22011205)",pytorch
39913,peterjc123,pr,2020-06-12T01:24:39Z,[test only] Test CUDA debug,,pytorch
39926,peterjc123,pr,2020-06-12T05:46:50Z,[TEST] Try cuda debug info with host,,pytorch
39975,ssnl,pr,2020-06-12T23:59:57Z,[DataLoader] add repr for WorkerInfo,,pytorch
39977,ssnl,pr,2020-06-13T01:14:18Z,Register view relation for split & chunk,"Fixes #39976

",pytorch
39987,peterjc123,pr,2020-06-13T05:56:45Z,Improve cuda error message for MSVC,,pytorch
39994,peterjc123,pr,2020-06-13T16:27:02Z,Adding /FS for NVCC if /Zi is used,Fixes https://github.com/pytorch/pytorch/issues/39989.,pytorch
40039,supriyar,pr,2020-06-15T18:38:54Z,[quant][graphmode] Refactor dynamic quant tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #40040 [quant][graphmode] Test JIT tracing for dynamic quant cases
* **#40039 [quant][graphmode] Refactor dynamic quant tests**

Summary:
Similar to static quant, break it up into op level tests and tests for jit passes

Test Plan:
python test/test_quantization.py TestQuantizeScriptPTDQOps
python test/test_quantization.py TestDynamicQuantizeScriptJitPasses
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22071278](https://our.internmc.facebook.com/intern/diff/D22071278)",pytorch
40040,supriyar,pr,2020-06-15T18:39:01Z,[quant][graphmode] Test JIT tracing for dynamic quant cases,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40040 [quant][graphmode] Test JIT tracing for dynamic quant cases**
* #40039 [quant][graphmode] Refactor dynamic quant tests

Summary:

Test Plan:
python test/test_quantization.py TestQuantizeDynamicScriptJitPasses

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22071277](https://our.internmc.facebook.com/intern/diff/D22071277)",pytorch
40066,rohan-varma,pr,2020-06-16T00:53:21Z,Add prefix of remote events for RPC profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40066 Add prefix of remote events for RPC profiling**

Builds on top of the previous PR to ensure that all remotely profiled events are prefixed with the key for the RPC that generated them. 

The key is generated by the result of `_build_rpc_profiling_key` in `rpc/internal.py` and prefixed onto the event name. In order to do this, we set the current-key when creating the RPC in Python, retrieve the currently-set key in C++ and save a GloballyUniqueId -> key mapping to an in-memory map. When we receive an RPC with profiling information, we expect to receive this ID back, and look up the corresponding profiling key in the map.

The key is then added to all the remote events.

Tested by adding tests to ensure the key is added to all the remote events. Also added a UT which tests in under the multi-threading scenario, to ensure that the mapping's correctness is maintained when several RPCs are in the process of being created at once.

Paste of ex. profiler output: 

```
---------------------------------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                                                                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Node ID
---------------------------------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
rpc_async#udf_with_torch_ops(worker1 -> worker3)                             0.00%            0.000us          0                3.270ms          3.270ms          1                1
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: ones             10.26%           31.168us         24.81%           75.387us         37.693us         2                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: empty            18.12%           55.066us         18.12%           55.066us         9.178us          6                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: fill_            5.01%            15.221us         6.15%            18.679us         9.339us          2                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: is_complex       1.14%            3.458us          1.14%            3.458us          1.729us          2                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: add              17.51%           53.212us         20.37%           61.907us         61.907us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: mul              12.62%           38.364us         15.18%           46.131us         46.131us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: relu             7.03%            21.358us         16.14%           49.041us         49.041us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: threshold        6.79%            20.640us         9.11%            27.683us         27.683us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: sigmoid          9.62%            29.242us         23.50%           71.427us         71.427us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: sigmoid_out      10.04%           30.517us         11.90%           36.164us         36.164us         1                3
rpc_async#udf_with_torch_ops(worker1 -> worker3)#remote_op: resize_          1.86%            5.647us          1.86%            5.647us          5.647us          1                3
---------------------------------------------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Self CPU time total: 303.893us
```
Differential Revision: [D22040035](https://our.internmc.facebook.com/intern/diff/D22040035/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22040035/)!",pytorch
40080,peterjc123,pr,2020-06-16T05:50:00Z,Improve cuda error message for MSVC (v1.5.1),,pytorch
40108,ngimel,pr,2020-06-16T18:56:22Z,Resubmit Remove `THTensor_(fill)` & `THTensor_(zero)` (#39727),"Summary:
Remove `THTensor_(fill)` & `THTensor_(zero)` following the PR https://github.com/pytorch/pytorch/pull/39042 as reference
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39727

Test Plan: buck test caffe2/caffe2/fb/python:pytorch_func_test

Differential Revision: D22070199

Pulled By: ngimel

",pytorch
40115,rohan-varma,pr,2020-06-16T20:08:08Z,Add guard for non-default stream in DDP's autograd engine callback,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40115 Add guard for non-default stream in DDP's autograd engine callback**

Closes https://github.com/pytorch/pytorch/issues/37790
Closes https://github.com/pytorch/pytorch/issues/37944

A user may wish to run DDP's forward + backwards step under a non-default CUDA stream such as those created by `with torch.cuda.Stream(stream)`. In this case, the user should be responsible for synchronizing events on this stream with other streams used in the program (per the documentation at https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics), but currently DDP has a bug which causes DDP under non-default streams to fail.

If a user does the following:
```
stream = torch.cuda.Stream()
with torch.cuda.stream(stream):
model = DDP(...)
loss = model(inptut).sum()
loss.backward()
grad = model.module.weight.grad()
average = dist.all_reduce(grad)
```

There is a chance that `average` and `grad` will not be equal. This is because the CUDA kernels corresponding to the  `all_reduce` call may run before `loss.backward()`'s kernels are finished. Specifically, in DDP we copy the allreduced gradients back to the model parameter gradients in an autograd engine callback, but this callback runs on the default stream. Note that this can also be fixed by the application synchronizing on the current stream, although this should not be expected, since the application is not using the current stream at all.

This PR fixes the issue by passing the current stream into DDP's callback.

Tested by adding a UT `test_DistributedDataParallel_non_default_stream` that fails without this PR

Differential Revision: [D22073353](https://our.internmc.facebook.com/intern/diff/D22073353/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22073353/)!",pytorch
40127,supriyar,pr,2020-06-16T22:13:27Z,[quant][graphmode] Refactor dynamic quant tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #40128 [quant][graphmode] Test JIT tracing for dynamic quant cases
* **#40127 [quant][graphmode] Refactor dynamic quant tests**

Summary:
Reland PR.
Similar to static quant, break it up into op level tests and tests for jit passes

Test Plan:
python test/test_quantization.py TestQuantizeScriptPTDQOps
python test/test_quantization.py TestDynamicQuantizeScriptJitPasses
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22081259](https://our.internmc.facebook.com/intern/diff/D22081259)",pytorch
40128,supriyar,pr,2020-06-16T22:13:35Z,[quant][graphmode] Test JIT tracing for dynamic quant cases,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40128 [quant][graphmode] Test JIT tracing for dynamic quant cases**
* #40127 [quant][graphmode] Refactor dynamic quant tests

Summary:
Reland PR

Test Plan:
python test/test_quantization.py TestQuantizeDynamicScriptJitPasses

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22081258](https://our.internmc.facebook.com/intern/diff/D22081258)",pytorch
40141,rohan-varma,pr,2020-06-17T04:10:49Z,Fix flaky rref timeout test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40141 Fix flaky rref timeout test**

This rref timeout test could be flaky because we could end up processing `RRefUserDelete` messages on the owner node before processing the to_here message. This would result in a hang in `ProcessGroupAgent::sync()` that eventually results in a timeout.

The rough sequence of what happens is:
0) Node 0 creates RRef on node 1 with rpc.remote() call
1) rref.to_here() is called with a timeout. Because of delay injection, the processing of this message can be delayed (this is also technically possible in applications without delay injection)
2) At some point, callbacks corresponding to rpc.remote() runs and confirms the rref, adding it as a confirmed user
3) RPC shutdown starts, as part of which we send out RRef user deletes. In this case, since 0 has a confirmed user, 0 sends an RRef user delete to 1, and node 1 removes the owner from the `owners_` field.
4) The `to_here()` message is finally processed by node 1. But since we have deleted the `owner_`, while processing this message we create a future that will be complete when the owner exists (this is to account for the case of to_here() arriving here rpc.remote). But this future will never complete, since the owner is already deleted, so we hang indefnitely

As a workaround for now, we can force `to_here()` to run before RPC shutdown by adding a blocking `to_here()` call with no timeout. As a result, since it's blocking, we are guaranteed that the remote end will process `to_here()` before the RRefUserDeletes. 

A more robust, longer-term fix would be to detect if an owner has been previously deleted (such as by an RRefUserDelete), and then error out when processing a `to_here()`.

Differential Revision: [D22084735](https://our.internmc.facebook.com/intern/diff/D22084735/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22084735/)!",pytorch
40191,supriyar,pr,2020-06-17T21:25:35Z,[quant][bug] Fix histogram observer with 0 input,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40191 [quant][bug] Fix histogram observer with 0 input**

Summary:
When the first couple of inputs passed to histogram observer are all 0's subsequent non-zero inputs cause a div by 0 error

Test Plan:
python test/test_quantization.py TestHistogramObserver.test_histogram_observer_zero_inputs
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22119422](https://our.internmc.facebook.com/intern/diff/D22119422)",pytorch
40281,ssnl,pr,2020-06-19T15:40:47Z,Fix IterableDataset len warning,"See https://github.com/pytorch/pytorch/pull/38925#issuecomment-646695044



cc @jlucier ",pytorch
40310,supriyar,pr,2020-06-19T21:07:54Z,[quant][bug] Histogram observer bug fix with min == max,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40310 [quant][bug] Histogram observer bug fix with min == max**

Summary:

Test Plan:
python test/test_quantization.py test_histogram_observer_same_inputs

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22145908](https://our.internmc.facebook.com/intern/diff/D22145908)",pytorch
40326,rohan-varma,pr,2020-06-20T00:52:13Z,Add callback with TLS state API in futures,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40326 Add callback with TLS state API in futures**

Adds a helper function `wrapPropagateTLSState` that wraps a function into another function that runs with the `at::ThreadLocalState` of the caller. The motivation is to compose a function with this wrapper when calling APIs such as `addCallback` or `then` so that different threads run callbacks with the right tls state.

With the API, the below

```
at::ThreadLocalState tls_state;
fut->addCallback([tls_state = std::move(tls_state)]() {
at::ThreadLocalStateGuard g(tls_state);
some_cb_that_requires_tls_state();
}
```

becomes

```
fut->addCallback(wrapPropagateTLSState<returnType>(some_cb_that_requires_state));
```

Differential Revision: [D22147634](https://our.internmc.facebook.com/intern/diff/D22147634/)",pytorch
40338,peterjc123,pr,2020-06-20T11:38:07Z,Fix update_s3_html for nightly jobs,Fixes #40337.,pytorch
40349,ngimel,pr,2020-06-20T21:58:41Z,restore generic IndexToScatterGatherOffset specialization,"#39963 erroneously removed template specialization to compute offsets, causing cases relying on this specialization (topk for 4d+ tensors with topk dimension >= 1024/2048 depending on the type) to produce bogus results.",pytorch
40352,peterjc123,pr,2020-06-21T02:33:42Z,Append forward slashes to PIP_UPLOAD_FOLDER,,pytorch
40365,peterjc123,pr,2020-06-22T05:16:01Z,Improve Dynamic Library for Windows,"1. Use LoadLibraryEx if available
2. Print more info on error",pytorch
40369,peterjc123,pr,2020-06-22T06:33:00Z,Pin the version of scipy for Windows test jobs,Fixes #40366.,pytorch
40380,rohan-varma,pr,2020-06-22T17:19:59Z,[DDP Note] Remove refs to RoundRobin PG until we officially support it,Removes line mentioning `ProcessGroupRoundRobin` since we don't intend it to be used as a public API just yet. We can add this back when we officially support the API,pytorch
40474,rohan-varma,pr,2020-06-24T00:08:09Z,"Move list size constants for profiler::Event and profiler::ProfilerConfig into
enum.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40474 Move list size constants for profiler::Event and profiler::ProfilerConfig into
enum.**
enum.**
enum.**
enum.**
enum.**
enum.**
* #40326 Add callback with TLS state API in futures
enum.**
* #40326 Add callback with TLS state API in futures

These constants are unnecessary since there is an enum, and we can add
the size at the end of the enum and it will be equal to the list size. I
believe that this is the typical pattern used to represent enum sizes.

Differential Revision: [D22147754](https://our.internmc.facebook.com/intern/diff/D22147754/)",pytorch
40486,peterjc123,pr,2020-06-24T03:10:10Z,Skip test_mem_leak on Windows,Addresses https://github.com/pytorch/pytorch/issues/40485.,pytorch
40487,peterjc123,pr,2020-06-24T03:25:18Z,Fixes caffe2 loading issues on Windows (v1.6.0),This is a cherry-pick of https://github.com/pytorch/pytorch/pull/39513 into 1.6.0,pytorch
40489,peterjc123,pr,2020-06-24T03:37:52Z,Improve Dynamic Library for Windows (v1.6.0),"Summary:
1. Use LoadLibraryEx if available
2. Print more info on error

(cherry picked from commit a2d4d9eca6bfda3283bf2534260c0a8ba201b9e8 original PR https://github.com/pytorch/pytorch/pull/40365)

",pytorch
40498,peterjc123,pr,2020-06-24T10:08:52Z,Skip test_mem_leak on Windows (v1.6.0),"(cherry picked from commit 3fb6f038256a3a5ce43e857409ce4ffb807d93a5)

",pytorch
40531,ssnl,pr,2020-06-24T20:30:04Z,Use device objects in c++ comm methods,"One step towards 
1. moving reduce/ reduce_add_coalesced to c++, add out=

to close https://github.com/pytorch/pytorch/issues/38911",pytorch
40573,ngimel,pr,2020-06-25T17:45:59Z,relax size check in flatten_for_scatter_gather,"Summary: Per title, to workaround apex sbn bug.

Test Plan: Covered by existing tests

Differential Revision: D22236942

",pytorch
40589,vfdev-5,pr,2020-06-25T21:38:52Z,Fixes bug with ParameterList/Dict replication in DP (#36035),"Fixes #36035

Description:
- Fixes bug with `ParameterList`, `ParameterDict` replication in DP

TODO: 
- [x] check correctness of `replica._former_parameters[key] = param` if replica is `ParameterList`, `ParameterDict`
- [x] added tests
  - [x] added DP basic test.
  - [x] added a test to make sure that gradients are propagated correctly on parameters.
  - [x] DDP test on parameters in single process multiple-devices use-case
- [x] Update docstring

@albanD could you please review the approach",pytorch
40602,rohan-varma,pr,2020-06-26T01:25:37Z,Configure RPC metrics handlers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40602 Configure RPC metrics handlers**

Differential Revision: [D22250592](https://our.internmc.facebook.com/intern/diff/D22250592/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22250592/)!",pytorch
40617,vfdev-5,pr,2020-06-26T13:56:56Z,Removed misleading docstring about grads #40158,"Fixes #40158 

Description
- docs update: removed incorrect statements

",pytorch
40652,rohan-varma,pr,2020-06-27T00:47:53Z,[RFC] Profile rpc_async call from JIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40652 [RFC] Profile rpc_async call from JIT**

Resolves https://github.com/pytorch/pytorch/issues/40304, but looking for
feedback on whether there is a better approach for this (cc @ilia-cher).

In order to profile `rpc_async` calls made within a torchscript function, we
add the profiling logic to `rpcTorchscript` which is the point where the RPC is
dispatched and is called by the jit `rpc_async` operator. We take a somewhat
similar approach to how this is done in the python API. If profiling is
enabled, we call `record_function_enter` which creates a `RecordFunction`
object and runs its starting callbacks. Then, we schedule end callbacks for
this `RecordFunction` to be run when the jit future completes.

One caveat is that `rpcTorchscript` can also be called by rpc_async from a
non-JIT function, in which case the profiling logic lives in Python. We add a
check to ensure that we don't double profile in this case.

Differential Revision: [D22270608](https://our.internmc.facebook.com/intern/diff/D22270608/)",pytorch
40669,ssnl,pr,2020-06-28T00:03:30Z,[typing] fix register_buffer/parameter,,pytorch
40675,peterjc123,pr,2020-06-28T05:59:34Z,Remove constexpr for NVCC on Windows,They are not well supported. Fixes https://github.com/pytorch/pytorch/issues/40393 and https://github.com/pytorch/pytorch/issues/39394.,pytorch
40676,peterjc123,pr,2020-06-28T06:04:08Z,Remove constexpr for NVCC on Windows (v1.6),#40675 for v1.6,pytorch
40677,peterjc123,pr,2020-06-28T06:21:04Z,Update the version of ninja and scipy,Update scipy to 1.15 and ninja to 1.10.,pytorch
40708,supriyar,pr,2020-06-29T17:47:26Z,[quant][graphmode] Add FP16 quant support - Insert Noop Observers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #40710 [quant][graphmode] FP16 quant support - Operator Fusion
* #40709 [quant][graphmode] FP16 quant support - Insert cast operators
* **#40708 [quant][graphmode] Add FP16 quant support - Insert Noop Observers**

Summary:
Insert NoopObservers for activations and weight tensors for FP16

Test Plan:
python test/test_quantization.py test_prepare_dynamic

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22335976](https://our.internmc.facebook.com/intern/diff/D22335976)",pytorch
40709,supriyar,pr,2020-06-29T17:47:37Z,[quant][graphmode] FP16 quant support - Insert cast operators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #40710 [quant][graphmode] FP16 quant support - Operator Fusion
* **#40709 [quant][graphmode] FP16 quant support - Insert cast operators**
* #40708 [quant][graphmode] Add FP16 quant support - Insert Noop Observers

Summary:
Cast to kHalf and back to kFloat before the linear operator to mimic FP16 quant support

Test Plan:
python test/test_quantization.py test_convert_dynamic_fp16

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22335977](https://our.internmc.facebook.com/intern/diff/D22335977)",pytorch
40710,supriyar,pr,2020-06-29T17:47:47Z,[quant][graphmode] FP16 quant support - Operator Fusion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40710 [quant][graphmode] FP16 quant support - Operator Fusion**
* #40709 [quant][graphmode] FP16 quant support - Insert cast operators
* #40708 [quant][graphmode] Add FP16 quant support - Insert Noop Observers

Summary:

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22335975](https://our.internmc.facebook.com/intern/diff/D22335975)",pytorch
40754,rohan-varma,pr,2020-06-30T01:41:22Z,Repro profiler issue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40754 Repro profiler issue**

Differential Revision: [D22301379](https://our.internmc.facebook.com/intern/diff/D22301379/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22301379/)!",pytorch
40782,peterjc123,pr,2020-06-30T15:46:27Z,[TEST ONLY] VS 14.13 builder jobs,Testing https://github.com/pytorch/builder/pull/464.,pytorch
40783,peterjc123,pr,2020-06-30T15:57:30Z,Fix zip serialization for file > 2GiB for Windows,`long long == int64_t != long` in MSVC,pytorch
40794,peterjc123,pr,2020-06-30T17:54:48Z,Fix wrong MSVC version constraint for CUDA 9.2,Tested with https://github.com/pytorch/pytorch/pull/40782.,pytorch
40849,peterjc123,pr,2020-07-01T03:48:15Z,Fix wrong MSVC version constraint for CUDA 9.2 (v1.6),"Summary:
Tested with https://github.com/pytorch/pytorch/pull/40782.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40794

Differential Revision: D22318045

Pulled By: malfet

fbshipit-source-id: a737ffd7cb8a6a9efb62b84378318f4c3800ad8f

",pytorch
40852,peterjc123,pr,2020-07-01T04:35:18Z,Fix zip serialization for file > 2GiB for Windows (v1.6),Cherry-pick of https://github.com/pytorch/pytorch/pull/40783,pytorch
40853,ngimel,pr,2020-07-01T06:22:18Z,fix pca_lowrank memory consumption,"Per title, fixes #40768",pytorch
40857,peterjc123,pr,2020-07-01T11:38:10Z,[TEST ONLY] Debug RoiAlignTest,,pytorch
40879,ssnl,pr,2020-07-01T18:57:37Z,typing for tensor.T/grad_fn torch.Size,"fixes  https://github.com/pytorch/pytorch/issues/40658 https://github.com/pytorch/pytorch/issues/40658

",pytorch
40884,ssnl,pr,2020-07-01T21:10:28Z,Use unbind for tensor.__iter__,"Unbind, which has a special backward with cat, is arguably better than multiple selects, whose backward is creating & adding a bunch of tensors as big as `self`.

",pytorch
40894,supriyar,pr,2020-07-02T02:06:36Z,[quant][graphmode] Refactor quantization patterns,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #41049 [quant][graphmode] Fp16 quant support - match numerics with eager mode
* **#40894 [quant][graphmode] Refactor quantization patterns**

Summary:

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22403901](https://our.internmc.facebook.com/intern/diff/D22403901)",pytorch
40897,cloudhan,pr,2020-07-02T02:57:07Z,[jit] Fix jit not round to even if const is folded,Fixed #40771,pytorch
40901,peterjc123,pr,2020-07-02T06:21:36Z,Re-enable Caffe2 test `RoiAlignTest.CheckCPUGPUEqual`,Fixes https://github.com/pytorch/pytorch/issues/35547.,pytorch
40923,ssnl,pr,2020-07-02T15:40:53Z,Add `module.training` to docs,"A lot of people ask https://discuss.pytorch.org/t/check-if-model-is-eval-or-train/9395/3

",pytorch
40957,ngimel,pr,2020-07-03T01:17:50Z,[do not merge] add an exact edges test,,pytorch
40976,peterjc123,pr,2020-07-04T03:19:39Z,DEBUG: SciPy pip DLL load issue,"* do not merge--debug only! I tried to replicate
pytorch CI in my fork but this is a bit easier
if not too troublesome

* this is an attempt to check for the fix for
SciPy DLL load issue reported in pytorch CI
in issue #40366 with our `1.5.0` release; at the moment,
`pytorch` CI is using a conda install to avoid the DLL
load issue observed with our PyPI wheel for Windows
Python 3.6

* the specific wheel in use in this feature branch
contains a variety of attempted fixes--if they work,
I will aim to backport them to a `1.5.1` release
of SciPy

",pytorch
40979,ssnl,pr,2020-07-04T06:05:28Z,Support kBool in comm NCCL,fixes https://github.com/pytorch/pytorch/issues/40978,pytorch
40999,ngimel,pr,2020-07-06T04:20:59Z,run single-threaded gradgradcheck in test_nn,"Most time-consuming tests in test_nn (taking about half the time) were gradgradchecks on Conv3d. Reduce their sizes, and, most importantly, run gradgradcheck single-threaded, because that cuts the time of conv3d tests by an order of magnitude, and barely affects other tests. 
These changes bring test_nn time down from 1200 s to ~550 s on my machine. ",pytorch
41000,peterjc123,pr,2020-07-06T04:22:47Z,Add the missing `resource_class` key in the update_s3_htmls job,"Fixes https://github.com/pytorch/pytorch/issues/40998.

Actually I don't know why it is needed. But without it, the build won't start. See my rerun of the update_s3_html3 job: https://app.circleci.com/pipelines/github/pytorch/pytorch/187926/workflows/432dbe98-ca2f-484d-acc7-0482cb3fd01f/jobs/6121551/steps.",pytorch
41028,ngimel,pr,2020-07-06T19:18:11Z,Return atomic,"Per title. This is not used currently in the pytorch codebase, but it is a legitimate usecase, and we have extensions that want to do that and are forced to roll their own atomic implementations for non-standard types. Whether atomic op returns old value or not should not affect performance, compiler is able to generate correct code depending on whether return value is used. https://godbolt.org/z/DBU_UW. 
Atomic operations for non-standard integer types (1,2 and 8 byte-width) are left as is, with void return. ",pytorch
41042,rohan-varma,pr,2020-07-06T21:59:26Z,Barrier test ,,pytorch
41049,supriyar,pr,2020-07-07T00:02:55Z,[quant][graphmode] Fp16 quant support - match numerics with eager mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41049 [quant][graphmode] Fp16 quant support - match numerics with eager mode**
* #40894 [quant][graphmode] Refactor quantization patterns

Summary:
In eager mode there is no cast operator for the activation tensor in the fbgemm fp16 operator
Remove it from graph mode.
For the weight tensor we handle saturation by clipping the values to fp16 range
This makes numerics match between debug model and final quantized model.

Test Plan:
python test/test_quantization.py test_linear_dynamic_fp16
Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
41112,ngimel,pr,2020-07-08T02:57:22Z,Speed tests all,,pytorch
41121,vfdev-5,pr,2020-07-08T15:11:23Z,Typo fix,"Description:
- Typo fix in the docstring",pytorch
41125,ssnl,pr,2020-07-08T16:01:50Z,[typing] tensor._version is int,,pytorch
41126,ngimel,pr,2020-07-08T16:20:19Z,Speed tests all,,pytorch
41147,ngimel,pr,2020-07-08T21:43:02Z,Run single-threaded gradgradcheck in testnn,Reland #40999 ,pytorch
41151,rohan-varma,pr,2020-07-08T22:08:06Z,[1.6 cherry pick] Add guard for non-default stream in DDP's autograd engine callback,"Port of https://github.com/pytorch/pytorch/pull/40115, to merge into release/1.6.

Original commit description:

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40115

Closes https://github.com/pytorch/pytorch/issues/37790
Closes https://github.com/pytorch/pytorch/issues/37944

A user may wish to run DDP's forward + backwards step under a non-default CUDA stream such as those created by `with torch.cuda.Stream(stream)`. In this case, the user should be responsible for synchronizing events on this stream with other streams used in the program (per the documentation at https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics), but currently DDP has a bug which causes DDP under non-default streams to fail.

If a user does the following:
```
stream = torch.cuda.Stream()
with torch.cuda.stream(stream):
model = DDP(...)
loss = model(inptut).sum()
loss.backward()
grad = model.module.weight.grad()
average = dist.all_reduce(grad)
```

There is a chance that `average` and `grad` will not be equal. This is because the CUDA kernels corresponding to the  `all_reduce` call may run before `loss.backward()`'s kernels are finished. Specifically, in DDP we copy the allreduced gradients back to the model parameter gradients in an autograd engine callback, but this callback runs on the default stream. Note that this can also be fixed by the application synchronizing on the current stream, although this should not be expected, since the application is not using the current stream at all.

This PR fixes the issue by passing the current stream into DDP's callback.

Tested by adding a UT `test_DistributedDataParallel_non_default_stream` that fails without this PR
ghstack-source-id: 106481208

Differential Revision: D22073353

fbshipit-source-id: 70da9b44e5f546ff8b6d8c42022ecc846dff033e

",pytorch
41171,rohan-varma,pr,2020-07-09T02:45:18Z,Allow drop_last option in DistributedSampler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41171 Allow drop_last option in DistributedSampler**

Closes https://github.com/pytorch/pytorch/issues/25162. 

DistributedSampler allows data to be split evenly across workers in
DDP, but it has always added additional samples in order for the data to be
evenly split in the case that the # of samples is not evenly divisible by the
number of workers. This can cause issues such as when doing distributed
validation accuracy, where multiple samples could be considered twice. Also applications may not want to repeat the data within a single epoch when training and would rather drop the tail. 

This PR adds a drop_last option where the tail of the data is dropped such that
the effective dataset size is still evenly divisible across the workers. This
ensures that DDP can train fine (there is no uneven inputs) and each replica
gets an equal number of data indices.

The change is backwards compatible because the default value of `drop_last` is `False` so the old behavior is the default. 

Differential Revision: [D22449974](https://our.internmc.facebook.com/intern/diff/D22449974/)",pytorch
41175,ssnl,pr,2020-07-09T05:28:55Z,Make IterableDataset dataloader.__len__ warning clearer,"Based on discussion with @jlucier (https://github.com/pytorch/pytorch/pull/38925#issuecomment-655859195) . `batch_size` change isn't made because data loader only has the notion of `batch_sampler`, not batch size. If `batch_size` dependent sharding is needed, users can still access it from their own code.",pytorch
41183,ssnl,pr,2020-07-09T14:22:58Z,[1.6] Make IterableDataset DataLoader.__len__ warning clearer,"https://github.com/pytorch/pytorch/pull/41175

cc @zou3519 ",pytorch
41184,ssnl,pr,2020-07-09T14:29:32Z,[1.6] Proposed warning change for DataLoader with IterableDataset,"cherry pick https://github.com/pytorch/pytorch/pull/40344

cc @zou3519 ",pytorch
41185,ssnl,pr,2020-07-09T15:18:01Z,[1.6] Make IterableDataset DataLoader.__len__ warning clearer,,pytorch
41217,rohan-varma,pr,2020-07-10T00:26:51Z,Fix flaky test_udf_remote_message_delay_timeout_to_self,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41217 Fix flaky test_udf_remote_message_delay_timeout_to_self**

Closes https://github.com/pytorch/pytorch/issues/41019. Due to the possibility of callback
finishCreatingOwnerRRef running after request_callback has processed and
created the owner RRef, we could actually end up with 0 owners on the node,
since the callback removes from the owners_ map. In this case, shutdown is fine
since there are no owners. On the other hand, if the callback runs first, there
will be 1 owner which we will delete in shutdown when we detect it has no
forks. So either way, shutdown works fine and we don't need to enforce there to
be 1 owner.

Differential Revision: [D22469806](https://our.internmc.facebook.com/intern/diff/D22469806/)",pytorch
41218,rohan-varma,pr,2020-07-10T00:28:53Z,Remove unnecessary test in rpc_test.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41218 Remove unnecessary test in rpc_test.py**
* #41217 Fix flaky test_udf_remote_message_delay_timeout_to_self

This test doesn't assert anything and was accidentally committed as
part of a larger diff a few months ago.

Differential Revision: [D22469852](https://our.internmc.facebook.com/intern/diff/D22469852/)",pytorch
41225,ngimel,pr,2020-07-10T03:43:06Z,test nan and inf in TestTorchMathOps,"Per title. `lgamma` produces a different result for `-inf` compared to scipy, so there comparison is skipped. ",pytorch
41287,rohan-varma,pr,2020-07-10T23:46:30Z,Fix flaky profiler and test_callback_simple RPC tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41287 Fix flaky profiler and test_callback_simple RPC tests**

Profiler tests that test profiling with builtin functions and `test_callback_simple` test has been broken for a while. This diff fixes that by preferring c10 ops to non-c10 ops in our operation matching logic.

The result of this is that these ops go through the c10 dispatch and thus have profiling enabled. For `test_callback_simple` this results in the effect that we choose `aten::add.Tensor` over `aten::add.Int` which fixes the type issue.

Differential Revision: [D22489197](https://our.internmc.facebook.com/intern/diff/D22489197/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22489197/)!",pytorch
41293,supriyar,pr,2020-07-11T00:29:11Z,[quant][pyper] Add embedding_bag weight quantize and dequantize ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41293 [quant][pyper] Add embedding_bag weight quantize and dequantize ops**

Summary:

Add new operators that does quantize and packing for 8 bit and 4 bit embedding bag operators.
This is an initial change to help unblock testing. This will be follwed by adding graph mode passes to enable quantization of embedding_bag module

Note to reviewers: Future PRs will replace this op with a separate quantize and pack operator and add support for floating point scale and zero point.

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingBag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22506700](https://our.internmc.facebook.com/intern/diff/D22506700)",pytorch
41318,rohan-varma,pr,2020-07-12T08:09:34Z,NCCL Backend support for torch.bool,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41318 NCCL Backend support for torch.bool**

Closes https://github.com/pytorch/pytorch/issues/24137. 
This PR adds support for the `torch.bool` tensor type to ProcessGroupNCCL. For most types we use the existing mapping, but since `bool` is not supported as a native `ncclDataType_t`, we add the following logic:
1) Map `at::kBool` to `ncclUint8`
2) During reduction (allreduce for example), if the operation is SUM, we instead override to to a MAX, to avoid overflow issues. The rest of the operations work with no changes. In the boolean case, changing sum to max makes no correctness difference since they both function as a bitwise OR. 

The reduction logic (for example for reduce/allreduce) is as follows:
sum, max = bitwise or
product, min = bitwise and

Note that this PR doesn't add support for BAND/BOR/BXOR. That is because these reduction ops currently are not supported by NCCL backend, see https://github.com/pytorch/pytorch/issues/41362

Tests are added to ensure that the reductions work as expected. 
Differential Revision: [D22496604](https://our.internmc.facebook.com/intern/diff/D22496604/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22496604/)!",pytorch
41387,powderluv,pr,2020-07-14T07:13:10Z,[cmake] Use PROJECT_SOURCE_DIR instead of CMAKE_*,"Add support for including pytorch via an add_subdirectory()
This requires using PROJECT_* instead of CMAKE_* which refer to
the top-most project including pytorch.

TEST=add_subdirectory() into a pytorch checkout and build.
There are still some hardcoded references to TORCH_SRC_DIR, I will
fix in a follow on commit. For now you can create a symlink to
 <pytorch>/torch/ in your project.

Change-Id: Ic2a8aec3b08f64e2c23d9e79db83f14a0a896abc

",pytorch
41441,zheng-xq,pr,2020-07-15T01:02:55Z,DO NOT SOBUMIT: math test hack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #41442 Fix the test_tensorexpr locally.
* **#41441 DO NOT SOBUMIT: math test hack**

",pytorch
41442,zheng-xq,pr,2020-07-15T01:03:01Z,Fix the test_tensorexpr locally.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41442 Fix the test_tensorexpr locally.**
* #41441 DO NOT SOBUMIT: math test hack

",pytorch
41444,zheng-xq,pr,2020-07-15T01:06:28Z,Fix the test_tensorexpr locally.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41444 Fix the test_tensorexpr locally.**

",pytorch
41446,ngimel,pr,2020-07-15T01:47:44Z,fix division by low precision scalar,"Before, inverse for division by scalar is calculated in the precision of the non-scalar operands, which can lead to underflow:
```
>>> x = torch.tensor([3388.]).half().to(0)
>>> scale = 524288.0
>>> x.div(scale)
tensor([0.], device='cuda:0', dtype=torch.float16)
>>> x.mul(1. / scale)
tensor([0.0065], device='cuda:0', dtype=torch.float16)
```
This PR makes results of multiplication by inverse and division the same. ",pytorch
41472,peterjc123,pr,2020-07-15T13:25:10Z,Disable failed caffe2 tests for BoundShapeInference on Windows,"Related: 
https://github.com/pytorch/pytorch/issues/40861
https://github.com/pytorch/pytorch/issues/41471",pytorch
41612,supriyar,pr,2020-07-17T22:34:23Z,[quant] Add Graph Mode Passes to quantize EmbeddingBag operators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#41612 [quant] Add Graph Mode Passes to quantize EmbeddingBag operators**

Summary:
This change adds preliminary support to quantize the EmbeddingBag operators. We currently support 4-bit and 8-bit quantization+packing of the weights.

To quantize these operators, specify the operator name in the `custom_op_name` field of the NoopObserver. Based on the op name (4bit or 8bit) we call the corresponding quantization functions.
Refer to the testplan for how to invoke the qconfig for the embedding_bag ops.

Future versions of this will support 4-bit and 2-bit qtensors with native support to observe and quantize it.

NB - This version assumes that the weights in the EmbeddingBag Module reside on the same device.

Test Plan:
python test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22609342](https://our.internmc.facebook.com/intern/diff/D22609342)",pytorch
41619,cloudhan,pr,2020-07-18T03:52:48Z,update CONTRIBUTING.md for ccache,"ccache now use cmake for building, update installation script.",pytorch
41621,ngimel,pr,2020-07-18T05:58:27Z,workaround segfault in deviceGuard construction,"Summary:
Per title. In some situation, deviceGuard constructor in mul_kernel_cuda segfaults, so construct deviceGuard conditionally only when first argument is scalar.
This does not root cause why deviceGuard constructor segfaults, so the issue might come back.

Test Plan: pytorch oss CI

Differential Revision: D22616460

",pytorch
41648,peterjc123,pr,2020-07-19T13:17:25Z,Skip warning 4522 with MSVC,"Suppresses ""multiple assignment operators specified"" warning.",pytorch
41654,ngimel,pr,2020-07-19T17:25:54Z,disable mkl for expm1,On some systems/mkl versions it produces expm1(nan)=-1,pytorch
41686,rohan-varma,pr,2020-07-20T18:03:36Z,[Not for review] Debug parameter issue in https://github.com/pytorch/pytorch/issues/41324,,pytorch
41769,rohan-varma,pr,2020-07-21T01:42:55Z,Enable test_distributed to work with spawn mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42932 Rename test_distributed to test_distributed_fork
* **#41769 Enable test_distributed to work with spawn mode**

Closes https://github.com/pytorch/pytorch/issues/36663. Currently the tests in `test_distributed` only work with the `fork` mode multiprocessing, this PR introduces support for `spawn` mode multiprocessing as well (while keeping the `fork` mode intact).

Motivations for the change:
1) Spawn multiprocessing is the default on MacOS, so it better emulates how MacOS users would use distributed
2) With python 3.8+, spawn is the default on linux, so we should have test coverage for this
3) PT multiprocessing suggests using spawn/forkserver over fork, for sharing cuda tensors: https://pytorch.org/docs/stable/multiprocessing.html
4) Spawn is better supported with respect to certain sanitizers such as TSAN, so adding this sanitizer coverage may help us uncover issues.

How it is done:
1) Move `test_distributed` tests in `_DistTestBase` class to a shared file `distributed_test` (similar to how the RPC tests are structured)
2) For `Barrier`, refactor the setup of temp directories, as the current version did not work with spawn, each process would get a different randomly generated directory and thus would write to different barriers. 
3) Add all the relevant builds to run internally and in OSS.
Running test_distributed with spawn mode in OSS can be done with:
`python test/run_test.py -i distributed/test_distributed_spawn -v`

Differential Revision: [D22408023](https://our.internmc.facebook.com/intern/diff/D22408023/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22408023/)!",pytorch
41830,peterjc123,pr,2020-07-22T01:31:06Z,Win jit test failure,,pytorch
41841,ngimel,pr,2020-07-22T06:57:17Z,fix masked_select for discontiguous outputs,"This fixes #41473 for discontiguous input, mask and out. Tests to follow. Reverting #33269 is not a great solution because I'm told masked_select was needed for printing complex tensors. 
cc @gchanan , @zou3519, @ezyang ",pytorch
41856,ngimel,pr,2020-07-22T18:20:48Z,[DO NOT MERGE} test if windows bfloat16/rand work,,pytorch
41861,ngimel,pr,2020-07-22T19:23:21Z,add scatter sub/div deprecation warning,"scatter div/subtract are not used, so we will deprecate them",pytorch
41908,ngimel,pr,2020-07-23T05:30:47Z,move min/max tests to TestTorchDeviceType,"so that testing _min_max on the different devices is easier, and min/max operations have better CUDA test coverage. 
",pytorch
41959,rohan-varma,pr,2020-07-23T23:27:44Z,[Resubmit #41318] NCCL backend support for torch bool,"Resubmit of #41318 pushed to ci-all branch.

Original description:
Closes #24137.
This PR adds support for the torch.bool tensor type to ProcessGroupNCCL. For most types we use the existing mapping, but since bool is not supported as a native ncclDataType_t, we add the following logic:

Map at::kBool to ncclUint8
During reduction (allreduce for example), if the operation is SUM, we instead override to to a MAX, to avoid overflow issues. The rest of the operations work with no changes. In the boolean case, changing sum to max makes no correctness difference since they both function as a bitwise OR.
The reduction logic (for example for reduce/allreduce) is as follows:
sum, max = bitwise or
product, min = bitwise and

Note that this PR doesn't add support for BAND/BOR/BXOR. That is because these reduction ops currently are not supported by NCCL backend, see #41362",pytorch
42002,peterjc123,pr,2020-07-24T12:39:22Z,Rc3 no sccache test,"Fixes #{issue number}
",pytorch
42036,ngimel,pr,2020-07-24T21:33:40Z,Enable non-synchronizing cub scan for cum* operations,"This uses cub for cum* operations, because, unlike thrust, cub is non-synchronizing. 
Cub does not support more than `2**31` element tensors out of the box (in fact, due to cub bugs the cutoff point is even smaller)
so to support that I split the tensor into `2**30` element chunks, and modify the first value of the second and subsequent chunks to contain the cumsum result of the previous chunks. Since modification is done inplace on the source tensor, if something goes wrong and we error out before the source tensor is reverted back to its original state, source tensor will be corrupted, but in most cases errors will invalidate the full coda context. 
",pytorch
42042,ngimel,pr,2020-07-24T23:02:12Z,update cub submodule,Per title,pytorch
42126,rohan-varma,pr,2020-07-27T19:40:10Z,[Testing] CI-all for https://github.com/pytorch/pytorch/pull/41171,"Fixes #{issue number}
",pytorch
42147,supriyar,pr,2020-07-28T01:13:30Z,[quant] Add saturate_to_fp16 op for FP16 quant support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42348 [quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver
* #42222 [quant][graph] Add support for FP16 dynamic quant
* #42221 [quant] Add FP16Observer for fp16 quant support
* **#42147 [quant] Add saturate_to_fp16 op for FP16 quant support**

Summary:
Op to check the range of a tensor and clamp the values to fp16 range
This operator will be inserted into the graph in subsequent diffs.

Test Plan:
python test/test_quantization.py TestQuantizedTensor.test_fp16_saturate_op

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22849221](https://our.internmc.facebook.com/intern/diff/D22849221)",pytorch
42189,rohan-varma,pr,2020-07-28T20:41:04Z,All Gather and gather APIs for Python Objects ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42189 All Gather and gather APIs for Python Objects**

Rehash of https://github.com/pytorch/pytorch/pull/28811, which was several months old.

As part of addressing https://github.com/pytorch/pytorch/issues/23232, this PR adds support for the following APIs:

`allgather_object` and `gather_object` to support gather/allgather of generic, pickable Python objects. This has been a long-requested feature so PyTorch should provide these helpers built-in.

The methodology is what is proposed in the original issue:
1) Pickle object to ByteTensor using torch.save
2) Comm. tensor sizes
3) Copy local ByteTensor into a tensor of maximal size
4) Call tensor-based collectives on the result of (3)
5) Unpickle back into object using torch.load

Note that the API is designed to match the tensor-based collectives other than supporting `async_op`. For now, it is a blocking call. If we see demand to support `async_op`, we will have to make more progress on merging work/future to support this.

If this is a suitable approach, we can support `scatter`, `broadcast` in follow up PRs.

Differential Revision: [D22785387](https://our.internmc.facebook.com/intern/diff/D22785387/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22785387/)!",pytorch
42221,supriyar,pr,2020-07-29T04:43:09Z,[quant] Add FP16Observer for fp16 quant support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42348 [quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver
* #42222 [quant][graph] Add support for FP16 dynamic quant
* **#42221 [quant] Add FP16Observer for fp16 quant support**
* #42147 [quant] Add saturate_to_fp16 op for FP16 quant support

Summary:
Adds a new observer that emits a warning if the range of tensor is beyond fp16 range. This will be further used in graph mode quantization to insert the cast to fp16 ops in the graph

Test Plan:
python test/test_quantizaton.py TestObserver.test_fp16_observer

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22849222](https://our.internmc.facebook.com/intern/diff/D22849222)",pytorch
42222,supriyar,pr,2020-07-29T04:43:20Z,[quant][graph] Add support for FP16 dynamic quant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42348 [quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver
* **#42222 [quant][graph] Add support for FP16 dynamic quant**
* #42221 [quant] Add FP16Observer for fp16 quant support
* #42147 [quant] Add saturate_to_fp16 op for FP16 quant support

Summary:
This change adds the necessary passes to perform FP16 dynamic quantization.
We skip inserting observers for activations based on the dtype (torch.float16) and only insert the Fp16Observer for weights

Test Plan:
python test/test_quantization.py TestQuantizeJitOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22849220](https://our.internmc.facebook.com/intern/diff/D22849220)",pytorch
42256,rohan-varma,pr,2020-07-29T21:56:39Z,[Testing] test_distributed_spawn runs in CI ,[WIP] CI-all for https://github.com/pytorch/pytorch/pull/41769,pytorch
42348,supriyar,pr,2020-07-31T01:28:02Z,[quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42348 [quant] Use PlaceholderObserver instead of Fp16Observer and NoopObserver**
* #42222 [quant][graph] Add support for FP16 dynamic quant
* #42221 [quant] Add FP16Observer for fp16 quant support
* #42147 [quant] Add saturate_to_fp16 op for FP16 quant support

Summary:
Use the dtype info in placeholderObserver to decide what ops to insert in the graph
In the next PR we can delete NoopObserver

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22859457](https://our.internmc.facebook.com/intern/diff/D22859457)",pytorch
42408,peterjc123,pr,2020-08-01T14:37:07Z,[DON'T MERGE] Trigger builds for MAGMA on Windows,"Fixes #{issue number}
",pytorch
42410,peterjc123,pr,2020-08-01T16:39:52Z,Update MAGMA to 2.5.3 for Windows,In order to introduce CUDA 11 build jobs.,pytorch
42420,peterjc123,pr,2020-08-02T02:43:16Z,Add CUDA 11 builds for Windows CI,"Stacked on https://github.com/pytorch/pytorch/pull/42410.
",pytorch
42507,ngimel,pr,2020-08-04T03:46:11Z,fix discontiguous inputs/outputs for cummin/cummax,"Fixes #42363
",pytorch
42558,rohan-varma,pr,2020-08-04T20:11:37Z,Ci all/rohan/all obj,"Fixes #{issue number}
",pytorch
42577,rohan-varma,pr,2020-08-05T03:07:50Z,Join-based API to support DDP uneven inputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42577 Join-based API to support DDP uneven inputs**

Closes https://github.com/pytorch/pytorch/issues/38174. Implements a join-based API to support training with the DDP module in the scenario where different processes have different no. of inputs. The implementation follows the description in https://github.com/pytorch/pytorch/issues/38174. Details are available in the RFC, but as a summary, we make the following changes:

#### Approach
1) Add a context manager that is owned by `class DistributedDataParallel` to coordinate the below process.
2) In the forward pass, we schedule a ""present"" allreduce where non-joined process contribute 1 and joined processes contribute 0. This lets us keep track of joined processes and know when all procs are joined.
3) When a process depletes its input and exits the context manager, it enters ""joining"" mode and attempts to ""shadow"" the collective comm. calls made in the model's forward and backward pass. For example we schedule the same allreduces in the same order as the backward pass, but with zeros
    a) There are a number of scenarios where in the backward pass, we have more than an allreduce for all tensors. For example, unused param detection and bucket rebuilding requires collective comm. 
4) We provide an option of whether we should divide by the initial world_size or effective world_size when some ranks are gone (default to initial world_size). If dividing by effective world_size, we adjust the allreduce division logic to divide by the effective world size (no. of non-joined procs) rather than the absolute world size.
5) At the end of training, the last joined process is selected to be the ""authoritative"" model copy and broadcasts its parameters.

We also make the following smaller changes to support the above:
- Add a `rank` argument to `_distributed_broadcast_coalesced` to specify which rank should do the broadcast, instead of forcing rank 0. This is needed because we cannot select rank 0 arbitrarily in the join-mode.
- Add a helper function to `DistributedDataParallel` which will have all processes agree on a common rank based on some condition. This common rank is then used for broadcasting final model params and module buffers throughout training.
- Expose several helper methods on `Reducer` such as getters for the `Reducer`s `Bucket`s and the ability to invoke `rebuildBuckets()` from Python, to support ""shadowing"" collective calls in join mode.

#### How is it tested?
We have tests covering the following models/scenarios:
- [x] Simple linear model
- [x] Large convolutional model
- [x] Large model with module buffers that are broadcast in the forward pass (resnet). We verify this with a helper function `will_sync_module_buffers` and ensure this is true for ResNet (due to batchnorm)
- [x] Scenario where a rank calls join() without iterating at all, so without rebuilding buckets (which requires collective comm)
- [x] Model with unused params (with find unused parameters=True)
- [x] Scenarios where different processes iterate for a varying number of different iterations.
- [x] Test consistency in tie-breaking when multiple ranks are the last ones to join
- [x] Test gradient division by the effective world_size (no. of unjoined processes) and the static world_size
- [x] Test that exceptions during training are correctly propagated by the context manager
- [x] Test expected behavior when the manager is disabled with `enable=False` (for debug purposes)
- [x] Test expected behavior when > 1 process joins early (at different iterations)
- [x] Test model equivalence to local training when used with join API.

#### How to run the tests
The main test can be run with `touch /tmp/barrier && TEMP_DIR=""/tmp"" BACKEND=""nccl"" WORLD_SIZE=""2"" python test/distributed/test_distributed.py -v TestDistBackend.test_ddp_uneven_inputs`

#### Performance implications

###### Trunk vs PR patched, 32 GPUs, batch size = 32
P50, forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.087 369/s vs 0.087 368/s

###### join(enable=True) vs without join, 32 GPUs, batch size = 32, even inputs
P50, forward + backward + optimizer batch latency & total QPS: 0.120 265/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.088 364/s vs 0.087 368/s

###### join(enable=False) vs without join, 32 GPUs, batch size = 32, even inputs
P50 forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.087 368/s vs 0.087 368/s

###### join(enable=True) with uneven inputs (offset = 2000), 32 GPUs, batch size = 32
P50 forward + backward + optimizer batch latency & total QPS: 0.183 174/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.150 213/s vs 0.087 368/s

###### join(enable=True) with uneven inputs (offset = 2000), 8 GPUs, batch size = 32
P50 forward + backward + optimizer batch latency & total QPS: 0.104 308/s vs 0.104 308/s
P50 backwards only batch latency & total QPS: 0.070 454/s vs 0.070 459/s

The 2 above uneven inputs benchmark was conducted 32 GPUs and 4 GPUs immediately depleting their inputs and entering ""join"" mode (i.e. not iterating at all), while the other 28 iterating as normal. It looks like there is a pretty significant perf hit for this case when there are uneven inputs and multi-node training. Strangely, when there is a single node (8 GPUs), this does not reproduce.

###### join(enable=True) with uneven inputs (offset = 10), 8 GPUs, batch size = 32
P50 forward + backward + optimizer batch latency & total QPS: 0.120 265/s vs 0.120   265/s
P50 backwards only batch latency & total QPS: 0.087 367/s vs 0.087   367/s
This means that there is only a difference of 10 in the uneven inputs, i.e. the early joined ranks only iterate 10 times less than the ones that iterate for the full N, instead of an all-or-nothing in the above tests.


#### Limitations
1) This is only implemented for MPSD, not SPMD. Per a discussion with @mrshenli we want to encourage the use of MPSD over SPMD for DDP.
2) This does not currently work with SyncBN or custom collective calls made in the model's forward pass. This is because the `join` class only shadows the `broadcast` for buffers in the forward pass, the gradient allreduces in the bwd pass, unused parameters reduction, and (optionally) the rebuild buckets broadcasting in the backwards pass. Supporting this will require additional design thought.
3) Has not been tested with the [DDP comm. hook](https://github.com/pytorch/pytorch/issues/39272) as this feature is still being finalized/in progress. We will add support for this in follow up PRs.
4) Has not been thoroughly tested with DDP + RPC. We plan to add support for this in follow up PRs. 

Differential Revision: [D22893859](https://our.internmc.facebook.com/intern/diff/D22893859/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D22893859/)!",pytorch
42612,supriyar,pr,2020-08-05T18:24:55Z,[quant] Create PerRowQuantizer for floating point scale and zero_point,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42881 [quant] Add torchbind support for embedding_bag packed weights
* #42762 [quant] Add embeddingbag_prepack function that works on quantized tensor.
* #42690 [quant] Make PerChannel Observer work with float qparams
* **#42612 [quant] Create PerRowQuantizer for floating point scale and zero_point**

Summary:
Add a new Quantizer that supports an input zero point (bias) that can be float.
The quantization equation in this case is

Xq = (Xf - bias) * inv_scale, where bias is float zero_point value
We start with per-row implementation and can extend to per-tensor in the future, if necessary

Test Plan:
python test/test_quantization.py TestQuantizedTensor

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D22960142](https://our.internmc.facebook.com/intern/diff/D22960142)",pytorch
42690,supriyar,pr,2020-08-06T18:41:48Z,[quant] Make PerChannel Observer work with float qparams,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42881 [quant] Add torchbind support for embedding_bag packed weights
* #42762 [quant] Add embeddingbag_prepack function that works on quantized tensor.
* **#42690 [quant] Make PerChannel Observer work with float qparams**
* #42612 [quant] Create PerRowQuantizer for floating point scale and zero_point

Summary:
Add implementation for new qscheme per_channel_affine_float_qparams in observer

Test Plan:
python test/test_quantization.py TestObserver.test_per_channel_observers

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23070633](https://our.internmc.facebook.com/intern/diff/D23070633)",pytorch
42731,vfdev-5,pr,2020-08-07T12:21:54Z,Minor typo fix,Just fixed a typo in test/test_sparse.py,pytorch
42746,rohan-varma,pr,2020-08-07T19:59:26Z,Skip test_c10d.ProcessGroupNCCLTest under TSAN,"Summary:
All of these tests fail under TSAN since we fork in a multithreaded
environment.

Test Plan: CI

Differential Revision: D23007746

",pytorch
42750,rohan-varma,pr,2020-08-07T20:42:38Z,Skip test_c10d.ProcessGroupNCCLTest under TSAN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42750 Skip test_c10d.ProcessGroupNCCLTest under TSAN**

All of these tests fail under TSAN since we fork in a multithreaded
environment.

Differential Revision: [D23007746](https://our.internmc.facebook.com/intern/diff/D23007746/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23007746/)!",pytorch
42762,supriyar,pr,2020-08-07T23:02:31Z,[quant] Add embeddingbag_prepack function that works on quantized tensor.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42881 [quant] Add torchbind support for embedding_bag packed weights
* **#42762 [quant] Add embeddingbag_prepack function that works on quantized tensor.**

Summary:
Use a prepack function that accepts qtensor as an input. The output is a byte tensor with packed data.
This is currently implemented only for 8-bit. In the future once we add 4-bit support this function will be extended to support that too.

Note -In the following change I will add TorchBind support for this to support serialization of packed weights.

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingBag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23070632](https://our.internmc.facebook.com/intern/diff/D23070632)",pytorch
42822,rohan-varma,pr,2020-08-10T18:40:38Z,Document unavailable reduction ops with NCCL backend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42822 Document unavailable reduction ops with NCCL backend**

These ops arent supported with NCCL backend and used to silently error.
We disabled them as part of addressing https://github.com/pytorch/pytorch/issues/41362, so
document that here.

Differential Revision: [D23023046](https://our.internmc.facebook.com/intern/diff/D23023046/)",pytorch
42881,supriyar,pr,2020-08-11T21:50:45Z,[quant] Add torchbind support for embedding_bag packed weights,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42881 [quant] Add torchbind support for embedding_bag packed weights**
* #42762 [quant] Add embeddingbag_prepack function that works on quantized tensor.

Summary:
This enables serialization/de-serialization of embedding packed params using getstate/setstate calls.
Added version number to deal with changes to serialization formats in future.

This can be extended in the future to support 4-bit/2-bit once we add support for that.

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingBag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23070634](https://our.internmc.facebook.com/intern/diff/D23070634)",pytorch
42922,ngimel,pr,2020-08-12T17:15:52Z,streamline stride propagation logic in TensorIterator,"Fixes #41314 among other things. 
This PR streamlines layout propagation logic in TensorIterator and removes almost all cases of channels-last hardcoding. The new rules and changes are as follows:
1) behavior of undefined `output` and defined output of the wrong (e.g. 0) size is always the same (before this PR the behavior was divergent)
2) in obvious cases (unary operation on memory-dense tensors, binary operations on memory-dense tensors with the same layout) strides are propagated (before propagation was inconsistent) (see footnote)
3) in other cases the output permutation is obtained as inverse permutation of sorting inputs by strides. Sorting is done with comparator obeying the following rules: strides of broadcasted dimensions are set to 0, and 0 compares equal to anything. Strides of not-broadcasted dimensions (including dimensions of size `1`) participate in sorting. Precedence is given to the first input, in case of a tie in the first input, first the corresponding dimensions are considered, and if that does not indicate that swap is needed, strides of the same dimension in subsequent inputs are considered. See changes in `reorder_dimensions` and `compute_strides`. Note that first inspecting dimensions of the first input allows us to better recover it's permutation (and we select this behavior because it more reliably propagates channels-last strides) but in some rare cases could result in worse traversal order for the second tensor. 
 
These rules are enough to recover previously hard-coded behavior related to channels last, so all existing tests are passing.
In general, these rules will produce intuitive results, and in most cases permutation of the full size input (in case of broadcasted operation) will be recovered, or permutation of the first input (in case of same sized inputs) will be recovered, including cases with trivial (1) dimensions. As an example of the latter, the following tensor
```
x=torch.randn(2,1,3).permute(1,0,2)
```
will produce output with the same stride (3,3,1) in binary operations with 1d tensor. Another example is a tensor of size N1H1 that has strides `H,H,1,1` when contiguous and `H, 1, 1, 1` when channels-last. The output retains these strides in binary operations when another 1d tensor is broadcasted on this one.  

Footnote: for ambiguous cases where all inputs are memory dense and have the same physical layout that nevertheless can correspond to different permutations, such as e.g. NC11-sized physically contiguous tensors, regular contiguous tensor is returned, and thus permutation information of the input is lost (so for NC11 channels-last input had the strides `C, 1, C, C`, but output will have the strides `C, 1, 1, 1`). This behavior is unchanged from before and consistent with numpy, but it still makes sense to change it. The blocker for doing it currently is performance of `empty_strided`. Once we make it on par with `empty` we should be able to propagate layouts in these cases. For now, to not slow down common contiguous case, we default to contiguous. 
The table below shows how in some cases current behavior loses permutation/stride information, whereas new behavior propagates permutation. 
| code                                                                                                                                                                                           | old                                                   | new                                                  |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|------------------------------------------------------|
| #strided tensors<br>a=torch.randn(2,3,8)[:,:,::2].permute(2,0,1)<br>print(a.stride())<br>print(a.exp().stride())<br>print((a+a).stride())<br>out = torch.empty(0)<br>torch.add(a,a,out=out)<br>print(out.stride()) | (2, 24, 8) <br>(6, 3, 1) <br>(1, 12, 4) <br>(6, 3, 1) | (2, 24, 8)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4) |
| #memory dense tensors<br>a=torch.randn(3,1,1).as_strided((3,1,1), (1,3,3))<br>print(a.stride(), (a+torch.randn(1)).stride())<br>a=torch.randn(2,3,4).permute(2,0,1)<br>print(a.stride())<br>print(a.exp().stride())<br>print((a+a).stride())<br>out = torch.empty(0)<br>torch.add(a,a,out=out)<br>print(out.stride())                                                                                                                                                                                               |  (1, 3, 3) (1, 1, 1)<br>(1, 12, 4)<br>(6, 3, 1)<br>(1, 12, 4)<br>(6, 3, 1)                                                       |  (1, 3, 3) (1, 3, 3)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4)<br>(1, 12, 4) |




",pytorch
42924,supriyar,pr,2020-08-12T17:27:21Z,[quant][pyper] Make offsets an optional paramter in the qembedding_bag op,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42924 [quant][pyper] Make offsets an optional paramter in the qembedding_bag op**

Summary:
offsets is an optional paramter in the python module currently. So we update the operator to follow suit
in order to avoid bad optional access

Test Plan:
python test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23081152](https://our.internmc.facebook.com/intern/diff/D23081152)",pytorch
42932,rohan-varma,pr,2020-08-12T18:45:28Z,Rename test_distributed to test_distributed_fork,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42932 Rename test_distributed to test_distributed_fork**
* #41769 Enable test_distributed to work with spawn mode

Follow up from https://github.com/pytorch/pytorch/pull/41769, rename `test_distributed` to `test_distributed_fork` to make it explicit that it forks.

New command to run test:
`python test/run_test.py -i distributed/test_distributed_fork -v`

Differential Revision: [D23072201](https://our.internmc.facebook.com/intern/diff/D23072201/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23072201/)!",pytorch
43050,vfdev-5,pr,2020-08-14T08:45:32Z,Fix typo in collect_env.py,"Minor typo fix introduced in yesterdays PR: https://github.com/pytorch/pytorch/pull/42961
",pytorch
43088,supriyar,pr,2020-08-15T00:13:55Z,[quant] Create nn.quantized.dynamic.EmbeddingBag,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43296 [quant] Add benchmarks for quantized embeddingbag module
* #43176 [quant] Enable from_float for quantized Embedding_Bag
* #43090 [quant] Make offsets an optional argument
* **#43088 [quant] Create nn.quantized.dynamic.EmbeddingBag**

Summary:
Create quantized module that the user can use to perform embedding bag quantization
The module uses the EmbeddingPackedParams to store the weights which can be serialized /deserialized
using TorchBind custom classes (C++ get/setstate code)
Following PR will add support for `from_float` to convert from float to quantized module

Test Plan:
python test/test_quantization.py TestDynamicQuantizedModule.test_embedding_bag_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23167519](https://our.internmc.facebook.com/intern/diff/D23167519)",pytorch
43090,supriyar,pr,2020-08-15T00:21:12Z,[quant] Make offsets an optional argument,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43296 [quant] Add benchmarks for quantized embeddingbag module
* #43176 [quant] Enable from_float for quantized Embedding_Bag
* **#43090 [quant] Make offsets an optional argument**
* #43088 [quant] Create nn.quantized.dynamic.EmbeddingBag

Summary:
To match the floating point module

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23167518](https://our.internmc.facebook.com/intern/diff/D23167518)",pytorch
43148,ngimel,pr,2020-08-17T18:42:05Z,remove dot from TH,"small cleanup of dead code
",pytorch
43176,supriyar,pr,2020-08-17T23:49:53Z,[quant] Enable from_float for quantized Embedding_Bag,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43296 [quant] Add benchmarks for quantized embeddingbag module
* **#43176 [quant] Enable from_float for quantized Embedding_Bag**
* #43090 [quant] Make offsets an optional argument
* #43088 [quant] Create nn.quantized.dynamic.EmbeddingBag

Summary:
Convert floating point nn.EmbeddingBag module to
nn.quantized.dynamic.EmbeddingBag module

Test Plan:
python test/test_quantization.py TestDynamicQuantizedModule.test_embedding_bag_api
python test/test_quantization.py TestPostTrainingDynamic.test_embedding_quantization

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23200196](https://our.internmc.facebook.com/intern/diff/D23200196)",pytorch
43184,peterjc123,pr,2020-08-18T02:01:32Z,Pin VC++ version to 14.26 ,"VC++14.27 fails to compile mkl-dnn, see oneapi-src/oneDNN#812",pytorch
43291,supriyar,pr,2020-08-19T22:03:03Z,[quant] Add benchmakrs for embedding_bag coversion ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43291 [quant] Add benchmakrs for embedding_bag coversion ops**

Summary:
Test Float2Fused and Fused2Float conversion operators for embedding_bag byte and 4-bit ops

Test Plan:

```
python -m pt.qembedding_pack_tes
```

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23231641](https://our.internmc.facebook.com/intern/diff/D23231641)",pytorch
43292,supriyar,pr,2020-08-19T22:36:00Z,[quant] Add benchmarks for quantized embeddingbag module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43292 [quant] Add benchmarks for quantized embeddingbag module**
* #43176 [quant] Enable from_float for quantized Embedding_Bag
* #43090 [quant] Make offsets an optional argument
* #43088 [quant] Create nn.quantized.dynamic.EmbeddingBag

Summary:
Use common config for float and quantized embedding_bag modules

Test Plan:
```
python -m pt.qembeddingbag_test

Forward Execution Time (us) : 35.738

Forward Execution Time (us) : 62.708

python -m pt.embeddingbag_test

Forward Execution Time (us) : 46.878

Forward Execution Time (us) : 103.904

```

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
43295,supriyar,pr,2020-08-19T22:46:40Z,[quant] Add benchmarks for quantized embeddingbag module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43295 [quant] Add benchmarks for quantized embeddingbag module**
* #43176 [quant] Enable from_float for quantized Embedding_Bag
* #43090 [quant] Make offsets an optional argument
* #43088 [quant] Create nn.quantized.dynamic.EmbeddingBag

Summary:
Use common config for float and quantized embedding_bag modules

Test Plan:
```
python -m pt.qembeddingbag_test

Forward Execution Time (us) : 35.738

Forward Execution Time (us) : 62.708

python -m pt.embeddingbag_test

Forward Execution Time (us) : 46.878

Forward Execution Time (us) : 103.904

```

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
43296,supriyar,pr,2020-08-19T22:49:14Z,[quant] Add benchmarks for quantized embeddingbag module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43296 [quant] Add benchmarks for quantized embeddingbag module**
* #43176 [quant] Enable from_float for quantized Embedding_Bag
* #43090 [quant] Make offsets an optional argument
* #43088 [quant] Create nn.quantized.dynamic.EmbeddingBag

Summary:
Use common config for float and quantized embedding_bag modules

Test Plan:
```
python -m pt.qembeddingbag_test

 Benchmarking PyTorch: qEmbeddingBag
 Mode: Eager
 Name: qEmbeddingBag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
 Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Forward Execution Time (us) : 35.738

 Benchmarking PyTorch: qEmbeddingBag
 Mode: Eager
 Name: qEmbeddingBag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
 Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Forward Execution Time (us) : 62.708

python -m pt.embeddingbag_test

 Benchmarking PyTorch: embeddingbag
 Mode: Eager
 Name: embeddingbag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
 Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Forward Execution Time (us) : 46.878

 Benchmarking PyTorch: embeddingbag
 Mode: Eager
 Name: embeddingbag_embeddingbags10_dim4_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
 Input: embeddingbags: 10, dim: 4, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Forward Execution Time (us) : 103.904

```

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23245531](https://our.internmc.facebook.com/intern/diff/D23245531)",pytorch
43330,vfdev-5,pr,2020-08-20T14:34:05Z,Added sparse support for abs and sign functions,"Description:

- [x] added a template implementation for unary sparse functions
- [x] added C++ code for sparse `abs` and `sign` ops similarly to `log1p` op
- [x] refactored `log1p`, `asin`, `neg` ops
- [x] temporarily refactored sparse tests for above functions => in other PR we can rewrite tests with `UnaryUfuncInfo` (https://github.com/pytorch/pytorch/pull/44028#issuecomment-696542189)
- Backprop is for other PRs

--- 
OLD

- [ ] added C++ code for sparse `abs` and `sign` ops similarly to `log1p` op
- [ ] added tests
  - [ ] coalesced input CPU/CUDA
  - [ ] uncoalesced input CPU/CUDA
  - [ ] backprop for `abs`. 
  - [ ] TODO: backprop for `sign` -> to fix in another PR
- [ ] added tests for `absolute`   


Blocked by https://github.com/pytorch/pytorch/issues/43699

",pytorch
43433,supriyar,pr,2020-08-21T22:41:48Z,[quant] Support set API for EmbeddingBag quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43433 [quant] Support set API for EmbeddingBag quantization**

Summary:
Add support for torch.quint8 dtype

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23277002](https://our.internmc.facebook.com/intern/diff/D23277002)",pytorch
43462,ssnl,pr,2020-08-23T02:02:20Z,Fix unintended error when worker force kill happens #43455,Fixes #43455,pytorch
43468,rohan-varma,pr,2020-08-23T07:54:42Z,"Allow GPU skip decorators to report the right number of GPUs required in
skipped tests","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43468 Allow GPU skip decorators to report the right number of GPUs required in
skipped tests**
skipped tests**

Closes https://github.com/pytorch/pytorch/issues/41378.
https://github.com/pytorch/pytorch/pull/41973 enhanced the skip decorators to
report the right no. of GPUs required, but this information was not passed to
the main process where the message is actually displayed. This PR uses a
`multiprocessing.Manager()` so that the dictionary modification is reflected
correctly in the main process.

With this diff, we can run a test in https://github.com/pytorch/pytorch/pull/42577 that requires 4 GPUs on a 2 GPU machine, and we get the expected message:

```
test_ddp_uneven_inputs_replicated_error (test_distributed.TestDistBackend) ... skipped 'Need at least 4 CUDA devices'
```

Differential Revision: [D23285790](https://our.internmc.facebook.com/intern/diff/D23285790/)",pytorch
43479,peterjc123,pr,2020-08-23T15:31:07Z,Correct the windows docs,"Fixes https://discuss.pytorch.org/t/i-cannot-use-the-pytorch-that-was-built-successfully-from-source-dll-initialization-routine-failed-error-loading-caffe2-detectron-ops-gpu-dll/93243/5?u=peterjc123.
",pytorch
43543,rohan-varma,pr,2020-08-25T08:54:20Z,Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43543 Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce**

Closes https://github.com/pytorch/pytorch/issues/14691. This is not needed in the multiple outputs case, because gloo allreduce
broadcasts the result tensor to all the outputs. See
https://github.com/facebookincubator/gloo/issues/152 and commit
https://github.com/facebookincubator/gloo/commit/9cabb5aaa4f02356bc8db05e5630cb550b3f5b5c
for more details. Came across this when debugging https://github.com/pytorch/pytorch/pull/42577.

This effectively reverts https://github.com/pytorch/pytorch/pull/14688 while keeping the tests.

Tested by ensuring `test_allreduce_basics` in `test_c10d.py` still works as expected.

Differential Revision: [D23173945](https://our.internmc.facebook.com/intern/diff/D23173945/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23173945/)!",pytorch
43594,supriyar,pr,2020-08-26T00:03:07Z,[quant] Add custom freeze pass for quantization attributes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43595 [quant] Expose quantization freeze pass in convert API
* **#43594 [quant] Add custom freeze pass for quantization attributes**

Summary:

This PR decouples quantization finalize pass from the geenric `freeze_module` pass
It only focuses on freezing the attributes related to quantization params so that they can later
be folded into the graph, and used by subsequent JIT pass like prepack_folding

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23333263](https://our.internmc.facebook.com/intern/diff/D23333263)",pytorch
43595,supriyar,pr,2020-08-26T00:03:13Z,[quant] Expose quantization freeze pass in convert API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43595 [quant] Expose quantization freeze pass in convert API**
* #43594 [quant] Add custom freeze pass for quantization attributes

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23333265](https://our.internmc.facebook.com/intern/diff/D23333265)",pytorch
43648,rohan-varma,pr,2020-08-26T19:21:25Z,[Not for review] Ci all ddp uneven inputs,"Fixes #{issue number}
",pytorch
43657,rohan-varma,pr,2020-08-26T21:01:09Z,"[RPC profiling] Add test to ensure using record_function works for RPC
functions","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43657 [RPC profiling] Add test to ensure using record_function works for RPC
functions**
functions**
functions**
functions**

We didn't have a test that ensures functions ran over RPC that are being profiled can use `with record_function()` to profile specific blocks in the function execution. This is useful if the user wants certain information about specific blocks in the function ran over RPC composed of many torch ops and some custom logic, for example.

Currently, this will not work if the function is TorchScripted since `with record_function()` is not torchscriptable yet. We can add support for this in future PRs so that torchscript RPC functions can also be profiled like this.

Differential Revision: [D23355215](https://our.internmc.facebook.com/intern/diff/D23355215/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23355215/)!",pytorch
43773,ngimel,pr,2020-08-28T16:30:36Z,"unify empty_cpu and empty_cuda, call native::empty where appropriate","per title
",pytorch
43818,ngimel,pr,2020-08-29T01:16:40Z,"Back out ""[pytorch][PR] Improve zero sized input for addmv""","Summary: Original commit changeset: aa6af96fe74e

Test Plan: existing tests

Reviewed By: malfet

Differential Revision: D23412113

",pytorch
43828,peterjc123,pr,2020-08-29T04:54:20Z,Check for intel-openmp DLLs before importing torch,Fixes https://github.com/pytorch/pytorch/issues/35803#issuecomment-682356457,pytorch
43887,rohan-varma,pr,2020-08-31T17:59:37Z,broadcast_object API for c10d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43932 [Docs] Add examples for new object-based c10d APIs
* #43930 scatter_object_list API for c10d
* **#43887 broadcast_object API for c10d**

As part of addressing #23232, this PR adds support for `broadcast_object_list` which is an API to broadcast arbitrary picklable objects to all the other ranks.  This has been a long-requested feature, so would be good for Pytorch to natively support this.

The implementation approach follows a similar approach as https://github.com/pytorch/pytorch/pull/42189. The input is a list of objects to be broadcasted and it is in place, meaning all ranks part of the group will have their input list modified to contain the broadcasted objects from the src rank.

Note that the API is designed to match the tensor-based collectives other than supporting async_op. For now, it is a blocking call. If we see demand to support async_op, we will have to make more progress on merging work/future to support this.

Differential Revision: [D23422577](https://our.internmc.facebook.com/intern/diff/D23422577/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23422577/)!",pytorch
43930,rohan-varma,pr,2020-09-01T02:02:01Z,scatter_object_list API for c10d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43930 scatter_object_list API for c10d**

Closes #23232. As part of addressing #23232, this PR adds support for scatter_object_list which is an API to scatter arbitrary picklable objects to all the other ranks.

The implementation approach follows a similar approach as https://github.com/pytorch/pytorch/pull/42189. The result of the `scatter` is stored as the first element of `scatter_object_output_list`, and the src rank is expected to provide an input list `scatter_object_input_list` which contains the objects to scatter.

Note that this API requires 1 broadcast and 2 scatters. This is because we must communicate the maximum object size to be scattered, which only the src rank knows about. After that, we also need to communicate the objects themselves as well as the true sizes of the object.

Note that the API is designed to match the tensor-based collectives other than supporting async_op. For now, it is a blocking call. If we see demand to support async_op, we will have to make more progress on merging work/future to support this.

It only works for Gloo because NCCL doesn't support scatter.

Differential Revision: [D23430686](https://our.internmc.facebook.com/intern/diff/D23430686/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23430686/)!",pytorch
43932,rohan-varma,pr,2020-09-01T02:16:29Z,[Docs] Add examples for new object-based c10d APIs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43932 [Docs] Add examples for new object-based c10d APIs**
* #48909 Add object-based collective APIs to public docs

Adds some basic examples to the documentation for each of the newly added
object-based collectives.

Differential Revision: [D23441838](https://our.internmc.facebook.com/intern/diff/D23441838/)",pytorch
43935,fritzo,pr,2020-09-01T03:52:59Z,Add broadcast_shapes() function and use it in MultivariateNormal,"Fixes #43837

This adds a `torch.broadcast_shapes()` function similar to Pyro's [broadcast_shape()](https://github.com/pyro-ppl/pyro/blob/7c2c22c10dffda8a33ffbd593cc8d58819959e40/pyro/distributions/util.py#L151) and JAX's [lax.broadcast_shapes()](https://jax.readthedocs.io/en/test-docs/_modules/jax/lax/lax.html). This helper is useful e.g. in multivariate distributions that are parameterized by multiple tensors and we want to `torch.broadcast_tensors()` but the parameter tensors have different ""event shape"" (e.g. mean vectors and covariance matrices). This helper is already heavily used in Pyro's distribution codebase, and we would like to start using it in `torch.distributions`.

## Tasks
- [x] add a `torch.broadcast_shapes()` function similar to Pyro's [broadcast_shape()](https://github.com/pyro-ppl/pyro/blob/7c2c22c10dffda8a33ffbd593cc8d58819959e40/pyro/distributions/util.py#L151).
- [x] refactor `MultivariateNormal`'s expansion logic to use `torch.broadcast_shapes()`
- [x] add unit tests for `torch.broadcast_shapes()`
- [x] add docs

cc @neerajprad ",pytorch
43973,rohan-varma,pr,2020-09-01T19:15:15Z,Polish DDP join API docstrings,Polishes DDP join api docstrings and makes a few minor cosmetic changes. ,pytorch
43989,supriyar,pr,2020-09-01T21:25:39Z,[quant][pyper] Support aten::embedding_bag quantization in graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44048 [quant][pyper] Support quantization of ops in fork-wait subgraph
* #44008 [quant][pyper] make embedding_bag quantization static
* **#43989 [quant][pyper] Support aten::embedding_bag quantization in graph mode**

Summary:
When we trace the model it produces aten::embedding_bag node in the graph,
Add necessary passes in graph mode to help support quantizing it as well

Test Plan:
python test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23460485](https://our.internmc.facebook.com/intern/diff/D23460485)",pytorch
44000,rohan-varma,pr,2020-09-02T00:16:59Z,Document the default behavior for dist.new_group() when ranks=None,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44000 Document the default behavior for dist.new_group() when ranks=None**

This wasn't documented, so add a doc saying all ranks are used when
ranks=None

Differential Revision: [D23465034](https://our.internmc.facebook.com/intern/diff/D23465034/)",pytorch
44007,peterjc123,pr,2020-09-02T01:16:38Z,[WIP] Re-enable Windows binary jobs for CUDA 11,"Fixes https://github.com/pytorch/pytorch/pull/43366/files#r474333051.
",pytorch
44008,supriyar,pr,2020-09-02T01:23:23Z,[quant][pyper] make embedding_bag quantization static,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44048 [quant][pyper] Support quantization of ops in fork-wait subgraph
* **#44008 [quant][pyper] make embedding_bag quantization static**
* #43989 [quant][pyper] Support aten::embedding_bag quantization in graph mode

Summary:
embedding_bag requires only quantization of weights (no dynamic quantization of inputs)
So the type of quantization is essentially static (without calibration)
This will enable pyper to do fc and embedding_bag quantization using the same API call

Test Plan:
python test/test_quantization.py test_embedding_bag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23467019](https://our.internmc.facebook.com/intern/diff/D23467019)",pytorch
44028,vfdev-5,pr,2020-09-02T15:27:04Z,"Added sparse support for asin and neg functions, updated log1p","

Description:

- [x] added C++ code for sparse `asin` and `neg` ops similarly to `log1p` op
- [x] added tests
  - [x] coalesced input CPU/CUDA
  - [x] uncoalesced input CPU/CUDA
- [x] added tests for `negative`  and `arcsin` 

Backprop will be addressed in another PR.",pytorch
44048,supriyar,pr,2020-09-02T19:06:11Z,[quant][pyper] Support quantization of ops in fork-wait subgraph,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44048 [quant][pyper] Support quantization of ops in fork-wait subgraph**
* #44008 [quant][pyper] make embedding_bag quantization static
* #43989 [quant][pyper] Support aten::embedding_bag quantization in graph mode

Summary:
Inline the fork-wait calls to make sure we can see the ops to be quantized in the main graph

Also fix the InlineForkWait JIT pass to account for the case where the aten::wait call isn't present in the main graph
and we return future tensor from subgraph

Example

```
graph(%self.1 : __torch__.dper3.core.interop.___torch_mangle_6325.DperModuleWrapper,
       %argument_1.1 : Tensor,
       %argument_2.1 : Tensor):
   %3 : Future[Tensor[]] = prim::fork_0(%self.1, %argument_1.1, %argument_2.1) # :0:0
   return (%3)
 with prim::fork_0 = graph(%self.1 : __torch__.dper3.core.interop.___torch_mangle_5396.DperModuleWrapper,
       %argument_1.1 : Tensor,
       %argument_2.1 : Tensor):
   %3 : __torch__.dper3.core.interop.___torch_mangle_6330.DperModuleWrapper = prim::GetAttr[name=""x""](%self.1)
   %4 : __torch__.dper3.core.interop.___torch_mangle_5397.DperModuleWrapper = prim::GetAttr[name=""y""](%self.1)
   %5 : __torch__.dper3.core.interop.___torch_mangle_6327.DperModuleWrapper = prim::GetAttr[name=""z""](%4)
   %6 : Tensor = prim::CallMethod[name=""forward""](%5, %argument_1.1, %argument_2.1) # :0:0
   %7 : None = prim::CallMethod[name=""forward""](%3, %6) # :0:0
   %8 : Tensor[] = prim::ListConstruct(%6)
   return (%8)
```

Test Plan:
python test/test_quantization.py test_interface_with_fork

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23481003](https://our.internmc.facebook.com/intern/diff/D23481003)",pytorch
44058,vfdev-5,pr,2020-09-02T20:34:06Z,torch.empty_like and torch.zeros_like raise error if any memory format is provided with sparse input (#43699),"Fixes #43699 

- Changed the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`
inside `empty_like` method.

- [x] Added tests

EDIT: 

More details on that and why we can not take zeros_like  approach.
Python code :
```python
res = torch.zeros_like(input_coalesced, memory_format=torch.preserve_format)
```
is routed to
```c++
// TensorFactories.cpp
Tensor zeros_like(
    const Tensor& self,
    const TensorOptions& options,
    c10::optional<c10::MemoryFormat> optional_memory_format) {
  if (options.layout() == kSparse && self.is_sparse()) {
    auto res = at::empty({0}, options); // to be resized
    res.sparse_resize_and_clear_(
        self.sizes(), self.sparse_dim(), self.dense_dim());
    return res;
  }
  auto result = at::empty_like(self, options, optional_memory_format);
  return result.zero_();
}
```
and passed to `if (options.layout() == kSparse && self.is_sparse())`

When we call in Python
```python
res = torch.empty_like(input_coalesced, memory_format=torch.preserve_format)
```
it is routed to
```c++
Tensor empty_like(
    const Tensor& self,
    const TensorOptions& options_,
    c10::optional<c10::MemoryFormat> optional_memory_format) {
  TORCH_CHECK(
    !(options_.has_memory_format() && optional_memory_format.has_value()),
    ""Cannot set memory_format both in TensorOptions and explicit argument; please delete ""
    ""the redundant setter."");
  TensorOptions options =
      self.options()
          .merge_in(options_)
          .merge_in(TensorOptions().memory_format(optional_memory_format));
  TORCH_CHECK(
      !(options.layout() != kStrided &&
          optional_memory_format.has_value()),
      ""memory format option is only supported by strided tensors""); 
  if (options.layout() == kSparse && self.is_sparse()) {
    auto result = at::empty({0}, options); // to be resized
    result.sparse_resize_and_clear_(
        self.sizes(), self.sparse_dim(), self.dense_dim());
    return result;
  }
```

cc @pearu 
",pytorch
44085,peterjc123,pr,2020-09-03T01:06:14Z,Fix CUDA debug nightly build failure,"Fixes https://github.com/pytorch/pytorch/issues/43607.
Tested in https://github.com/pytorch/pytorch/pull/44007.",pytorch
44086,peterjc123,pr,2020-09-03T01:13:40Z,Enable CUDA 11 jobs for Windows nightly builds,"Fixes https://github.com/pytorch/pytorch/pull/43366/files#r474333051.
Tested with https://github.com/pytorch/pytorch/pull/44007.
",pytorch
44207,supriyar,pr,2020-09-04T18:06:26Z,[quant] Support quantization of embedding lookup operators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44217 [quant] Move EmbeddingBag eager quantization to static
* #44208 [quant] Add quantized Embedding module
* **#44207 [quant] Support quantization of embedding lookup operators**

Summary:
Use existing embedding_bag operator but set offsets to [0, 1, .. len(indices)]
Test Plan:
python test/test_quantization.py TestEmbeddingOps.test_embedding_byte

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23547385](https://our.internmc.facebook.com/intern/diff/D23547385)",pytorch
44208,supriyar,pr,2020-09-04T18:06:33Z,[quant] Add quantized Embedding module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44217 [quant] Move EmbeddingBag eager quantization to static
* **#44208 [quant] Add quantized Embedding module**
* #44207 [quant] Support quantization of embedding lookup operators

Summary:
Add quantized module in static quantization namespace. Embedding
quantization requires only weights to be quantized so it is static.
Internally it calls the embedding_bag_byte op with the offsets set corresponding to the
indices.

Future PR will move EmbeddingBag quantization from dynamic to static as well.

Test Plan:
python test/test_quantization.py test_embedding_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23547384](https://our.internmc.facebook.com/intern/diff/D23547384)",pytorch
44217,supriyar,pr,2020-09-04T19:33:09Z,[quant] Move EmbeddingBag eager quantization to static,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44217 [quant] Move EmbeddingBag eager quantization to static**
* #44208 [quant] Add quantized Embedding module
* #44207 [quant] Support quantization of embedding lookup operators

Summary:
Move the tests to static ones as well

Test Plan:
 python test/test_quantization.py TestStaticQuantizedModule.test_embedding_bag_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23547386](https://our.internmc.facebook.com/intern/diff/D23547386)",pytorch
44220,rohan-varma,pr,2020-09-04T20:16:25Z,Correctly convert namedtuples in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44220 Correctly convert namedtuples in DDP**

Closes https://github.com/pytorch/pytorch/issues/44009
Currently if a dataloader returns objects created with a
`collections.namedtuple` or `typing.NamedTuple`, this will incorrectly be cast to a tuple. As a result, if we have data of these types, there can be runtime errors during the forward pass if the module is expecting a named tuple.

Fix this in
`scatter_gather.py` to resolve the issue reported in
https://github.com/pytorch/pytorch/issues/44009

Differential Revision: [D23536752](https://our.internmc.facebook.com/intern/diff/D23536752/)",pytorch
44259,ngimel,pr,2020-09-06T06:13:56Z,Port nonzero cuda from THC to ATen,"1) Ports nonzero from THC to ATen
2) replaces most thrust uses with cub, to avoid synchronization and to improve performance. There is still one necessary synchronization point, communicating number of nonzero elements from GPU to CPU
3) slightly changes algorithm, now we first compute the number of nonzeros, and then allocate correct-sized output, instead of allocating full-sized output as was done before, to account for possibly all elements being non-zero
4) unfortunately, since the last transforms are still done with thrust, 2) is slightly beside the point, however it is a step towards a future without thrust
4) hard limits the number of elements in the input tensor to MAX_INT. Previous implementation allocated a Long tensor with the size ndim*nelements, so that would be at least 16 GB for a tensor with MAX_INT elements. It is reasonable to say that larger tensors could not be used anyway. 

Benchmarking is done for tensors with approximately half non-zeros
<details><summary>Benchmarking script</summary>
<p>

```
import torch
from torch.utils._benchmark import Timer
from torch.utils._benchmark import Compare
import sys

device = ""cuda""
results = []
for numel in (1024 * 128,):#, 1024 * 1024, 1024 * 1024 * 128):
    inp = torch.randint(2, (numel,), device=""cuda"", dtype=torch.float)
    for ndim in range(2,3):#(1,4):
        if ndim == 1:
            shape = (numel,)
        elif ndim == 2:
            shape = (1024, numel // 1024)
        else:
            shape = (1024, 128, numel // 1024 // 128)
        inp = inp.reshape(shape)
        repeats = 3
        timer = Timer(stmt=""torch.nonzero(inp, as_tuple=False)"", label=""Nonzero"", sub_label=f""number of elts {numel}"",
        description = f""ndim {ndim}"", globals=globals())
        for i in range(repeats):
            results.append(timer.blocked_autorange())
        print(f""\rnumel {numel} ndim {ndim}"", end="""")
        sys.stdout.flush()

comparison = Compare(results)
comparison.print()
```
</p>
</details>

### Results
Before:
```
[--------------------------- Nonzero ---------------------------]
                                 |  ndim 1  |   ndim 2  |   ndim 3
 1 threads: ------------------------------------------------------
       number of elts 131072     |    55.2  |     71.7  |     90.5
       number of elts 1048576    |   113.2  |    250.7  |    497.0
       number of elts 134217728  |  8353.7  |  23809.2  |  54602.3

 Times are in microseconds (us).
```
After:
```
[-------------------------- Nonzero --------------------------]
                                |  ndim 1  |  ndim 2  |  ndim 3
1 threads: ----------------------------------------------------
      number of elts 131072     |    48.6  |    79.1  |    90.2
      number of elts 1048576    |    64.7  |   134.2  |   161.1
      number of elts 134217728  |  3748.8  |  7881.3  |  9953.7

Times are in microseconds (us).

```
There's a real regression for smallish 2D tensor due to added work of computing number of nonzero elements, however, for other sizes there are significant gains, and there are drastically lower memory requirements. Perf gains would be even larger for tensors with fewer nonzeros.",pytorch
44261,rohan-varma,pr,2020-09-06T08:05:31Z,Ci all/rohan/ci all rename fork,"Fixes #{issue number}
",pytorch
44280,peterjc123,pr,2020-09-07T14:42:53Z,Don't use VCOMP if Intel OMP is used,"Fixes https://github.com/pytorch/pytorch/issues/44096.
",pytorch
44345,rohan-varma,pr,2020-09-09T00:07:17Z,TorchScript with record_function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44419 Support record_shapes in RPC profiling
* **#44345 TorchScript with record_function**

As part of enhancing profiler support for RPC, when executing TorchScript functions over RPC, we would like to be able to support user-defined profiling scopes created by `with record_function(...)`.


Since after https://github.com/pytorch/pytorch/pull/34705, we support `with` statements in TorchScript, this PR adds support for `with torch.autograd.profiler.record_function` to be used within TorchScript.

This can be accomplished via the following without this PR:
```
torch.opts.profiler._record_function_enter(...)
# Script code, such as forward pass
torch.opts.profiler._record_function_exit(....)
```

This is a bit hacky and it would be much cleaner to use the context manager now that we support `with` statements. Also, `_record_function_` type operators are internal operators that are subject to change, this change will help avoid BC issues in the future.

Tested with `python test/test_jit.py TestWith. test_with_record_function -v`

Differential Revision: [D23332074](https://our.internmc.facebook.com/intern/diff/D23332074/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23332074/)!",pytorch
44419,rohan-varma,pr,2020-09-09T20:21:55Z,Support record_shapes in RPC profiling ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44419 Support record_shapes in RPC profiling**

Closes https://github.com/pytorch/pytorch/issues/39969

This PR adds support for propagation of input shapes over the wire when the profiler is invoked with `record_shapes=True` over RPC. Previously, we did not respect this argument.

This is done by saving the shapes as an ivalue list and recovering it as the type expected (`std::vector<std::vector<int>>` on the client). Test is added to ensure that remote ops have the same `input_shapes` as if the op were run locally.

Remote ops profiler output before (run with `group_by_input_shapes=True`):

```
rpc_sync#aten::add(worker1 -> worker2)#remote_op: aten::add        0.18%            67.229us         0.21%            76.449us         76.449us         1                2                []
rpc_sync#aten::add(worker1 -> worker2)#remote_op: aten::empty      0.03%            9.220us          0.03%            9.220us          9.220us          1                2                []
```

After:
```
rpc_sync#aten::add(worker1 -> worker2)#remote_op: aten::add        0.14%            91.001us         0.16%            102.094us        102.094us        1                2                [[100], [100], []]
rpc_sync#aten::add(worker1 -> worker2)#remote_op: aten::empty      0.02%            11.093us         0.02%            11.093us         11.093us         1                2                [[], [], [], [], [], []]
```

Differential Revision: [D23591274](https://our.internmc.facebook.com/intern/diff/D23591274/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23591274/)!",pytorch
44423,ngimel,pr,2020-09-09T21:05:53Z,Add reset_grad() function (#42754),"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42754

Test Plan: Imported from OSS

Differential Revision: D23010859

Pulled By: firstprayer

",pytorch
44427,rohan-varma,pr,2020-09-09T21:30:10Z,Fix DDP join() API in the case of model.no_sync(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44427 Fix DDP join() API in the case of model.no_sync()**

Closes https://github.com/pytorch/pytorch/issues/44425
DDP join API currently does not work properly with `model.no_sync()`, see https://github.com/pytorch/pytorch/issues/44425 for details. This PR fixes the problem via the approach mentioned in the issue, namely scheduling an allreduce that tells joined ranks whether to sync in the backwards pass or not. Tests are added for skipping gradient synchronization for various `sync_interval`s. 

Differential Revision: [D23609070](https://our.internmc.facebook.com/intern/diff/D23609070/)",pytorch
44439,rohan-varma,pr,2020-09-09T22:53:09Z,Add a test to ensure DDP join works with RPC,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44439 Add a test to ensure DDP join works with RPC**

Adds a test to ddp_under_dist_autograd_test to enusre that that uneven
inputs join() API works properly when DDP + RPC is combined. We test that when
running in outside DDP mode (DDP applied to whole hybrid module) we can
correctly process uneven inputs across different trainers.

Differential Revision: [D23612409](https://our.internmc.facebook.com/intern/diff/D23612409/)",pytorch
44445,ssnl,pr,2020-09-10T01:10:28Z,Clarify track_running_stats docs; Make SyncBatchNorm track_running_stats behavior consistent,"context: https://github.com/pytorch/pytorch/pull/38084

Fixes #{issue number}
",pytorch
44525,rohan-varma,pr,2020-09-11T00:55:21Z,"In common_distributed, fix TEST_SKIPS multiprocessing manager","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44525 In common_distributed, fix TEST_SKIPS multiprocessing manager**

Since `TEST_SKIPS` is a global multiprocessing.manager, this was causing
issues when one test would fail and make the rest of the tests fail during
setup due to networking errors.

See the failed CI job: https://app.circleci.com/pipelines/github/pytorch/pytorch/212491/workflows/0450151d-ca09-4cf6-863d-272de6ed917f/jobs/7389065 for an example, where `test_ddp_backward` failed but then caused the rest of the tests to fail at the line `test_skips.update(TEST_SKIPS)`.

To fix this issue, at the end of every test we revert `TEST_SKIPS` back to a regular dict, and redo the conversion to a `mulitiprocessing.Manager` in the next test, which prevents these errors.

Differential Revision: [D23641618](https://our.internmc.facebook.com/intern/diff/D23641618/)",pytorch
44574,supriyar,pr,2020-09-11T23:31:27Z,[wip][prototype] creating quint4x2 dtype for quantized tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44574 [wip][prototype] creating quint4x2 dtype for quantized tensors**

Summary:
This is a prototype PR that introduces 4 bit qtensors. The new dtype added for this is c10::quint4x2
The underlying storage for this is still uint8_t, so we pack 2 4-bit values in a byte while quantizing it.

This change uses most of the existing scaffolding for qtensor storage. We allocate storage
based on the dtype before creating a new qtensor.

It also adds a dispatch mechanism for this dtype so that in the future we can use this to get the bitrate
while packing the qtensor (when we add 2-bit qtensor)

Kernels that use this dtype should be aware of the packing format.

Test Plan:
Locally tested
```
x = torch.ones((100, 100), dtype=torch.float)
qx_8bit = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint8)
qx = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint4x2)

torch.save(x, ""temp.p"")
print('Size float (B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx_8bit, ""temp.p"")
print('Size quantized 8bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx, ""temp.p"")
print('Size quantized 4bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')
```

Size float (B): 40760
Size quantized 8bit(B): 10808
Size quantized 4bit(B): 5816

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
44583,peterjc123,pr,2020-09-12T02:44:25Z,Patch generate files for system protobuf,"Fixes https://github.com/pytorch/pytorch/issues/42939
",pytorch
44603,peterjc123,pr,2020-09-13T02:12:54Z,Split CUDA_NVCC_FLAGS by space,"Fixes https://github.com/pytorch/pytorch/issues/44599
",pytorch
44604,ngimel,pr,2020-09-13T02:30:57Z,use std::tuple for Future stream,"future callbacks are supported only in single-gpu per process mode, so there's no need to create a stream on every device for them (or even allocate an array without creating streams). Using std::tuple instead of std::vector makes this semantics a bit clearer. 

Also, what do you guys think of adding tests that would make sure that additional contexts are not created, to prevent breakages similar to one fixed by #44097? Creating extra contexts is very bad, and for multi-gpu jobs leads to half of GPU memory being thrown away. 
",pytorch
44616,ngimel,pr,2020-09-13T17:49:01Z,adds list_gpu_processes function,"per title, to make it easier to track the creation of stray contexts:
```
python -c ""import torch; a=torch.randn(1, device='cuda'); print(torch.cuda.memory.list_gpu_processes(0)); print(torch.cuda.memory.list_gpu_processes(1))""
GPU:0
process      79749 uses      601.000 MB GPU memory
GPU:1
no processes are running
```
",pytorch
44646,rohan-varma,pr,2020-09-14T17:54:50Z,Remove thread_local RecordFunctionGuard from profiler.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44967 Fallback to CPU when remote end does not have CUDA for profiling
* #44923 [RPC profiling] Add tests to ensure RPC profiling works on single threaded
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* **#44646 Remove thread_local RecordFunctionGuard from profiler.**

Per a discussion with @ilia-cher, this is not needed anymore and
removing it would make some future changes to support async RPC profiling
easier. Tested by ensuring profiling tests in `test_autograd.py` still pass.

Differential Revision: [D23683998](https://our.internmc.facebook.com/intern/diff/D23683998/)",pytorch
44653,rohan-varma,pr,2020-09-14T19:58:57Z,[RPC profiling] Allow disableProfiler() to be called from another thread. ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44967 Fallback to CPU when remote end does not have CUDA for profiling
* #44923 [RPC profiling] Add tests to ensure RPC profiling works on single threaded
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* **#44653 [RPC profiling] Allow disableProfiler() to be called from another thread.**
* #44646 Remove thread_local RecordFunctionGuard from profiler.

This changes the profiler per a discussion with @ilia-cher offline that enables `disableProfiler()` event consolidation logic to be called from different threads (i.e. threads where the profiler was not explicitly enabled). This is needed to support the functionality enabled by D23638387 where we defer profiling event collection until executing an async callback that can execute on a different thread, to support RPC async function profiling.

This is done by introducing 2 flags `cleanupTLSState` and `consolidate` which controls whether we should clean up thread local settings (we don't do this when calling `disableProfiler()` on non-main threads) and whether we should consolidate all profiled events. Backwards compatiblity is ensured since both options are true by default.

Added a test in `test_misc.cpp` to test this.

Differential Revision: [D23638499](https://our.internmc.facebook.com/intern/diff/D23638499/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23638499/)!",pytorch
44655,rohan-varma,pr,2020-09-14T20:07:43Z,[RPC profiling] Don't wrap toHere() calls with profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44967 Fallback to CPU when remote end does not have CUDA for profiling
* #44923 [RPC profiling] Add tests to ensure RPC profiling works on single threaded
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* **#44655 [RPC profiling] Don't wrap toHere() calls with profiling**
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.

Since `toHere()` does not execute operations (torch operators) over RPC and simply
transfers the value to the local node, we don't need to enable the profiler
remotely for this message. This causes unnecessary overhead and is not needed.

Since `toHere` is a blocking call, we already profile the call on the local node using `RECORD_USER_SCOPE`, so this does not change the expected profiler results (validated by ensuring all remote profiling tests pass).

Differential Revision: [D23641466](https://our.internmc.facebook.com/intern/diff/D23641466/)",pytorch
44664,rohan-varma,pr,2020-09-14T21:28:35Z,[RPC profiling] Extend RPC profiling to support async function execution over RPC.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44967 Fallback to CPU when remote end does not have CUDA for profiling
* #44923 [RPC profiling] Add tests to ensure RPC profiling works on single threaded
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* **#44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.**
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.

Closes https://github.com/pytorch/pytorch/issues/39971. This PR adds support for functions decorated with `@rpc.functions.async_execution` to be profiled over RPC as builtins, jit functions, and blocking python UDFs currently can be. The reasoning for this is to provide complete feature support in terms of RPC profiling and the various types of functions users can run.

To enable this, the PR below this enables calling `disableProfiler()` safely from another thread. We use that functionality to defer disabling the profiler on the server until the future corresponding to the RPC request completes (rather than only the blocking `processRPC` call as was done previously). Since when the future completes we've kicked off the async function and the future corresponding to it has completed, we are able to capture any RPCs the function would have called and the actual work done on the other node.

For example, if the following async function is ran on a server over RPC:

```
def slow_add(x, y):
    time.sleep(1)
    return torch.add(x, y)


@rpc.functions.async_execution
def slow_async_add(to, x, y):
    return rpc.rpc_async(to, slow_add, args=(x, y))
```

we expect to see the original RPC profiled, the nested RPC profiled, and the actual torch.add() work. All of these events should be recorded with the correct node id. Here is an example profiling output:

```
-------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------
-------  ---------------  ---------------  ---------------
Name                                                                                                                       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Node ID
-------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------
-------  ---------------  ---------------  ---------------                                                                                                                            rpc_async#slow_async_add(worker1 -> worker2)                                                                               0.00%            0.000us          0                1.012s
         1.012s           1                1
aten::empty                                                                                                                7.02%            11.519us         7.02%            11.519us         11.519us         1                1
rpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)                             0.00%            0.000us          0                1.006s
         1.006s           1                2                                                                                                                                          rpc_async#slow_async_add(worker1 -> worker2)#remote_op: aten::empty                                                        7.21%            11.843us         7.21%            11.843us
         11.843us         1                2
rpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)#remote_op: aten::add        71.94%           118.107us        85.77%           140.802us        140.802us        1                3
rpc_async#slow_async_add(worker1 -> worker2)#remote_op: rpc_async#slow_add(worker2 -> worker3)#remote_op: aten::empty      13.82%           22.695us         13.82%           22.695us
         22.695us         1                3                                                                                                                                          -------------------------------------------------------------------------------------------------------------------------  ---------------  ---------------  ---------------  --------
-------  ---------------  ---------------  ---------------
Self CPU time total: 164.164us
```

This PR also moves a bunch of the profiling logic to `rpc/utils.cpp` to declutter `request_callback` code.

Differential Revision: [D23638387](https://our.internmc.facebook.com/intern/diff/D23638387/)",pytorch
44678,supriyar,pr,2020-09-15T00:32:20Z,[quant] creating quint4x2 dtype for quantized tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45594 [quant] PerChannelFloatQParams support for quint4x2 dtype
* **#44678 [quant] creating quint4x2 dtype for quantized tensors**

Summary:
This is a prototype PR that introduces 4 bit qtensors. The new dtype added for this is c10::quint4x2
The underlying storage for this is still uint8_t, so we pack 2 4-bit values in a byte while quantizing it.

This change uses most of the existing scaffolding for qtensor storage. We allocate storage
based on the dtype before creating a new qtensor.

It also adds a dispatch mechanism for this dtype so we can use this to get the bitwidth, qmin and qmax info
while quantizing and packing the qtensor (when we add 2-bit qtensor)

Kernels that use this dtype should be aware of the packing format.

Test Plan:
Locally tested
```
x = torch.ones((100, 100), dtype=torch.float)
qx_8bit = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint8)
qx = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint4x2)

torch.save(x, ""temp.p"")
print('Size float (B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx_8bit, ""temp.p"")
print('Size quantized 8bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx, ""temp.p"")
print('Size quantized 4bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')
```

Size float (B): 40760
Size quantized 8bit(B): 10808
Size quantized 4bit(B): 5816

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23993134](https://our.internmc.facebook.com/intern/diff/D23993134)",pytorch
44681,ngimel,pr,2020-09-15T01:32:43Z,"fixes lda condition for blas functions, fixes bug with beta=0 in addmv slow path","per title. If `beta=0` and slow path was taken, `nan` and `inf` in the result were not masked as is the case with other linear algebra functions. Similarly, since `mv` is implemented as `addmv` with `beta=0`, wrong results were sometimes produced for `mv` slow path. 
",pytorch
44749,supriyar,pr,2020-09-15T22:32:30Z,[quant][qat] Ensure observers and fq modules are scriptable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44773 [quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule
* #44765 [quant][fx] Add node name as prefix to observer module name
* **#44749 [quant][qat] Ensure observers and fq modules are scriptable**

Summary:
Ensure fx module is scriptable after calling prepare_qat on it

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_qat_and_script

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23718380](https://our.internmc.facebook.com/intern/diff/D23718380)",pytorch
44765,supriyar,pr,2020-09-16T00:52:03Z,[quant][fx] Add node name as prefix to observer module name,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44846 [quant] Add quant APIs to save/load observer state_dict
* #44773 [quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule
* **#44765 [quant][fx] Add node name as prefix to observer module name**

Summary:

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_save_observer_state_dict

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23741355](https://our.internmc.facebook.com/intern/diff/D23741355)",pytorch
44773,supriyar,pr,2020-09-16T04:34:09Z,[quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44846 [quant] Add quant APIs to save/load observer state_dict
* **#44773 [quant][qat] Ensure fake_quant and observer can be disabled on scriptmodule**
* #44765 [quant][fx] Add node name as prefix to observer module name

Summary:
The model is created and prepared using fx APIs and then scripted for training.
In order to test QAT on scriptmodel we need to be able to disable/enable fake_quant
and observer modules on it.

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_qat_and_script

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23741354](https://our.internmc.facebook.com/intern/diff/D23741354)",pytorch
44826,rohan-varma,pr,2020-09-16T21:15:18Z,Add an option to DDP to take a list of parameters to ignore upfront.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44826 Add an option to DDP to take a list of parameters to ignore upfront.**

 As described in https://github.com/pytorch/pytorch/issues/43690, there
is a need for DDP to be able to ignore certain parameters in the module (not
install allreduce hooks) for certain use cases. `find_unused_parameters` is
sufficient from a correctness perspective, but we can get better performance
with this upfront list if users know which params are unused, since we won't
have to traverse the autograd graph every iteration.

For now, we have a private static method in DDP to set an attribute for parameters and buffers to be ignored by DDP. The final version of this API still requires additional discussion. 

Differential Revision: [D23740639](https://our.internmc.facebook.com/intern/diff/D23740639/)",pytorch
44846,supriyar,pr,2020-09-16T23:08:31Z,[quant] Add quant APIs to save/load observer state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44846 [quant] Add quant APIs to save/load observer state_dict**

Summary:
The save function traverses the model state dict to pick out the observer stats
load function traverse the module hierarchy to load the state dict into module attributes depending on observer type

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_save_observer_state_dict

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23746821](https://our.internmc.facebook.com/intern/diff/D23746821)",pytorch
44923,rohan-varma,pr,2020-09-18T00:36:41Z,"[RPC profiling] Add tests to ensure RPC profiling works on single threaded
server","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44967 Fallback to CPU when remote end does not have CUDA for profiling
* **#44923 [RPC profiling] Add tests to ensure RPC profiling works on single threaded
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server**
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.

server

This ensures that RPC profiling works in single-threaded server
scenarios and that we won't make the assumption that we'll have multiple
threads when working on this code. For example, this assumption resulted in a
bug in the previous diff (which was fixed). 

Differential Revision: [D23691304](https://our.internmc.facebook.com/intern/diff/D23691304/)",pytorch
44967,rohan-varma,pr,2020-09-18T19:34:36Z,Fallback to CPU when remote end does not have CUDA for profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44967 Fallback to CPU when remote end does not have CUDA for profiling**
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.
server
* #44664 [RPC profiling] Extend RPC profiling to support async function execution over RPC.
* #44655 [RPC profiling] Don't wrap toHere() calls with profiling
* #44653 [RPC profiling] Allow disableProfiler() to be called from another thread.
* #44646 Remove thread_local RecordFunctionGuard from profiler.

A comment from @mrshenli on #44664 led us to the following concern: when enabling profiler on server, if it is a different machine it may
not have CUDA while caller does. In this case, we would crash but now we
fallback to CPU and log a warning.

For testing, I forced it to return CUDA profiler state, and validated that it falls back. Not sure how to add a unittest given that we have single machine tests and the machine either has or doesn't have cuda. 
Differential Revision: [D23790729](https://our.internmc.facebook.com/intern/diff/D23790729/)",pytorch
45001,supriyar,pr,2020-09-19T00:15:35Z,[quant] Make observer fields as paramteres instead of buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45001 [quant] Make observer fields as paramteres instead of buffers**
* #44846 [quant] Add quant APIs to save/load observer state_dict

Summary:
Currently the observer stats don't show up as a part of named_parameters in the module.
The difference between paramteres and bufers is that buffers do not update the gradient at every step.
We can get around this by specifying requires_grad=False for the parameters

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
45003,supriyar,pr,2020-09-19T00:27:55Z,[quant] Make observer fields as paramteres instead of buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45003 [quant] Make observer fields as paramteres instead of buffers**
* #44846 [quant] Add quant APIs to save/load observer state_dict

Summary:
Currently the observer stats don't show up as a part of named_parameters in the module.
The difference between paramteres and bufers is that buffers do not update the gradient at every step.
We can get around this by specifying requires_grad=False for the parameters

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
45008,ngimel,pr,2020-09-19T01:20:18Z,update gloo submodule,"Revert accidental gloo submodule changes in #41977
",pytorch
45050,supriyar,pr,2020-09-20T18:33:18Z,[quant][fx][bug] Fix error in convert step for QAT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45050 [quant][fx][bug] Fix error in convert step for QAT**

Summary:
Update tests to actually test for QAT

Test Plan:
python test/test_quantization.py TestQuantizeFxOps.test_linear

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23808022](https://our.internmc.facebook.com/intern/diff/D23808022)",pytorch
45054,supriyar,pr,2020-09-21T05:39:30Z,[quant] Make observer fields as paramteres instead of buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45054 [quant] Make observer fields as paramteres instead of buffers**

Summary:
Currently the observer stats don't show up as a part of named_parameters in the module.
The difference between paramteres and bufers is that buffers do not update the gradient at every step.
We can get around this by specifying requires_grad=False for the parameters

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23810726](https://our.internmc.facebook.com/intern/diff/D23810726)",pytorch
45149,supriyar,pr,2020-09-22T18:11:20Z,[quant] Add optimized approach to calculate qparams for qembedding_bag,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45149 [quant] Add optimized approach to calculate qparams for qembedding_bag**

Summary:
The choose_qparams_optimized calculates the the optimized qparams.
It uses a greedy approach to nudge the min and max and calculate the l2 norm
  and tries to minimize the quant error by doing `torch.norm(x-fake_quant(x,s,z))`
Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23848060](https://our.internmc.facebook.com/intern/diff/D23848060)",pytorch
45162,rohan-varma,pr,2020-09-22T21:14:05Z,Fix test_rpc_profiling_remote_record_function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45162 Fix test_rpc_profiling_remote_record_function**

Closes https://github.com/pytorch/pytorch/issues/45067, https://github.com/pytorch/pytorch/issues/45145. This test was flaky because it was not able to validate that the
overall record_function's CPU times are greater than the sum of its children.
It turns out that this is a general bug in the profiler that can be reproduced
without RPC, see https://github.com/pytorch/pytorch/issues/45160. This means that it should be fixed and tested at the profiler, not RPC, level. Hence,
removing this from the test and replacing it by just validating the expected
children.

Ran the test 1000 times and they all passed.

Differential Revision: [D23851854](https://our.internmc.facebook.com/intern/diff/D23851854/)",pytorch
45209,ngimel,pr,2020-09-23T17:17:12Z,add self cuda time to avoid double/quadruple counting,"In profiler, cuda did not report self time, so for composite functions there was no way to determine which function is really taking time. In addition, ""total cuda time"" reported was frequently more than total wallclock time. This PR adds ""self CUDA time"" in profiler, and computes total cuda time based on self cuda time, similar to how it's done for CPU. Also, slight formatting changes to make table more compact. Before:
```
--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  
Name                  Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  
--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  
aten::matmul          0.17%            890.805us        99.05%           523.401ms        5.234ms          49.91%           791.184ms        7.912ms          100              
aten::mm              98.09%           518.336ms        98.88%           522.511ms        5.225ms          49.89%           790.885ms        7.909ms          100              
aten::t               0.29%            1.530ms          0.49%            2.588ms          25.882us         0.07%            1.058ms          10.576us         100              
aten::view            0.46%            2.448ms          0.46%            2.448ms          12.238us         0.06%            918.936us        4.595us          200              
aten::transpose       0.13%            707.204us        0.20%            1.058ms          10.581us         0.03%            457.802us        4.578us          100              
aten::empty           0.14%            716.056us        0.14%            716.056us        7.161us          0.01%            185.694us        1.857us          100              
aten::as_strided      0.07%            350.935us        0.07%            350.935us        3.509us          0.01%            156.380us        1.564us          100              
aten::stride          0.65%            3.458ms          0.65%            3.458ms          11.527us         0.03%            441.258us        1.471us          300              
--------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  
Self CPU time total: 528.437ms
CUDA time total: 1.585s

Recorded timeit time:  789.0814 ms

```
Note recorded timeit time (with proper cuda syncs) is 2 times smaller than ""CUDA time total"" reported by profiler

After
```
--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
        aten::matmul         0.15%     802.716us        99.06%     523.548ms       5.235ms     302.451us         0.04%     791.151ms       7.912ms           100  
            aten::mm        98.20%     519.007ms        98.91%     522.745ms       5.227ms     790.225ms        99.63%     790.848ms       7.908ms           100  
             aten::t         0.27%       1.406ms         0.49%       2.578ms      25.783us     604.964us         0.08%       1.066ms      10.662us           100  
          aten::view         0.45%       2.371ms         0.45%       2.371ms      11.856us     926.281us         0.12%     926.281us       4.631us           200  
     aten::transpose         0.15%     783.462us         0.22%       1.173ms      11.727us     310.016us         0.04%     461.282us       4.613us           100  
         aten::empty         0.11%     591.603us         0.11%     591.603us       5.916us     176.566us         0.02%     176.566us       1.766us           100  
    aten::as_strided         0.07%     389.270us         0.07%     389.270us       3.893us     151.266us         0.02%     151.266us       1.513us           100  
        aten::stride         0.60%       3.147ms         0.60%       3.147ms      10.489us     446.451us         0.06%     446.451us       1.488us           300  
--------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 528.498ms
CUDA time total: 793.143ms

Recorded timeit time:  788.9832 ms

```",pytorch
45235,rohan-varma,pr,2020-09-23T21:36:53Z,Add link to profiling recipe from rpc main docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45235 Add link to profiling recipe from rpc main docs**

This is so that users know that the profiler works as expected with
RPC and they can learn how to use it to profile RPC-based workloads.

Screenshot:
![profiler_ss](https://user-images.githubusercontent.com/8039770/94075767-a145d080-fdb0-11ea-872a-f88a997c0b2a.png)

Differential Revision: [D23777888](https://our.internmc.facebook.com/intern/diff/D23777888/)",pytorch
45238,rohan-varma,pr,2020-09-23T22:29:36Z,Add a warning log when there is high skew of uneven inputs in DDP training,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45238 Add a warning log when there is high skew of uneven inputs in DDP training**

This request came up in feature review for DDP uneven inputs, so this PR adds a warning when there is much higher than expected amount of
discrepancy of inputs across different processes when running with uneven
inputs. This is because a skew in the thousands can reduce performance a
nontrivial amount as shown in benchmarks in https://github.com/pytorch/pytorch/pull/42577, and it was proposed to add this
warning as a result. Tested by running the tests so the threshold is hit and
observing the output.

Differential Revision: [D23719270](https://our.internmc.facebook.com/intern/diff/D23719270/)",pytorch
45257,supriyar,pr,2020-09-24T02:59:00Z,[quant][graph] Remove redundant aten::wait calls in the graph,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45257 [quant][graph] Remove redundant aten::wait calls in the graph**

Summary:
Currently we inline fork-wait calls when we insert observers for quantization
In the case where fork and wait are in different subgraphs, inlining the fork-wait calls
only gets rid of the fork. This leaves the aten::wait call in the graph with a torch.Tensor as input,
which is currently not supported.
To avoid this we check to make sure input to all wait calls in the graph is of type Future[tensor]
in the cleanup phase

Test Plan:
python test/test_quantization.py TestQuantizeJitPasses.test_quantize_fork_wait

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D23895412](https://our.internmc.facebook.com/intern/diff/D23895412)",pytorch
45376,peterjc123,pr,2020-09-26T16:41:06Z,[DON'T MERGE] Test Windows container,"Fixes #{issue number}
",pytorch
45391,ngimel,pr,2020-09-27T04:57:47Z,Allow outputs to have different strides for reductions with multiple outputs (min and max),"Fixes #42364 by creating temporaries in case reduction outputs have different strides. This situation is exceedingly rare, and no attempt is made to improve performance in this case. However, this fix does not regress performance for the common cases. 
Note: turns out `var_mean` does not have `out` variant so presently it cannot hit this issue (#45446). ",pytorch
45530,supriyar,pr,2020-09-29T21:23:39Z,[quant] Make choose_qparams_optimized return Tensors to preserve dtype,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45530 [quant] Make choose_qparams_optimized return Tensors to preserve dtype**

Summary:
Returning double values requires special handling as a return type for aten functions.
Instead return tensors where the type is preserved in the tensor dtype

Test Plan:
python test/test_quantization.py TestQuantizedTensor.test_choose_qparams_optimized

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24001134](https://our.internmc.facebook.com/intern/diff/D24001134)",pytorch
45555,supriyar,pr,2020-09-30T03:06:35Z,[quant] PerChannelFloatQParams support for quint4x2 dtype,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45555 [quant] PerChannelFloatQParams support for quint4x2 dtype**
* #44678 [quant] creating quint4x2 dtype for quantized tensors

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
45557,ngimel,pr,2020-09-30T04:29:51Z,Use addmm directly for 1x1 convolution,"Fixes #45274 
Based on #44041, sets intermediate for backward computation (otherwise, backward tests are failing).
",pytorch
45594,supriyar,pr,2020-09-30T18:59:40Z,[quant] PerChannelFloatQParams support for quint4x2 dtype,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45594 [quant] PerChannelFloatQParams support for quint4x2 dtype**
* #44678 [quant] creating quint4x2 dtype for quantized tensors

Summary:
Adds support for Per-channel quantization using float qparams for 4-bit dtype
We use the new dispatch mechanism and use existing quantize/dequantize kernels to pack the
4-bit data depending on the bit_width.
Size of 4-bit quantized tensor is half that of 8-bit quantized tensor.

Test Plan:
python test/test_quantization.py TestQuantizedTensor.test_quantize_per_channel_sub_byte

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24025595](https://our.internmc.facebook.com/intern/diff/D24025595)",pytorch
45648,neerajprad,pr,2020-10-01T05:11:09Z,Support pytest for distribution testing,"In response to https://github.com/pytorch/pytorch/issues/11578. This is a test run to see if CI (and other internal systems) works fine with pytest style tests.
 - Creates a separate `distributions` directory within `test`.
 - For testing, this rewrites the `constraint` tests as parameterized tests in pytest. I don't plan to convert any other tests to pytest style, but only expose this option for adding new tests, if required.

If this is a success, we can move `EXAMPLES` in `test_distributions` into a separate file that can be imported by both pytest and unittest style tests. cc. @fritzo ",pytorch
45709,rohan-varma,pr,2020-10-02T00:19:12Z,Fix distributed documentation for asynchronous collective Work objects,"Closes https://github.com/pytorch/pytorch/issues/42247. Clarifies some documentation related to `Work` object semantics (outputs of async collective functions). Clarifies the difference between CPU operations and CUDA operations (on Gloo or NCCL backend), and provides an example where the difference in CUDA operation's wait() semantics is necessary to understand for correct code. 
![sync](https://user-images.githubusercontent.com/8039770/94875710-6f64e780-040a-11eb-8fb5-e94fd53534e5.png)
",pytorch
45748,ngimel,pr,2020-10-02T17:25:27Z,unify reproducibility notes,"Many of our functions contain same warnings about results reproducibility. Make them use common template. 
",pytorch
45751,supriyar,pr,2020-10-02T18:00:19Z,[quant] Add 4-bit embedding_bag prepack/unpack support using quint4x2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45881 [quant] Refactor qembeddingbag to remove duplicate code
* #45865 [quant] Support for 4-bit quantized EmbeddingBag module
* #45752 [quant] Support 4-bit embedding_bag operators using the dtype quint4x2
* **#45751 [quant] Add 4-bit embedding_bag prepack/unpack support using quint4x2**

Summary:
Use the torch.quint4x2 dtype to create 4-bit packed tensors

Test Plan:
python test/test_quantization.py TestEmbeddingBagOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24120997](https://our.internmc.facebook.com/intern/diff/D24120997)",pytorch
45752,supriyar,pr,2020-10-02T18:00:34Z,[quant] Support 4-bit embedding_bag operators using the dtype quint4x2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45881 [quant] Refactor qembeddingbag to remove duplicate code
* #45865 [quant] Support for 4-bit quantized EmbeddingBag module
* **#45752 [quant] Support 4-bit embedding_bag operators using the dtype quint4x2**
* #45751 [quant] Add 4-bit embedding_bag prepack/unpack support using quint4x2

Summary:
Use the torch.quint4x2 dtype to create 4-bit packed tensors in the previous PR.
These packed tensors can be directly consumed by the operator.
Serialization of the packed tensors is supported using torchbind custom class.
Module support will follow in a later PR.

Test Plan:
python test/test_quantization.py TestEmbeddingBagOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24120996](https://our.internmc.facebook.com/intern/diff/D24120996)",pytorch
45829,peterjc123,pr,2020-10-05T07:08:46Z,Don't export enums for CUDA sources on Windows,"Fixes #{issue number}
",pytorch
45830,peterjc123,pr,2020-10-05T07:30:24Z,Enable XNNPACK on Windows & Update XNNPACK,"Fixes #44283.
",pytorch
45842,peterjc123,pr,2020-10-05T15:19:27Z,Compress NVCC flags for Windows,"Fixes #{issue number}
This makes the command line shorter.
Also updates `randomtemp` in which the previous version has a limitation that the length of the argument cannot exceed 260.",pytorch
45865,supriyar,pr,2020-10-05T20:57:09Z,[quant] Support for 4-bit quantized EmbeddingBag module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45881 [quant] Refactor qembeddingbag to remove duplicate code
* **#45865 [quant] Support for 4-bit quantized EmbeddingBag module**
* #45752 [quant] Support 4-bit embedding_bag operators using the dtype quint4x2
* #45751 [quant] Add 4-bit embedding_bag prepack/unpack support using quint4x2

Summary:

Test Plan:
python test/test_quantization.py TestPostTrainingStatic.test_quantized_embedding_bag
python test/test_quantization.py TestStaticQuantizedModule.test_embedding_bag_api

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24120995](https://our.internmc.facebook.com/intern/diff/D24120995)",pytorch
45881,supriyar,pr,2020-10-06T00:45:08Z,[quant] Refactor qembeddingbag to remove duplicate code,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45881 [quant] Refactor qembeddingbag to remove duplicate code**
* #45865 [quant] Support for 4-bit quantized EmbeddingBag module
* #45752 [quant] Support 4-bit embedding_bag operators using the dtype quint4x2
* #45751 [quant] Add 4-bit embedding_bag prepack/unpack support using quint4x2

Summary:

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingBagOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24127892](https://our.internmc.facebook.com/intern/diff/D24127892)",pytorch
45933,rohan-varma,pr,2020-10-06T23:36:46Z,Prioritize raising error message about unused parameters when rebuild_buckets fails,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45950 [NCCL] Provide additional information about NCCL error codes.
* #45942 Only populate grad accumulator to var mapping for find_unused_parameters=True in DDP
* **#45933 Prioritize raising error message about unused parameters when rebuild_buckets fails**

Occasionally users run DDP with models with unused params, in this
case we would like to surface an error message telling them to run with
find_unused_params=True. However, a recent change to rebuild_buckets logic (https://github.com/pytorch/pytorch/pull/44798) made
it so that we raise a size mismatch error when this happens, but the
information about unused parameters is likely to be more useful and likely to
be the most common case of failure. Prefer raising this error over the
subsequent size mismatch errors.

Differential Revision: [D24151256](https://our.internmc.facebook.com/intern/diff/D24151256/)",pytorch
45942,rohan-varma,pr,2020-10-07T01:52:57Z,Only populate grad accumulator to var mapping for find_unused_parameters=True in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45950 [NCCL] Provide additional information about NCCL error codes.
* **#45942 Only populate grad accumulator to var mapping for find_unused_parameters=True in DDP**

We only need to keep track of this for traversing the autograd graph
when find_unused_parameters=True. Without that, we populate and keep this
mapping in memory, which occupies sizeof(pointer) * number of grad accumulators
of extra memory.

Also renames the variable to something more meaningful. 

Differential Revision: [D24154407](https://our.internmc.facebook.com/intern/diff/D24154407/)",pytorch
45950,rohan-varma,pr,2020-10-07T04:19:28Z,[NCCL] Provide additional information about NCCL error codes.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45950 [NCCL] Provide additional information about NCCL error codes.**
* #45942 Only populate grad accumulator to var mapping for find_unused_parameters=True in DDP

A pain point for debugging failed training jobs due to NCCL errors has
been understanding the source of the error, since NCCL does not itself report
too many details (usually just ""unhandled {system, cuda, internal} error"").

In this PR, we add some basic debug information about what went wrong. The information is collected by grepping the NCCL codebase for when these errors are thrown. For example, `ncclSystemError` is what is thrown when system calls such as malloc or munmap fail.

Tested by forcing `result = ncclSystemError` in the macro. The new error
message looks like:

```RuntimeError: NCCL error in:
caffe2/torch/lib/c10d/ProcessGroupNCCL.cpp:759, unhandled system error, NCCL
version 2.7.3
ncclSystemError: System call (socket, malloc, munmap, etc) failed.
```

The last line is what we have added to the message.

In the future, we will also evaluate setting NCCL_DEBUG=WARN, by which NCCL
provides more details about errors sa well.

Differential Revision: [D24155894](https://our.internmc.facebook.com/intern/diff/D24155894/)",pytorch
45958,peterjc123,pr,2020-10-07T07:46:55Z,[TEST ONLY] Test new randomtemp,"Fixes #{issue number}
",pytorch
45959,peterjc123,pr,2020-10-07T08:22:21Z,[TEST ONLY] Test new randomtemp,"Fixes #{issue number}
",pytorch
45960,peterjc123,pr,2020-10-07T08:23:56Z,[TEST ONLY] Test new randomtemp ,"Fixes #{issue number}
",pytorch
45961,peterjc123,pr,2020-10-07T08:33:52Z,[TEST ONLY] Test new randomtemp,"Fixes #{issue number}
",pytorch
45963,vfdev-5,pr,2020-10-07T09:15:13Z,Fixes bug in sspaddmm (#45113),"Fixes #45113

Description:
- Fixed bug in sspaddmm by calling contiguous on indices. 
- Added tests

We have to make indices contiguous as we use `indices.data_ptr` in `_to_csr` which assumes row-contiguous storage:
https://github.com/pytorch/pytorch/blob/be45c3401af8186f97f0e2b269ff3bafaf16157f/aten/src/ATen/native/sparse/SparseTensorMath.cpp#L1087-L1090


> Part 1 of fixing this is probably to document sspaddmm. Part 2 may be to rewrite it using other ops. (https://github.com/pytorch/pytorch/issues/45113#issuecomment-700166809)

- Docs will be written here: https://github.com/pytorch/pytorch/pull/45400",pytorch
45989,rohan-varma,pr,2020-10-07T20:02:06Z,[Flaky tests] Fix test_all_gather_timeout test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45989 [Flaky tests] Fix test_all_gather_timeout test**

This test was failing internally for the Thrift-based RPC agent, since
it has a different error regex. Use `self.get_timeout_error_regex` which gets
the timeout error string for each backend to fix this.

Differential Revision: [D24170394](https://our.internmc.facebook.com/intern/diff/D24170394/)",pytorch
45997,supriyar,pr,2020-10-07T21:56:39Z,[quant][pyper] Set sparse to False for embedding_bag ops in graph mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46003 [quant][pyper] Rename the sparse argument for embedding_bag ops
* **#45997 [quant][pyper] Set sparse to False for embedding_bag ops in graph mode**

Summary:
The current sparse field using in the float module is for sparse gradients, which is not applicable
to inference. The sparse field in the quantizd ops denotes pruned weights.

Test Plan:
python test/test_quantization.py TestQuantizeDynamicJitOps.test_embedding_bag

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24176543](https://our.internmc.facebook.com/intern/diff/D24176543)",pytorch
46003,supriyar,pr,2020-10-07T23:20:55Z,[quant][pyper] Rename the sparse argument for embedding_bag ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46003 [quant][pyper] Rename the sparse argument for embedding_bag ops**
* #45997 [quant][pyper] Set sparse to False for embedding_bag ops in graph mode

Summary:
sparse is confusing because itt is used in training for sparse gradients

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24178248](https://our.internmc.facebook.com/intern/diff/D24178248)",pytorch
46025,peterjc123,pr,2020-10-08T08:57:09Z,Update randomtemp to v0.3,"Fixes #45982.
",pytorch
46026,peterjc123,pr,2020-10-08T08:59:30Z,[TEST ONLY] Test randomtemp update for binary jobs,Testing https://github.com/pytorch/builder/pull/545.,pytorch
46055,neerajprad,pr,2020-10-08T20:55:03Z,Fix error in Binomial to retain lazy logit initialization,"Some internal tests were sporadically failing for https://github.com/pytorch/pytorch/pull/45648. The cause of this is a bug in `Binomial.__init__` that references the lazy `logits` attribute and sets it when not needed. This cleans up the `is_scalar` logic too which isn't needed given that `broadcast_all` will convert `Number` to a `tensor`. 

The reason for the flakiness is the mutation of the params dict by the first test, which is fixed by doing a shallow copy. It will be better to convert this into a pytest parameterized test once https://github.com/pytorch/pytorch/pull/45648 is merged.

cc. @fritzo, @ezyang ",pytorch
46075,rohan-varma,pr,2020-10-09T01:48:32Z,Remove object-based collective APIs from public docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46075 Remove object-based collective APIs from public docs**

Removes these from public docs for now as we are still
iterating/formalizing these APIs. Will add them back once they are part of a
PyTorch release.

Differential Revision: [D24211510](https://our.internmc.facebook.com/intern/diff/D24211510/)",pytorch
46099,rohan-varma,pr,2020-10-09T17:06:30Z,[v1.7 patch] Prioritize raising error message about unused parameters when rebuild_buckets fails,"Summary:

Note: This PR has been merged into master at 62554a3 after the 1.7 branch cut
(see original PR: #45933). This PR is to merge it into the 1.7 branch.

---- Original Commit Description Follows ---

#45933 Prioritize raising error message about unused parameters when rebuild_buckets fails

Occasionally users run DDP with models with unused params, in this
case we would like to surface an error message telling them to run with
find_unused_params=True. However, a recent change to rebuild_buckets logic (#44798) made
it so that we raise a size mismatch error when this happens, but the
information about unused parameters is likely to be more useful and likely to
be the most common case of failure. Prefer raising this error over the
subsequent size mismatch errors.

Differential Revision: D24151256",pytorch
46109,rohan-varma,pr,2020-10-09T19:45:53Z,[v1.7 patch] Remove object-based collective APIs from public docs,"Summary:

Note: This PR has been merged into master at 362d9a9 after the 1.7 branch cut
(see original PR: https://github.com/pytorch/pytorch/pull/46075). This PR is to merge it into the 1.7 branch.

---- Original Commit Description Follows ---

#46075 Remove object-based collective APIs from public docs
Removes these from public docs for now as we are still
iterating/formalizing these APIs. Will add them back once they are part of a
PyTorch release.

Differential Revision: D24211510",pytorch
46113,supriyar,pr,2020-10-09T20:57:41Z,[wip] fused fake_quant linear impl,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46113 [wip] fused fake_quant linear impl**

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
46167,ngimel,pr,2020-10-11T19:51:41Z,WIP - experiment with unique APIs,"Our current `unique` function is fragile and almost impossible to modify. This is an attempt to see how unique might look like in a saner world. 
This PR adds _unique_good function that always returns named tuple that contains `value`, `inverse`, `indices` and `counts` fields:
```
func: _uniq(Tensor self, *, bool return_inverse=False, bool return_indices=False, bool return_counts=False) -> (Tensor values, Tensor inverse, Tensor indices, Tensor counts)
```
When some of these values are not needed due to corresponding function argument being `False`, 0-element tensor or undefined tensor (None) can be returned (more on that below, this PR returns 0-element tensors in this case to satisfy scripting limitations) 
Pros:
- return type is always the same (tuple)
- scripting almost works - scripting can't handle undefined (None) tensor, so if we agree that 0-element return in this case is acceptable, it solves scripting problem
- provides numpy functionality (return_indices) that is currently very challenging to add to existing `unique`
- named tuple is a much better UX when there can be up to 4 returns, relying on return order is fragile (especially when things to return are controlled by kwargs and not positional args)

Cons:
- if we decide to change `unique` behavior to this, it's bc-breaking
- it is not numpy compatible (numpy returns either a single nd-array, or a tuple of nd-arrays, not named tuple always)
- returning 0-element tensors to satisfy scripting concerns is worse UX than returning undefined tensors that turn to None in the named tuple. 
- even `None` elements in the named tuple are not the best, ideally they should be omitted but as far as I can see it can be solved only by an additional layer of python wrapping, and that will complicate scripting. 
- if in the future we'll need additional returns, it will also be bc-breaking, since the number of elements in the return tuple is fixed
",pytorch
46250,supriyar,pr,2020-10-13T16:26:47Z,[quant][graph][fix] Set type for GetAttr nodes in remapTypes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46250 [quant][graph][fix] Set type for GetAttr nodes in remapTypes**

Summary:
Previously the type of GetAttr nodes was getting set incorrectly and wasn't matching the module type

Test Plan:
Existing quantization tests

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24279872](https://our.internmc.facebook.com/intern/diff/D24279872)",pytorch
46304,rohan-varma,pr,2020-10-14T02:58:39Z,Avoid scatter for single-device case in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46304 Avoid scatter for single-device case in DDP**

In the case that a single process operates only on one GPU, we can
avoid this scatter and instead replace it with a recursive version of `to`
which transfers the input tensors to the correct device.

The implementation of `_recursive_to` is modeled after `scatter` in https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py, in order to keep parity with the previous conventions (i.e. custom types not having their tensors moved).

Differential Revision: [D24296377](https://our.internmc.facebook.com/intern/diff/D24296377/)",pytorch
46372,rohan-varma,pr,2020-10-15T01:47:29Z,[RPC] print exception message on workers that run python functions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46372 [RPC] print exception message on workers that run python functions**

Currently, in `_run_function`, we catch an exception from the python
function which is run, and report it back to the master. However in some large
scale training jobs, it would be valuable to also log the error on the trainer
itself for faster debugging.

Differential Revision: [D24324578](https://our.internmc.facebook.com/intern/diff/D24324578/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D24324578/)!",pytorch
46414,r-barnes,pr,2020-10-15T19:56:14Z,"Add type-safe, const-safe loops to c10","Pull Request resolved: https://github.com/pytorch/pytorch/pull/46414

For loops are often written with mismatched data types which causes silent type and sign coercion in the absence of integer conversion warnings. Getting around this in templated code requires convoluted patterns such as
```
for(auto i=decltype(var){0};i<var;i++)
```
(the above pattern appears ~31 times in the code base) with this diff we can instead write
```
for(const auto i = c10::ranges::indices(var))
```
Note that this loop is type-safe and const-safe. (Type-unsafe loops occur an unknown number of times do to the lack of conversion warnings on compilation.)

The functions introduced here (`c10::ranges::ints` and `c10::ranges::indices`) allow for type-safety and const-ness within for loops, which prevents the accidental truncation or modification of integers and other types, improving code safety. Further, they use the same naming convention as the ranges-v3 library and offer an easy migration path to C++20 functionality.

Test Plan:
```
buck run //caffe2/c10:c10_test_0
```

Differential Revision: D24334732

",pytorch
46445,r-barnes,pr,2020-10-16T05:04:48Z,Fix implicit cast in custom_function,"Summary: Fix an instance in which a truncated integer prevents downstream type safety checks.

Test Plan: I'm not sure what's appropriate here.

Differential Revision: D24339292

",pytorch
46448,supriyar,pr,2020-10-16T05:28:33Z,[quant][graph] Move removeExtraWaitCalls from freezing code to quantization code,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46448 [quant][graph] Move removeExtraWaitCalls from freezing code to quantization code**

Summary:
The dangling wait calls are introduced due to calling inline-fork-wait on the graph

Test Plan:
python test/test_quantization.py TestQuantizeJitPasses.test_quantize_fork_wait

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24376906](https://our.internmc.facebook.com/intern/diff/D24376906)",pytorch
46474,r-barnes,pr,2020-10-16T15:53:41Z,Ensure kernel launches are checked,"Summary:
Caffe2 and Torch currently does not have a consistent mechanism for determining if a kernel has launched successfully. The result is difficult-to-detect or silent errors. This diff provides functionality to fix that. Subsequent diffs on the stack fix the identified issues.

Kernel launch errors may arise if invalid launch parameters (number of blocks, number of threads, shared memory, or stream id) are specified incorrectly for the hardware or for other reasons. Interestingly, unless these launch errors are specifically checked for CUDA will silently fail and return garbage answers which can affect downstream computation. Therefore, catching launch errors is important.

Launches are currently checked by placing
```
AT_CUDA_CHECK(cudaGetLastError());
```
somewhere below the kernel launch. This is bad for two reasons.
1. The check may be performed at a site distant to the kernel launch, making debugging difficult.
2. The separation of the launch from the check means that it is difficult for humans and static analyzers to determine whether the check has taken place.

This diff defines a macro:
```
#define TORCH_CUDA_KERNEL_LAUNCH_CHECK() AT_CUDA_CHECK(cudaGetLastError())
```
which clearly indicates the check.

This diff also introduces a new test which analyzes code to identify kernel launches and determines whether the line immediately following the launch contains `TORCH_CUDA_KERNEL_LAUNCH_CHECK();`.

A search of the Caffe2 codebase identifies 104 instances of `AT_CUDA_CHECK(cudaGetLastError());` while the foregoing test identifies 1,467 launches which are not paired with a check. Visual inspection indicates that few of these are false positives, highlighting the need for some sort of static analysis system.

Test Plan:
The new test is run with:
```
buck run //caffe2/test:kernel_launch_error_check
```
And should be launched automatically with the other land tests. (TODO: Is it?)

The test is currently set up only to provide warnings but can later be adjusted to require checks.

Otherwise, I rely on the existing test frameworks to ensure that changes resulting from reorganizing existing launch checks don't cause regressions.

Differential Revision: D24309971

",pytorch
46477,rohan-varma,pr,2020-10-16T16:27:12Z,[Not for land] break ddp uneven inputs to test windows ci,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46477 [Not for land] break ddp uneven inputs to test windows ci**

title

Differential Revision: [D24363580](https://our.internmc.facebook.com/intern/diff/D24363580/)",pytorch
46617,r-barnes,pr,2020-10-20T21:30:59Z,Minor cleaning of `test_cuda.py`,"Summary: Sort includes, fix deprecated test warning

Test Plan:
```
buck run mode/dev-nosan //caffe2/test:cuda
```

Reviewed By: drdarshan

Differential Revision: D24429247

",pytorch
46666,r-barnes,pr,2020-10-21T18:10:37Z,Fix interval midpoint calculation,"Summary: Interval midpoint calculations can overflow (integers) this diff fixes such an instance.

Test Plan: Standard test rig

Differential Revision: D23997893

",pytorch
46677,supriyar,pr,2020-10-21T21:07:36Z,[quant][fx] Embedding quantization support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46678 [quant][fx] EmbeddingBag quantization support
* **#46677 [quant][fx] Embedding quantization support**

Summary:
Add support for weight only embedding quantization

Test Plan:
python test/test_quantization.py TestQuantizeFxOps.test_qembedding_module

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24463305](https://our.internmc.facebook.com/intern/diff/D24463305)",pytorch
46678,supriyar,pr,2020-10-21T21:07:49Z,[quant][fx] EmbeddingBag quantization support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46678 [quant][fx] EmbeddingBag quantization support**
* #46677 [quant][fx] Embedding quantization support

Summary:

Test Plan:
python test/test_quantization.py TestQuantzeFxOps.test_qembedding_bag_module

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24463306](https://our.internmc.facebook.com/intern/diff/D24463306)",pytorch
46688,supriyar,pr,2020-10-21T22:31:56Z,[quant] consolidate chooseQuantizationParams in the codebase,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46688 [quant] consolidate chooseQuantizationParams in the codebase**

Summary:
Ideally this shouldn't depend on USE_FBGEMM macro since this is just a util function that can be used even when fbgemm is disabled

Test Plan:
python test/test_quantization.py
CI tests

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
46715,r-barnes,pr,2020-10-22T15:23:11Z,Clarify timing of GetDeviceProperty(),"Test Plan: N/A

Reviewed By: ezyang

Differential Revision: D24455538

",pytorch
46727,r-barnes,pr,2020-10-22T18:24:53Z,Add and adjust kernel launch checks,"Summary: This adds kernel launch safety checks to a number of kernels. See D24309971 for context.

Test Plan: The existing pre-commit test rigs are used.

Differential Revision: D24334303

",pytorch
46730,r-barnes,pr,2020-10-22T18:40:44Z,Eliminate narrowing conversion,"Summary: A narrowing conversion on `last_idx` raises a compiler warning. This fixes that.

Test Plan: Standard pre-commit test rig.

Reviewed By: EscapeZero

Differential Revision: D24481497

",pytorch
46755,rohan-varma,pr,2020-10-23T03:08:28Z,Fix DDP issue where parameters share same grad_accumulator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46755 Fix DDP issue where parameters share same grad_accumulator**

Closes  https://github.com/pytorch/pytorch/issues/41324
As reported in https://github.com/pytorch/pytorch/issues/41324, there is a bug in DDP when `find_unused_parameters=True` and 2 or more parameters share the same gradient accumulator.

In the reducer, we currently keep a mapping of grad accumulator to index and populate it with map[accumulator] = index, but this overwrites indices when the accumulator is the same. To fix this, switch the mapping values to a vector of indices to hold all such indices that share the same accumulator.

Differential Revision: [D24497388](https://our.internmc.facebook.com/intern/diff/D24497388/)",pytorch
46769,r-barnes,pr,2020-10-23T16:48:48Z,Fix hash type,"Summary: Value to be hashed is `int64_t`, but hash is `int`. This results in a downconversion which throws out bits which would otherwise be hashed.

Test Plan: Standard pre-commit test rig

Reviewed By: malfet

Differential Revision: D24480962

",pytorch
46770,r-barnes,pr,2020-10-23T16:55:42Z,Fix type warning,"Test Plan: Standard pre-commit test rig.

Reviewed By: malfet

Differential Revision: D24480898

",pytorch
46771,r-barnes,pr,2020-10-23T16:57:41Z,Perform explicit cast,"Summary:
`std::ceil` returns a `float` which is cast to `size_t` by the `max` operation.

We convert to `int64_t` to suppress the warning while matching the type of `newDims[0]`.

Since the types match, we don't need an explicit template type for `max`. This allows `max` to take `int64_t` as its values, matching the type of `newCapacity`.

Test Plan: Standard pre-commit test rig.

Reviewed By: malfet

Differential Revision: D24481684

",pytorch
46812,r-barnes,pr,2020-10-24T18:18:29Z,Fix compiler warning,"Summary: `sizeof` returns an unsigned, so comparison against `-1` is a warning. This fixes that.

Test Plan: Standard pre-commit test rig.

Reviewed By: bhosmer

Differential Revision: D24506390

",pytorch
46833,r-barnes,pr,2020-10-26T06:08:13Z,Fix implicit conversion,"Summary: Implicit integer conversions are causing compiler warnings. Since in this case the logs make it pretty clear that the `unsigned` types won't overflow despite 64-bit inputs, we fix the issue by making the downconversion explicit.

Test Plan: Standard test rig.

Differential Revision: D24481377

",pytorch
46834,r-barnes,pr,2020-10-26T06:08:58Z,Fix signed-to-unsigned conversion warning,"Summary: `next` is set to `-1`, presumably to avoid an ""undefined variable"" warning. However, Setting `next=-1` gives a signed-to-unsigned warning. In practice, the `-1` wraps around to `size_t::max`. So we set this to `size_t::max` from the get go to avoid all warnings.

Test Plan: Standard pre-commit test rig.

Differential Revision: D24481068

",pytorch
46835,r-barnes,pr,2020-10-26T06:09:13Z,Make conversions explicit,"Summary: We make explicit a couple of previously implicit down/narrowing conversions. This fixes a couple of compiler warnings.

Test Plan: Standard pre-commit test rig.

Differential Revision: D24481427

",pytorch
46836,r-barnes,pr,2020-10-26T06:09:30Z,Fix integer conversion warnings,"Summary: The various types `DEFINE_TO` converts between cause integer conversion and floating-point narrowing warnings. We use a pragma to temporarily disable conversion warnings for this part of the code.

Test Plan: Standard test rig

Differential Revision: D24481102

",pytorch
46837,r-barnes,pr,2020-10-26T06:11:55Z,Fix bit math,"Summary:
Formerly `static_cast<StreamId>(bits)` and `static_cast<DeviceIndex>(bits)` were and-ed against `ull` types resulting in an integer promotion which later raised a warning in downcasting passes to  `Stream` and `Device`.

Moving the `&` operation inside the cast results in two `uint64_t` being operated on and then cast to the correct type, eliminating the warning.

Test Plan: Standard pre-commit test rig.

Differential Revision: D24481292

",pytorch
46838,r-barnes,pr,2020-10-26T06:21:06Z,Fix unused variable warning,"Summary: `SCALAR_TYPE` may be unused in some contexts where the macro is used. We use the standard `(void)var` trick to suppress the compiler warning in these instances.

Test Plan: Standard pre-commit tests.

Differential Revision: D24481142

",pytorch
46839,r-barnes,pr,2020-10-26T06:24:53Z,Fix interval midpoint calculation in vulkan,"Summary: Interval midpoint calculations can overflow (integers). This fixes such an instance.

Test Plan: Standard test rig.

Differential Revision: D24392545

",pytorch
46861,rohan-varma,pr,2020-10-26T18:01:45Z,Fix DDP documentation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46861 Fix DDP documentation**

Noticed that in the DDP documentation:
https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel
there were some examples with `torch.nn.DistributedDataParallel`, fix this to
read `torch.nn.parallel.DistributedDataParallel`.

Differential Revision: [D24534486](https://our.internmc.facebook.com/intern/diff/D24534486/)",pytorch
46897,rohan-varma,pr,2020-10-27T01:49:52Z,"Fix object-based collectives API to use torch.cuda.current_device instead of
rank","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46897 Fix object-based collectives API to use torch.cuda.current_device instead of
rank**
rank**
rank**
rank**

These APIs implicitly assumed that gpu for rank == rank index, but
that is not necessarily true. For example, the first GPU could be used for a
different purpose and rank 0 could use GPU 1, rank 1 uses GPU 2, etc. Thus, we
mandate that the user specify the device to use via `torch.cuda.set_device()`
before making calls to this API. This expectation should be okay since we
clearly document it, and we expect the user to set this for
DistributedDataParallel as well. Backwards compatibility is not an issue since these APIs have not been publicly announced yet. 

Also adds/tidies up some documentation.

Differential Revision: [D24556177](https://our.internmc.facebook.com/intern/diff/D24556177/)",pytorch
46956,r-barnes,pr,2020-10-27T22:50:51Z,Proposing some safety exceptions,"Summary: These safety exceptions cover some common C++/CUDA checks in at least one internal code file. Perhaps they'd be useful elsewhere in the codebase?

Test Plan: N/A

Differential Revision: D24578774

",pytorch
46957,supriyar,pr,2020-10-27T23:13:57Z,[quant] Fix flaky test test_histogram_observer_against_reference,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46957 [quant] Fix flaky test test_histogram_observer_against_reference**

Summary:
Possibly due to use of large tensor in hypothesis. Reducing the size to see if it helps

Test Plan:
python test/test_quantization.py TestRecordHistogramObserver.test_histogram_observer_against_reference

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24580137](https://our.internmc.facebook.com/intern/diff/D24580137)",pytorch
46997,ngimel,pr,2020-10-28T17:22:10Z,fix calculation of number of elements to not overflow,"Possibly fixes #46764. 
Computing number of tensor elements in many cases is written as 
```
int64_t numel = std::accumulate(oldshape.begin(), oldshape.end(), 1,
                                  std::multiplies<int64_t>());
```
This computes the product with the type of `1` literal, which is `int`. When there's more than INT_MAX elements, result overflows. In #46746, the tensor that was sent to reshape had 256^4 elements, and that was computed as `0`, so reshape was not done correctly. 
I've audited usages of std::accumulate and changed them to use int64_t as `init` type. ",pytorch
47148,ngimel,pr,2020-10-31T06:21:10Z,check sparse sizes,"checks sizes of sparse tensors when comparing them in assertEqual.
Removes additional checks in safeCoalesce, safeCoalesce should not be a test for `.coalesce()` function. 
",pytorch
47150,peterjc123,pr,2020-10-31T08:55:41Z,Use local env for building CUDA extensions on Windows,"Fixes https://github.com/pytorch/vision/pull/2818#issuecomment-719167504
After activating the VC env multiple times, the following error will be raised when building a CUDA extension.
```
FAILED: C:/tools/MINICO~1/CONDA-~2/TORCHV~1/work/build/temp.win-amd64-3.8/Release/tools/MINICO~1/CONDA-~2/TORCHV~1/work/torchvision/csrc/cuda/PSROIAlign_cuda.obj 
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0\bin\nvcc -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /EHsc -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -DWITH_CUDA -Dtorchvision_EXPORTS -IC:\tools\MINICO~1\CONDA-~2\TORCHV~1\work\torchvision\csrc -I%PREFIX%\lib\site-packages\torch\include -I%PREFIX%\lib\site-packages\torch\include\torch\csrc\api\include -I%PREFIX%\lib\site-packages\torch\include\TH -I%PREFIX%\lib\site-packages\torch\include\THC ""-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0\include"" -I%PREFIX%\include -I%PREFIX%\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt"" -I%PREFIX%\Library\include -c C:\tools\MINICO~1\CONDA-~2\TORCHV~1\work\torchvision\csrc\cuda\PSROIAlign_cuda.cu -o C:\tools\MINICO~1\CONDA-~2\TORCHV~1\work\build\temp.win-amd64-3.8\Release\tools\MINICO~1\CONDA-~2\TORCHV~1\work\torchvision\csrc\cuda\PSROIAlign_cuda.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_50,code=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0
'cl.exe' is not recognized as an internal or external command,
operable program or batch file.
```",pytorch
47192,rohan-varma,pr,2020-11-02T15:34:14Z,Add ivalue conversion helpers to rpc::Message,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47192 Add ivalue conversion helpers to rpc::Message**

To make progress on retiring torch/csrc/utils/future.h as mentioned in
https://github.com/pytorch/pytorch/issues/41574, we add some helpers to convert
back and forth from message to ivalue tuple representation of the message. We
can then use these APIs in order to convert messages to ivalue, and then wrap
then with at::IValue::Future instead of using torch::utils::Future.

In particular, we introduce `toIValueTuple` which converts the message to an
ivalue tuple and a static `Message::fromIValueTuple` which does the opposite.

Tested by adding a test to test_wire_serialization.cpp

As the next step, in follow up PRs we can begin removing instances of `FutureMessage` by replacing them using this conversion. 

Differential Revision: [D24682943](https://our.internmc.facebook.com/intern/diff/D24682943/)",pytorch
47206,rohan-varma,pr,2020-11-02T18:21:58Z,Add tests for DDP control flow models.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47206 Add tests for DDP control flow models.**

As discussed offline with @pritamdamania87, add testing to ensure per-iteration and rank-dependent control flow works as expected in DDP with `find_unused_parameters=True`.

Differential Revision: [D24659901](https://our.internmc.facebook.com/intern/diff/D24659901/)",pytorch
47328,supriyar,pr,2020-11-03T23:32:02Z,[quant] Add testing coverage for 4-bit embedding_bag sparse lookup op,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47329 [quant][pyper] Add support for pruned weights in embedding_bag_byte lookup
* **#47328 [quant] Add testing coverage for 4-bit embedding_bag sparse lookup op**

Summary:
Extend tests to cover case for pruned weights with mapping table.
Support for 8-bits sparse lookup to follow

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24719910](https://our.internmc.facebook.com/intern/diff/D24719910)",pytorch
47329,supriyar,pr,2020-11-03T23:32:14Z,[quant][pyper] Add support for pruned weights in embedding_bag_byte lookup,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47329 [quant][pyper] Add support for pruned weights in embedding_bag_byte lookup**
* #47328 [quant] Add testing coverage for 4-bit embedding_bag sparse lookup op

Summary:
Supports pruned weights along with mapping for the compressed indices

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24719909](https://our.internmc.facebook.com/intern/diff/D24719909)",pytorch
47394,rohan-varma,pr,2020-11-04T20:50:28Z,Refactor DDP uneven inputs control flags,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47394 Refactor DDP uneven inputs control flags**
uneven inputs.
* **#47394 Refactor DDP uneven inputs control flags**
uneven inputs.
* **#47394 Refactor DDP uneven inputs control flags**
uneven inputs.
* **#47394 Refactor DDP uneven inputs control flags**
uneven inputs.
* **#47394 Refactor DDP uneven inputs control flags**
uneven inputs.
* **#47394 Refactor DDP uneven inputs control flags**

This is a preliminary refactor for the next diff that will add an
additional flag to control whether we throw a StopIteration or not (see https://github.com/pytorch/pytorch/issues/47250 for details). In this change, we move the flags for ddp uneven inputs to a simple class.

Differential Revision: [D24739509](https://our.internmc.facebook.com/intern/diff/D24739509/)",pytorch
47470,rohan-varma,pr,2020-11-05T22:53:38Z,[Reland] Add tests for DDP control flow models.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47470 [Reland] Add tests for DDP control flow models.**

Reland of https://github.com/pytorch/pytorch/pull/47206, which was reverted due to failing multigpu tests.

The fix to make multigpu tests work is to compare against `torch.tensor([world_size, 0])`, not hardcode `torch.tensor([2, 0]` which assumes a world size of 2.

Original commit description:

As discussed offline with @pritamdamania87, add testing to ensure per-iteration and rank-dependent control flow works as expected in DDP with find_unused_parameters=True.

Differential Revision: [D24767893](https://our.internmc.facebook.com/intern/diff/D24767893/)",pytorch
47471,rohan-varma,pr,2020-11-05T22:56:48Z,[ci-all][Not for land] Add tests for DDP control flow models.,"Reland of https://github.com/pytorch/pytorch/pull/47206, which was reverted due to failing multigpu tests.

The fix to make multigpu tests work is to compare against `torch.tensor([world_size, 0])`, not hardcode `torch.tensor([2, 0]` which assumes a world size of 2.

Original commit description:

As discussed offline with @pritamdamania87, add testing to ensure per-iteration and rank-dependent control flow works as expected in DDP with find_unused_parameters=True.
ghstack-source-id: 115993934

Differential Revision: [D24767893](https://our.internmc.facebook.com/intern/diff/D24767893/)

[ghstack-poisoned]

Fixes #{issue number}
",pytorch
47488,rohan-varma,pr,2020-11-06T03:07:50Z,[WIP] Add an option to throw StopIteration when a rank terminates early for DDPuneven inputs.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47488 Add an option to throw StopIteration when a rank terminates early for DDP
uneven inputs.**
* #47394 Refactor DDP uneven inputs control flags
uneven inputs.**
* #47394 Refactor DDP uneven inputs control flags
uneven inputs.**
* #47394 Refactor DDP uneven inputs control flags
uneven inputs.**
* #47394 Refactor DDP uneven inputs control flags
uneven inputs.**
* #47394 Refactor DDP uneven inputs control flags

uneven inputs.

Adds a flag to ddp join() context manager that enables throwing a
StopIteration across all ranks when this flag is specified.

To do this, we implement the design in https://github.com/pytorch/pytorch/issues/47250. When running with this flag, we schedule an additional allreduce in the case that a joined rank needs to throw a `StopIteration`. In non-joined ranks forward pass, we match this allreduce and if at least one rank tells us to throw, we raise a `StopIteration`.

Will be updated soon with perf benchmarks, but one idea for better performance is to await this additional allreduce in the backwards pass for better overlap.

Tested by modifying UT to add a series of tests that run under this flag.

Differential Revision: [D24743437](https://our.internmc.facebook.com/intern/diff/D24743437/)",pytorch
47514,supriyar,pr,2020-11-06T20:12:34Z,[quant][qat] Ensure observer respects device affinity,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47514 [quant][qat] Ensure observer respects device affinity**

Summary:

fixed a bug where we were always setting the device to 'cuda' (irrespective of the device id)
in the calculate_qparams function

Fixes #46533

Test Plan:
python test/test_quantization.py TestObserver.test_observer_qparams_respects_device_affinity

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24800495](https://our.internmc.facebook.com/intern/diff/D24800495)",pytorch
47545,OverLordGoldDragon,pr,2020-11-07T03:04:16Z,Fix docstring typo,It's its.,pytorch
47602,peterjc123,pr,2020-11-09T15:48:21Z,Fix python 3.9 builds on Windows,"Fixes https://github.com/pytorch/pytorch/issues/47460.
",pytorch
47642,ngimel,pr,2020-11-10T01:06:44Z,"Back out ""[pytorch][PR] The dimension being reduced should not be coalesced by TensorIterator""","Summary: Original commit changeset: 02bb2b15694c

Test Plan: Covered by CI tests

Reviewed By: anjali411

Differential Revision: D24849072

",pytorch
47663,crcrpar,pr,2020-11-10T09:45:30Z,Update links in DDP note,"Update the links in https://pytorch.org/docs/stable/notes/ddp.html#.
",pytorch
47726,ngimel,pr,2020-11-11T02:29:42Z,Unbreak backward compatibility tests,"Per title
",pytorch
47762,bharatr21,pr,2020-11-11T17:01:02Z,[DOCS]: Correct docs for torch.lu_solve,"Fixes #43498 by correcting the function signature of `torch.lu_solve`
",pytorch
47766,supriyar,pr,2020-11-11T18:11:19Z,[quant] Add support for 2D indices for quantized embedding operators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47766 [quant] Add support for 2D indices for quantized embedding operators**

Summary:
The operator now supports accepting 2D indices as inputs.
For embedding operators, we set the default offsets in the op since the FBGEMM kernel expects it to be set
Output shape depends on the shape if the indices.

For embedding_bag operator, if indices is 2D (B, N) then offsets should be set to None by user. In this case
the input is interpreted as B bags each of fixed length N. Output shape is still 2-D in this case.

Test Plan:
python test/test_quantization.py TestQuantizedEmbeddingOps.test_embedding_bag_2d_indices
python test/test_quantization.py TestQuantizedEmbeddingOps.test_embedding_2d_indices

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24895048](https://our.internmc.facebook.com/intern/diff/D24895048)",pytorch
47800,supriyar,pr,2020-11-11T23:08:49Z,[quant] skip tests without fbgemm support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47800 [quant] skip tests without fbgemm support**

Summary:
Fixes #47748

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D24904885](https://our.internmc.facebook.com/intern/diff/D24904885)",pytorch
47874,r-barnes,pr,2020-11-12T18:41:37Z,Improve dimensionality mismatch warning,"Test Plan: N/A

Differential Revision: D24926123

",pytorch
47876,r-barnes,pr,2020-11-12T19:06:45Z,"Fix ""pointless comparison"" warning","Summary: Fixes a pointless comparison against zero warning that arises for some scalar types

Test Plan:
Arises with
```
xbuck test mode/dev-nosan //caffe2/torch/fb/sparsenn:gpu_test -- test_prior_correction_calibration_prediction_binary
```

Differential Revision: D24925955

",pytorch
47895,rohan-varma,pr,2020-11-13T05:34:56Z,Log timeout in ProcessGroupNCCL for NCCL_BLOCKING_WAIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47895 Log timeout in ProcessGroupNCCL for NCCL_BLOCKING_WAIT**

It would be useful in error messages to have the timeout in addition
to the operation timed out message.

Differential Revision: [D24943318](https://our.internmc.facebook.com/intern/diff/D24943318/)",pytorch
47896,rohan-varma,pr,2020-11-13T05:35:07Z,Pass in smaller timeout into init_process_group for distributed_test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47896 Pass in smaller timeout into init_process_group for distributed_test**

Closes https://github.com/pytorch/pytorch/issues/47892. Since we have a 100s timeout on the entire test, we should have a smaller timeout than the default 30 min for the process group used for the test.

This diff sets the timeout to 60s. For example, this is useful when running tests with NCCL_BLOCKING_WAIT so that we get the op timed out error instead of the test itself timing out. 

Differential Revision: [D24943323](https://our.internmc.facebook.com/intern/diff/D24943323/)",pytorch
47919,r-barnes,pr,2020-11-13T16:33:56Z,Fix a downcast,"Summary: Suppresses a downcast warning.

Test Plan:
Reproduces with
```
buck test mode/dev-nosan //caffe2/torch/fb/sparsenn:gpu_test
```

Differential Revision: D24866987

",pytorch
47920,r-barnes,pr,2020-11-13T16:34:34Z,Be more careful with device ids,"Summary:
A common pattern is for device ids to be integers of various sizes; however, all of these are internally stored as int16_t. At the moment this conversion happens silently, so erroneous values may not be caught.

This diff causes the device constructor to take larger ints as inputs and value check them before downcasting, catching potential issues before they happen.

Test Plan:
```
buck test mode/dev-nosan //caffe2/torch/fb/sparsenn:gpu_test
```

Differential Revision: D24866941

",pytorch
48001,rohan-varma,pr,2020-11-16T06:46:39Z,[Not for land] RPC double init issue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48001 [Not for land] RPC double init issue**

Differential Revision: [D24985808](https://our.internmc.facebook.com/intern/diff/D24985808/)",pytorch
48031,rohan-varma,pr,2020-11-16T19:42:41Z,[Don't review] CI all multigpu tests,"Fixes #{issue number}
",pytorch
48041,neerajprad,pr,2020-11-16T20:32:12Z,Add support for CorrCholeskyTransform,"This adds a transform to convert a real vector of (D * (D-1))/2 dimension into the cholesky factor of a D x D correlation matrix. This follows the implementation in [NumPyro](https://github.com/pyro-ppl/numpyro/blob/master/numpyro/distributions/transforms.py) by @fehiepsi. This is needed for the LKJDistribution which will be added in a subsequent PR.

Also in line with the ongoing effort to refactor distributions test, this moves the transforms test into its own file that uses pytest with parametrized fixtures. 

For review:
 @fehiepsi - could you help review the math?
 @fritzo - do you have any suggestions for what to do about the event dimension (more details are in the comment below)? 
 @ezyang - could you review the changes in `run_test.py`? Instead of a separate `PYTEST_TESTS`, I have clubbed these tests in `USE_PYTEST_LIST` to avoid duplicate logic. The only difference is that we do not anymore check if pytest is not installed and exclude the tests in the list. I figured that if existing tests are already using pytest, this should not matter. 

TODOs (probably not all can be satisfied at the same time):
 - [x] Use operations that are JIT friendly, i.e. the transform works with different sized input under JIT.
 - [x] Resolve test failures - currently `arange(scalar_tensor)` fails on certain backends but this is needed for JIT. Maybe we should only support same sized tensor under JIT?
 - [x] Add tests to check that the transform gives correct gradients and is in agreement with the `log_det_jacobian`.
 - [x] Add `input_event_dim` and `output_event_dim` to `CorrCholeskyTransform`.",pytorch
48104,ssnl,pr,2020-11-17T16:50:45Z,Fix jit doc model loading example,"Fixes #{issue number}
",pytorch
48129,rohan-varma,pr,2020-11-17T22:35:39Z,Disable distributed collectives profiling tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48194 [WIP] Fix and re-enable distributed profiling tests
* **#48129 Disable distributed collectives profiling tests**

It looks like all test failures in distributed_test have to do with
profiling, so disabling them in this PR (by setting `expect_event=False`
always), so that the distributed profiling tests don't run.

Created https://github.com/pytorch/pytorch/issues/48127 to track the fix.
Will verify with CI all that re-enabling distributed tests passes as expected.

CI-all version of PR: https://github.com/pytorch/pytorch/pull/48132

Differential Revision: [D25034888](https://our.internmc.facebook.com/intern/diff/D25034888/)",pytorch
48132,rohan-varma,pr,2020-11-17T22:46:46Z,[Ci-all] Check that disabling dist profiling tests makes dist tests succeed,Actual PR: https://github.com/pytorch/pytorch/pull/48129,pytorch
48194,rohan-varma,pr,2020-11-18T19:03:40Z,[WIP] Fix and re-enable distributed profiling tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48194 [WIP] Fix and re-enable distributed profiling tests**

Fix and re-enable distributed profiling tests

CI-all version of PR: https://github.com/pytorch/pytorch/pull/48196

Differential Revision: [D25036865](https://our.internmc.facebook.com/intern/diff/D25036865/)",pytorch
48196,rohan-varma,pr,2020-11-18T19:12:07Z,[WIP][CI-all] Fix profiling tests,ci-all for https://github.com/pytorch/pytorch/pull/48194,pytorch
48231,rohan-varma,pr,2020-11-19T02:47:44Z,Dont skip NCCL backend when testing all_reduce_cuda,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48231 Dont skip NCCL backend when testing all_reduce_cuda**

Noticed that these tests were being skipped with NCCL backend, but
there doesn't appear to be a valid reason to. Enabled these tests and verify
that they pass with 500 stress runs.

Differential Revision: [D25079030](https://our.internmc.facebook.com/intern/diff/D25079030/)",pytorch
48248,rohan-varma,pr,2020-11-19T09:04:33Z,Add an option to run RPC tests with TCP init,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48248 Add an option to run RPC tests with TCP init**

We have found a few bugs where initializing/de-initializing/re-initializing RPC, and using RPC along with process groups does not work as expected, usually under TCP/env initialization (which is used over `file` which is the init that we use in our test in multi-machine scenarios).

Due to this motivation, this PR adds an environment variable `RPC_INIT_WITH_TCP` that allows us to run any RPC test with TCP initialization. 

To avoid port collisions, we use `common.find_free_port()`.
Differential Revision: [D25085458](https://our.internmc.facebook.com/intern/diff/D25085458/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D25085458/)!",pytorch
48272,r-barnes,pr,2020-11-19T20:40:41Z,Suppress unsigned warning,"Summary: Fixes a pointless comparison against zero warning that arises for some scalar types

Test Plan:
Arises with
```
xbuck test mode/dev-nosan //caffe2/torch/fb/sparsenn:gpu_test -- test_prior_correction_calibration_prediction_binary
```

Fixes issues raised by #47876 - `std::is_signed` was a poor choice `std::is_unsigned` is a better choice. Surprisingly, the two are non-reciprocal. ",pytorch
48285,vishwakftw,pr,2020-11-20T00:49:50Z,Implement Kumaraswamy Distribution,"This PR implements the Kumaraswamy distribution.

cc: @fritzo @alicanb @sdaulton ",pytorch
48293,rohan-varma,pr,2020-11-20T04:28:08Z,Enable creation and transfer of ScriptModule over RPC,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48339 RRef proxy support for ScriptModule methods
* **#48293 Enable creation and transfer of ScriptModule over RPC**

This PR enables a `ScriptModule` to be created on a remote worker, retrieve it to current device, and run methods on it remotely.

In order to do this, we define custom pickling for `ScriptModule` in our `InternalRPCPickler`. The pickling basically uses torch.save and torch.load to save/recover the ScriptModule.

We test that we can create and retrieve a ScriptModule with rpc_sync, and create RRef to ScriptModule with rpc.remote. We can also run remote methods on the rref and transfer it to current worker.

Although we can run methods remotely on the RRef to ScriptModule, this does not currently work with RRef helper, support is added in the next (wip) PR.

Future PRs will address:
1. Calling local ScriptModule method remotely 
2. Calling `to_here()` on ScriptModule in JIT (currently disabled, see https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/rref_impl.cpp#L147). 

Differential Revision: [D25107773](https://our.internmc.facebook.com/intern/diff/D25107773/)",pytorch
48296,r-barnes,pr,2020-11-20T06:43:26Z,Fix cmake warning,"Building with cmake currently gives the warning:
```
CMake Warning (dev) at aten/src/ATen/CMakeLists.txt:74:
  Syntax Warning in cmake code at column 56

  Argument not separated from preceding token by whitespace.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Deprecation Warning at third_party/sleef/CMakeLists.txt:20 (cmake_policy):
  The OLD behavior for policy CMP0066 will be removed from a future version
  of CMake.

  The cmake-policies(7) manual explains that the OLD behaviors of all
  policies are deprecated and that a policy should be set to OLD only under
  specific short-term circumstances.  Projects should be ported to the NEW
  behavior and not rely on setting a policy to OLD.
```
This fixes that warning. The [cmake documentation](https://cmake.org/cmake/help/v3.7/command/file.html) indicates that a comma is not needed to separate globbing patterns.",pytorch
48298,peterjc123,pr,2020-11-20T07:28:26Z,Fix CUDA 11 test jobs on Windows,"Fixes #{issue number}
",pytorch
48339,rohan-varma,pr,2020-11-20T23:04:21Z,RRef proxy support for ScriptModule methods,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48339 RRef proxy support for ScriptModule methods**

Closes https://github.com/pytorch/pytorch/issues/48294
https://github.com/pytorch/pytorch/pull/48293 added creation and transfer of ScriptModule over RPC in python, but it did not work with ScriptModule.

This PR makes the above work with ScriptModule as per a discussion with @mrshenli:
1) We remove the `hasattr()` check and just let Python throw the exception as it would when accessing the py function with `getattr`
2) We  condition on `issubclass(type, ScriptModule)` when checking if it is wrapped with async_function, because `ScriptModule` does not have getattr implemented (this is because ScriptModule forward/function is not a python function, it is a torchscript specific function):
```
torch/jit/_script.py"", line 229, in __get__
    return self.__getattr__(""forward"")  # type: ignore
AttributeError: '_CachedForward' object has no attribute '__getattr__'
```

Differential Revision: [D25134423](https://our.internmc.facebook.com/intern/diff/D25134423/)",pytorch
48368,elfringham,pr,2020-11-22T00:04:30Z,Update sleef to 3.5.1 to remove unsupported use of __sizeless_struct,"Fixes pytorch/pytorch#35049

Versions of gcc that support arm_sve.h will fail to build for AARCH64 with the current sleef submodule which uses an unsupported type __sizeless_struct. Later versions of sleef were corrected to fix this.
",pytorch
48453,elfringham,pr,2020-11-25T12:43:45Z,Flake8 fixes,Quiet errors from flake8. Only a couple of code changes for deprecated Python syntax from before 2.4. The rest is just adding noqa markers.,pytorch
48469,peterjc123,pr,2020-11-26T03:04:58Z,"Update Windows CI to CUDA 11.1, cuDNN 8.0.5","Fixes #{issue number}
",pytorch
48474,zhuzilin,pr,2020-11-26T06:41:01Z,Add jit.ignore in custom_lstms.py to bypass issue #25135,"`custom_lstms.py` mentioned in https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/ is a really nice example on how to use torchscript to speed up models. However, due to issue #25135, the following function could not be added to a ScriptModule:
```python
def reverse(lst):
    # type: (List[Tensor]) -> List[Tensor]
    return lst[::-1]
```
This PR adds a `@jit.ignore` on this function to make the code runnable again.

Thank you for your time on reviewing this PR.",pytorch
48480,elfringham,pr,2020-11-26T09:23:17Z,Update sleef to address build failure on AARCH64,"Fixes #35049

Versions of gcc that support arm_sve.h will fail to build for AARCH64 with the current sleef submodule which uses an unsupported type __sizeless_struct. Later versions of sleef were corrected to fix this.",pytorch
48543,ssnl,pr,2020-11-28T22:07:10Z,Fix persistent_workers + pin_memory,"Fixes https://github.com/pytorch/pytorch/issues/48370 https://github.com/pytorch/pytorch/issues/47445

cc @emcastillo who authored the original functionality.",pytorch
48570,yaochengji,pr,2020-11-30T05:06:00Z,enable autocast for xla,"For enabling amp in torch/xla, see [this](https://github.com/pytorch/xla/pull/2654).",pytorch
48596,ssnl,pr,2020-11-30T18:06:53Z,Update loss module doc,"Fixes #{issue number}
",pytorch
48656,peterjc123,pr,2020-12-01T17:59:07Z,Update magma to 2.5.4 for Windows,"Fixes https://github.com/pytorch/pytorch/issues/48527
",pytorch
48658,peterjc123,pr,2020-12-01T18:09:23Z,Test binary jobs for Windows magma upgrade,"Testing https://github.com/pytorch/builder/pull/600
",pytorch
48662,rohan-varma,pr,2020-12-01T19:24:49Z,[wip] fix processgroupnccl profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48662 [wip] fix processgroupnccl profiling**

Differential Revision: [D25250227](https://our.internmc.facebook.com/intern/diff/D25250227/)",pytorch
48664,rohan-varma,pr,2020-12-01T19:28:26Z,[wip][ci-all] fix processgroupnccl profiling,"Differential Revision: [D25250227](https://our.internmc.facebook.com/intern/diff/D25250227/)

[ghstack-poisoned]

Fixes #{issue number}
",pytorch
48669,ssnl,pr,2020-12-01T22:15:44Z,Fix dataloader hang with large sampler,"Fixes https://github.com/pytorch/pytorch/issues/48666
",pytorch
48743,fritzo,pr,2020-12-02T21:07:39Z,Enable distribution validation if __debug__,"Fixes #47123
Follows https://github.com/pyro-ppl/pyro/pull/2701

This turns on `Distribution` validation by default. The motivation is to favor beginners by providing helpful error messages. Advanced users focused on speed can disable validation by calling
```py
torch.distributions.Distribution.set_default_validate_args(False)
```
or by disabling individual distribution validation via `MyDistribution(..., validate_args=False)`.

In practice I have found many beginners forget or do not know about validation. Therefore I have [enabled it by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro. I believe PyTorch could also benefit from this change. Indeed validation caught a number of bugs in `.icdf()` methods, in tests, and in PPL benchmarks, all of which have been fixed in this PR.

## Release concerns
- This may slightly slow down some models. Concerned users may disable validation.
- This may cause new `ValueErrors` in models that rely on unsupported behavior, e.g. `Categorical.log_prob()` applied to continuous-valued tensors (only {0,1}-valued tensors are supported).

We should clearly note this change in release notes.",pytorch
48755,r-barnes,pr,2020-12-02T23:38:12Z,Detect extraneous kernel launch checks,"Summary:
The code being modified detects CUDA kernels whose launches aren't provably being checked for failure. This extends that to locate extraneous instances of such checks.

These checks don't do anything valuable and are semantically misnamed if they don't follow a kernel launch, so it makes sense not to have them in the code.

Test Plan:
```
buck test //caffe2/test:kernel_launch_checks -- --print-passing-details
```

Differential Revision: D25286853

",pytorch
48765,r-barnes,pr,2020-12-03T03:15:21Z,Improve an autograd warning,"Fixes #48764
",pytorch
48798,neerajprad,pr,2020-12-03T20:07:55Z,Add LKJCholesky distribution,"As a follow up to #48041, this adds the `LKJCholesky` distribution that samples the Cholesky factor of positive definite correlation matrices.

This also relaxes the check on `tril_matrix_to_vec` so that it works for 2x2 matrices with `diag=-2`.

cc. @fehiepsi",pytorch
48884,peterjc123,pr,2020-12-05T16:00:06Z,[TEST ONLY] Chesterliu/dev/unicode api,"Test https://github.com/pytorch/pytorch/pull/47905 without setting LC_* variables
",pytorch
48893,r-barnes,pr,2020-12-05T23:12:50Z,Fix some convoluted(?) code,"Summary: This simplifies some code which is written in an interesting way. It may be that this was intentional, but I don't recognize the pattern being used.

Test Plan: N/A - Sandcastle

Differential Revision: D25358283

",pytorch
48894,r-barnes,pr,2020-12-05T23:20:29Z,[Feedback request] Check CUDA launch guards,"Summary:
CUDA kernel launches and API calls need to be made to the same device as the data the are affecting or Bad Things will happen.

In (parts of?) PyTorch we used `at::cuda::OptionalCUDAGuard` to ensure the correct device is used.

However, in a large codebase it can be difficult to locate code that isn't guarded, resulting in hard-to-find bugs (e.g., D25350560).

Here, we use a light-weight static analysis to quickly identify functions that use CUDA without appropriate guards.

The output looks like this:
```
Missing CUDA device_guard in '/data/sandcastle/boxes/fbsource/fbcode/caffe2/torch/fb/sparsenn/sparsenn_operators_gpu.cu'. Context:

1946: Tensor sigrid_hash_cuda(
1947:     const Tensor& indices,
1948:     const int64_t salt,
1949:     const int64_t maxValue,
1950:     const bool hashIntoInt32) {

Found 620 instances in 269 files where CUDA was used without device guards.
        /data/sandcastle/boxes/fbsource/fbcode/caffe2/aten/src/ATen/native/cuda/IndexKernel.cu
        /data/sandcastle/boxes/fbsource/fbcode/caffe2/aten/src/ATen/native/cuda/Indexing.cu
        /data/sandcastle/boxes/fbsource/fbcode/caffe2/aten/src/ATen/native/cuda/Loops.cuh
        /data/sandcastle/boxes/fbsource/fbcode/caffe2/aten/src/ATen/native/cuda/LossCTC.cu
```

Some further refinement will be necessary to ensure this checks only appropriate files.

By using some simple statement machines and heuristics the script is able to avoid using heavy dependencies at the cost of requiring that code be written in a generally sane way. D25358283 gives an instance where these assumptions break down and shows that it represents a non-standard case which can be easily corrected.

Test Plan:
```
python3 caffe2/torch/testing/check_launch_guards.py
```

Differential Revision: D25358200

",pytorch
48897,rohan-varma,pr,2020-12-06T03:10:54Z,Improve new_group example in the context of SyncBatchNorm ,"Closes https://github.com/pytorch/pytorch/issues/48804
Improves some documentation/example in SyncBN docs to clearly show that each rank must call into all `new_group()` calls for creating process subgroups, even if they are not going to be part of that particular subgroup.
We then pick the right group, i.e. the group that the rank is part of, and pass that into the SyncBN APIs. 

Doc rendering: 

<img width=""786"" alt=""syncbn_update"" src=""https://user-images.githubusercontent.com/8039770/101271959-b211ab80-373c-11eb-8b6d-d56483fd9f5d.png"">
",pytorch
48909,rohan-varma,pr,2020-12-06T23:51:54Z,Add object-based collective APIs to public docs ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43932 [Docs] Add examples for new object-based c10d APIs
* **#48909 Add object-based collective APIs to public docs**

Adds these new APIs to the documentation

Differential Revision: [D25363279](https://our.internmc.facebook.com/intern/diff/D25363279/)",pytorch
48913,ngimel,pr,2020-12-07T00:21:17Z,disable is_non_overlapping_and_dense for sparse tensors,"Fixes #48892
`is_non_overlapping_and_dense` is now virtual, like `is_contiguous`.

",pytorch
48946,rohan-varma,pr,2020-12-07T20:10:30Z,Fix ProcessGroupNCCL profiling when profiler is not run with use_cuda,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #49072 Test distributed collectives profiling with Gloo on GPU
* **#48946 Fix ProcessGroupNCCL profiling when profiler is not run with use_cuda**

Move `recordFunctionEndCallback` to after the blocking portion of launching the NCCL kernel, and remove `addCallback` since it runs the lambda inline anyways, and triggers unnecessary CUDA stream logic. If we want CUDA operations such as NCCL kernels accurately profiled, we should use the profiler with `use_cuda=True`. However, we are currently debugging a deadlock for the `use_cuda=True` case, fix is being tracked in https://github.com/pytorch/pytorch/issues/48987. 

To ensure that the tests are no longer flaky, submitted this PR to ci-all: https://github.com/pytorch/pytorch/pull/48947 and ran the test a bunch of times ssh'd into the CI machine and made sure they don't fail.

Differential Revision: [D25368322](https://our.internmc.facebook.com/intern/diff/D25368322/)",pytorch
48947,rohan-varma,pr,2020-12-07T20:21:53Z,[ci-all][not for review] Fix ProcessGroupNCCL profiling when profiler isâ€¦,"â€¦ not run with use_cuda

Differential Revision: [D25368322](https://our.internmc.facebook.com/intern/diff/D25368322/)

[ghstack-poisoned]

Fixes #{issue number}
",pytorch
49009,rohan-varma,pr,2020-12-08T08:53:13Z,Refactor RPC matchBuiltInOp to get rid of exception swallowing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49009 Refactor RPC matchBuiltInOp to get rid of exception swallowing**

As per the title, we should generally not have exception swalling and
this commit makes it so that if there is a true error in JIT operator
resolution, it is propagated back to the RPC callee and we don't silently
swallow any other exceptions that may happen. Swallowing the exceptions
previously resulted in hard to debug issues such as unexpected ops showing up
in profiler, and flaky tests which were fixed by
https://github.com/pytorch/pytorch/pull/41287

Added a unittest that validates the error that comes from `jit/pybind_utils.h`.

Differential Revision: [D25392905](https://our.internmc.facebook.com/intern/diff/D25392905/)",pytorch
49020,peterjc123,pr,2020-12-08T15:13:58Z,[collect_env] Acquire windows encoding using OEMCP,"Fixes https://github.com/pytorch/pytorch/issues/49010.
",pytorch
49021,peterjc123,pr,2020-12-08T15:32:31Z,[collect_env] Add candidate paths for nvidia-smi on Windows,"Recently, Nvidia tries to put nvidia-smi under SystemRoot.",pytorch
49025,peterjc123,pr,2020-12-08T16:56:24Z,Correctly apply WIN32_LEAN_AND_MEAN to the whole repo,"Fixes https://github.com/pytorch/pytorch/issues/48895
",pytorch
49027,supriyar,pr,2020-12-08T18:10:36Z,[quant][fix] Support quantization of ops where input is quantizable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49027 [quant][fix] Support quantization of ops where input is quantizable**

Summary:
For cat followed by linear since the output of cat is not quanitzed, we didnt quantize the linear
This checks the uses of the cat op to insert observers

Test Plan:
python test/test_quantization.py TestQuantizeJitOps.test_cat_linear

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D25403412](https://our.internmc.facebook.com/intern/diff/D25403412)",pytorch
49072,rohan-varma,pr,2020-12-09T05:44:41Z,Test distributed collectives profiling with Gloo on GPU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49072 Test distributed collectives profiling with Gloo on GPU**

As per the title, we should enable these tests for Gloo when run on GPU and the profiler is enabled with `use_cuda=True`. Enabling ProcessGroupNCCL profiling test to work with `use_cuda=True` is being tracked in https://github.com/pytorch/pytorch/issues/48987.

Also manually verified that it gives accurate results with GPU profiling, by timing the allreduce (which is blocking when we call wait() with gloo) with a vanilla `time.time()` and verified that the reported times are roughly similar.  

Also, we refactor some duplicated code in `test_reduce_multigpu` that makes it easier to read. 

Differential Revision: [D25388986](https://our.internmc.facebook.com/intern/diff/D25388986/)",pytorch
49141,rohan-varma,pr,2020-12-10T04:26:01Z,Fix link in distributed contributing doc and add link,"One of the links for ramp up tasks wasn't showing any results and the other was only RPC results. Instead of this, I just changed it to one link that has `pt_distributed_rampup` which seems reasonable as the developer will be able to see both RPC and distributed tasks.

Also added test command for DDP tests. ",pytorch
49158,vfdev-5,pr,2020-12-10T10:50:35Z,Refactor sparse unary ops tests,"Description:
- Added TestSparseUnaryUfuncs test case to test_sparse.py
- Removed old test code for neg, asin, log1p ops on sparse data
- Added sparse unary ops testing helpers in OpInfo, e.g. sample_sparse_inputs
- Added abs, sign UnaryUfuncInfo to `op_db`


cc @mruberry ",pytorch
49191,ngimel,pr,2020-12-10T21:58:49Z,inline `has`  function for DispatchKeySet,"inlines `has` function for DispatchKeySet, that is frequently used in TensorImpl in calls such as `is_sparse`, `is_cuda` etc. 
This increases `empty` instruction count (1853228 -> 1937428) without appreciable effect on runtime, and noticeably reduces instruction counts for `copy_` and friends that have to rely on `is_sparse`, `is_cuda` and the like a lot to decide which path to take (3269114 -> 2634114). 
",pytorch
49197,r-barnes,pr,2020-12-10T22:47:15Z,Suppress `warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed` warning,"Summary:
Compiling currently gives a number of these warnings:
```
caffe2/c10/util/TypeCast.h(39): warning: calling a constexpr __host__ function from a __host__ __devic
e__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow thi
s.
          detected during:
            instantiation of ""dest_t c10::static_cast_with_inter_type<dest_t, src_t>::apply(src_t) [wi
th dest_t=c10::complex<double>, src_t=__nv_bool]""
(157): here
            instantiation of ""To c10::convert<To,From>(From) [with To=c10::complex<double>, From=__nv_
bool]""
(169): here
            instantiation of ""To c10::checked_convert<To,From>(From, const char *) [with To=c10::compl
ex<double>, From=__nv_bool]""
caffe2/c10/core/Scalar.h(63): here

caffe2/c10/util/TypeCast.h(39): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.
          detected during:
            instantiation of ""dest_t c10::static_cast_with_inter_type<dest_t, src_t>::apply(src_t) [with dest_t=c10::complex<double>, src_t=int64_t]""
(157): here
            instantiation of ""To c10::convert<To,From>(From) [with To=c10::complex<double>, From=int64_t]""
(169): here
            instantiation of ""To c10::checked_convert<To,From>(From, const char *) [with To=c10::complex<double>, From=int64_t]""
caffe2/c10/core/Scalar.h(63): here
```
Here, we take the compiler up on its suggestion and use the `--expt-relaxed-constexpr` flag.

As noted [here](https://stackoverflow.com/questions/55481202/how-to-disable-cuda-host-device-warning-for-just-one-function) either `#pragma nv_exec_check_disable` or `#pragma hd_warning_disable` could also be used. This would have less over-all impact on the code (since using the flag applies to all files touched by NVCC within this target); however, their effects are not as well documented as `constexpr`.

Test Plan:
Compiling
```
buck build mode/dev-nosan -c=python.package_style=inplace dper3/dper3_models/experimental/pytorch/ads:ads_model_generation_script
```
shows this warning.

We rely on sandcastle for testing here.

Reviewed By: xw285cornell

Differential Revision: D25440771

",pytorch
49280,peterjc123,pr,2020-12-12T09:56:48Z,PyLong_{As/From}{Long/UnsignedLong} lint checks,"Fixes https://github.com/pytorch/pytorch/issues/45581
",pytorch
49351,r-barnes,pr,2020-12-14T19:54:02Z,Convert some files to Python3,"Test Plan: Standard sandcastle tests

Differential Revision: D25499576

",pytorch
49365,r-barnes,pr,2020-12-14T22:16:13Z,Fix check_kernel_launches.py for macros and provide extended context ,"`check_kernel_launches.py` currently gives a false positive in instances such as:
```
735:     <<<smallIndexGrid, smallIndexBlock, 0, stream>>>(                                   \
736:       outInfo, selfInfo, indicesInfo,                                                   \
737:       outSelectDim, selfSelectDim, static_cast<TYPE>(sliceSize),                        \
738:       selfSelectDimSize);                                                               \
739:     C10_CUDA_KERNEL_LAUNCH_CHECK();
```
because the newlines after the last `\` are not consumed by the regex. This fixes that.

In addition, the regex is modified to provide greater context for the start of the kernel launch. This changes the context from:
```
157:       (
158:           size, X_strides, Y_dims, X, Y);
```
to
```
157:       <<<M, CAFFE_CUDA_NUM_THREADS, 0, context->cuda_stream()>>>(
158:           size, X_strides, Y_dims, X, Y);
```",pytorch
49373,rohan-varma,pr,2020-12-15T00:05:08Z,Unescape string in RPC error message,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49373 Unescape string in RPC error message**

Unescaping the string in RPC error message to provide better error msg

Differential Revision: [D25511730](https://our.internmc.facebook.com/intern/diff/D25511730/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D25511730/)!",pytorch
49495,r-barnes,pr,2020-12-16T21:09:08Z,"Suppress ""statement is unreachable"" warning","Summary:
Compiling PyTorch currently generates a large number of warnings like this:
```
caffe2/aten/src/ATen/core/builtin_function.h(105): warning: statement is unreachable
```
The offending code
```
  std::string pretty_print_schema() const override {
    TORCH_INTERNAL_ASSERT(false);
    return """";
  }
```
has an unreachable return which prevents a ""no return"" warning.

We resolve the situation by using NVCC's pragma system to suppress this warning within this function.

Test Plan:
The warning appears when running:
```
buck build mode/dev-nosan //caffe2/torch/fb/sparsenn:test
```
As well as a number of other build commands.

Differential Revision: D25546542

",pytorch
49611,TylerADavis,pr,2020-12-18T19:56:36Z,Update `is_floating_point()` docs to mention bfloat16,"Fixes #49610 . Explicitly mentions that `is_floating_point()` will return `True` if passed a `bfloat16` tensor.
",pytorch
49640,peterjc123,pr,2020-12-19T07:46:19Z,Enable tests using named temp files on Windows,,pytorch
49674,rohan-varma,pr,2020-12-21T10:29:20Z,[Not for review] trigger ci for profiler,"Fixes #{issue number}
",pytorch
49725,ngimel,pr,2020-12-22T06:04:43Z,remove unused THCBlas,"removes unused THCBlas, call `at::cuda::blas::gemm` directly where needed. 
",pytorch
49736,SamuelMarks,pr,2020-12-22T12:08:14Z,"[*.py] Rename ""Arguments:"" to ""Args:""","I've written custom parsers and emitters for everything from docstrings to classes and functions. However, I recently came across an issue when I was parsing/generating from the TensorFlow codebase: inconsistent use of `Args:` and `Arguments:` in its docstrings.

```sh
(pytorch#c348fae)$ for name in 'Args:' 'Arguments:'; do
    printf '%-10s %04d\n' ""$name"" ""$(rg -IFtpy --count-matches ""$name"" | paste -s -d+ -- | bc)""; done
Args:      1095
Arguments: 0336
```

It is easy enough to extend my parsers to support both variants, however it looks like `Arguments:` is wrong anyway, as per:

  - https://google.github.io/styleguide/pyguide.html#doc-function-args @ [`ddccc0f`](https://github.com/google/styleguide/blob/ddccc0f/pyguide.md)

  - https://chromium.googlesource.com/chromiumos/docs/+/master/styleguide/python.md#describing-arguments-in-docstrings @ [`9fc0fc0`](https://chromium.googlesource.com/chromiumos/docs/+/9fc0fc0/styleguide/python.md)

  - https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html @ [`c0ae8e3`](https://github.com/sphinx-contrib/napoleon/blob/c0ae8e3/docs/source/example_google.rst)

Therefore, only `Args:` is valid. This PR replaces them throughout the codebase.

PS: For related PRs, see tensorflow/tensorflow/pull/45420

PPS: The trackbacks automatically appearing below are sending the same changes to other repositories in the [PyTorch](https://github.com/pytorch) organisation.",pytorch
49776,supriyar,pr,2020-12-23T01:52:26Z,[debug] Add module path to observer name,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49776 [debug] Add module path to observer name**

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
49788,ngimel,pr,2020-12-23T05:43:52Z,removes more unused THC functions,"per title
",pytorch
49801,r-barnes,pr,2020-12-23T17:36:37Z,Mod lists to neutral+descriptive terms in caffe2/caffe2/opt,"Summary:
Per ""https://fb.workplace.com/groups/e/permalink/3320810064641820/"" we can no longer use the terms ""whitelist"" and ""blacklist"", and editing any file containing them results in a critical error signal. Let's embrace the change.
This diff changes ""blacklist"" to ""blocklist"" in a number of non-interface contexts (interfaces would require more extensive testing and might interfere with reading stored data, so those are deferred until later).

Test Plan: Sandcastle

Differential Revision: D25686949

",pytorch
49803,r-barnes,pr,2020-12-23T17:42:46Z,Mod lists to neutral+descriptive terms in caffe2/docs,"Summary:
Per ""https://fb.workplace.com/groups/e/permalink/3320810064641820/"" we can no longer use the terms ""whitelist"" and ""blacklist"", and editing any file containing them results in a critical error signal. Let's embrace the change.
This diff changes ""blacklist"" to ""blocklist"" in a number of non-interface contexts (interfaces would require more extensive testing and might interfere with reading stored data, so those are deferred until later).

Test Plan: Sandcastle

Differential Revision: D25686924

",pytorch
49815,SamuelMarks,pr,2020-12-24T00:55:15Z,[c10/**] Fix typos,All pretty minor. I avoided renaming `class DestructableMock` to `class DestructibleMock` and similar such symbol renames (in this PR).,pytorch
49910,rohan-varma,pr,2020-12-29T00:47:07Z,[WIP] Demo for caching serialization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49910 [WIP] Demo for caching serialization**

Demo to demonstrate potential speedup if we cache serialization in RPC.

With this setup where we have several large networks being serialized to be send over network with rpc.remote, we can cache the serialization the first time and this results in some speedup. In particular, total execution time goes from 2s to 1s; each (cached) rpc.remote invocaton goes from ~0.16s -> ~0.05s.

Differential Revision: [D25720408](https://our.internmc.facebook.com/intern/diff/D25720408/)",pytorch
49938,r-barnes,pr,2020-12-29T20:45:43Z,Clean up type annotations in caffe2/torch/nn/modules,"Test Plan: Sandcastle tests

Differential Revision: D25718705

",pytorch
49939,r-barnes,pr,2020-12-29T20:50:52Z,Clean up some type annotations in torch/jit,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717573

",pytorch
49940,r-barnes,pr,2020-12-29T20:50:56Z,Clean up type annotations in caffe2/torch/nn/utils,"Differential Revision: D25718722

",pytorch
49941,r-barnes,pr,2020-12-29T20:52:24Z,Clean up type annotations in torch/nn/quantized/modules,"Differential Revision: D25718715

",pytorch
49942,r-barnes,pr,2020-12-29T21:00:32Z,Clean up some type annotations in caffe2/torch/quantization,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Reviewed By: vkuzo

Differential Revision: D25717551

",pytorch
49943,r-barnes,pr,2020-12-29T21:02:41Z,Clean up some type annotations in caffe2/test,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717534

",pytorch
49944,r-barnes,pr,2020-12-29T21:03:19Z,Clean up some type annotations in android,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717539

",pytorch
49945,r-barnes,pr,2020-12-29T21:04:02Z,Clean up some type annotations in caffe2/contrib/aten/gen_op,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717502

",pytorch
49946,r-barnes,pr,2020-12-29T21:04:42Z,Clean up some type annotations in benchmarks/fastrnns,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717510

",pytorch
49950,vfdev-5,pr,2020-12-29T21:42:42Z,Update MultiHeadAttention docstring,"Fixes MultiHeadAttention docstring.

Currently, https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention
is 

<img width=""648"" alt=""Screen Shot 2020-12-29 at 21 06 43"" src=""https://user-images.githubusercontent.com/2459423/103311124-cd10cc00-4a19-11eb-89c9-0ee261364963.png"">


and with the fix will be

<img width=""648"" alt=""Screen Shot 2020-12-29 at 22 41 35"" src=""https://user-images.githubusercontent.com/2459423/103315838-0dc31200-4a27-11eb-82e2-ca8f13d713a1.png"">

",pytorch
49953,r-barnes,pr,2020-12-30T00:15:53Z,Drop unused imports from leftovers,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727348

",pytorch
49955,r-barnes,pr,2020-12-30T00:17:50Z,Drop unused imports from benchmarks,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727355

",pytorch
49956,r-barnes,pr,2020-12-30T00:18:18Z,Drop unused imports from scripts,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727347

",pytorch
49957,r-barnes,pr,2020-12-30T01:30:37Z,Clean up type annotations in caffe2/torch/nn/modules,"Test Plan: Sandcastle

Differential Revision: D25729745

",pytorch
49972,r-barnes,pr,2020-12-30T17:39:30Z,Drop unused imports,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727352

",pytorch
49973,r-barnes,pr,2020-12-30T17:41:54Z,Drop unused imports from test,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727350

",pytorch
49974,r-barnes,pr,2020-12-30T17:44:04Z,Drop unused imports,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727358

",pytorch
49976,r-barnes,pr,2020-12-30T17:57:43Z,[Don't review] Clean up some type annotations in caffe2/torch,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717501

",pytorch
49980,r-barnes,pr,2020-12-30T19:43:08Z,Drop unused imports from caffe2/python,"Summary:
From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Test Plan: Standard sandcastle tests

Differential Revision: D25727359

",pytorch
49983,rohan-varma,pr,2020-12-31T00:39:43Z,[RPC] Relax some profiling tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49983 [RPC] Relax some profiling tests**

We have observed very rare flakiness in some profiling tests recently,
i.e.: https://github.com/pytorch/pytorch/issues/49635, https://github.com/pytorch/pytorch/issues/49634, https://github.com/pytorch/pytorch/issues/49633. Interestingly, they all seem to have failed on the same job: `pytorch_linux_xenial_cuda10_2_cudnn7_py3_multigpu_test`. However, we were not able to reproduce these even with thousands of
runs on the CI machines where the failure was originally reported. As a result,
relaxing these tests and re-enabling them to reduce failure rates.

Differential Revision: [D25739416](https://our.internmc.facebook.com/intern/diff/D25739416/)",pytorch
50057,zheng-xq,pr,2021-01-04T19:51:36Z,Add full reduction benchmark.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50193 Add CPP Full Reduction Benchmarks.
* **#50057 Add full reduction benchmark.**

As part of the effort to calibrate TE reduction performance, adding a full reduction benchmark.
Also add a ""skip_input_transformation"" option.
Fixed other reduction benchmarks to accept specific benchmarks that was listed.

Test plans:
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce_full
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce_full_fwd_cpu_16777216_s1
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce_full_fwd_cpu_16777216_s0
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce2d_inner
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce2d_inner_fwd_cpu_640_524288
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce2d_outer
* python -m benchmarks.tensorexpr --device=cpu --mode=fwd reduce2d_outer_fwd_cpu_640_524288

Differential Revision: [D25774138](https://our.internmc.facebook.com/intern/diff/D25774138)",pytorch
50077,r-barnes,pr,2021-01-04T23:41:22Z,Clean up some type annotations in caffe2/python,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717530

",pytorch
50078,r-barnes,pr,2021-01-04T23:41:40Z,Clean up some type annotations in torch/testing/_internal,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717560

",pytorch
50079,r-barnes,pr,2021-01-04T23:46:59Z,[Don't review] Clean up type annotations in caffe2/torch/nn,"Differential Revision: D25718694

",pytorch
50084,ngimel,pr,2021-01-05T06:18:21Z,fixes indices computation for trilinear interpolate backwards,"#48675 had some typos in indices computations so that results for trilinear interpolation where height is not equal to width were wrong. This PR fixes it. 
cc @xwang233 
",pytorch
50106,r-barnes,pr,2021-01-05T21:28:33Z,Clean up simple type annotations in nn/functional.py,"Differential Revision: D25787566

",pytorch
50131,rohan-varma,pr,2021-01-06T07:46:55Z,Address clang-tidy warning for push_back in ProcessGroupNCCL,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50180 [WIP] Use side-stream in CPU to GPU copies in DDP
* **#50131 Address clang-tidy warning for push_back in ProcessGroupNCCL**

Noticed that in the internal diff for
https://github.com/pytorch/pytorch/pull/49069 there was a clang-tidy warning to
use emplace instead of push_back. In this case, there should be a small perf win because we directly construct the object inside the container, instead of constructing a temporary and moving it into the container. 

Differential Revision: [D25800134](https://our.internmc.facebook.com/intern/diff/D25800134/)",pytorch
50133,rohan-varma,pr,2021-01-06T08:23:38Z,Warn user once for possible unnecessary find_unused_params in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50133 Warn user once for possible unnecessary find_unused_params in DDP**

`find_unused_parameters=True` is only needed when the model has unused parameters that are not known at model definition time or differ due to control flow.

Unfortunately, many DDP users pass this flag in as `True` even when they do not need it, sometimes as a precaution to mitigate possible errors that may be raised (such as the error we raise with not using all outputs).While this is a larger issue to be fixed in DDP, it would also be useful to warn once if we did not detect unused parameters.

The downside of this is that in the case of flow control models where the first iteration doesn't have unused params but the rest do, this would be a false warning. However, I think the warning's value exceeds this downside, and we have also indicated this possibility in the warning itself.

Differential Revision: [D25411118](https://our.internmc.facebook.com/intern/diff/D25411118/)",pytorch
50154,flx42,pr,2021-01-06T19:41:33Z,docker: add environment variable PYTORCH_VERSION,"The aim is being able to inspect a container image and determine immediately
which version of pytorch it contains.

Closes #48324

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>


@seemethere PTAL.
As you requested in https://github.com/pytorch/pytorch/issues/48324#issuecomment-754237156, I'm submitting the patch. But I could only do limited testing as I'm not sure these Makefile/Dockerfile are used for pushing the Docker Hub images (since the Makefile tags the image with a `v` prefix for the version, as in: `pytorch:v1.7.1`, but Docker Hub images don't have this prefix). 

Also on the master branch we currently have the following:
```
$ git describe --tags
v1.4.0a0-11171-g68a6e46379
```
So it's a little off, but it behaves as expected on the `release/1.7` branch.",pytorch
50156,r-barnes,pr,2021-01-06T20:04:22Z,[Don't review] Clean up some type annotations in test/jit/...../test_class_type.py,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25720035

",pytorch
50158,r-barnes,pr,2021-01-06T20:06:49Z,[Don't review] Clean up some type annotations in test/jit,"Summary: Upgrades type annotations from Python2 to Python3

Test Plan: Sandcastle tests

Differential Revision: D25717504

",pytorch
50180,rohan-varma,pr,2021-01-07T01:06:08Z,Use side-stream in CPU to GPU copies in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50180 [WIP] Use side-stream in CPU to GPU copies in DDP**

Resolves the regression in
https://github.com/pytorch/pytorch/issues/49819 by adding copy over background
stream similar to scatter. We gate this with an env var which is by default true, but off for internal needs. 

Differential Revision: [D25818170](https://our.internmc.facebook.com/intern/diff/D25818170/)",pytorch
50193,zheng-xq,pr,2021-01-07T07:00:44Z,Add CPP Full Reduction Benchmarks.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50193 Add CPP Full Reduction Benchmarks.**
* #50057 Add full reduction benchmark.

* Supports aten, native reference implementation, and NNC TE implementations.
* Support functionality checks against aten, in addition to performance checks.

Test plans:

* After enable ""BUILD_TENSOREXPR_BENCHMARK"" in CMakeLists.txt,
* bin/tensorexpr_bench --benchmark_filter=Reduce1D

Measurements:

On a Broadwell E5-2686 CPU,

Reduce1D/Torch/16777216            5638547 ns    5638444 ns        119 BYTES=11.902G/s
Reduce1D/Naive/16777216           19308235 ns   19308184 ns         36 BYTES=3.47567G/s
Reduce1D/NativeRfactor/16777216    8433348 ns    8433038 ns         85 BYTES=7.95785G/s
Reduce1D/NativeVector/16777216     5608836 ns    5608727 ns        124 BYTES=11.9651G/s
Reduce1D/NativeTiled/16777216      5550233 ns    5550221 ns        126 BYTES=12.0912G/s
Reduce1D/TeNaive/16777216         21451047 ns   21450752 ns         33 BYTES=3.12851G/s
Reduce1D/TeSplitTail/16777216     23701732 ns   23701229 ns         30 BYTES=2.83145G/s
Reduce1D/TeSplitMask/16777216     23683589 ns   23682978 ns         30 BYTES=2.83363G/s
Reduce1D/TeRfactorV2/16777216      5378019 ns    5377909 ns        131 BYTES=12.4786G/s

Result summary:

* The single-threaded performance with NNC TeRfactorV2 matches and exceeds Aten and avx2 naive counterpart.

Follow-up items:

* rfactor does not work well with split
* We don't have a multi-threaded implementation yet.
  * Missing ""parallel"" scheduling primitive, which is not different from what we need for pointwise ops.

Differential Revision: [D25821880](https://our.internmc.facebook.com/intern/diff/D25821880)",pytorch
50215,r-barnes,pr,2021-01-07T18:13:15Z,Drop some Python2 artefacts,"Differential Revision: D25810368

",pytorch
50293,r-barnes,pr,2021-01-08T22:11:41Z,Type annotations in test/jit,"Summary: Switching to type annotations for improved safety and import tracking.

Differential Revision: D25853949

",pytorch
50302,neerajprad,pr,2021-01-09T00:07:41Z,Raise warning during validation when arg_constraints not defined,"After we merged https://github.com/pytorch/pytorch/pull/48743, we noticed that some existing code that subclasses `torch.Distribution` started throwing `NotImplemenedError` since the constraints required for validation checks were not implemented.

```sh
File ""torch/distributions/distribution.py"", line 40, in __init__
  for param, constraint in self.arg_constraints.items():
File ""torch/distributions/distribution.py"", line 92, in arg_constraints
  raise NotImplementedError
```

This PR throws a UserWarning for such cases instead and gives a better warning message.

cc. @balandat",pytorch
50320,peterjc123,pr,2021-01-09T06:48:58Z,Add terminate handler for uncaught exceptions,"Addresses https://github.com/pytorch/pytorch/issues/50051
",pytorch
50368,vfdev-5,pr,2021-01-11T10:54:50Z,Introduced AliasInfo for OpInfo,"Introduced AliasInfo for OpInfo.

Context: Split of #49158

cc @mruberry , please let me know if you'd like to see here more code to cover 

> [ ] fold test_op_aliases.py into OpInfo-based testing in test_ops.py

from https://github.com/pytorch/pytorch/issues/50006

and/or add `UnaryUfuncInfo('abs')` as discussed https://github.com/pytorch/pytorch/pull/49158/files#r548774221",pytorch
50370,vfdev-5,pr,2021-01-11T11:44:35Z,Introduced operator variant to OpInfo,"Introduced operator variant to OpInfo

Context: Split of #49158

cc @mruberry",pytorch
50400,nSircombe,pr,2021-01-11T21:45:38Z,Enables build with oneDNN (MKL-DNN) on AArch64,"Since version 1.6, oneDNN has provided limited support for AArch64 builds.

This minor change is to detect an AArch64 CPU and permit the use of
`USE_MKLDNN` in that case.

Build flags for oneDNN are also modified accordingly.

Note: oneDNN on AArch64, by default, will use oneDNN's reference C++ kernels.
These are not optimised for AArch64, but oneDNN v1.7 onwards provides support
for a limited set of primitives based Arm Compute Library.
See: https://github.com/oneapi-src/oneDNN/pull/795
and: https://github.com/oneapi-src/oneDNN/pull/820
for more details. Support for ACL-based oneDNN primitives in PyTorch
will require some further modification,

Fixes #{issue number}
",pytorch
50418,supriyar,pr,2021-01-12T03:16:48Z,[quant] update embedding module to not store qweight,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50418 [quant] update embedding module to not store qweight**

Summary:
previously we were storing the quantized weight as a module attribute, which
was resulting in the weight getting stored as part of the model and getting stored twice (once as qweight and other as unpacked_weight)
We don't need this since we already store the unpacked weights as part of the model.

Test Plan:
Before
```
Archive:  tmp.pt
 Length   Method    Size  Cmpr    Date    Time   CRC-32   Name
--------  ------  ------- ---- ---------- ----- --------  ----
     586  Stored      586   0% 00-00-1980 00:00 5fefdda0  tmp/extra/producer_info.json
 1588700  Stored  1588700   0% 00-00-1980 00:00 04e0da4c  tmp/data/0
   63548  Stored    63548   0% 00-00-1980 00:00 0ceb1f45  tmp/data/1
   63548  Stored    63548   0% 00-00-1980 00:00 517bc3ab  tmp/data/2
 1588700  Stored  1588700   0% 00-00-1980 00:00 dbe88c73  tmp/data/3
   63548  Stored    63548   0% 00-00-1980 00:00 d8dc47c4  tmp/data/4
   63548  Stored    63548   0% 00-00-1980 00:00 b9e0c20f  tmp/data/5
    1071  Stored     1071   0% 00-00-1980 00:00 10dc9350  tmp/data.pkl
     327  Defl:N      203  38% 00-00-1980 00:00 dfddb661  tmp/code/__torch__/___torch_mangle_0.py
     185  Stored      185   0% 00-00-1980 00:00 308f580b  tmp/code/__torch__/___torch_mangle_0.py.debug_pkl
    1730  Defl:N      515  70% 00-00-1980 00:00 aa11f799  tmp/code/__torch__/torch/nn/quantized/modules/embedding_ops.py
    1468  Defl:N      636  57% 00-00-1980 00:00 779609a6  tmp/code/__torch__/torch/nn/quantized/modules/embedding_ops.py.debug_pkl
       0  Stored        0   0% 00-00-1980 00:00 00000000  tmp/code/__torch__/torch/classes/quantized.py
       6  Stored        6   0% 00-00-1980 00:00 816d0907  tmp/code/__torch__/torch/classes/quantized.py.debug_pkl
       4  Stored        4   0% 00-00-1980 00:00 57092f6d  tmp/constants.pkl
       2  Stored        2   0% 00-00-1980 00:00 55679ed1  tmp/version
--------          -------  ---                            -------
 3436971          3434800   0%                            16 files
```
After
```
Archive:  tmp.pt
 Length   Method    Size  Cmpr    Date    Time   CRC-32   Name
--------  ------  ------- ---- ---------- ----- --------  ----
 1588700  Stored  1588700   0% 00-00-1980 00:00 a4da6981  tmp/data/0
   63548  Stored    63548   0% 00-00-1980 00:00 74d9b607  tmp/data/1
   63548  Stored    63548   0% 00-00-1980 00:00 e346a0c2  tmp/data/2
     952  Stored      952   0% 00-00-1980 00:00 eff8706e  tmp/data.pkl
     375  Defl:N      227  40% 00-00-1980 00:00 96c77b68  tmp/code/__torch__/quantization/test_quantize/___torch_mangle_23.py
     228  Defl:N      162  29% 00-00-1980 00:00 6a378113  tmp/code/__torch__/quantization/test_quantize/___torch_mangle_23.py.debug_pkl
    1711  Defl:N      509  70% 00-00-1980 00:00 66d8fd61  tmp/code/__torch__/torch/nn/quantized/modules/embedding_ops.py
    1473  Defl:N      634  57% 00-00-1980 00:00 beb2323b  tmp/code/__torch__/torch/nn/quantized/modules/embedding_ops.py.debug_pkl
       0  Stored        0   0% 00-00-1980 00:00 00000000  tmp/code/__torch__/torch/classes/quantized.py
       6  Stored        6   0% 00-00-1980 00:00 816d0907  tmp/code/__torch__/torch/classes/quantized.py.debug_pkl
       4  Stored        4   0% 00-00-1980 00:00 57092f6d  tmp/constants.pkl
       2  Stored        2   0% 00-00-1980 00:00 55679ed1  tmp/version
--------          -------  ---                            -------
 1720547          1718292   0%                            12 files
```
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D25879879](https://our.internmc.facebook.com/intern/diff/D25879879)",pytorch
50445,rohan-varma,pr,2021-01-12T20:44:07Z,Add check to ensure CUDA event is created when synchronizing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50445 Add check to ensure CUDA event is created when synchronizing**

We have been seeing some silent correctness issues that only surface
later (i.e. after a device synchronize), and one potential source of this could be issues in stream
synchronization.

As per https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/CUDAEvent.h#L123, `cudaStreamWaitEvent` is only called if the event has been successfully created. It should indeed be created in the `WorkNCCL` constructor: https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp#L251, but just adding this check to catch any synchronization issues that may occur and raise a better error message.

Differential Revision: [D25889723](https://our.internmc.facebook.com/intern/diff/D25889723/)",pytorch
50477,r-barnes,pr,2021-01-13T16:33:28Z,Remove a blacklist reference,"Summary: See task for context

Differential Revision: D25893906

",pytorch
50478,r-barnes,pr,2021-01-13T16:35:07Z,Remove instance of blacklist,"Summary: See task for context

Differential Revision: D25893912

",pytorch
50479,r-barnes,pr,2021-01-13T16:36:09Z,"Remove ""blacklist"" from nvmifier","Summary: See task for context

Differential Revision: D25893971

",pytorch
50480,r-barnes,pr,2021-01-13T16:42:43Z,Drop blacklist from glow,"Differential Revision: D25893858

",pytorch
50482,r-barnes,pr,2021-01-13T18:11:12Z,"Fix warnings in ""ForeachOpsKernels""","Summary:
Compiling currently shows:
```
Jan 13 16:46:28 In file included from ../aten/src/ATen/native/ForeachOpsKernels.cpp:2:
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:28:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:44:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:149:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:164:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:183:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:198:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:150:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:74:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:150:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:84:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:151:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:74:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:151:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:84:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:158:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:158:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:159:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:159:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:160:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:160:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:161:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:161:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:163:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:53:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:163:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:63:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:164:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:53:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:164:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:63:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:195:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:115:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:195:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:125:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:196:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:115:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:196:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:125:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:198:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:135:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:198:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:145:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:199:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:135:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:199:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:145:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {
```
this diff fixes that

Differential Revision: D25901744

",pytorch
50484,r-barnes,pr,2021-01-13T18:14:42Z,Fix loop type,"Summary:
I currently see the compilation warning:
```
Jan 13 16:46:21 [3644/5223] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/core/ivalue.cpp.o
Jan 13 16:46:21 ../aten/src/ATen/core/ivalue.cpp:855:22: warning: comparison of integers of different signs: 'int' and 'std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >::size_type' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:21   for (auto i = 0; i < slots_.size(); ++i) {
```
This diff fixes that

Differential Revision: D25901674

",pytorch
50485,r-barnes,pr,2021-01-13T18:14:45Z,"Fix warnings in ""ForeachUtils.h""","Summary:
Compiling currently raises the warnings:
```
Jan 13 16:46:28 In file included from ../aten/src/ATen/native/ForeachOpsKernels.cpp:2:
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:28:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:44:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:149:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:164:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:183:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:198:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
```
This diff fixes that

Differential Revision: D25901713

",pytorch
50486,r-barnes,pr,2021-01-13T18:14:55Z,Fix warnings in TensorShape,"Summary:
Compiling currently gives:
```
an 13 16:46:39 In file included from ../aten/src/ATen/native/TensorShape.cpp:12:
Jan 13 16:46:39 ../aten/src/ATen/native/Resize.h:37:24: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     if (new_size_bytes > self->storage().nbytes()) {
Jan 13 16:46:39         ~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:32:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long long') [-Wsign-compare]
Jan 13 16:46:39   for (size_t i = 0; i < shape_tensor.numel(); ++i) {
Jan 13 16:46:39                      ~ ^ ~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:122:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < tensors.size(); i++) {
Jan 13 16:46:39                       ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:162:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int i = 0; i < tensors.size(); i++) {
Jan 13 16:46:39                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:300:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < s1.size(); ++i) {
Jan 13 16:46:39                       ~ ^ ~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:807:21: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     TORCH_CHECK(dim < self_sizes.size());
Jan 13 16:46:39                 ~~~ ^ ~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../c10/util/Exception.h:361:31: note: expanded from macro 'TORCH_CHECK'
Jan 13 16:46:39   if (C10_UNLIKELY_OR_CONST(!(cond))) {                                 \
Jan 13 16:46:39                               ^~~~
Jan 13 16:46:39 ../c10/util/Exception.h:244:47: note: expanded from macro 'C10_UNLIKELY_OR_CONST'
Jan 13 16:46:39 #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
Jan 13 16:46:39                                               ^
Jan 13 16:46:39 ../c10/macros/Macros.h:173:65: note: expanded from macro 'C10_UNLIKELY'
Jan 13 16:46:39 #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))
Jan 13 16:46:39                                                                 ^~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:855:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const int64_t' (aka 'const long long') [-Wsign-compare]
Jan 13 16:46:39   for (size_t i = 0; i < num_blocks; ++i) {
Jan 13 16:46:39                      ~ ^ ~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2055:23: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     for (int i = 0; i < vec.size(); i++) {
Jan 13 16:46:39                     ~ ^ ~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2100:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < src.size(); ++i) {
```
This fixes issues with loop iteration variable types

Differential Revision: D25901799

",pytorch
50492,fritzo,pr,2021-01-13T18:46:53Z,Validate args in HalfCauchy and HalfNormal,"Fixes #50404
Complementary to #50403

This also fixes `HalfCauchy.cdf()`, `HalfNormal.log_prob()`, `HalfNormal.cdf()` and ensures validation is not done twice.

cc @feynmanliang",pytorch
50493,r-barnes,pr,2021-01-13T19:00:41Z,Drop unused imports from caffe2/quantization (#49974),"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49974

From
```
./python/libcst/libcst codemod remove_unused_imports.RemoveUnusedImportsWithGlean --no-format caffe2/
```

Differential Revision: D25902417

",pytorch
50495,r-barnes,pr,2021-01-13T19:29:23Z,Generalize `sum_intlist`,"Differential Revision: D25902853

",pytorch
50498,rohan-varma,pr,2021-01-13T19:54:29Z,[RPC] Support timeout in rref._get_type(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50499 [RPC] Support timeout for RRef proxy functions
* **#50498 [RPC] Support timeout in rref._get_type()**

This change is mostly needed for the next diff in this stack, where
rref._get_type() is called in the rpc_async/rpc_sync RRef proxy function and
can block indefinitely if there is no timeout. It will also be useful to have a
timeout argument when we publicize this API to keep it consistent with other
RPC APIs.

Differential Revision: [D25897588](https://our.internmc.facebook.com/intern/diff/D25897588/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D25897588/)!",pytorch
50499,rohan-varma,pr,2021-01-13T19:54:39Z,[RPC] Support timeout for RRef proxy functions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50499 [RPC] Support timeout for RRef proxy functions**
* #50498 [RPC] Support timeout in rref._get_type()

Adds a timeout API to the following functions:
```
rref.rpc_sync()
rref.rpc_async()
rref.remote()
```
so that RPCs initiated by these proxy calls can be appropriately timed out similar to the regular RPC APIs. Timeouts are supported in the following use cases:

1. rpc.remote finishes in time and successfully, but function run by rref.rpc_async() is slow and times out. Timeout error will be raised
2. rref.rpc_async() function is fast, but rpc.remote() is slow/hanging. Then when rref.rpc_async() is called, it will still timeout with the passed in timeout (and won't block for the rpc.remote() to succeed, which is what happens currently). Although, the timeout will occur during the future creation itself (and not the wait) since it calls `rref._get_type` which blocks. We can consider making this nonblocking by modifying rref._get_type to return a future, although that is likely a larger change.

Differential Revision: [D25897495](https://our.internmc.facebook.com/intern/diff/D25897495/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D25897495/)!",pytorch
50526,r-barnes,pr,2021-01-14T06:05:54Z,Make `_s_where` warning much more useful,"Summary: It took me several hours to figure out why/how the error being modified arose. This is because the LOC the error traces back to is not necessarily the lines where the problem occurs. This error message should prevent others from suffering similarly.

Test Plan: Sandcastle tests

Differential Revision: D25908738

",pytorch
50541,r-barnes,pr,2021-01-14T17:07:15Z,Minor doc improvement(?) on ArrayRef::slice,"Summary: I found the current phrasing to be confusing

Test Plan: N/A

Reviewed By: ngimel

Differential Revision: D25909205

",pytorch
50547,fritzo,pr,2021-01-14T18:06:53Z,Independent constraint,"Addresses #50496 

This fixes a number of inconsistencies in torch.distributions.constraints as used for parameters and supports of probability distributions.
- Adds a `constraints.independent` and replaces `real_vector` with `independent(real, 1)`. (this pattern has long been used in Pyro)
- Adds an `.event_dim` attribute to all constraints.
- Tests that `constraint.check(data)` has the correct shape. (Previously the shapes were incorrect).
- Adds machinery to set static `.is_discrete` and `.event_dim` for `constraints.dependent`.
- Fixes constraints for a number of distributions.

## Tested
- added a new check to the constraints tests
- added a new check for `.event_dim`

cc @fehiepsi @feynmanliang @stefanwebb",pytorch
50581,fritzo,pr,2021-01-15T14:53:50Z,Fix TransformedDistribution shaping logic,"Fixes #50496
Fixes #34859
Fixes #21596

This fixes many bugs involving `TransformedDistribution` and `ComposeTransform` when the component transforms changed their event shapes. Part of the fix is to introduce an `IndependentTransform` analogous to `distributions.Independent` and `constraints.independent`, and to introduce methods `Transform.forward_shape()` and `.inverse_shape()`. I have followed @fehiepsi's suggestion and replaced `.input_event_dim` -> `.domain.event_dim` and `.output_event_dim` -> `.codomain.event_dim`. This allows us to deprecate `.event_dim` as an attribute.

## Summary of changes

- Fixes `TransformDistribution` and `ComposeTransform` shape errors.
- Fixes a behavior bug in `LogisticNormal`.
- Fixes `kl_divergence(TransformedDistribution, TransformedDistribution)`
- Adds methods `Transform.forward_shape()`, `.inverse_shape()` which are required for correct shape computations in `TransformedDistribution` and `ComposeTransform`.
- Adds an `IndependentTransform`.
- Adds a `ReshapeTransform` which is invaluable in testing shape logic in `ComposeTransform` and `TransformedDistribution` and which will be used by @stefanwebb flowtorch.
- Fixes incorrect default values in `constraints.dependent.event_dim`.
- Documents the `.event_dim` and `.is_discrete` attributes.

## Changes planned for follow-up PRs

- Memoize `@constraints.dependent_property` as we do with `@lazy_property`, since we now consult those properties much more often.

## Tested
- [x] added a test for `Dist.support` vs `Dist(**params).support` to ensure static and dynamic attributes agree.
- [x] refactoring is covered by existing tests
- [x] add test cases for `ReshapedTransform`
- [x] add a test for `TransformedDistribution` on a wide grid of input shapes
- [x] added a regression test for #34859

cc @fehiepsi @feynmanliang @stefanwebb",pytorch
50625,rohan-varma,pr,2021-01-15T22:57:30Z,[Collective APIs] Make python object collective API args consistent,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51341 [1.8] Add additional tests for object-based APIs
* **#50625 [Collective APIs] Make python object collective API args consistent**

Make API signatures consistent and provide default argument similar to
the tensor collectives.

Differential Revision: [D25932012](https://our.internmc.facebook.com/intern/diff/D25932012/)",pytorch
50659,ngimel,pr,2021-01-17T04:17:11Z,fix bn channels_last contiguity check,"Fixes #42588 
The contiguity check used to be for memory format suggested by `grad_output->suggest_memory_format()`, but an invariant guaranteed by derivatives.yaml is `input->suggest_memory_format()`
",pytorch
50660,peterjc123,pr,2021-01-17T04:23:39Z,Better split of the windows test jobs,"See discussion in https://github.com/pytorch/pytorch/pull/50320#discussion_r554447365.
",pytorch
50706,supriyar,pr,2021-01-19T00:38:09Z,[quant] Add non-fbgemm fallback implementation for embedding lookup ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50706 [quant] Add non-fbgemm fallback implementation for embedding lookup ops**

Summary:
Add a default CPU implementation for quantized embedding lookup operators.
This should enable the ops to execute on mobile as well where we don't have fbgemm.

Test Plan:
python test/test_quantization.py
and CI tests

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D25956842](https://our.internmc.facebook.com/intern/diff/D25956842)",pytorch
50783,r-barnes,pr,2021-01-20T02:10:29Z,"Fix warnings in ""ForeachOpsKernels"" with c10::irange","Summary:
Compiling currently shows:
```
Jan 13 16:46:28 In file included from ../aten/src/ATen/native/ForeachOpsKernels.cpp:2:
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:28:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:44:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:149:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:164:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:183:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachUtils.h:198:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28   for (int64_t i = 0; i < tensors1.size(); i++) {
Jan 13 16:46:28                       ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:150:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:74:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:150:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:84:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:151:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:74:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:151:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST_ALPHA(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:84:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST_ALPHA'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:158:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:158:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(add);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:159:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:159:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(sub);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:160:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:160:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:161:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:31:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:161:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_SCALARLIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:40:21: note: expanded from macro 'FOREACH_BINARY_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < tensors.size(); i++) {                                                                            \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:163:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:53:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:163:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(mul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:63:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:164:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:53:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:164:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_BINARY_OP_LIST(div);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:63:21: note: expanded from macro 'FOREACH_BINARY_OP_LIST'
Jan 13 16:46:28   for (int i = 0; i < tensors1.size(); i++) {                                                             \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:195:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:115:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:195:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:125:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:196:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:115:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:196:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALAR(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:125:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALAR'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                           \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:198:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:135:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:198:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcdiv);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:145:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:199:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:135:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {                                                                                                              \
Jan 13 16:46:28                   ~ ^ ~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:199:1: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:28 FOREACH_POINTWISE_OP_SCALARLIST(addcmul);
Jan 13 16:46:28 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:28 ../aten/src/ATen/native/ForeachOpsKernels.cpp:145:21: note: expanded from macro 'FOREACH_POINTWISE_OP_SCALARLIST'
Jan 13 16:46:28   for (int i = 0; i < input.size(); i++) {
```
this diff fixes that

Test Plan: Sandcastle tests

Differential Revision: D25935046

",pytorch
50784,r-barnes,pr,2021-01-20T02:16:24Z,Use c10::irange for great good,"Differential Revision: D25934820

",pytorch
50797,r-barnes,pr,2021-01-20T07:09:20Z,Modernize for-loops,"Summary:
Modernize for-loops throughout caffe2/ subdirs to use ranged-loops where possible (all `.cpp` files were examined).

```
find caffe2/ -iname ""*.cpp"" > /home/rbarnes/files
buck run mode/opt foundation/clangr:clangr_local -- -j 10 --file=/home/rbarnes/files --multi --apply-replacements=true tidy '--checks=-*,modernize-loop-convert'
```

Differential Revision: D25969148

",pytorch
50851,rohan-varma,pr,2021-01-20T23:14:39Z,[WIP][Grad Compression] Unittest to verify allreduce_hook parity,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50851 [WIP][Grad Compression] Unittest to verify allreduce_hook parity**

Improves upon the previous unittest to ensure allreduce_hook results in the same gradients as vanilla allreduce in DDP.

Differential Revision: [D25963654](https://our.internmc.facebook.com/intern/diff/D25963654/)",pytorch
50861,rohan-varma,pr,2021-01-21T00:28:35Z,Add note about TCP init in RPC tests to contributing doc.,"We added this option in https://github.com/pytorch/pytorch/pull/48248, but it would be good to document it somewhere as well, hence adding it to this contributing doc.",pytorch
50899,r-barnes,pr,2021-01-21T20:17:32Z,Modernize for-loops,"Differential Revision: D26001931

",pytorch
50912,r-barnes,pr,2021-01-21T20:44:18Z,Modernize for-loops,"Differential Revision: D26001948

",pytorch
50977,rohan-varma,pr,2021-01-23T03:09:00Z,[RPC] Add option to make rref.get_type not block.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50983 [JIT/Futures] Support set_exception API on python future.
* #50979 [torch.futures] Fix doc inconsistency about callback args
* #50978 [torch.futures] Clarify callback behavior when future is completed
* **#50977 [RPC] Add option to make rref.get_type not block.**

Adds a `blocking` flag that can be set to False to make this API return a `Future` to the type. This is to make this function non-blocking, mostly for a future change that will allow `rref.rpc_async()` to be completely non-blocking (it currently calls and waits for this function that issues an RPC in-line). 

Differential Revision: [D25944582](https://our.internmc.facebook.com/intern/diff/D25944582/)",pytorch
50978,rohan-varma,pr,2021-01-23T03:09:14Z,[torch.futures] Clarify callback behavior when future is completed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50983 [JIT/Futures] Support set_exception API on python future.
* #50979 [torch.futures] Fix doc inconsistency about callback args
* **#50978 [torch.futures] Clarify callback behavior when future is completed**
* #50977 [RPC] Add option to make rref.get_type not block.

Noticed that the documentation is not clear that the cbs are invoked
inline if the future is already completed. We should probably document this
behavior.

Differential Revision: [D25944636](https://our.internmc.facebook.com/intern/diff/D25944636/)",pytorch
50979,rohan-varma,pr,2021-01-23T03:09:29Z,[torch.futures] Fix doc inconsistency about callback args,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50983 [JIT/Futures] Support set_exception API on python future.
* **#50979 [torch.futures] Fix doc inconsistency about callback args**
* #50978 [torch.futures] Clarify callback behavior when future is completed
* #50977 [RPC] Add option to make rref.get_type not block.

Noticed that the documentation is inconsisntent about the arg needed
in the callback. It appears to require the future, so fix this in the docs.

Differential Revision: [D25944637](https://our.internmc.facebook.com/intern/diff/D25944637/)",pytorch
50983,rohan-varma,pr,2021-01-23T06:55:19Z,[JIT/Futures] Support set_exception API on python future.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50983 [JIT/Futures] Support set_exception API on python future.**
* #50979 [torch.futures] Fix doc inconsistency about callback args
* #50978 [torch.futures] Clarify callback behavior when future is completed
* #50977 [RPC] Add option to make rref.get_type not block.

There is currently no way to handle/propagate errors with the python-based futures API (they are raised correctly if set with an error, but this is only possible from C++).

This diff allows a `set_exception` API to allow this. This is done by setting `unwrap_func` to be a python function that raises an exception. This is mostly to support a future change that will enable `rref.rpc_async()` to be non-blocking (need this for error handling). 

Differential Revision: [D25950304](https://our.internmc.facebook.com/intern/diff/D25950304/)",pytorch
51053,neerajprad,pr,2021-01-25T20:52:03Z,Fix docstring to clarify logits usage for multiclass case,"Fixes #50378.

Additionally, this has some minor fixes:
 - [x] Fix mean for half-cauchy to return `inf` instead of `nan`.
 - [x] Fix constraints/support for the relaxed categorical distribution.",pytorch
51056,rohan-varma,pr,2021-01-25T22:13:30Z,ci for https://github.com/pytorch/pytorch/issues/45067,"Fixes #{issue number}
",pytorch
51064,rohan-varma,pr,2021-01-26T00:07:09Z,ci-all for https://github.com/pytorch/pytorch/issues/45067,"Fixes #{issue number}
",pytorch
51086,supriyar,pr,2021-01-26T04:28:43Z,[quant][fx] Scope support for call_function in QuantizationTracer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51259 [quant][fx] Update name of packed weight attributes
* #51171 [quant][fx] Make scale, zero_point buffers in the model, use FQN (for quantize_per_tensor ops)
* #51166 [quant][fx] Make scale, zero_point buffers in the model and use FQN (for quantized ops)
* **#51086 [quant][fx] Scope support for call_function in QuantizationTracer**

Summary:
Previously we only supported getting scope for call_module and custom qconfig dict for call_module.
This PR extends the Scope class to record the scope for all node types.
For call_function qconfig if module_name is specified it takes precedence over function qconfig.

Test Plan:
python test/test_quantization.py test_qconfig_for_call_func

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26077602](https://our.internmc.facebook.com/intern/diff/D26077602)",pytorch
51087,supriyar,pr,2021-01-26T04:28:53Z,[quant][fx] Make packed_weight a module buffer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51087 [quant][fx] Make packed_weight a module buffer**
* #51086 [quant][fx] Extend scope to support all node type

Summary:
This enables the packed weights to be stored as getattr nodes in the graph
And also allows for packed_weight to show up in the model state_dict of fx quantized model.

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_attrs_are_buffers

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26077601](https://our.internmc.facebook.com/intern/diff/D26077601)",pytorch
51095,rohan-varma,pr,2021-01-26T06:51:12Z,Fix benchmarks/distributed/ddp/benchmark.py,"Fixes the issue reported in https://github.com/pytorch/pytorch/issues/50679 by using built-in object-based collectives. User has verified this patch works

Test with:
RANK=0 python3 pytorch-dist-benchmark.py --world-size 2 --master-addr 127.0.0.1 --master-port 23456
RANK=1 python3 pytorch-dist-benchmark.py --world-size 2 --master-addr 127.0.0.1 --master-port 23456",pytorch
51146,r-barnes,pr,2021-01-26T22:16:43Z,Use `sum_intlist` and `prod_intlist`,"Differential Revision: D25903430

",pytorch
51166,supriyar,pr,2021-01-27T00:42:48Z,"[quant][fx] Make scale, zero_point buffers in the model and use FQN (for quantized ops)","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51259 [quant][fx] Update name of packed weight attributes
* #51171 [quant][fx] Make scale, zero_point buffers in the model, use FQN (for quantize_per_tensor ops)
* **#51166 [quant][fx] Make scale, zero_point buffers in the model and use FQN (for quantized ops)**
* #51086 [quant][fx] Scope support for call_function in QuantizationTracer

Summary:

Currently scale and zero_point values are stored as constant values in the graph.
This prevents these values from being updated in the graph and also does not enable saving
these values to state_dict

After this PR we store scale/zero_point values for quantized ops as buffers in the root module
and create get_attr nodes for them in the graph.

NOTE: The scale and zero_point values will now be stored as part of the model state_dict of the FX quantized model.
Before this PR, we were not storing any qparams in state_dict so saving/loading a quantized fx model didn't work.

We also use the FQN of the module where the quantized ops are present to name these attributes so
that they can be uniquely  identified and mapped to quantized ops.

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_qparams_buffers

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26092965](https://our.internmc.facebook.com/intern/diff/D26092965)",pytorch
51167,vfdev-5,pr,2021-01-27T00:48:21Z,OpInfo: Added clamp and trunc tests with aliases,"Description:
- Added clamp, trunc tests with aliases
- Added tests for aliases for asin(h), acos(h), etc
- fixed 'fix' alias implementation
- fixed annotations in test_jit_alias_remapping
- updated native_functions.yaml aliases guidelines

Blocked by #50368

cc @mruberry ",pytorch
51171,supriyar,pr,2021-01-27T01:41:15Z,"[quant][fx] Make scale, zero_point buffers in the model, use FQN (for quantize_per_tensor ops)","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51259 [quant][fx] Update name of packed weight attributes
* **#51171 [quant][fx] Make scale, zero_point buffers in the model, use FQN (for quantize_per_tensor ops)**
* #51166 [quant][fx] Make scale, zero_point buffers in the model and use FQN (for quantized ops)
* #51086 [quant][fx] Scope support for call_function in QuantizationTracer

Summary:
Following up on previous PR, this PR makes scale and zero_point for quantize_per_tensor to be
registered as buffers in the module.
Currently the dtype is still stored as attr (not registered as buffer) since we can only register tensor types.

Test Plan:
python test/test_quantization.py test_qparams_buffers

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26092964](https://our.internmc.facebook.com/intern/diff/D26092964)",pytorch
51251,ngimel,pr,2021-01-27T23:54:56Z,remove LegacyDefinitions as it is empty now,"Per title
",pytorch
51259,supriyar,pr,2021-01-28T01:21:16Z,[quant][fx] Update name of packed weight attributes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51259 [quant][fx] Update name of packed weight attributes**

Summary:

Store the FQN of the module that is using the packed weights (the quantized op)

In the case of fusion we update the scope mapping to store the module path of the fused node.

Test Plan:
python test/test_quantization.py test_packed_weight_fused_op

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26117964](https://our.internmc.facebook.com/intern/diff/D26117964)",pytorch
51321,rohan-varma,pr,2021-01-29T00:30:55Z,[1.8] Move object-based collective APIs to regular collectives,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51321 [1.8] Move object-based collective APIs to regular collectives**

Differential Revision: [D26137282](https://our.internmc.facebook.com/intern/diff/D26137282/)",pytorch
51322,rohan-varma,pr,2021-01-29T00:41:23Z,[1.8] Move object-based collective APIs to regular collectives,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51322 [1.8] Move object-based collective APIs to regular collectives**

Based on feedback we are moving these APIs such as `dist.broadcast_object_list` to instead directly use `dist.broadcast()` and overload the latter to support non-tensor types.

Currently only made changes in `distributed_c10d.py` and in `distributed_test.py`. If this change looks good, will go ahead and update all callsites as well as the documentation for these APIs.

Differential Revision: [D26137282](https://our.internmc.facebook.com/intern/diff/D26137282/)",pytorch
51341,rohan-varma,pr,2021-01-29T07:57:46Z,[1.8] Add additional tests for object-based APIs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51341 [1.8] Add additional tests for object-based APIs**
* #50625 [Collective APIs] Make python object collective API args consistent

Adds tests for objects that contain CPU/GPU tensors to ensure that
they can also be serialized/deserialized appropriately.

Differential Revision: [D26144100](https://our.internmc.facebook.com/intern/diff/D26144100/)",pytorch
51369,fritzo,pr,2021-01-29T21:09:52Z,Fix Dirichlet.arg_constraints event_dim,"This fix ensures
```py
Dirichlet.arg_constraints[""concentration""].event_dim == 1
```
which was missed in #50547

## Tested
- [x] added a regression test, covering all distributions",pytorch
51372,ngimel,pr,2021-01-29T21:33:28Z,expose memory_fraction and gpu_process docs,"Per title
",pytorch
51375,r-barnes,pr,2021-01-29T21:56:15Z,"""whitelist"" -> ""allowlist""","Differential Revision: D26150609

",pytorch
51395,ngimel,pr,2021-01-30T01:28:29Z,update profiler doc strings,"Fixes formatting for autograd.profiler doc string (was broken), slightly expands profiler.profile documentation. 
",pytorch
51405,peterjc123,pr,2021-01-30T10:09:20Z,Remove android toolchain in Windows CircleCI image,"Fixes #{issue number}
It can spare nearly 10 GB of disk space.",pytorch
51581,micmelesse,pr,2021-02-02T21:06:56Z,[ROCm] enable fft tests,"This PR enable some failing unit tests for fft in pytorch on ROCM.

The reason these tests were failing was due to hipfft clobbering inputs causing a mismatch in tests that were checking that applying ffts and their reverse would get you back to the input.

We solve this by cloning the input using an existing flag on the ROCM platform.

There PR doesnot enable all fft tests. There are other issues that need to be resolved before that can happen.",pytorch
51642,r-barnes,pr,2021-02-03T16:48:52Z,Fix warnings in TensorShape,"Summary:
Compiling currently gives:
```
an 13 16:46:39 In file included from ../aten/src/ATen/native/TensorShape.cpp:12:
Jan 13 16:46:39 ../aten/src/ATen/native/Resize.h:37:24: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     if (new_size_bytes > self->storage().nbytes()) {
Jan 13 16:46:39         ~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:32:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long long') [-Wsign-compare]
Jan 13 16:46:39   for (size_t i = 0; i < shape_tensor.numel(); ++i) {
Jan 13 16:46:39                      ~ ^ ~~~~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:122:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < tensors.size(); i++) {
Jan 13 16:46:39                       ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:162:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int i = 0; i < tensors.size(); i++) {
Jan 13 16:46:39                   ~ ^ ~~~~~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:300:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < s1.size(); ++i) {
Jan 13 16:46:39                       ~ ^ ~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:807:21: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     TORCH_CHECK(dim < self_sizes.size());
Jan 13 16:46:39                 ~~~ ^ ~~~~~~~~~~~~~~~~~
Jan 13 16:46:39 ../c10/util/Exception.h:361:31: note: expanded from macro 'TORCH_CHECK'
Jan 13 16:46:39   if (C10_UNLIKELY_OR_CONST(!(cond))) {                                 \
Jan 13 16:46:39                               ^~~~
Jan 13 16:46:39 ../c10/util/Exception.h:244:47: note: expanded from macro 'C10_UNLIKELY_OR_CONST'
Jan 13 16:46:39 #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
Jan 13 16:46:39                                               ^
Jan 13 16:46:39 ../c10/macros/Macros.h:173:65: note: expanded from macro 'C10_UNLIKELY'
Jan 13 16:46:39 #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))
Jan 13 16:46:39                                                                 ^~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:855:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const int64_t' (aka 'const long long') [-Wsign-compare]
Jan 13 16:46:39   for (size_t i = 0; i < num_blocks; ++i) {
Jan 13 16:46:39                      ~ ^ ~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2055:23: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39     for (int i = 0; i < vec.size(); i++) {
Jan 13 16:46:39                     ~ ^ ~~~~~~~~~~
Jan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2100:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Jan 13 16:46:39   for (int64_t i = 0; i < src.size(); ++i) {
```
This fixes issues with loop iteration variable types

Test Plan: Sandcastle tests

Differential Revision: D25935136

",pytorch
51653,vfdev-5,pr,2021-02-03T17:53:09Z,Optimized bilinear interpolation using TensorIterator,"Related to #10482

Description:

- Optimized bilinear interpolation for 1d, 2d, 3d cases using TensorIterator

### Results


<details>
<summary>
Interpolation 2d - 6 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 320, 320] | [256, 256] | True | False | 0.3938 | 0.0782 | 5.0339
[1, 3, 320, 320] | [512, 512] | True | False | 1.5585 | 0.4105 | 3.7965
[1, 3, 320, 320] | [256, 256] | False | False | 0.3481 | 0.0760 | 4.5780
[1, 3, 320, 320] | [512, 512] | False | False | 1.5848 | 0.4091 | 3.8734
[1, 3, 320, 320] | [256, 256] | False | True | 1.2058 | 1.2034 | 1.0020
[1, 3, 320, 320] | [512, 512] | False | True | 4.8691 | 4.8537 | 1.0032
[32, 128, 64, 64] | [32, 32] | False | True | 6.3915 | 6.4041 | 0.9980
[32, 128, 64, 64] | [128, 128] | False | True | 166.1769 | 164.5621 | 1.0098
[32, 128, 64, 64] | [32, 32] | True | False | 3.7194 | 2.4720 | 1.5046
[32, 128, 64, 64] | [128, 128] | True | False | 86.6704 | 52.3754 | 1.6548
[1, 3, 500, 500] | [256, 256] | True | False | 0.3270 | 0.0792 | 4.1307
[1, 3, 500, 500] | [800, 800] | True | False | 3.3116 | 0.5567 | 5.9482
[1, 3, 500, 500] | [256, 256] | False | False | 0.3763 | 0.0773 | 4.8700
[1, 3, 500, 500] | [800, 800] | False | False | 3.2577 | 0.5590 | 5.8279


</details>

<details>
<summary>
Interpolation 1d - 6 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[4, 512, 320] | 256 | True | False | 0.2795 | 0.1032 | 2.7089
[4, 512, 320] | 512 | True | False | 0.5533 | 0.1888 | 2.9303


</details>

<details>
<summary>
Interpolation 3d - 6 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 16, 320, 320] | [8, 256, 256] | True | False | 4.4105 | 2.1236 | 2.0769
[1, 3, 16, 320, 320] | [32, 512, 512] | True | False | 83.9426 | 42.6641 | 1.9675
[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 15.5736 | 15.5758 | 0.9999
[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 272.4795 | 273.2745 | 0.9971


</details>

<details>
<summary>
Interpolation 2d - 1 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 320, 320] | [256, 256] | True | False | 1.0240 | 0.4145 | 2.4705
[1, 3, 320, 320] | [512, 512] | True | False | 4.0771 | 1.3836 | 2.9467
[1, 3, 320, 320] | [256, 256] | False | False | 0.9771 | 0.3270 | 2.9878
[1, 3, 320, 320] | [512, 512] | False | False | 4.1732 | 1.2209 | 3.4180
[1, 3, 320, 320] | [256, 256] | False | True | 1.5466 | 1.5363 | 1.0067
[1, 3, 320, 320] | [512, 512] | False | True | 6.1555 | 6.1199 | 1.0058
[32, 128, 64, 64] | [32, 32] | False | True | 27.6362 | 27.5901 | 1.0017
[32, 128, 64, 64] | [128, 128] | False | True | 468.6442 | 465.5163 | 1.0067
[32, 128, 64, 64] | [32, 32] | True | False | 20.1495 | 10.0694 | 2.0011
[32, 128, 64, 64] | [128, 128] | True | False | 400.0401 | 204.0662 | 1.9603
[1, 3, 500, 500] | [256, 256] | True | False | 0.8956 | 0.3366 | 2.6606
[1, 3, 500, 500] | [800, 800] | True | False | 8.6554 | 2.9530 | 2.9310
[1, 3, 500, 500] | [256, 256] | False | False | 1.0921 | 0.3385 | 3.2263
[1, 3, 500, 500] | [800, 800] | False | False | 8.9594 | 2.9627 | 3.0241


</details>

<details>
<summary>
Interpolation 1d - 1 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[4, 512, 320] | 256 | True | False | 1.5233 | 0.5027 | 3.0301
[4, 512, 320] | 512 | True | False | 3.0302 | 0.9735 | 3.1128


</details>

<details>
<summary>
Interpolation 3d - 1 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 16, 320, 320] | [8, 256, 256] | True | False | 12.0477 | 11.3196 | 1.0643
[1, 3, 16, 320, 320] | [32, 512, 512] | True | False | 222.8618 | 209.9955 | 1.0613
[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 17.9883 | 17.9937 | 0.9997
[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 380.7244 | 380.1916 | 1.0014


</details>


<details>
<summary>
Versions and build configs
</summary>

PyTorch master: 1.9.0.dev20210223
PyTorch master build setting:
```
BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,
```

PR : 1.9.0a0+74b172b
PR build setting:
```
BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/g++-7, CXX_FLAGS=-O3 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON,
```
</details>





This description is based on the benchmarks and the code from [here](https://github.com/vfdev-5/interpolate-tensoriterator/tree/master/step_six).

TL;DR
- Linear upsampling generic implementation using TensorIterator for Nd case (single loop function for 1d, 2d and 3d cases)
  - can be generalized to nearest, bicubic interpolation modes.
- works for channels first and last cases.
 
Joint work with Francisco Massa (@fmassa).
",pytorch
51695,rohan-varma,pr,2021-02-04T03:17:42Z,[RPC] Move confirmation future in rref context to jit future,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51696 [RPC] waitForThreadLocalRRefs returns jitFuture
* **#51695 [RPC] Move confirmation future in rref context to jit future**

As part of the plan to completely eliminate torch/csrc/utils/future.h,
we are converting this to JitFuture (c10::ivalue::Future).

Differential Revision: [D26238873](https://our.internmc.facebook.com/intern/diff/D26238873/)",pytorch
51696,rohan-varma,pr,2021-02-04T03:17:57Z,[RPC] waitForThreadLocalRRefs returns jitFuture,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51696 [RPC] waitForThreadLocalRRefs returns jitFuture**
* #51695 [RPC] Move confirmation future in rref context to jit future

Modify this API to use JitFuture.

Differential Revision: [D26239132](https://our.internmc.facebook.com/intern/diff/D26239132/)",pytorch
51697,rohan-varma,pr,2021-02-04T03:18:11Z,[RPC] Refactor rref_context to not use utils::Future,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51698 [RPC] delete torch/csrc/utils/future.h
* **#51697 [RPC] Refactor rref_context to not use utils::Future**

Refactors the rest of rref_context, specifically pendingOwners map and `getOwnerRRef` to use JitFuture.

Differential Revision: [D26243268](https://our.internmc.facebook.com/intern/diff/D26243268/)",pytorch
51698,rohan-varma,pr,2021-02-04T03:18:26Z,[RPC] delete torch/csrc/utils/future.h,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51698 [RPC] delete torch/csrc/utils/future.h**
* #51697 [RPC] Refactor rref_context to not use utils::Future

Completely eliminates torch::utils::Future as we are now full relying on JitFuture.
Closes https://github.com/pytorch/pytorch/issues/41574

Made sure this does not regress perf: 
RPC `test_stress_heavy_rpc` results before and after:
Before this diff:
```
test_stress_heavy_rpc (fb.test_tensorpipe_agent.TensorPipeRpcTestWithFork) ... Rank 0 finished testing 20 t
imes in 0.10975527763366699 seconds.
Rank 3 finished testing 20 times in 0.11303091049194336 seconds.
Rank 1 finished testing 20 times in 0.13047003746032715 seconds.
Rank 2 finished testing 20 times in 0.13222789764404297 seconds.
```

After:
```
test_stress_heavy_rpc (fb.test_tensorpipe_agent.TensorPipeRpcTestWithFork) ... Rank 0 finished testing 20 times in 0.11118173599243164 seconds.
Rank 3 finished testing 20 times in 0.11451172828674316 seconds.
Rank 2 finished testing 20 times in 0.12779712677001953 seconds.
Rank 1 finished testing 20 times in 0.12971091270446777 seconds.
```

RPC `test_nested_rref_stress` before and after (added timing stats):
Before this diff:
```
test_nested_rref_stress (fb.test_tensorpipe_agent.TensorPipeRpcTestWithFork) ... Rank 0 finished testing 20 times in 0.13818001747131348 seconds.
Rank 3 finished testing 20 times in 0.13921260833740234 seconds.
Rank 2 finished testing 20 times in 0.14690589904785156 seconds.
Rank 1 finished testing 20 times in 0.15098071098327637 seconds.
```

After this diff:
```
test_nested_rref_stress (fb.test_tensorpipe_agent.TensorPipeRpcTestWithFork) ... Rank 3 finished testing 20
 times in 0.1366286277770996 seconds.
Rank 2 finished testing 20 times in 0.1391909122467041 seconds.
Rank 1 finished testing 20 times in 0.1434028148651123 seconds.
Rank 0 finished testing 20 times in 0.15080523490905762 seconds.
```


Differential Revision: [D26243735](https://our.internmc.facebook.com/intern/diff/D26243735/)",pytorch
51702,ngimel,pr,2021-02-04T04:16:09Z,unbreak bc test,"Caused by #48223 revert
",pytorch
51753,r-barnes,pr,2021-02-04T23:39:08Z,Pyre for training_toolkit,"Differential Revision: D26258217

",pytorch
51770,supriyar,pr,2021-02-05T05:48:59Z,[quant] add docs for embedding/embedding_bag,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51770 [quant] add docs for embedding/embedding_bag**

Summary:

Test Plan:
tested locally on mac

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26279112](https://our.internmc.facebook.com/intern/diff/D26279112)",pytorch
51821,rohan-varma,pr,2021-02-06T00:57:46Z,Dist profiling supports tensor shapes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51822 [Dist Profiling] Support shape recording for profiling collectives in gloo
* **#51821 Dist profiling supports tensor shapes**

Differential Revision: [D26280595](https://our.internmc.facebook.com/intern/diff/D26280595/)",pytorch
51822,rohan-varma,pr,2021-02-06T00:58:01Z,[Dist Profiling] Support shape recording for profiling collectives in gloo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52031 [Dist Profiling] Enable dist profiling for DDP
* #52004 [Dist Profiling] Enable profiling for gloo send/recv
* **#51822 [Dist Profiling] Support shape recording for profiling collectives in gloo**

Adds support for shape recording for profiling distributed collectives, for gloo. Added
both cpp and python tests to ensure that shapes are recorded properly.

Differential Revision: [D26291739](https://our.internmc.facebook.com/intern/diff/D26291739/)",pytorch
51825,ngimel,pr,2021-02-06T04:14:38Z,[wip] doc_fix,"tries to fix doc_test
",pytorch
51827,ngimel,pr,2021-02-06T07:04:58Z,fix multi_output_kernel,"With @zasdfgbnm's help and with his small TensorIterator kernel repro https://github.com/zasdfgbnm/tensoriterator we've found a workaround for what looks like a compiler bug in multi_output_kernel that manifests itself with cuda 10.2 and cuda 11 when there is a non-trivial OffsetCalculator. 
It looks like those nvcc versions cannot handle inheritance in device structs, so instead of inheriting `multi_outputs_unroll` from `unroll` we make it independent. 
cc @vkuzo, @haichuan-fb I verified that reverting #49315 to bring back multi_output_kernel and running `test_learnable_backward_per_channel_cuda` test passes, but I didn't do it in this PR - can you take it up as a follow-up?
",pytorch
51869,zhuzilin,pr,2021-02-08T08:28:26Z,Improve the performance of repeat_interleave,"Fixes #31980

This PR adds a short cut for `repeat_interleave` when the `repeats` is an `int64_t`. The old implementation will turn the int `results` into a tensor which has the same size of the selected dimension of `input` for the generalized form and do complex operations like `cumsum` and `index_select`. The new implementation will reuse the `repeat` operation.

This is the benchmark from the origin issue. The performance without the shortcut is:
```bash
>>> import torch
>>> import timeit
>>> timeit.timeit('torch.randn(10000).repeat_interleave(100, -1)', 'import torch', number=100)
0.22591369319707155
>>> timeit.timeit('torch.randn(10000)[..., None].expand(-1, 100).flatten(-2, -1)', 'import torch', number=100)
0.02309961011633277
```
With this shortcut:
```bash
>>> import torch
>>> import timeit
>>> timeit.timeit('torch.randn(10000).repeat_interleave(100, -1)', 'import torch', number=100)
0.023523901589214802
>>> timeit.timeit('torch.randn(10000)[..., None].expand(-1, 100).flatten(-2, -1)', 'import torch', number=100)
0.023197444155812263
```

Thank you for your time on this PR.

Gently ping @zou3519  @VitalyFedyunin",pytorch
51936,peterjc123,pr,2021-02-09T03:32:05Z,Stop installing libuv on Windows,"Fixes #{issue number}
@gunandrose4u ",pytorch
51937,peterjc123,pr,2021-02-09T03:51:57Z,Link onnx_library when BUILD_TEST=0 for Windows,"Fixes https://github.com/pytorch/pytorch/issues/51877
cc @antoniovs1029",pytorch
52004,rohan-varma,pr,2021-02-09T23:52:02Z,[Dist Profiling] Enable profiling for gloo send/recv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52031 [Dist Profiling] Enable dist profiling for DDP
* **#52004 [Dist Profiling] Enable profiling for gloo send/recv**
* #51822 [Dist Profiling] Support shape recording for profiling collectives in gloo

Enables profiling of p2p collectives for Gloo. Modified/added relevant unittests.

Differential Revision: [D26347164](https://our.internmc.facebook.com/intern/diff/D26347164/)",pytorch
52029,rohan-varma,pr,2021-02-10T08:28:35Z,[Dist Profiling] Enable dist profiling for DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Closes https://github.com/pytorch/pytorch/issues/52020
Ensures that we can profile collectives in DDP by propagating the profiler threadLocalState appropriately. As described in the above issue, before this wouldn't work as the profiler would only be enabled on the main thread.

Differential Revision: [D26356192](https://our.internmc.facebook.com/intern/diff/D26356192/)",pytorch
52031,rohan-varma,pr,2021-02-10T08:43:33Z,[Dist Profiling] Enable dist profiling for DDP (gloo-only),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52031 [Dist Profiling] Enable dist profiling for DDP (gloo-only)**

Closes https://github.com/pytorch/pytorch/issues/52020
Ensures that we can profile collectives in DDP by propagating the profiler threadLocalState appropriately. As described in the above issue, before this wouldn't work as the profiler would only be enabled on the main thread.

Example profiler output:
```
                           aten::empty_like         0.11%       9.503ms         0.23%      20.060ms       7.039us          2850  
                                 aten::view         0.13%      11.117ms         0.13%      11.117ms       2.059us          5400  
                                aten::relu_         0.23%      20.549ms         0.64%      56.259ms      22.963us          2450  
                           aten::clamp_min_         0.09%       7.586ms         0.41%      35.710ms      14.576us          2450  
                            aten::clamp_min         0.32%      28.124ms         0.32%      28.124ms      11.479us          2450  
                           aten::max_pool2d         0.00%     309.374us         0.03%       2.818ms      56.352us            50  
              aten::max_pool2d_with_indices         0.02%       1.781ms         0.03%       2.508ms      50.164us            50  
                                 aten::add_         4.10%     359.388ms         4.10%     359.388ms      14.498us         24789  
                  aten::adaptive_avg_pool2d         0.00%     241.042us         0.02%       1.900ms      37.999us            50  
                                 aten::mean         0.02%       1.471ms         0.02%       1.659ms      33.178us            50  
                              aten::flatten         0.00%     300.594us         0.01%     646.465us      12.929us            50  
                               aten::linear         0.00%     351.813us         0.07%       6.076ms     121.529us            50  
                                    aten::t         0.01%       1.300ms         0.03%       2.484ms       9.937us           250  
                            aten::transpose         0.01%     872.023us         0.01%       1.184ms       4.737us           250  
                           aten::as_strided         0.01%     991.673us         0.01%     991.673us       1.941us           511  
                                aten::addmm         0.05%       4.620ms         0.06%       5.101ms     102.011us            50  
                               aten::expand         0.01%     448.976us         0.01%     576.925us       5.769us           100  
                          aten::log_softmax         0.00%     242.182us         0.02%       1.952ms      39.036us            50  
                         aten::_log_softmax         0.02%       1.372ms         0.02%       1.710ms      34.193us            50  
                             aten::nll_loss         0.00%     197.138us         0.02%       1.806ms      36.116us            50  
                     aten::nll_loss_forward         0.02%       1.609ms         0.02%       1.609ms      32.173us            50  
                               ##backward##        71.21%        6.243s        71.23%        6.244s     124.884ms            50  
                            aten::ones_like         0.00%     326.458us         0.02%       1.556ms      31.114us            50  
                        aten::empty_strided         0.01%     953.270us         0.01%     953.270us       4.518us           211  
                            NllLossBackward         0.01%     956.132us         0.05%       4.363ms      87.265us            50  
                    aten::nll_loss_backward         0.04%       3.407ms         0.04%       3.407ms      68.142us            50  
                         LogSoftmaxBackward         0.00%     371.859us         0.02%       2.149ms      42.976us            50  
           aten::_log_softmax_backward_data         0.01%       1.204ms         0.02%       1.777ms      35.539us            50  
                              AddmmBackward         0.01%     839.859us         0.07%       5.702ms     114.050us            50  
                                 aten::conj         0.00%     138.479us         0.00%     138.479us       1.385us           100  
                                   aten::mm         0.03%       2.734ms         0.04%       3.263ms      32.626us           100  
            torch::autograd::AccumulateGrad         0.32%      28.308ms         1.82%     159.868ms      19.859us          8050  
                                 aten::div_         1.93%     169.297ms         1.93%     169.297ms      21.031us          8050  
                                  TBackward         0.00%     122.898us         0.01%     521.645us      10.433us            50  
                               ViewBackward         0.00%     150.556us         0.01%     566.579us      11.332us            50  
                              aten::reshape         0.00%     162.456us         0.00%     416.023us       8.320us            50  
                              MeanBackward1         0.00%     295.767us         0.03%       2.573ms      51.455us            50  
                                  aten::div         0.02%       1.569ms         0.02%       1.831ms      36.621us            50  
                              ReluBackward1         0.11%       9.844ms         0.75%      65.363ms      26.679us          2450  
                   aten::threshold_backward         0.53%      46.600ms         0.63%      55.519ms      22.661us          2450  
                               AddBackward0         0.01%     790.913us         0.01%     790.913us       0.989us           800  
                     CudnnBatchNormBackward         0.31%      26.949ms         2.21%     193.873ms      73.160us          2650  
            aten::cudnn_batch_norm_backward         1.49%     130.999ms         1.90%     166.924ms      62.990us          2650  
                   CudnnConvolutionBackward         0.24%      20.836ms         4.90%     429.205ms     161.964us          2650  
           aten::cudnn_convolution_backward         0.41%      35.645ms         4.66%     408.369ms     154.102us          2650  
     aten::cudnn_convolution_backward_input         1.95%     170.946ms         2.09%     183.224ms      70.471us          2600  
    aten::cudnn_convolution_backward_weight         1.94%     169.685ms         2.16%     189.500ms      71.510us          2650  
                            gloo:all_reduce         0.00%       0.000us             0       22.056s      88.224ms           250  
                                 aten::set_         0.01%       1.185ms         0.01%       1.185ms       4.702us           252  
                                aten::copy_         0.14%      12.215ms         0.14%      12.215ms      21.133us           578  
               MaxPool2DWithIndicesBackward         0.01%     492.740us         0.05%       4.528ms      90.565us            50  
     aten::max_pool2d_with_indices_backward         0.02%       1.822ms         0.05%       4.035ms      80.710us            50  
                           aten::zeros_like         0.00%     296.133us         0.02%       1.972ms      39.430us            50  
                           aten::resize_as_         0.00%     202.059us         0.00%     241.797us       4.836us            50  
                              ##optimizer##         0.02%       1.679ms         6.30%     551.889ms      11.038ms            50  
                    Optimizer.step#SGD.step         1.69%     147.963ms         6.27%     549.782ms      10.996ms            50  
                                aten::clone         0.01%       1.140ms         0.04%       3.665ms      22.765us           161  
                               aten::detach         0.00%     128.894us         0.00%     295.912us       1.838us           161  
                                     detach         0.00%     167.018us         0.00%     167.018us       1.037us           161  
                             gloo:broadcast         0.00%       0.000us             0     976.757us     488.379us             2  
                                 aten::mul_         1.06%      92.993ms         1.06%      92.993ms      11.788us          7889  
-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  

```

Differential Revision: [D26356192](https://our.internmc.facebook.com/intern/diff/D26356192/)",pytorch
52037,vfdev-5,pr,2021-02-10T10:30:52Z,Added negative alias to neg opinfo,"
Description:
- Added negative alias to neg opinfo

Refactored from https://github.com/pytorch/pytorch/pull/49158",pytorch
52101,r-barnes,pr,2021-02-10T21:53:01Z,Use c10::irange for great good,"Differential Revision: D26378826

",pytorch
52123,r-barnes,pr,2021-02-11T00:54:22Z,Fix compiler warning for MathConstants.h,"Summary:
Compiler currently complains:
```
caffe2/c10/util/MatchConstants.h(18): warning: calling a constexpr __host__ function(""from_bits"") from a __host__ __device__ function(""pi"") is not allowed.
```

Differential Revision: D26379485

",pytorch
52150,r-barnes,pr,2021-02-11T19:22:17Z,op_whitelist -> op_allowlist,"Differential Revision: D26405520

",pytorch
52153,r-barnes,pr,2021-02-11T19:31:53Z,Use c10::irange for great good,"Differential Revision: D26407087

",pytorch
52166,rohan-varma,pr,2021-02-11T22:08:37Z,[RPC] Allow rref.rpc_async() to be non blocking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52167 [RPC] Move rref proxy timeout tests to general RpcTest
* **#52166 [RPC] Allow rref.rpc_async() to be non blocking**

In RRef proxy, `rref.rpc_async()` has some blocking behavior due to blocking on getting the RRef type. This can cause performance hits and it would be good to make this truly async.

This diff changes the call to `_get_type()` to return a future, and then runs the actual `rpc_async` call as a callback. We use a `torch.futures.Future()` to store the result of RPC API so that we can return it without incurring any blocking behavior. 

Differential Revision: [D25944751](https://our.internmc.facebook.com/intern/diff/D25944751/)",pytorch
52167,rohan-varma,pr,2021-02-11T22:08:51Z,[RPC] Move rref proxy timeout tests to general RpcTest,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52167 [RPC] Move rref proxy timeout tests to general RpcTest**
* #52166 [RPC] Allow rref.rpc_async() to be non blocking

These tests were mistakenly written only under the TensorPipe Test
class, hence enabling them for all agents.

Differential Revision: [D25950283](https://our.internmc.facebook.com/intern/diff/D25950283/)",pytorch
52172,r-barnes,pr,2021-02-11T23:33:32Z,Check kernel launches in caffe2/aten/src/THCUNN,"Differential Revision: D26408802

",pytorch
52173,r-barnes,pr,2021-02-11T23:33:41Z,Check kernel launches in caffe2/caffe2/image,"Differential Revision: D26408885

",pytorch
52174,r-barnes,pr,2021-02-11T23:33:41Z,Check kernel launches in caffe2/aten/src/THC,"Differential Revision: D26408837

",pytorch
52185,r-barnes,pr,2021-02-12T02:37:29Z,Kernel launch checks for aten/src/ATen,"Differential Revision: D26408276

",pytorch
52203,lezcano,pr,2021-02-12T10:52:49Z,Port index_copy from TH to ATen,"The design of the `TensorIterator` was similar to that in https://github.com/pytorch/pytorch/pull/50578

Resolves https://github.com/pytorch/pytorch/issues/24670
Resolves https://github.com/pytorch/pytorch/issues/24523

Timings:
<details>
<summary>Script</summary>

```python
from IPython import get_ipython
import torch

torch.manual_seed(13)
torch.set_num_threads(1)

ipython = get_ipython()

cpu = torch.device('cpu')
cuda = torch.device('cuda')


def run_test(ndims, size, index_len, device):
    print(f""ndims: {ndims}, tensor_size: {size}, index_len: {index_len}, device: {device}"")

    x = torch.rand(*([size] * ndims), device=device)
    index = torch.randint(size, (index_len,), dtype=torch.long, device=device)
    for d in range(ndims):
        shape_t = [size] * d + [index_len] + [size] * (ndims - d - 1)
        t = torch.rand(*shape_t, device=device)
        command = ""x.index_copy(d, index, t)""
        if device == cuda:
            command = command + ""; torch.cuda.synchronize()""
        ipython.magic(f""timeit {command}"")
    print()


run_test(3, 700, 10, cpu)
run_test(3, 700, 100, cpu)
run_test(3, 700, 700, cpu)
run_test(2, 10000, 10000, cpu)

run_test(3, 700, 10, cuda)
run_test(3, 700, 100, cuda)
run_test(3, 700, 700, cuda)
run_test(2, 10000, 10000, cuda)
```

</details>

<details>
<summary>CPU ATen</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cpu                                                                                                                                                          
327 ms Â± 309 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                              
329 ms Â± 456 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                              
378 ms Â± 1.44 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                             
                                                                                                                                                                                                                
ndims: 3, tensor_size: 700, index_len: 100, device: cpu                                                                                                                                                         
348 ms Â± 1.52 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                             
359 ms Â± 330 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                              
526 ms Â± 686 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                              
                                                                                                                                                                                                                
ndims: 3, tensor_size: 700, index_len: 700, device: cpu                                                                                                                                                         
560 ms Â± 19 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                               
552 ms Â± 2.61 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                             
932 ms Â± 2.52 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)                                                                                                                                             
                                                                                                                                                                                                                
ndims: 2, tensor_size: 10000, index_len: 10000, device: cpu                                                                                                                                                     
163 ms Â± 5.05 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)                                                                                                                                           
302 ms Â± 5.75 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```                                                                                                                                           
</details>

<details>
<summary>CUDA ATen</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cuda
9.63 ms Â± 441 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
9.65 ms Â± 230 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
12.4 ms Â± 881 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 700, index_len: 100, device: cuda
10.8 ms Â± 1.51 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
11 ms Â± 417 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
21.2 ms Â± 18.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 700, index_len: 700, device: cuda
19 ms Â± 4.42 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
17.8 ms Â± 493 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
25.8 ms Â± 1.22 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cuda
5.59 ms Â± 109 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
10 ms Â± 25.5 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

</details>

<details>
<summary>CPU TH</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cpu
333 ms Â± 2.42 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
327 ms Â± 1.04 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
366 ms Â± 753 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 100, device: cpu
336 ms Â± 1.24 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
345 ms Â± 914 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
884 ms Â± 4.32 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 700, device: cpu
441 ms Â± 3.58 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
514 ms Â± 1.17 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
7.46 s Â± 6.46 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cpu
141 ms Â± 233 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
1.13 s Â± 855 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

</details>


<details>
<summary>CUDA TH</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cuda
9.64 ms Â± 390 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
9.68 ms Â± 3.26 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
13.9 ms Â± 928 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 700, index_len: 100, device: cuda
11.6 ms Â± 1.38 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
12.1 ms Â± 3.72 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
30.3 ms Â± 27.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 700, index_len: 700, device: cuda
27.2 ms Â± 19.8 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
30.6 ms Â± 43.6 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
146 ms Â± 204 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cuda
6.5 ms Â± 3.99 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
64.7 ms Â± 55.5 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```

</details>

According to these we see a slight performance improvement across both CPU and GPU.


cc: @nikitaved ",pytorch
52212,zhuzilin,pr,2021-02-12T16:41:42Z,Make bias in lazy modules lazy and avoid create empty tensors,"Some minor improvement for lazy modules introduced in #44538, #47350 and #51548.

This PR mainly turn the bias to `UninitializedParameter` and instead of creating empty tensors like 
```python
self.bias = Parameter(torch.Tensor(0))
self.bias = UninitializedParameter()
```
I think it would be better to 
```python
self.register_parameter('bias', None)
self.bias = UninitializedParameter()
```

In addition, I change the constructor of the `LazyBatchNorm` from
```python
self.running_mean = UninitializedBuffer()
```
to
```python
self.register_buffer('running_mean', UninitializedBuffer())
```
as the original one would not change the underlying `self._buffers`.

Thank you for your time on reviewing this PR :).

Gently ping @albanD, @mruberry",pytorch
52240,r-barnes,pr,2021-02-12T21:45:52Z,Check kernel launches in caffe2/operators,"Differential Revision: D26408330

",pytorch
52269,rohan-varma,pr,2021-02-15T10:49:22Z,Side stream patch,"Cherry pick for https://github.com/pytorch/pytorch/pull/50180 into 1.8 release. Original commit description:

Stack from ghstack:

#50180 [WIP] Use side-stream in CPU to GPU copies in DDP
Resolves the regression in
#49819 by adding copy over background
stream similar to scatter. We gate this with an env var which is by default true, but off for internal needs.

Differential Revision: D25818170",pytorch
52270,rohan-varma,pr,2021-02-15T10:53:21Z,[1.8 patch] Use side-stream in CPU to GPU copies in DDP,"Cherry pick #50180 into 1.8
Original commit description:
Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50180

Resolves the regression in
https://github.com/pytorch/pytorch/issues/49819 by adding copy over background
stream similar to scatter. For internal use cases, this is gated with an env var that maintains the previous behavior when it is off.

Test Plan: CI

Reviewed By: mrshenli, ngimel

Differential Revision: D25818170

fbshipit-source-id: e50c76c035504b2a44e2be084701cee45c90df75

Fixes #{issue number}
",pytorch
52280,micmelesse,pr,2021-02-15T19:42:03Z,Rnn miopen fp16 v1.8,"Fixes #{issue number}
",pytorch
52281,micmelesse,pr,2021-02-15T19:58:10Z,enable miopen for rnn fp16,"Fixes #{issue number}
",pytorch
52330,r-barnes,pr,2021-02-16T23:48:39Z,Modernize for-loops,"Differential Revision: D26001961

",pytorch
52384,rohan-varma,pr,2021-02-17T20:38:45Z,[DDP] unittest for when params arent used in backward pass,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52481 Add distributed debug mode func to python
* #52391 [DDP] Separate error messages for unused params in forward and not all outputs
used in loss computation
* #52385 [DDP] Enhance warning for find_unused_params
* **#52384 [DDP] unittest for when params arent used in backward pass**
used in loss computation
* #52385 [DDP] Enhance warning for find_unused_params
* **#52384 [DDP] unittest for when params arent used in backward pass**
used in loss computation
* #52385 [DDP] Enhance warning for find_unused_params
* **#52384 [DDP] unittest for when params arent used in backward pass**
used in loss computation
* #52385 [DDP] Enhance warning for find_unused_params
* **#52384 [DDP] unittest for when params arent used in backward pass**

Adds a simple UT with unittest that we can modify when we enable DDP backward without needing all parameters to get gradient.

Differential Revision: [D26482479](https://our.internmc.facebook.com/intern/diff/D26482479/)",pytorch
52385,rohan-varma,pr,2021-02-17T21:03:12Z,[DDP] Enhance warning for find_unused_params,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52481 Add distributed debug mode func to python
* #52391 [DDP] Separate error messages for unused params in forward and not all outputs
used in loss computation
* **#52385 [DDP] Enhance warning for find_unused_params**
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation
* **#52385 [DDP] Enhance warning for find_unused_params**
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation
* **#52385 [DDP] Enhance warning for find_unused_params**
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation
* **#52385 [DDP] Enhance warning for find_unused_params**
* #52384 [DDP] unittest for when params arent used in backward pass

This warning should specify that we did not find unused params in the
_forward_ pass, which is when we log this warning. This is to avoid confusion
when we get an error because not all outputs were used to compute loss, which
also raises an error about unused parameters (to be fixed in the next diff)

Differential Revision: [D26494136](https://our.internmc.facebook.com/intern/diff/D26494136/)",pytorch
52391,rohan-varma,pr,2021-02-17T22:07:27Z,"[DDP] Separate error messages for unused params in forward and not all outputs
used in loss computation","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52391 [DDP] Separate error messages for unused params in forward and not all outputs
used in loss computation**
used in loss computation**
* #52385 [DDP] Enhance warning for find_unused_params
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation**
* #52385 [DDP] Enhance warning for find_unused_params
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation**
* #52385 [DDP] Enhance warning for find_unused_params
* #52384 [DDP] unittest for when params arent used in backward pass
used in loss computation**
* #52385 [DDP] Enhance warning for find_unused_params
* #52384 [DDP] unittest for when params arent used in backward pass

used in loss computation

There are 2 ways DDP can throw the exception refactored here -
1) Unused params in the forward pass. We provide `find_unused_parameters=True` for this.
2) All params used in fwd pass, but not all outputs used in loss computation. There are a few workarounds for this but we do not provide native support.

Previously, these 2 issues were combined into 1 error message but that has historically resulted in confusion, with users reporting getting this error even when they enable `find_unused_parameters=True` (which they expect to fix this error). As a result there is additional churn to debug these issues because the true cause (1) vs (2) is not known.

This commit helps to fix the issue by separating out the 2 error messages depending on if we ran with unused parameter detection or not. Hopefully this should make the error message much more clear and actionable.

error msg with `find_unused_params=True`:
```
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
```
error msg without `find_unused_params` specified:
```
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
```

Differential Revision: [D26496688](https://our.internmc.facebook.com/intern/diff/D26496688/)",pytorch
52452,r-barnes,pr,2021-02-18T19:46:35Z,Modernize for-loops in torch misc,"Differential Revision: D26520760

",pytorch
52475,micmelesse,pr,2021-02-19T01:15:45Z,[ROCM] enable miopen for rnn f16,"This PR enables using MIOpen for RNN FP16 on ROCM. 

It does this by altering use_miopen to allow fp16.  In the special case where LSTMs use projections we use the default implementation, as it is not implemented in MIOpen at this time. We do send out a warning once to let the user know.

We then remove the various asserts that are no longer necessary since we handle the case.",pytorch
52481,rohan-varma,pr,2021-02-19T03:35:11Z,Add distributed debug mode func to python,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52803 Log nccl debug level in ProcessGroupNCCL
* **#52481 Add distributed debug mode func to python**
used in loss computation
* #52385 [DDP] Enhance warning for find_unused_params
* #52384 [DDP] unittest for when params arent used in backward pass

Adds an API `get_debug_mode` that can be used by distributed package and users to retrieve debug mode. Currently no functionality changes, but wanted to get the bare bones function out and add relevant debug mode logging in follow up diffs.

Detailed docs will be added once functionality is enabled.

Differential Revision: [D26508972](https://our.internmc.facebook.com/intern/diff/D26508972/)",pytorch
52503,peterjc123,pr,2021-02-19T12:15:03Z,Automatically set BUILD_SPLIT_CUDA for cpp exts,"Fixes https://github.com/pytorch/vision/pull/3418#issuecomment-781673110
",pytorch
52506,peterjc123,pr,2021-02-19T16:14:54Z,Fix undefined symbol for CUDA 11.1 Windows,"Fixes https://github.com/pytorch/pytorch/issues/52467.
",pytorch
52539,silvasean,pr,2021-02-19T23:49:40Z,"Remove notion of ""level"" from `Module::dump_to_str`.","The code uses `torch::jit::jit_log_prefix` for handling recursive
indenting in most places in this function. There was one place that was
using ""level"", but it was buggy -- it would result in a compounding
superlinear indent. Note that changing it to ""level+1"" doesn't fix the
bug.

Before/after:
https://gist.github.com/silvasean/8ee3ef115a48de6c9c54fbc40838d8d7

The new code establishes a recursive invariant for
`Module::dump_to_str`: the function returns the module printed at the
base indent level (i.e. no indent). `torch::jit:log_prefix` is used
to prefix recursive calls. The code was already nearly there, except for
this spurious use of ""level"".
",pytorch
52609,ngimel,pr,2021-02-22T18:57:58Z,remove pointless test,"Fixes T81870118
",pytorch
52618,r-barnes,pr,2021-02-22T20:49:46Z,Modernize for-loops in caffe2/torch (#50797),"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50797

Modernize for-loops throughout caffe2/ subdirs to use ranged-loops where possible (all `.cpp` files were examined).

```
find caffe2/ -iname ""*.cpp"" > /home/rbarnes/files
buck run mode/opt foundation/clangr:clangr_local -- -j 10 --file=/home/rbarnes/files --multi --apply-replacements=true tidy '--checks=-*,modernize-loop-convert'
```

Test Plan: Sandcastle tests

Differential Revision: D26585065

",pytorch
52619,rohan-varma,pr,2021-02-22T21:21:56Z,Run distributed_test with NCCL_ASYNC_ERROR_HANDLING,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52887 [Dist Debugability] Refactor and fix DDP model check during init
* **#52619 Run distributed_test with NCCL_ASYNC_ERROR_HANDLING**

Runs this test suite with nccl_async_error_handling enabled. It is the
default to run many distributed training jobs, and can also help catch
errors/hangs in tests more easily. We don't expect any changes in the actual
existing tests since they shouldn't have any hangs.

Also removes a commented out line

Differential Revision: [D26588108](https://our.internmc.facebook.com/intern/diff/D26588108/)",pytorch
52643,r-barnes,pr,2021-02-23T01:03:54Z,Drop unused variables,"Test Plan: Sandcastle

Differential Revision: D26588961

",pytorch
52652,r-barnes,pr,2021-02-23T03:24:58Z,Remove unused variable,"Differential Revision: D26589146

",pytorch
52653,r-barnes,pr,2021-02-23T03:25:10Z,Suppress unsigned comparison warning,"Summary:
Fixes:
```
caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu(105): warning: pointless comparison of unsigned integer with zero
```

Differential Revision: D26588918

",pytorch
52665,lezcano,pr,2021-02-23T11:24:41Z,Remove _th_take,These definitions of TH functions were left in the codebase after they were ported to ATen in https://github.com/pytorch/pytorch/pull/45283 and https://github.com/pytorch/pytorch/pull/45430,pytorch
52702,r-barnes,pr,2021-02-23T22:16:54Z,Fix constexpr __host__ warning,"Summary:
Fixes:
```
stderr: caffe2/c10/util/MathConstants.h(22): warning: calling a constexpr __host__ function(""from_bits"") from a __host__ __device__ function(""pi"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.
```

Differential Revision: D26589533

",pytorch
52752,r-barnes,pr,2021-02-24T17:27:36Z,Fix compiler warning for cast,"Summary:
Fixes the warning:
```
Feb 23 23:24:27 In file included from ../aten/src/ATen/native/BatchLinearAlgebra.cpp:9:
Feb 23 23:24:27 ../aten/src/ATen/native/Resize.h:39:24: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]
Feb 23 23:24:27     if (new_size_bytes > self->storage().nbytes()) {
Feb 23 23:24:27         ~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~
Feb 23 23:24:27 1 warning generated.
```

Differential Revision: D26621748

",pytorch
52763,neerajprad,pr,2021-02-24T20:08:39Z,Add sample validation for LKJCholesky.log_prob,"Fixes #52724.

This fixes the following for the LKJCholesky distribution in master:
 - `log_prob` does sample validation when `validate_args=True`.
 - exposes documentation for the LKJCholesky distribution.

cc. @fehiepsi, @fritzo 
",pytorch
52803,rohan-varma,pr,2021-02-25T01:29:55Z,Log nccl debug level in ProcessGroupNCCL,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52803 Log nccl debug level in ProcessGroupNCCL**

This is useful for double checking we have the expected nccl_debug
level when debugging problematic jobs.

New logs:

When default is warn:
```
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 60000
USE_HIGH_PRIORITY_STREAM: 0
NCCL_DEBUG: WARN
```
unset:

```
NCCL_ASYNC_ERROR_HANDLING: 0
NCCL_BLOCKING_WAIT: 0
TIMEOUT(ms): 1800000
USE_HIGH_PRIORITY_STREAM: 0
NCCL_DEBUG: N/A
```

Differential Revision: [D26653699](https://our.internmc.facebook.com/intern/diff/D26653699/)",pytorch
52835,rohan-varma,pr,2021-02-25T08:38:22Z,Fix grammar in reducer warning,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52835 Fix grammar in reducer warning**

Addresses comment in https://github.com/pytorch/pytorch/pull/52385
that was missed before landing the PR

Differential Revision: [D26660764](https://our.internmc.facebook.com/intern/diff/D26660764/)",pytorch
52887,rohan-varma,pr,2021-02-25T23:37:21Z,[Dist Debugability] Refactor and fix DDP model check during init,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52966 [DDP Logging] Log comm. hook in ddp logging
* #52965 [DDP Logging] Log nccl_async_error_handling
* #52957 [Dist Debugality] Log key DDP metrics to stderr under debug mode.
* **#52887 [Dist Debugability] Refactor and fix DDP model check during init**

This diff changes the way to do model consistency check (i.e. `_verify_replicas_across_processes`) in DDP.

There were a few things that could be improved with the way we verify model across processes in DDP initialization:

1. We should do this check before syncing module states in DDP init, otherwise with Gloo backend this will throw but we would like to throw the error corresponding to different models on different ranks. To do this, we move the methods to be standalone C++ functions (not part of reducer) and move this check to before synchronizing parameters.
2. Refactor DDP init in the following ways:
- Run model consistency check before creating reducer, 2
- add helper functions to build params to pass into reducer
- add helper function to call `_verify_model_across_ranks`
- move `def parameters` to a helper function `_get_parameters` to be used more broadly within DDP

In follow up changes we will add the ability to detect which rank had inconsistent model (https://github.com/pytorch/pytorch/issues/52876 would be useful for this to determine which ranks(s) had errors).

Differential Revision: [D26565290](https://our.internmc.facebook.com/intern/diff/D26565290/)",pytorch
52904,neerajprad,pr,2021-02-26T05:35:57Z,Expose documentation for LKJCholesky distribution,"This is already added to the master branch in https://github.com/pytorch/pytorch/pull/52763.

",pytorch
52906,rohan-varma,pr,2021-02-26T06:30:31Z,torch_warn_once for calling completed() on cuda future,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52906 torch_warn_once for calling completed() on cuda future**

CUDAFuture completed() semantics are different from at::ivalue::Future semantics, hence adding a warning only once when this is called to warn user about this.

This could be helpful for both DDP developers and comm. hook developers to avoid bugs that can arise from this assumption.

Differential Revision: [D26659806](https://our.internmc.facebook.com/intern/diff/D26659806/)",pytorch
52947,ngimel,pr,2021-02-26T22:38:37Z,fix logcumsumexp functor to properly handle infs and nans,"Fixes #52213
Nans were previously inconsistently propagated due to std::min always returning first argument if one of the args in nan
when reduction functor was called on 2 `-inf` arguments, `std::min(x,y) - std::max(x,y)` resulted in `-inf - (-inf)` = nan, even though logcumsumexp is well defined for `-inf, -inf` pair. ",pytorch
52949,rohan-varma,pr,2021-02-26T23:04:09Z,[Dist Profiling] Enable dist profiling for MPI backend for collectives and send/recv ,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52949 [Dist Profiling] Enable dist profiling for MPI backend for collectives and send/recv**

Enables distributed profiling which we have for gloo and nccl for the MPI backend. Enables it for all collectives as well as send/recv, and also adds shape recording. 

Differential Revision: [D26591590](https://our.internmc.facebook.com/intern/diff/D26591590/)",pytorch
52957,rohan-varma,pr,2021-02-27T03:22:01Z,[Dist Debugality] Log key DDP metrics to stderr under debug mode.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52966 [DDP Logging] Log comm. hook in ddp logging
* #52965 [DDP Logging] Log nccl_async_error_handling
* **#52957 [Dist Debugality] Log key DDP metrics to stderr under debug mode.**
* #52887 [Dist Debugability] Refactor and fix DDP model check during init

This diff:
1. Under TORCH_DISTRIBUTED_DEBUG=INFO or DETAIL, logs DDP information during init time (all stats in ddp_logging_data_)
2. Under TORCH_DISTRIBUTED_DEBUG=DETAIL, logs runtime stats when they are collected (first 10 iterations and then once every 100 iterations). Avoiding logging every iteration to not spam logs.
3. Dedupes the logic to set the above debug level as per request.

Verified by inspecting logs:

```
I0226 19:12:47.109243 2818475 logger.cpp:140] [Rank 1]: DDP Initialized with:
world_size: 2 module_name: Linear device_ids: 1 output_device: 1 backend_name: nccl parameter_dtype: float total
_parameter_size_in_bytes: 40 num_parameter_tensors: 2 bucket_sizes: 40 CUDA_VISIBLE_DEVICES: N/Abroadcast_buffer
s: 1 bucket_cap_mb: 25 find_unused_parameters: 0 gradient_as_bucket_view: 0
 Backend Info: nccl_socket_ifname: N/A nccl_blocking_wait: N/A nccl_debug: WARN nccl_nthreads: N/A nccl_ib_timeo
ut: N/A
I0226 19:12:47.109252 2818473 logger.cpp:140] [Rank 0]: DDP Initialized with:
world_size: 2 module_name: Linear device_ids: 0 output_device: 0 backend_name: nccl parameter_dtype: float total
_parameter_size_in_bytes: 40 num_parameter_tensors: 2 bucket_sizes: 40 CUDA_VISIBLE_DEVICES: N/Abroadcast_buffer
s: 1 bucket_cap_mb: 25 find_unused_parameters: 0 gradient_as_bucket_view: 0
 Backend Info: nccl_socket_ifname: N/A nccl_blocking_wait: N/A nccl_debug: WARN nccl_nthreads: N/A nccl_ib_timeo
ut: N/A
```

```
I0226 19:12:48.117936 2818473 logger.cpp:286] [Rank 0 / 2] Training Linear unused_parameter_size=0
 Avg forward compute time: 568944
 Avg backward compute time: 885504
Avg backward comm. time: 692496
 Avg backward comm/comp overlap time: 113536
I0226 19:12:48.118517 2818475 logger.cpp:286] [Rank 1 / 2] Training Linear unused_parameter_size=0
 Avg forward compute time: 565584
 Avg backward compute time: 876992
Avg backward comm. time: 201872
 Avg backward comm/comp overlap time: 128624
```

Differential Revision: [D26708184](https://our.internmc.facebook.com/intern/diff/D26708184/)",pytorch
52964,peterjc123,pr,2021-02-27T06:35:59Z,Update mkl to 2020.2.254,"Fixes https://github.com/pytorch/pytorch/issues/52907
",pytorch
52965,rohan-varma,pr,2021-02-27T07:54:56Z,[DDP Logging] Log nccl_async_error_handling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #52966 [DDP Logging] Log comm. hook in ddp logging
* **#52965 [DDP Logging] Log nccl_async_error_handling**
* #52957 [Dist Debugality] Log key DDP metrics to stderr under debug mode.
* #52887 [Dist Debugability] Refactor and fix DDP model check during init

Logs nccl async error handling in ddp logger

Differential Revision: [D26709030](https://our.internmc.facebook.com/intern/diff/D26709030/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26709030/)!",pytorch
52966,rohan-varma,pr,2021-02-27T07:55:03Z,[DDP Logging] Log comm. hook in ddp logging,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52966 [DDP Logging] Log comm. hook in ddp logging**
* #52965 [DDP Logging] Log nccl_async_error_handling
* #52957 [Dist Debugality] Log key DDP metrics to stderr under debug mode.
* #52887 [Dist Debugability] Refactor and fix DDP model check during init

Logs registerd comm hook if there is one
Differential Revision: [D26709388](https://our.internmc.facebook.com/intern/diff/D26709388/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26709388/)!",pytorch
53059,r-barnes,pr,2021-03-01T23:19:55Z,More types for torch/fb/hpc,"Differential Revision: D26707036

",pytorch
53117,vfdev-5,pr,2021-03-02T16:23:56Z,[operator_benchmark] Added channels last 3d option to interpolate test,"Description:

- Added channels last 3d option to interpolate test
  - split config non-4d into two : 3d and 5d


",pytorch
53166,supriyar,pr,2021-03-03T03:08:20Z,[quant][fx] add _remove_qconfig flag to convert_fx,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53166 [quant][fx] add _remove_qconfig flag to convert_fx**

Summary:
Context: For fx modules that consist of scriptmodules, calling
delattr(module, 'qconfig') throws an attribute error. will follow up
with a separate issue/repro to fix this problem

This PR adds a temporary flag to convert_fx API to preserve the qconfig attributes on the converted model
We will remove this flag once we reach a conclusion on calling delattr on scriptmodules

Test Plan:

python test/test_quantization.py test_preserve_qconfig
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D26771518](https://our.internmc.facebook.com/intern/diff/D26771518)",pytorch
53186,vfdev-5,pr,2021-03-03T15:15:53Z,[operator benchmarks] Added more modes to interpolation tests,"Description:
- Added more modes: bicubic and nearest to interpolation tests
- Added a test case for downsampling a small image",pytorch
53211,vfdev-5,pr,2021-03-03T22:03:15Z,Optimized bilinear interpolation channels last case using TensorIterator,"Related to #10482

A follow-up PR to https://github.com/pytorch/pytorch/pull/51653/ 

Description:
- Replaced linear channels last manually vectorized implementation with TensorIterator channels last implementation (vectorizable by compiler).

**Important:** this implementation is not fast for all tested cases. Positive speedups ranges from 1.5x - 22x. But
there is a slowdown for [32, 128, 64, 64] -> [32, 128, 32, 32] case. See below.

### Results

<details>
<summary>
Interpolation 2d - 6 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 320, 320] | [256, 256] | False | True | 1.2061 | 0.0754 | 16.0017
[1, 3, 320, 320] | [512, 512] | False | True | 4.8571 | 0.4026 | 12.0629
--> Slowdown [32, 128, 64, 64] | [32, 32] | False | True | 6.4191 | 8.9945 | 0.7137
[32, 128, 64, 64] | [128, 128] | False | True | 164.4186 | 67.4578 | 2.4374
--> Unreliable result [32, 128, 64, 64] | [32, 32] | True | False | 2.3113 | 2.3300 | 0.9920
[32, 128, 64, 64] | [128, 128] | True | False | 52.5961 | 51.9378 | 1.0127
[1, 3, 500, 500] | [256, 256] | False | True | 1.2072 | 0.0818 | 14.7502
[1, 3, 500, 500] | [800, 800] | False | True | 12.1309 | 0.5503 | 22.0435


</details>

<details>
<summary>
Interpolation 3d - 6 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 15.5705 | 1.2826 | 12.1394
[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 271.9735 | 25.1804 | 10.8010


</details>

<details>
<summary>
Interpolation 2d - 1 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 320, 320] | [256, 256] | False | True | 1.5425 | 0.3242 | 4.7578
[1, 3, 320, 320] | [512, 512] | False | True | 6.2044 | 1.6701 | 3.7151
--> Slowdown [32, 128, 64, 64] | [32, 32] | False | True | 27.6637 | 44.8813 | 0.6164
[32, 128, 64, 64] | [128, 128] | False | True | 465.7710 | 284.7859 | 1.6355
--> Unreliable result [32, 128, 64, 64] | [32, 32] | True | False | 9.9654 | 13.8053 | 0.7219
[32, 128, 64, 64] | [128, 128] | True | False | 205.2611 | 204.0933 | 1.0057
[1, 3, 500, 500] | [256, 256] | False | True | 1.5461 | 0.3591 | 4.3060
[1, 3, 500, 500] | [800, 800] | False | True | 15.3655 | 2.9365 | 5.2326

</details>

<details>
<summary>
Interpolation 3d - 1 thread(s)
</summary>


In | Out | Is contiguous | Channels last | master | this PR | speed-up
---|---|---|---|---|---|---
[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 17.9978 | 6.0110 | 2.9942
[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 381.1965 | 112.3014 | 3.3944


</details>


<details>
<summary>
Versions and build configs
</summary>

PyTorch master: 1.9.0.dev20210302+cpu
PyTorch master build setting:
```
BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON,
```

PR : 1.9.0a0+git3d72ad9
PR build setting:
```
BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/lib/ccache/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON,
```
</details>

Full results for all cases are [here](https://github.com/vfdev-5/interpolate-tensoriterator/blob/master/step_six/pr_1.9.0a0%2Bgit3d72ad9_vs_pth_1.9.0.dev20210302%2Bcpu_results.md.3). Raw data: [here](https://github.com/vfdev-5/interpolate-tensoriterator/blob/master/step_six/pr_results_1.9.0a0%2Bgit3d72ad9.log.save.3) and [here](https://github.com/vfdev-5/interpolate-tensoriterator/blob/master/step_six/pth_nightly_results_1.9.0.dev20210302%2Bcpu.log.save.3).

This description is based on the benchmarks and the code from [here](https://github.com/vfdev-5/interpolate-tensoriterator/tree/master/step_six).


Joint work with Francisco Massa (@fmassa).

",pytorch
53212,rohan-varma,pr,2021-03-03T22:14:42Z,[torch.futures] Add note about error handling for non-chained futures.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53212 [torch.futures] Add note about error handling for non-chained futures.**

Ran into a strange issue with error handling in future callbacks, more
details in https://github.com/pytorch/pytorch/issues/52166, but essentially,
after a callback throws all additional processing stops, and other futures can
never be completed, resulting in a hang. Add a note to warn about this.

In particular, the following can result in a hang:

```
    ret_fut = torch.futures.Future9)
    def test_rpc_err_in_cb(self):
        if self.rank != 0:
            return
        # this will cause the RPC to be marked with an error due to timeout. 
        fut = rpc.rpc_async(worker_name(self.rank + 1), slow_add, args=(torch.tensor(1), torch.tensor(1)), timeout=0.01)
        def cb(fut):
            print(f""Fut result is {fut.wait()}"") # throws
            ret_fut.set_result(fut.wait())

        fut.add_done_callback(cb)
        ret_fut.wait()
```

Differential Revision: [D26793310](https://our.internmc.facebook.com/intern/diff/D26793310/)",pytorch
53243,zheng-xq,pr,2021-03-04T06:09:22Z,Adding parallel support for the LLVM backend.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53243 Adding parallel support for the LLVM backend.**

Differential Revision: [D26906509](https://our.internmc.facebook.com/intern/diff/D26906509)",pytorch
53286,r-barnes,pr,2021-03-04T19:22:16Z,Add more kernel launch checks,"Differential Revision: D26818164

",pytorch
53347,rohan-varma,pr,2021-03-05T08:30:14Z,[JIT/futures] Raise the correct exception type,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53347 [JIT/futures] Raise the correct exception type**

Closes https://github.com/pytorch/pytorch/issues/51673
Modifies at::ivalue::Future exception handling code when running `then` callbacks to throw the correct error (i.e. the python error that was encountered in user-defined cb, as opposed to always RuntimeError).

This allows errors to be caught and handled appropriately by python users.

Differential Revision: [D26800749](https://our.internmc.facebook.com/intern/diff/D26800749/)",pytorch
53356,lezcano,pr,2021-03-05T13:17:56Z,Port put_ and take from TH to ATen,"The two ports were don together, as they can be implemented with the same kernel. In TH, they were already implemented with the same kernel.

Resolves https://github.com/pytorch/pytorch/issues/24751 
Resolves https://github.com/pytorch/pytorch/issues/24614 
Resolves https://github.com/pytorch/pytorch/issues/24640 
Resolves https://github.com/pytorch/pytorch/issues/24772 

This port makes sure that it interacts correctly with the ""deterministic algorithms"" flag, as done in https://github.com/pytorch/pytorch/pull/51388

This PR also makes these two functions correct in the following aspects (all of them added to the tests as well):
- Support for complex numbers
- Correct handling of scalar inputs and zero-dimensional inputs
- Implementation that does not do any copies nor sorting of any of the input tensors
- Faster and more correct implementation of the backwards (now it works as it should when `source.shape() != index.shape()`)
- Now `put_(..., accumulate=True)` is implemented correctly with atomic operations on GPU / CPU (when possible) and is deterministic (modulo the loss of precision that might happen due to the reordering of a sum of floats)
- Adds the `torch.put` function that was missing, (`index_put` exists, for example)
- Corrected docs

It also adds a much more thorough testing to the operations and their gradients.

There is a BC-breaking change, and that is that now we check that the inputs do not overlap in the `put_` operation. This was handled (some of the cases, other cases were wrong) in the TH implementation by making contiguous copies of the inputs. How should we handle this one?

**Edit.** Benchmarks:
<details>
<summary>Script</summary>

```python
from IPython import get_ipython
import torch
from itertools import product

torch.manual_seed(13)
torch.set_num_threads(1)

ipython = get_ipython()

cpu = torch.device('cpu')
cuda = torch.device('cuda')


def run_test(ndims, size, index_len, device, cmd):
    print(f""cmd: {cmd}, ndims: {ndims}, tensor_size: {size}, index_len: {index_len}, device: {device}"")

    large_tensor = torch.rand(*([size] * ndims), device=device)
    small_tensor = torch.rand((index_len,), device=device)
    index = torch.randint(size * ndims, (index_len,), dtype=torch.long, device=device)
    if cmd == ""put"":
        command = ""large_tensor.put_(index, small_tensor, accumulate=False)""
        if device == cuda:
            command += ""; torch.cuda.synchronize()""
    elif cmd == ""accumulate"":
        command = ""large_tensor.put_(index, small_tensor, accumulate=True)""
        if device == cuda:
            command += ""; torch.cuda.synchronize()""
    elif cmd == ""take"":
        command = ""torch.take(large_tensor, index)""
        if device == cuda:
            command += ""; torch.cuda.synchronize()""
    ipython.magic(f""timeit {command}"")
    print()


for method, device in product([""accumulate"", ""put"", ""take""], [cpu, cuda]):
    run_test(3, 1000, 10, device, method)
    run_test(3, 1000, 1000, device, method)
    run_test(3, 1000, 10000, device, method)
    run_test(2, 10000, 100000, device, method)
```
</details>

```python
put_(accumulate=False)
```

<details>
<summary>ATen CPU (1.5x - 2x speedup)</summary>

```python
cmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
1.05 Âµs Â± 2.35 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
3.15 Âµs Â± 5.13 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
21.6 Âµs Â± 13.1 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
238 Âµs Â± 781 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```
</details>

<details>
<summary>TH CPU</summary>

```python
cmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
722 ns Â± 2.67 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
4.89 Âµs Â± 18.1 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
42.5 Âµs Â± 96.3 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
428 Âµs Â± 774 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```
</details>
<details>
<summary>ATen GPU (same speed)</summary>

```python
cmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
8.99 Âµs Â± 16 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
10.4 Âµs Â± 24.4 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
10.4 Âµs Â± 11.2 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
15.6 Âµs Â± 1.12 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```
</details>

<details>
<summary>TH GPU</summary>

```python
cmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
8.44 Âµs Â± 31.4 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
9.09 Âµs Â± 4.3 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
9.77 Âµs Â± 0.998 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
15.8 Âµs Â± 5.7 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```
</details>

```python
put_(accumulate=True)
```

<details>
<summary>ATen CPU (x2 speedup)</summary>

```python
cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
1.12 Âµs Â± 2.91 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
3.14 Âµs Â± 2.05 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
20.8 Âµs Â± 25.9 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
264 Âµs Â± 263 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```
</details>

<details>
<summary>TH CPU</summary>

```python
cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
814 ns Â± 1.87 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
5.11 Âµs Â± 6.02 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
43.9 Âµs Â± 49.4 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
442 Âµs Â± 1.07 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```
</details>
<details>
<summary>ATen GPU (3x - 11x speedup)</summary>

```python
cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
9.01 Âµs Â± 14.1 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
10.4 Âµs Â± 15.6 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
10.3 Âµs Â± 44.3 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
12.6 Âµs Â± 19 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```
</details>

<details>
<summary>TH GPU</summary>

```python
cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
34.7 Âµs Â± 131 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
38.2 Âµs Â± 116 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
61.2 Âµs Â± 50.4 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)

cmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
140 Âµs Â± 24.2 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```
</details>

```python
take()
```

<details>
<summary>ATen CPU (1.1x speedup)</summary>

```python
cmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
1.18 Âµs Â± 2.34 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
2.79 Âµs Â± 2.96 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
16.6 Âµs Â± 10.4 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
161 Âµs Â± 984 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```
</details>

<details>
<summary>TH CPU</summary>

```python
cmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu
1.1 Âµs Â± 3.14 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu
2.93 Âµs Â± 7.31 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu
18.6 Âµs Â± 14.5 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu
178 Âµs Â± 139 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```
</details>
<details>
<summary>ATen GPU (same speed)</summary>

```python
cmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
9.38 Âµs Â± 23.1 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
10.7 Âµs Â± 9.77 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
10.6 Âµs Â± 107 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
11.5 Âµs Â± 21.1 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```
</details>

<details>
<summary>TH GPU</summary>

```python
cmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda
9.31 Âµs Â± 7.57 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda
9.52 Âµs Â± 5.78 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda
9.73 Âµs Â± 17.6 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

cmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda
11.7 Âµs Â± 5.7 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```
</details>


cc @mruberry 

",pytorch
53408,micmelesse,pr,2021-03-05T21:30:43Z,[ROCM] load hipfft separately from rocfft,"This PR makes changes to how hipfft is loaded in pytorch. hipfft is packaged in a separate library to rocfft following rocm 4.1. 

We check the rocm version and if it is past rocm 4.1 we load hipfft in addition to rocfft. ",pytorch
53411,micmelesse,pr,2021-03-05T21:50:33Z,[ROCM] Fix hipfft transform type error,"This PR enable some failing unit tests for fft in pytorch on ROCM.

The reason these tests were failing was due to an error in how hipfft was executed for different transform types for float inputs causing a mismatch error when compared to baselines.

We solved the problem by calling hipfft with the right config for each transformation type.

There PR doesnot enable all fft tests. There are still other issues that need to be resolved before that can happen.
",pytorch
53480,ngimel,pr,2021-03-08T00:34:15Z,Filter 0's returned by exponential distribution,"Fixes #48841 for half datatype (it was fixed for other datatypes before). 
The reason for #48841 happening for half was that `exponential_` for half was producing 0s. 
Exponential distribution implementation on cuda is here https://github.com/pytorch/pytorch/blob/e08aae261397b8da3e71024bbeddfe0487185d1d/aten/src/ATen/native/cuda/DistributionTemplates.h#L535-L545
with `transformation::exponential` defined here
https://github.com/pytorch/pytorch/blob/e08aae261397b8da3e71024bbeddfe0487185d1d/aten/src/ATen/core/TransformationHelper.h#L113-L123
It takes a uniformly distributed random number and takes `log` of it. If necessary, the result is then converted to low precision datatype (half). To avoid 0's, before applying `log`,  ones are replaced with std::nextafter(1,0). This seems fine, because log(1-eps) is still representable in half precision (`torch.tensor([1.], device=""cuda"").nextafter(torch.tensor([0.], device=""cuda"")).log().half()` produces 5.96e-8) , so casting to `scalar_t` should work. However, since fast log approximation is used (`__logf`), the log result is ~3e-9 instead of more accurate 5.96e-8, and underflows when casting to half. Using `::log` instead of fast approximation fixes it, however, it comes with ~20% perf penalty on exponential kernel for fp32 datatype, probably more for half. 

Edit: alternative approach used now is to filter all small values returned by transformation. The result is equivalent to squashing of 1's to 1-eps that was used before, and computing correct log of 1-eps (which is -eps, exactly equal even for doubles). This doesn't incur noticeable performance hit. 
",pytorch
53668,r-barnes,pr,2021-03-10T00:17:35Z,Type annotations for torch.jit.export and friends,"Differential Revision: D26927210

",pytorch
53711,lezcano,pr,2021-03-10T14:29:36Z,Faster backwards for cumsum and cumprod,"Provides a faster formula for `cumprod` in the case when the input has zeros. This formula is non-differentiable, so we leave the previous formula for the cases when `at::GradMode::is_enabled()`.

This new formula gives up to x10 and x30 speed-ups in CPU and GPU (see the benchmarks below).

The `cumsum` backward formula was rewritten so that no copies are necessary. We also removed a double negation in its formula. This gives a significant speed-up in CPU, while being almost as efficient as the formula with copies in GPU. We can see this speed-up when comparing the ""No zeros"" part of the benchmark.

Benchmarks:

nb. It is worth noting that the script tests the forward and the backward for `cumprod`, so the speed-ups should be even larger than those announced here.
<details>
<summary>Script</summary>

```python
from IPython import get_ipython
import torch
from itertools import product

torch.manual_seed(13)
torch.set_num_threads(1)

ipython = get_ipython()

cpu = torch.device('cpu')
cuda = torch.device('cuda')


def run_test(ndims, size, size_prod, zeros, device):
    print(f""ndims: {ndims}, tensor_size: {size}, size_prod: {size_prod}, zeros: {zeros}, device: {device}"")

    for dim in range(ndims):
        sizes = ndims * [size]
        sizes[dim] = size_prod
        tensor = torch.rand(*sizes, device=device)
        with torch.no_grad():
            if zeros:
                # Set 0.1 of them to zero
                p_drop = 0.1
                mask = torch.full_like(tensor, 1.0 - p_drop)
                tensor = tensor * torch.bernoulli(mask)
            else:
                tensor = tensor + 1e-3
        tensor.requires_grad_()
        grad = torch.ones_like(tensor)
        # We test both forward + backward, meaning that the speed-up is actually greater than reported
        # That being said, this is more realistic than doing `retain_graph=True`
        command = ""torch.autograd.grad([tensor.cumprod(dim)], [tensor], grad_outputs=[grad])""
        if device == cuda:
            command += ""; torch.cuda.synchronize()""
        ipython.magic(f""timeit {command}"")
    print()


for device, zeros in product([cuda, cpu], [True, False]):
    run_test(3, 300, 10, zeros, device)
    run_test(3, 300, 100, zeros, device)
    if device == cuda:
        run_test(3, 300, 300, zeros, device)
```


</details>

<details>
<summary>CPU This PR  (Some regression small tensors, x4 speed-up large tensors)</summary>

```
Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cpu
28.2 ms Â± 12.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
29.8 ms Â± 78.9 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
24.5 ms Â± 29.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cpu
414 ms Â± 3.63 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
428 ms Â± 4.12 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
382 ms Â± 3.18 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

No Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cpu
3.11 ms Â± 9.72 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
3.83 ms Â± 3.7 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
4.08 ms Â± 10.1 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cpu
92.2 ms Â± 113 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
101 ms Â± 101 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
87 ms Â± 170 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```                                                                                                                                           
</details>

<details>
<summary>CUDA This PR (7-30x speed-up)</summary>

```

Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cuda
1.46 ms Â± 2.07 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.48 ms Â± 3.51 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.93 ms Â± 8.07 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cuda
10.5 ms Â± 914 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
10.6 ms Â± 509 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
11.7 ms Â± 864 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 300, zeros: True, device: cuda
30.3 ms Â± 5.16 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
30.6 ms Â± 6.44 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
32.2 ms Â± 2.34 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

No Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cuda
248 Âµs Â± 335 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
252 Âµs Â± 186 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
438 Âµs Â± 254 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cuda
2.1 ms Â± 193 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
2.16 ms Â± 380 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
2.59 ms Â± 398 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 300, zeros: False, device: cuda
6.3 ms Â± 857 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
6.39 ms Â± 288 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
7.15 ms Â± 233 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

</details>

<details>
<summary>CPU master</summary>

```
Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cpu
8.27 ms Â± 12.4 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
10.8 ms Â± 13.2 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
28.2 ms Â± 74.4 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cpu
1.53 s Â± 116 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.95 s Â± 4.38 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.86 s Â± 3.58 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

No Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cpu
3.42 ms Â± 20 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
4.25 ms Â± 3.65 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
4.34 ms Â± 3.04 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cpu
104 ms Â± 148 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
117 ms Â± 99.5 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
94.8 ms Â± 125 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```

</details>


<details>
<summary>CUDA master</summary>

```
Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cuda
912 Âµs Â± 431 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.05 ms Â± 2.46 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
2.74 ms Â± 381 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cuda
71.3 ms Â± 7.91 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
85.4 ms Â± 9.82 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
119 ms Â± 6.21 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 300, size_prod: 300, zeros: True, device: cuda
646 ms Â± 103 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
776 ms Â± 81.7 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
917 ms Â± 160 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)

No Zeros:
ndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cuda
301 Âµs Â± 893 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
308 Âµs Â± 236 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
592 Âµs Â± 140 ns per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

ndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cuda
2.61 ms Â± 375 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
2.68 ms Â± 524 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
3.38 ms Â± 736 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 300, size_prod: 300, zeros: False, device: cuda
7.89 ms Â± 848 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
8.03 ms Â± 517 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
9.24 ms Â± 405 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

</details>

cc @nikitaved ",pytorch
53746,lithuak,pr,2021-03-10T21:21:20Z,"Fix typo ""informations"" -> ""information""","Hey, fixing the [uncountable](https://www.oxfordlearnersdictionaries.com/definition/american_english/information) noun to the proper form.
",pytorch
53763,neerajprad,pr,2021-03-10T22:53:47Z,[CherryPick] Fixes for distribution validation checks,"This has fixes for distribution validation checks from the following PRs in master:
 - #52763
 - #53600

These fixes are required for a consistent behavior given that distribution argument validation is now enabled by default. Together these PRs ensure that all distribution `log_prob` methods call sample validation when `validate_args=True` and, conversely, turn off all validation checks when `validate_args=False`.",pytorch
53773,rohan-varma,pr,2021-03-10T23:43:23Z,Gloo-only CPU-based monitored barrier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* #54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* **#53773 Gloo-only CPU-based monitored barrier**

Closes https://github.com/pytorch/pytorch/issues/52876

Implements a barrier by doing send/recv to rank 0, and rank 0 waits for these requests and on timeout, throws an exception indicating which rank did not join in the given timeout.

This barrier is only intended for CPU use cases and built into process group gloo, and will be used for debugging synchronization/hang issues.

Differential Revision: [D26921357](https://our.internmc.facebook.com/intern/diff/D26921357/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26921357/)!",pytorch
53787,rohan-varma,pr,2021-03-11T03:29:49Z,Expose dist.monitored_barrier() API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* #54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* **#53787 Expose dist.monitored_barrier() API**
* #53773 Gloo-only CPU-based monitored barrier

Per title, exposes a python-based monitored barrier API that we can use as part of debugability and may be useful for user applications.

Differential Revision: [D26965127](https://our.internmc.facebook.com/intern/diff/D26965127/)",pytorch
53793,rohan-varma,pr,2021-03-11T06:02:00Z,[DDP] Fix wrong call to dist.get_rank(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #53795 [DDP] add _distributed_rank helper function
* **#53793 [DDP] Fix wrong call to dist.get_rank()**

This call should pass in the process group so it works appropriately
for subgroups instead of whole world being passed into DDP.

Aside: This wasn't caught by tests since we don't have good testing around
passing subgroups into DDP, I believe nearly all tests use the entire world.
Should we add better testing for subgroups which may potentially bring up more
subtle bugs?

Differential Revision: [D26972367](https://our.internmc.facebook.com/intern/diff/D26972367/)",pytorch
53795,rohan-varma,pr,2021-03-11T06:02:07Z,[DDP] add _distributed_rank helper function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53795 [DDP] add _distributed_rank helper function**
* #53793 [DDP] Fix wrong call to dist.get_rank()

There are 4 calls in ddp implementation to dist.get_rank(), move these
to a helper property to ensure that users don't actually call `dist.get_rank()`
instead of `dist.get_rank(self.process_group)`.

Keeping API private for now because not sure if there is a user need to call `model.distributed_rank`, but can make it public if we think it's a useful api.

Differential Revision: [D26972368](https://our.internmc.facebook.com/intern/diff/D26972368/)",pytorch
53829,r-barnes,pr,2021-03-11T18:12:10Z,More types for torch/fb/hpc,"Differential Revision: D26955421

",pytorch
53898,lezcano,pr,2021-03-12T15:47:12Z,Support for Half / bfloat16 / index_select and better testing,"Added the support for half / bfloat / bool for `index_select`, as suggested by @ngimel in
https://github.com/pytorch/pytorch/issues/49707#issuecomment-788140578

For the tests to pass, I also added the support for `index_add`.

I added `OpInfo` tests for `index_add` and more thorough forward tests for `index_select` to test these changes. 

While doing so, I found that the support for scalar types in the derivative of `index_add` was not correct, so I corrected it.

Resolves https://github.com/pytorch/pytorch/issues/49707

It should also resolve similar issues that I encountered when porting `index_copy`, `take` and `put`. ",pytorch
53913,rohan-varma,pr,2021-03-12T18:41:42Z,[DDP] remove dedupe check in reducer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

https://github.com/pytorch/pytorch/pull/53279/files has landed
deduplicating the shared params in python before constructing reducer. Because
of this, we no longer need the changes in
https://github.com/pytorch/pytorch/pull/46755/files.

This is already tested by `test_ddp_shared_grad_acc_unused_params` and
`test_ddp_weight_sharing`

Differential Revision: [D27015466](https://our.internmc.facebook.com/intern/diff/D27015466/)",pytorch
53919,rohan-varma,pr,2021-03-12T20:26:34Z,[DDP] remove duplicate var check in reducer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53919 [DDP]  remove dedupe check in reducer**

https://github.com/pytorch/pytorch/pull/53279/files has landed
deduplicating the shared params in python before constructing reducer. Because
of this, we no longer need the changes in
https://github.com/pytorch/pytorch/pull/46755/files.

This is already tested by `test_ddp_shared_grad_acc_unused_params` and
`test_ddp_weight_sharing`

Differential Revision: [D27015466](https://our.internmc.facebook.com/intern/diff/D27015466/)",pytorch
54002,lezcano,pr,2021-03-15T14:27:16Z,SVD docs improved,"- Corrected a few errata in the SVD docs
- Made the notation more uniform (refer to `Vh` in `linalg.svd`, always use double tilts...)
- Wrote a better explanation about why the gradients of `U` and `V` are not well-defined when the input is complex or real but has repeated singular values. The previous one pointed to a somewhat obscure post on gauge theory.",pytorch
54037,r-barnes,pr,2021-03-15T23:33:33Z,More types for torch,"Differential Revision: D27054755

",pytorch
54084,r-barnes,pr,2021-03-16T19:24:00Z,Pyre-ify torch.jit.interface's,"Differential Revision: D27075597

",pytorch
54117,rohan-varma,pr,2021-03-17T00:27:24Z,[NCCL] Add more details for checkForNCCLErrors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54117 [NCCL] Add more details for checkForNCCLErrors**

https://github.com/pytorch/pytorch/pull/45950 enhanced our NCCL logging errors so that we add some basic debug information about what when wrong when erroring out with a NCCL error.

However, that PR only used the added function for `C10D_NCCL_CHECK` which is used to check the return values of NCCL calls. However, in ProcessGroupNCCL we also have `checkForNCCLErrors` which checks for errors on nccl communicators, and in case of errors it would be good to have this logging there too.

Also renames the function s/errorMessage/getNcclErrorDetailStr

Differential Revision: [D27100497](https://our.internmc.facebook.com/intern/diff/D27100497/)",pytorch
54122,zheng-xq,pr,2021-03-17T04:05:56Z,Resubmit: Adding parallel support for the LLVM backend.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54122 Resubmit: Adding parallel support for the LLVM backend.**

Test plan:
  * USE_TBB=1 ATEN_THREADING=TBB python setup.py develop --cmake
  * USE_TBB=1 ATEN_THREADING=NATIVE python setup.py develop --cmake
  * USE_TBB=1 ATEN_THREADING=OMP python setup.py develop --cmake
  * cd build; ninja bin/tensorexpr_bench
  * bin/test_tensorexpr --gtest_filter=""*Parallel*""

Differential Revision: [D27109802](https://our.internmc.facebook.com/intern/diff/D27109802)",pytorch
54214,r-barnes,pr,2021-03-17T21:53:44Z,Final kernel launch checks,"Differential Revision: D27138004

",pytorch
54219,rohan-varma,pr,2021-03-17T23:10:06Z,[DDP] Remove redundant pass statement,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* #54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* **#54219 [DDP] Remove redundant pass statement**
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

There is no need for this ``pass``.

Differential Revision: [D27105234](https://our.internmc.facebook.com/intern/diff/D27105234/)",pytorch
54220,rohan-varma,pr,2021-03-17T23:10:12Z,[DDP] Use monitored barrier in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* #54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54220 [DDP] Use monitored barrier in DDP**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54220 [DDP] Use monitored barrier in DDP**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54220 [DDP] Use monitored barrier in DDP**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* **#54220 [DDP] Use monitored barrier in DDP**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

Adds `dist.monitored_barrier()` call when after verifying model consistency across ranks to DDP constructor.

This is only enabled for debug modes > OFF, otherwise there is a slight perf hit during initialization. Will post benchmarks showing perf impact once they are done running.

Tested by ensuring the proper error propagation for the various debug modes when `dist.monitored_barrier()` is called.

Differential Revision: [D27106698](https://our.internmc.facebook.com/intern/diff/D27106698/)",pytorch
54221,rohan-varma,pr,2021-03-17T23:15:27Z,Tag distributed team for review for /torch/nn/parallel,This folder contains the DDP python interface as well as several misc. communication files. ,pytorch
54349,micmelesse,pr,2021-03-19T20:07:52Z,[ROCM] load only hipfft separately past rocm4.1,"This PR is a follow up to https://github.com/pytorch/pytorch/pull/53408. 

It only loads hipfft if the version is rocm 4.1 or after and stops loading rocfft. This was done to resolve some issues observed in our internal ci due to conflicts. 


",pytorch
54386,peterjc123,pr,2021-03-21T07:27:04Z,[test] vc 14.28 fix for nvcc,"Fixes #{issue number}
",pytorch
54391,peterjc123,pr,2021-03-21T17:42:35Z,[test] alternative fix for vc 16.8 with nvcc,"Fixes #{issue number}
",pytorch
54453,rohan-varma,pr,2021-03-22T21:14:11Z,[DDP] Remove data_ptr assert check after pin_memory,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54453 [DDP] Remove data_ptr assert check after pin_memory**

It seems like this assert is not always necessarily true, such as if
the tensor being pinned was already somehow pinned, or if cuda is run with UVM
enabled. This check was added in
https://github.com/pytorch/pytorch/pull/53160/files but should not be necessary
to fix the race condition that the PR fixed.

Differential Revision: [D27243485](https://our.internmc.facebook.com/intern/diff/D27243485/)",pytorch
54458,rohan-varma,pr,2021-03-22T23:21:47Z,"Back out ""[pytorch][PR] Avoid DDP race condition with find_unused_parameters=True when all params are used""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54458 Back out ""[pytorch][PR] Avoid DDP race condition with find_unused_parameters=True when all params are used""**

Original commit changeset: 3e0ed91b7b5c

Differential Revision: [D27247595](https://our.internmc.facebook.com/intern/diff/D27247595/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D27247595/)!",pytorch
54475,rohan-varma,pr,2021-03-23T02:35:50Z,[Futures] set completed_ after postMarkCompletedHook,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54476 [Futures] Bump log verbosity when ignoring cb errors in python future.
* **#54475 [Futures] set completed_ after postMarkCompletedHook**

If completed_ is set before postMarkCompletedHook, errors in postMarkCompletedHook may not be correctly propagated as when we call `setError`, we throw an error claiming the future is already completed.

Since `postMarkCompletedHook` is additional processing installed by the framework for e.g. proper cuda stream handling, it makes sense that we only consider a future as completed_ after this processing has completed.

Differential Revision: [D27252965](https://our.internmc.facebook.com/intern/diff/D27252965/)",pytorch
54476,rohan-varma,pr,2021-03-23T02:35:57Z,[Futures] Bump log verbosity when ignoring cb errors in python future.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54476 [Futures] Bump log verbosity when ignoring cb errors in python future.**
* #54475 [Futures] set completed_ after postMarkCompletedHook

Per title. For `add_done_callback`, we log but swallow exceptions in order to keep consistent with what concurrent.futures python library does, see discussion in https://github.com/pytorch/pytorch/pull/45675.

Although, it would be good to improve the verbosity here as this can be a source of confusion if users are setting a different future via `add_done_callback`, and an error is hit resulting in an unexpected hang (see https://github.com/pytorch/pytorch/issues/52132 for more details on how this can happen).

Differential Revision: [D27253004](https://our.internmc.facebook.com/intern/diff/D27253004/)",pytorch
54500,vfdev-5,pr,2021-03-23T12:48:50Z,Optimized generic interpolation using TensorIterator (keeps original 2d/3d channels last impl),"Related to #10482

A follow-up PR to https://github.com/pytorch/pytorch/pull/51653/ 

Description:
- Replaces nearest/linear/cubic implementations with generic interpolation implementation
- Retains 2d/3d channels last implementation due to perf slowdown for 1 thread (see below appendix note)

Speed-ups for cases:
- upsample_nearest channels first
- upsample_bicubic channels first/last


### Results for this PR

<details>
<summary>

Benchmark results between 8518b0e (master) and 73137d8 (this PR)

</summary>

```
Description:
- 20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.6
- 20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.1
- 20210331-092940_pr_results_1.9.0a0+git73137d8.6
- 20210331-092940_pr_results_1.9.0a0+git73137d8.1

[---------- upsample_bilinear2d channels_first contiguous torch.float32 ----------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          331.8       |          334.6     
      [1, 3, 320, 320] -> (512, 512)   |         1261.7       |         1271.5     
      [32, 128, 64, 64] -> (32, 32)    |        10164.6       |        10251.4     
      [32, 128, 64, 64] -> (128, 128)  |       195966.1       |       197141.8     
      [1, 3, 500, 500] -> (256, 256)   |          347.7       |          348.3     
      [1, 3, 500, 500] -> (800, 800)   |         3044.9       |         3071.4     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |           76.1       |           77.0     
      [1, 3, 320, 320] -> (512, 512)   |          244.8       |          247.6     
      [32, 128, 64, 64] -> (32, 32)    |         2329.4       |         2315.8     
      [32, 128, 64, 64] -> (128, 128)  |        47855.3       |        49047.7     
      [1, 3, 500, 500] -> (256, 256)   |           78.1       |           78.7     
      [1, 3, 500, 500] -> (800, 800)   |          569.3       |          575.6     

Times are in microseconds (us).

[------- upsample_bilinear2d channels_first non-contiguous torch.float32 --------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         339.0        |         340.3      
      [1, 3, 320, 320] -> (512, 512)  |        1266.1        |        1277.3      
      [1, 3, 500, 500] -> (256, 256)  |         348.8        |         351.3      
      [1, 3, 500, 500] -> (800, 800)  |        3054.5        |        3077.3      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |          76.6        |          77.4      
      [1, 3, 320, 320] -> (512, 512)  |         246.0        |         248.1      
      [1, 3, 500, 500] -> (256, 256)  |          78.3        |          79.5      
      [1, 3, 500, 500] -> (800, 800)  |         572.2        |         580.0      

Times are in microseconds (us).

[--------- upsample_bilinear2d channels_last non-contiguous torch.float32 --------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         965.4        |         964.9      
      [1, 3, 320, 320] -> (512, 512)   |        3856.2        |        3866.8      
      [32, 128, 64, 64] -> (32, 32)    |        5808.3        |        5812.8      
      [32, 128, 64, 64] -> (128, 128)  |       99575.2        |       97226.2      
      [2, 128, 64, 46] -> (32, 32)     |         110.5        |         109.0      
      [2, 128, 64, 46] -> (128, 128)   |        1662.3        |        1612.0      
      [1, 128, 64, 46] -> (32, 32)     |          55.6        |          55.5      
      [1, 128, 64, 46] -> (128, 128)   |         467.0        |         463.9      
      [1, 3, 500, 500] -> (256, 256)   |         967.7        |         966.7      
      [1, 3, 500, 500] -> (800, 800)   |        9394.7        |        9436.6      
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         962.2        |         965.4      
      [1, 3, 320, 320] -> (512, 512)   |        3844.3        |        3844.3      
      [32, 128, 64, 64] -> (32, 32)    |        2270.0        |        2267.6      
      [32, 128, 64, 64] -> (128, 128)  |       31909.7        |       32106.5      
      [2, 128, 64, 46] -> (32, 32)     |          61.3        |          59.9      
      [2, 128, 64, 46] -> (128, 128)   |         912.3        |         893.5      
      [1, 128, 64, 46] -> (32, 32)     |          55.5        |          55.3      
      [1, 128, 64, 46] -> (128, 128)   |         467.0        |         466.4      
      [1, 3, 500, 500] -> (256, 256)   |         967.2        |         971.1      
      [1, 3, 500, 500] -> (800, 800)   |        9383.2        |        9417.4      

Times are in microseconds (us).

[------ upsample_linear1d channels_first contiguous torch.float32 -------]
                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        513.5         |         521.8      
      [4, 512, 320] -> [512]  |        999.0         |        1011.8      
6 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        103.7         |         104.9      
      [4, 512, 320] -> [512]  |        192.2         |         194.9      

Times are in microseconds (us).

[------------- upsample_trilinear3d channels_first contiguous torch.float32 -------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          5.4         |          5.5       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        111.2         |        111.1       
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          1.1         |          1.0       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |         23.4         |         23.2       

Times are in milliseconds (ms).

[----------- upsample_trilinear3d channels_last non-contiguous torch.float32 ------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        13521.9       |        12939.9     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       244561.3       |       236595.6     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          362.2       |          365.5     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        38141.4       |        37957.7     
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        12980.4       |        12962.7     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       236256.4       |       236364.5     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          367.9       |          393.2     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        38222.5       |        38198.3     

Times are in microseconds (us).

[----------- upsample_nearest2d channels_first contiguous torch.float32 ----------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         1205.7       |          107.2     
      [1, 3, 320, 320] -> (512, 512)   |         4793.5       |          357.7     
      [32, 128, 64, 64] -> (32, 32)    |        26550.0       |         6227.1     
      [32, 128, 64, 64] -> (128, 128)  |       341140.3       |       116404.4     
      [1, 3, 500, 500] -> (256, 256)   |         1208.6       |          122.9     
      [1, 3, 500, 500] -> (800, 800)   |        11648.0       |          848.1     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          220.5       |           32.6     
      [1, 3, 320, 320] -> (512, 512)   |          865.4       |           78.1     
      [32, 128, 64, 64] -> (32, 32)    |         4890.9       |         2201.2     
      [32, 128, 64, 64] -> (128, 128)  |        73533.8       |        32315.4     
      [1, 3, 500, 500] -> (256, 256)   |          222.3       |           35.0     
      [1, 3, 500, 500] -> (800, 800)   |         2107.5       |          170.7     

Times are in microseconds (us).

[----------- upsample_nearest2d channels_first contiguous torch.uint8 -----------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        1457.0        |         310.7      
      [1, 3, 320, 320] -> (512, 512)  |        5808.0        |        1196.6      
      [1, 3, 500, 500] -> (256, 256)  |        1460.9        |         312.7      
      [1, 3, 500, 500] -> (800, 800)  |       14094.3        |        2903.5      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         264.8        |          66.8      
      [1, 3, 320, 320] -> (512, 512)  |        1046.0        |         228.9      
      [1, 3, 500, 500] -> (256, 256)  |         266.0        |          68.0      
      [1, 3, 500, 500] -> (800, 800)  |        2546.6        |         535.8      

Times are in microseconds (us).

[-------- upsample_nearest2d channels_first non-contiguous torch.float32 --------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        1284.3        |        109.9       
      [1, 3, 320, 320] -> (512, 512)  |        4870.0        |        361.6       
      [1, 3, 500, 500] -> (256, 256)  |        1482.8        |        123.3       
      [1, 3, 500, 500] -> (800, 800)  |       12050.3        |        858.8       
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         240.2        |         32.8       
      [1, 3, 320, 320] -> (512, 512)  |         886.1        |         78.4       
      [1, 3, 500, 500] -> (256, 256)  |         274.9        |         34.9       
      [1, 3, 500, 500] -> (800, 800)  |        2188.8        |        174.0       

Times are in microseconds (us).

[--------- upsample_nearest2d channels_first non-contiguous torch.uint8 ---------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        1501.9        |         312.2      
      [1, 3, 320, 320] -> (512, 512)  |        5853.4        |        1202.1      
      [1, 3, 500, 500] -> (256, 256)  |        1574.0        |         313.9      
      [1, 3, 500, 500] -> (800, 800)  |       14210.2        |        2904.5      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         277.2        |          67.2      
      [1, 3, 320, 320] -> (512, 512)  |        1059.8        |         228.9      
      [1, 3, 500, 500] -> (256, 256)  |         292.2        |          68.1      
      [1, 3, 500, 500] -> (800, 800)  |        2574.4        |         536.2      

Times are in microseconds (us).

[--------- upsample_nearest2d channels_last non-contiguous torch.float32 ---------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         746.0        |         751.1      
      [1, 3, 320, 320] -> (512, 512)   |        2967.6        |        2979.2      
      [32, 128, 64, 64] -> (32, 32)    |        3408.5        |        3379.0      
      [32, 128, 64, 64] -> (128, 128)  |       90166.4        |       90023.0      
      [2, 128, 64, 46] -> (32, 32)     |          74.8        |          74.5      
      [2, 128, 64, 46] -> (128, 128)   |        1591.2        |        1594.3      
      [1, 128, 64, 46] -> (32, 32)     |          39.3        |          39.2      
      [1, 128, 64, 46] -> (128, 128)   |         420.3        |         419.1      
      [1, 3, 500, 500] -> (256, 256)   |         751.6        |         756.3      
      [1, 3, 500, 500] -> (800, 800)   |        7222.2        |        7268.6      
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         144.9        |         140.1      
      [1, 3, 320, 320] -> (512, 512)   |         560.7        |         540.6      
      [32, 128, 64, 64] -> (32, 32)    |        1418.1        |        1418.6      
      [32, 128, 64, 64] -> (128, 128)  |       28158.4        |       26411.4      
      [2, 128, 64, 46] -> (32, 32)     |          18.4        |          17.8      
      [2, 128, 64, 46] -> (128, 128)   |         532.3        |         552.0      
      [1, 128, 64, 46] -> (32, 32)     |          13.9        |          13.6      
      [1, 128, 64, 46] -> (128, 128)   |          81.3        |          82.9      
      [1, 3, 500, 500] -> (256, 256)   |         145.9        |         141.6      
      [1, 3, 500, 500] -> (800, 800)   |        1363.4        |        1316.2      

Times are in microseconds (us).

[---------- upsample_nearest2d channels_last non-contiguous torch.uint8 ----------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         795.7        |         824.1      
      [1, 3, 320, 320] -> (512, 512)   |        3163.4        |        3274.8      
      [32, 128, 64, 64] -> (32, 32)    |         798.8        |         812.2      
      [32, 128, 64, 64] -> (128, 128)  |       25259.6        |       25453.1      
      [2, 128, 64, 46] -> (32, 32)     |          39.3        |          39.9      
      [2, 128, 64, 46] -> (128, 128)   |         493.7        |         499.9      
      [1, 128, 64, 46] -> (32, 32)     |          22.6        |          22.9      
      [1, 128, 64, 46] -> (128, 128)   |         249.7        |         254.0      
      [32, 64, 128, 64] -> (32, 32)    |         475.3        |         507.4      
      [32, 64, 128, 64] -> (128, 128)  |       13709.7        |       13767.5      
      [1, 3, 500, 500] -> (256, 256)   |         804.0        |         827.6      
      [1, 3, 500, 500] -> (800, 800)   |        7764.9        |        7982.7      
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         150.1        |         151.4      
      [1, 3, 320, 320] -> (512, 512)   |         589.5        |         592.6      
      [32, 128, 64, 64] -> (32, 32)    |         141.3        |         194.5      
      [32, 128, 64, 64] -> (128, 128)  |        6916.5        |        7445.0      
      [2, 128, 64, 46] -> (32, 32)     |          10.0        |          12.5      
      [2, 128, 64, 46] -> (128, 128)   |          95.8        |         141.1      
      [1, 128, 64, 46] -> (32, 32)     |           8.1        |          10.0      
      [1, 128, 64, 46] -> (128, 128)   |          52.5        |          74.3      
      [32, 64, 128, 64] -> (32, 32)    |          79.8        |         123.7      
      [32, 64, 128, 64] -> (128, 128)  |        3639.9        |        4087.9      
      [1, 3, 500, 500] -> (256, 256)   |         150.7        |         152.2      
      [1, 3, 500, 500] -> (800, 800)   |        1430.9        |        1440.7      

Times are in microseconds (us).

[------ upsample_nearest1d channels_first contiguous torch.float32 ------]
                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        1601.7        |        241.7       
      [4, 512, 320] -> [512]  |        3188.5        |        435.7       
6 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |         291.9        |         53.3       
      [4, 512, 320] -> [512]  |         577.8        |         88.1       

Times are in microseconds (us).

[------- upsample_nearest1d channels_first contiguous torch.uint8 -------]
                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        2010.1        |         532.3      
      [4, 512, 320] -> [512]  |        3999.7        |        1011.4      
6 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |         364.2        |         104.6      
      [4, 512, 320] -> [512]  |         722.8        |         193.5      

Times are in microseconds (us).

[-------------- upsample_nearest3d channels_first contiguous torch.float32 --------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        14801.0       |         977.5      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       217368.5       |       41577.3      
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         2670.3       |         210.7      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        42023.6       |       10971.6      

Times are in microseconds (us).

[--------------- upsample_nearest3d channels_first contiguous torch.uint8 ---------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        17151.7       |        3195.8      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       221221.0       |       50524.5      
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         3085.3       |         588.6      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        39842.0       |        9141.0      

Times are in microseconds (us).

[------------ upsample_nearest3d channels_last non-contiguous torch.float32 -------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         7694.1       |         7729.0     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       138104.6       |       138158.0     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          251.1       |          252.4     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        28991.5       |        28882.8     
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1398.3       |         1402.6     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        28056.5       |        28123.2     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |           50.8       |           51.1     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |         7595.7       |         7540.7     

Times are in microseconds (us).

[------------- upsample_nearest3d channels_last non-contiguous torch.uint8 --------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         8147.8       |         8176.2     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       114658.1       |       114992.7     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          364.3       |          356.0     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        17276.0       |        16331.0     
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1469.4       |         1476.1     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        20647.1       |        20722.6     
      [1, 16, 32, 64, 64] -> [16, 32, 32]     |           69.7       |           68.4     
      [1, 16, 32, 64, 64] -> [64, 128, 128]   |         3125.7       |         2948.2     

Times are in microseconds (us).

[----------- upsample_bicubic2d channels_first contiguous torch.float32 ----------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          5961.0      |         1680.2     
      [1, 3, 320, 320] -> (512, 512)   |         23803.7      |         6591.0     
      [32, 128, 64, 64] -> (32, 32)    |        620609.4      |        37981.6     
      [32, 128, 64, 64] -> (128, 128)  |      10120286.1      |       646305.5     
      [1, 3, 500, 500] -> (256, 256)   |          6005.4      |         1694.6     
      [1, 3, 500, 500] -> (800, 800)   |         58271.9      |        16047.6     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          6218.5      |          347.1     
      [1, 3, 320, 320] -> (512, 512)   |         24144.6      |         1253.4     
      [32, 128, 64, 64] -> (32, 32)    |        612762.5      |         6934.8     
      [32, 128, 64, 64] -> (128, 128)  |       9906221.2      |       127411.1     
      [1, 3, 500, 500] -> (256, 256)   |          6241.9      |          350.2     
      [1, 3, 500, 500] -> (800, 800)   |         59052.2      |         2984.8     

Times are in microseconds (us).

[-------- upsample_bicubic2d channels_first non-contiguous torch.float32 --------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        6050.9        |        1694.3      
      [1, 3, 320, 320] -> (512, 512)  |       23897.1        |        6607.9      
      [1, 3, 500, 500] -> (256, 256)  |        6282.8        |        1693.9      
      [1, 3, 500, 500] -> (800, 800)  |       58608.1        |       16061.0      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        6243.7        |         347.6      
      [1, 3, 320, 320] -> (512, 512)  |       24779.9        |        1253.8      
      [1, 3, 500, 500] -> (256, 256)  |        6348.0        |         350.7      
      [1, 3, 500, 500] -> (800, 800)  |       59255.6        |        2983.8      

Times are in microseconds (us).

[--------- upsample_bicubic2d channels_last non-contiguous torch.float32 ---------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          6117.0      |         1688.2     
      [1, 3, 320, 320] -> (512, 512)   |         23967.4      |         6644.8     
      [32, 128, 64, 64] -> (32, 32)    |        679574.0      |        78477.4     
      [32, 128, 64, 64] -> (128, 128)  |      10334325.5      |       817649.0     
      [2, 128, 64, 46] -> (32, 32)     |          9828.0      |         4449.2     
      [2, 128, 64, 46] -> (128, 128)   |        134989.3      |        42817.4     
      [1, 128, 64, 46] -> (32, 32)     |          4508.2      |         2228.6     
      [1, 128, 64, 46] -> (128, 128)   |         59404.9      |        21400.4     
      [1, 3, 500, 500] -> (256, 256)   |          6359.0      |         1712.7     
      [1, 3, 500, 500] -> (800, 800)   |         58717.6      |        16086.6     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          6922.0      |          349.5     
      [1, 3, 320, 320] -> (512, 512)   |         24916.5      |         1260.2     
      [32, 128, 64, 64] -> (32, 32)    |        454240.4      |        16491.4     
      [32, 128, 64, 64] -> (128, 128)  |       7198101.5      |       159921.9     
      [2, 128, 64, 46] -> (32, 32)     |         10082.8      |          891.1     
      [2, 128, 64, 46] -> (128, 128)   |        151037.0      |         7704.2     
      [1, 128, 64, 46] -> (32, 32)     |          4325.5      |          633.9     
      [1, 128, 64, 46] -> (128, 128)   |         62400.4      |         3853.5     
      [1, 3, 500, 500] -> (256, 256)   |          6374.9      |          354.9     
      [1, 3, 500, 500] -> (800, 800)   |         58638.8      |         2992.0     

Times are in microseconds (us).


Intermediate benchmark sources:

- results/20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.log.save
- results/20210331-092940_pr_results_1.9.0a0+git73137d8.log.save
```

[Source file](https://raw.githubusercontent.com/vfdev-5/interpolate-tensoriterator/master/step_seven/results/20210326-061238_pr_1.9.0a0%2Bgita17040a_vs_pth_1.9.0a0%2Bgit8518b0e_results.md)

</details>

This description is based on the benchmarks and the code from [here](https://github.com/vfdev-5/interpolate-tensoriterator/tree/master/step_seven).

Joint work with Francisco Massa (@fmassa).

--- 

Appendix: Results without original 2d/3d channels last implementation


<details>
<summary>

Quick benchmark results between 8518b0e (master) and [this branch](https://github.com/pytorch/pytorch/compare/master...Quansight:vfdev-5/generic-upsample-tensor-iterator)

</summary>

```
Description:
- 20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.opencv.6
- 20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.opencv.1
- 20212303-061238_pr_results_1.9.0a0+gite3a9544.opencv.6
- 20212303-061238_pr_results_1.9.0a0+gite3a9544.opencv.1

[----------------- upsample_bilinear2d channels_first contiguous -----------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          348.5       |          331.7     
      [1, 3, 320, 320] -> (512, 512)   |         1254.0       |         1178.1     
      [32, 128, 64, 64] -> (32, 32)    |        10409.4       |        10009.1     
      [32, 128, 64, 64] -> (128, 128)  |       210175.8       |       204542.5     
      [1, 3, 500, 500] -> (256, 256)   |          348.5       |          329.5     
      [1, 3, 500, 500] -> (800, 800)   |         3079.8       |         2890.1     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |           76.4       |           73.4     
      [1, 3, 320, 320] -> (512, 512)   |          247.1       |          232.0     
      [32, 128, 64, 64] -> (32, 32)    |         2371.1       |         2340.5     
      [32, 128, 64, 64] -> (128, 128)  |        62182.6       |        54089.9     
      [1, 3, 500, 500] -> (256, 256)   |           78.2       |           75.8     
      [1, 3, 500, 500] -> (800, 800)   |          569.0       |          541.3     

Times are in microseconds (us).

[-------------- upsample_bilinear2d channels_first non-contiguous ---------------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         340.5        |         321.9      
      [1, 3, 320, 320] -> (512, 512)  |        1256.1        |        1179.0      
      [1, 3, 500, 500] -> (256, 256)  |         351.4        |         332.0      
      [1, 3, 500, 500] -> (800, 800)  |        3089.1        |        2898.6      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |          77.2        |          75.0      
      [1, 3, 320, 320] -> (512, 512)  |         246.6        |         232.7      
      [1, 3, 500, 500] -> (256, 256)  |          78.6        |          75.4      
      [1, 3, 500, 500] -> (800, 800)  |         576.3        |         539.6      

Times are in microseconds (us).

[------------------------ upsample_bilinear2d channels_last non-contiguous ------------------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1
1 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          971.9       |         1324.6       |       99.6   
      [1, 3, 320, 320] -> (512, 512)   |         3867.8       |         5329.9       |      271.5   
      [32, 128, 64, 64] -> (32, 32)    |         6010.6       |         6304.3       |              
      [32, 128, 64, 64] -> (128, 128)  |       112299.9       |       116956.8       |              
      [2, 128, 64, 46] -> (32, 32)     |          110.1       |          133.2       |              
      [2, 128, 64, 46] -> (128, 128)   |         1690.1       |         1838.6       |              
      [1, 128, 64, 46] -> (32, 32)     |           55.8       |           73.4       |      185.8   
      [1, 128, 64, 46] -> (128, 128)   |          474.5       |          684.9       |     1445.7   
      [1, 3, 500, 500] -> (256, 256)   |          972.9       |         1343.0       |      149.5   
      [1, 3, 500, 500] -> (800, 800)   |         9460.2       |        12925.8       |      685.1   
6 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          956.6       |          260.1       |       27.1   
      [1, 3, 320, 320] -> (512, 512)   |         3867.3       |          967.1       |       63.6   
      [32, 128, 64, 64] -> (32, 32)    |         2489.4       |         2427.0       |              
      [32, 128, 64, 64] -> (128, 128)  |        37462.1       |        41329.8       |              
      [2, 128, 64, 46] -> (32, 32)     |           61.2       |           38.9       |              
      [2, 128, 64, 46] -> (128, 128)   |          904.2       |          652.0       |              
      [1, 128, 64, 46] -> (32, 32)     |           57.1       |           32.0       |      191.1   
      [1, 128, 64, 46] -> (128, 128)   |          491.4       |          138.1       |     1485.8   
      [1, 3, 500, 500] -> (256, 256)   |          977.0       |          257.8       |       36.6   
      [1, 3, 500, 500] -> (800, 800)   |         9470.0       |         2696.0       |      142.8   

Times are in microseconds (us).

[------------- upsample_linear1d channels_first contiguous --------------]
                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        516.5         |         524.7      
      [4, 512, 320] -> [512]  |        993.8         |        1008.0      
6 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        104.3         |         105.4      
      [4, 512, 320] -> [512]  |        193.5         |         195.6      

Times are in microseconds (us).

[-------------------- upsample_trilinear3d channels_first contiguous --------------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          5.5         |         11.5       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        116.3         |        213.1       
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          1.1         |          2.1       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |         36.1         |         47.2       

Times are in milliseconds (ms).

[------------------ upsample_trilinear3d channels_last non-contiguous -------------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         13.1         |         19.9       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        242.3         |        349.4       
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         13.1         |          4.4       
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        242.4         |         87.2       

Times are in milliseconds (ms).

[------------------ upsample_nearest2d channels_first contiguous -----------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         1194.5       |          107.8     
      [1, 3, 320, 320] -> (512, 512)   |         4813.8       |          365.5     
      [32, 128, 64, 64] -> (32, 32)    |        26745.6       |         6280.6     
      [32, 128, 64, 64] -> (128, 128)  |       357686.7       |       129032.9     
      [1, 3, 500, 500] -> (256, 256)   |         1205.9       |          123.8     
      [1, 3, 500, 500] -> (800, 800)   |        11770.3       |          879.2     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          220.2       |           32.7     
      [1, 3, 320, 320] -> (512, 512)   |          867.2       |           78.7     
      [32, 128, 64, 64] -> (32, 32)    |         5789.6       |         2241.8     
      [32, 128, 64, 64] -> (128, 128)  |        89125.3       |        41881.3     
      [1, 3, 500, 500] -> (256, 256)   |          224.3       |           34.8     
      [1, 3, 500, 500] -> (800, 800)   |         2182.8       |          176.6     

Times are in microseconds (us).

[--------------- upsample_nearest2d channels_first non-contiguous ---------------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        1279.5        |        110.2       
      [1, 3, 320, 320] -> (512, 512)  |        4908.1        |        367.1       
      [1, 3, 500, 500] -> (256, 256)  |        1488.1        |        123.4       
      [1, 3, 500, 500] -> (800, 800)  |       12186.4        |        879.3       
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |         241.8        |         32.6       
      [1, 3, 320, 320] -> (512, 512)  |         889.0        |         79.2       
      [1, 3, 500, 500] -> (256, 256)  |         279.2        |         35.6       
      [1, 3, 500, 500] -> (800, 800)  |        2226.5        |        174.3       

Times are in microseconds (us).

[------------------------ upsample_nearest2d channels_last non-contiguous -------------------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1
1 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          752.1       |          487.2       |      75.5    
      [1, 3, 320, 320] -> (512, 512)   |         2992.6       |         1880.0       |     251.4    
      [32, 128, 64, 64] -> (32, 32)    |         3458.6       |         3466.5       |              
      [32, 128, 64, 64] -> (128, 128)  |       102350.7       |       103919.4       |              
      [2, 128, 64, 46] -> (32, 32)     |           75.2       |           85.2       |              
      [2, 128, 64, 46] -> (128, 128)   |         1637.0       |         1690.4       |              
      [1, 128, 64, 46] -> (32, 32)     |           39.6       |           47.2       |      37.6    
      [1, 128, 64, 46] -> (128, 128)   |          426.3       |          449.0       |     412.4    
      [1, 3, 500, 500] -> (256, 256)   |          757.5       |          495.5       |      85.0    
      [1, 3, 500, 500] -> (800, 800)   |         7281.4       |         4532.6       |     622.8    
6 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |          139.3       |          104.1       |      75.7    
      [1, 3, 320, 320] -> (512, 512)   |          535.5       |          361.2       |      73.0    
      [32, 128, 64, 64] -> (32, 32)    |         1518.6       |         1458.2       |              
      [32, 128, 64, 64] -> (128, 128)  |        37117.7       |        40142.4       |              
      [2, 128, 64, 46] -> (32, 32)     |           17.6       |           26.6       |              
      [2, 128, 64, 46] -> (128, 128)   |          537.6       |          629.4       |              
      [1, 128, 64, 46] -> (32, 32)     |           13.7       |           22.1       |      38.8    
      [1, 128, 64, 46] -> (128, 128)   |           83.6       |           94.5       |     420.2    
      [1, 3, 500, 500] -> (256, 256)   |          140.8       |          104.9       |      87.8    
      [1, 3, 500, 500] -> (800, 800)   |         1317.8       |          853.8       |     139.7    

Times are in microseconds (us).

[------------- upsample_nearest1d channels_first contiguous -------------]
                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |        1594.3        |        247.4       
      [4, 512, 320] -> [512]  |        3222.6        |        440.4       
6 threads: ---------------------------------------------------------------
      [4, 512, 320] -> [256]  |         294.4        |         53.7       
      [4, 512, 320] -> [512]  |         575.0        |         88.5       

Times are in microseconds (us).

[--------------------- upsample_nearest3d channels_first contiguous ---------------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        14952.7       |        1005.7      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       224955.6       |       46228.0      
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         2887.2       |         206.2      
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        56872.0       |       13566.3      

Times are in microseconds (us).

[------------------- upsample_nearest3d channels_last non-contiguous --------------------]
                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         7772.3       |         4770.9     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       144655.1       |       108605.0     
6 threads: -------------------------------------------------------------------------------
      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1401.9       |          877.7     
      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        35939.6       |        28621.5     

Times are in microseconds (us).

[------------------ upsample_bicubic2d channels_first contiguous -----------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         6038.7       |         2340.4     
      [1, 3, 320, 320] -> (512, 512)   |        24040.6       |         9205.9     
      [32, 128, 64, 64] -> (32, 32)    |       471016.3       |        52059.1     
      [32, 128, 64, 64] -> (128, 128)  |      7705594.5       |       884743.9     
      [1, 3, 500, 500] -> (256, 256)   |         6061.5       |         2361.9     
      [1, 3, 500, 500] -> (800, 800)   |        58940.7       |        22401.8     
6 threads: ------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         6594.3       |          466.5     
      [1, 3, 320, 320] -> (512, 512)   |        25361.5       |         1729.1     
      [32, 128, 64, 64] -> (32, 32)    |       487783.5       |        11550.0     
      [32, 128, 64, 64] -> (128, 128)  |      7963636.6       |       196017.3     
      [1, 3, 500, 500] -> (256, 256)   |         6443.8       |          464.1     
      [1, 3, 500, 500] -> (800, 800)   |        61891.9       |         4257.2     

Times are in microseconds (us).

[--------------- upsample_bicubic2d channels_first non-contiguous ---------------]
                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544
1 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        6116.7        |        2357.0      
      [1, 3, 320, 320] -> (512, 512)  |       24182.0        |        9213.9      
      [1, 3, 500, 500] -> (256, 256)  |        6349.6        |        2358.5      
      [1, 3, 500, 500] -> (800, 800)  |       59365.2        |       22431.2      
6 threads: -----------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)  |        7155.1        |         464.6      
      [1, 3, 320, 320] -> (512, 512)  |       24566.8        |        1712.4      
      [1, 3, 500, 500] -> (256, 256)  |        7217.5        |         466.6      
      [1, 3, 500, 500] -> (800, 800)  |       59880.2        |        4148.8      

Times are in microseconds (us).

[------------------------ upsample_bicubic2d channels_last non-contiguous -------------------------]
                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1
1 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         6184.3       |         2360.0       |      215.0   
      [1, 3, 320, 320] -> (512, 512)   |        24499.7       |         9231.1       |      510.7   
      [32, 128, 64, 64] -> (32, 32)    |       548304.5       |        93517.8       |              
      [32, 128, 64, 64] -> (128, 128)  |      7810958.3       |      1086334.6       |              
      [2, 128, 64, 46] -> (32, 32)     |        10883.4       |         5594.9       |              
      [2, 128, 64, 46] -> (128, 128)   |       153253.2       |        57071.2       |              
      [1, 128, 64, 46] -> (32, 32)     |         4519.4       |         2826.5       |      619.7   
      [1, 128, 64, 46] -> (128, 128)   |        61339.7       |        28470.7       |     3654.5   
      [1, 3, 500, 500] -> (256, 256)   |         6444.8       |         2389.9       |      292.9   
      [1, 3, 500, 500] -> (800, 800)   |        59448.0       |        22479.1       |     1316.9   
6 threads: -----------------------------------------------------------------------------------------
      [1, 3, 320, 320] -> (256, 256)   |         6370.1       |          464.9       |       61.3   
      [1, 3, 320, 320] -> (512, 512)   |        25365.6       |         1767.5       |      145.7   
      [32, 128, 64, 64] -> (32, 32)    |       502888.7       |        22016.3       |              
      [32, 128, 64, 64] -> (128, 128)  |      8072918.9       |       234567.0       |              
      [2, 128, 64, 46] -> (32, 32)     |        11171.4       |         1049.5       |              
      [2, 128, 64, 46] -> (128, 128)   |       152612.5       |        11264.8       |              
      [1, 128, 64, 46] -> (32, 32)     |         4359.3       |          791.4       |      651.1   
      [1, 128, 64, 46] -> (128, 128)   |        61346.5       |         7563.9       |     3765.2   
      [1, 3, 500, 500] -> (256, 256)   |         6644.4       |          469.7       |       77.4   
      [1, 3, 500, 500] -> (800, 800)   |        59947.2       |         4154.3       |      313.2   

Times are in microseconds (us).


Intermediate benchmark sources:

- results/20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.log.save.opencv
- results/20212303-061238_pr_results_1.9.0a0+gite3a9544.log.save.opencv

```

[Source file](https://raw.githubusercontent.com/vfdev-5/interpolate-tensoriterator/master/step_seven/results/20212303-061238_pr_1.9.0a0%2Bgite3a9544_vs_pth_1.9.0a0%2Bgit8518b0e_results.opencv.md)
</details>
",pytorch
54557,rohan-varma,pr,2021-03-24T00:43:57Z,[NCCL] Enhance watchdog to log exceptions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* #54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* **#54557 [NCCL] Enhance watchdog to log exceptions**
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

When looping through the nccl communicator cache checking for errors, enhance the watchdog to log exceptions that are set on the communicator.

This will allow for better debugability since the NCCL error will be logged when the watchdog receives errors for the communicators and aborts them appropriately.

Tested by forcing a NCCL error with NCCL_BLOCKING_WAIT=1 and verifying that the exception is indeed logged.

Differential Revision: [D27106699](https://our.internmc.facebook.com/intern/diff/D27106699/)",pytorch
54558,rohan-varma,pr,2021-03-24T00:44:04Z,"[NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
* **#54558 [NCCL][Blocking Wait] Log set exceptions when checking for exceptions in
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait**
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

blocking wait

In blocking wait's polling synchronization loop, we frequently call checkAndSetException() as part of isCompleted() to check the status of nccl operations. It would be useful to log here in case we encounter any exceptions (which are later thrown by `checkAndThrowException`).

Also slightly refactors code previously added to make use of a helper function to get the error message given an `std::exception_ptr`.

Differential Revision: [D27136202](https://our.internmc.facebook.com/intern/diff/D27136202/)",pytorch
54584,vfdev-5,pr,2021-03-24T11:01:27Z,[operator benchmarks] Added more interpolation test cases,"Description:
- Added uint8 nearest test case
- Added 3d vectorization test case
",pytorch
54589,peterjc123,pr,2021-03-24T13:15:47Z,[test] vc toolchain modification,"Fixes #54502
Needs to be merged after https://github.com/pytorch/builder/pull/684",pytorch
54624,supriyar,pr,2021-03-24T20:40:37Z,"[quant][fx] store dtype, axis as literals in the graph","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54860 [quant][fx] Add pass in convert to fold quant-dequant sequence
* #54859 [quant][fx][pyper] Get first linear use of quantize_per_tensor for FQN
* **#54624 [quant][fx] store dtype, axis as literals in the graph**

Summary:
previously we were creating setattr nodes for dtype and axis.
The FX convention is that primitive types are embedded as literals in args/kwargs.

With this change we won't see getattr nodes in the graph anymore for dtype/axis

Test Plan:
python test/test_quantization.py TestQuantizeFx

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D27306898](https://our.internmc.facebook.com/intern/diff/D27306898)",pytorch
54649,rohan-varma,pr,2021-03-25T02:16:39Z,[DDP logging] Prefer use of c10::Join,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54649 [DDP logging] Prefer use of c10::Join**

Some operator<< code manually implemented string join in C++, turns
out there is a c10 util for this. Use the util instead of rolling our own.

Differential Revision: [D27316705](https://our.internmc.facebook.com/intern/diff/D27316705/)",pytorch
54691,lithuak,pr,2021-03-25T16:58:07Z,Drop Python 2 support in common_device_type.py,"Hey!

Just stumbled across these Python 2 fragments while reading the source code and thought it could be removed, since the Python 2 support has already been dropped.

@mruberry ",pytorch
54740,rohan-varma,pr,2021-03-25T22:55:38Z,Provide a decorator to set/unset nccl blocking wait for tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* #54741 test_c10d: Run tests with nccl_async_error_handling
* **#54740 Provide a decorator to set/unset nccl blocking wait for tests**
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

Adds a simple helper decorator to set/unset nccl blocking wait for
tests. This will make it easier than having to manually set/unset the
os.environ vars every time.

Differential Revision: [D27277222](https://our.internmc.facebook.com/intern/diff/D27277222/)",pytorch
54741,rohan-varma,pr,2021-03-25T22:55:45Z,test_c10d: Run tests with nccl_async_error_handling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54742 test_c10d: use with_nccl_blocking_wait decorator
* **#54741 test_c10d: Run tests with nccl_async_error_handling**
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

Similar to what we did for `distributed_test.py`, let `MultiProcessTests` that run collective comm. tests with `nccl` run under `nccl_async_error_handling`. This will better simulate real-world training scenarios.

Differential Revision: [D27277389](https://our.internmc.facebook.com/intern/diff/D27277389/)",pytorch
54742,rohan-varma,pr,2021-03-25T22:55:51Z,test_c10d: use with_nccl_blocking_wait decorator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54742 test_c10d: use with_nccl_blocking_wait decorator**
* #54741 test_c10d: Run tests with nccl_async_error_handling
* #54740 Provide a decorator to set/unset nccl blocking wait for tests
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier
blocking wait
* #54557 [NCCL] Enhance watchdog to log exceptions
* #54220 [DDP] Use monitored barrier in DDP
* #54219 [DDP] Remove redundant pass statement
* #53787 Expose dist.monitored_barrier() API
* #53773 Gloo-only CPU-based monitored barrier

Uses with_nccl_blocking_wait decorator for test_c10d.

Differential Revision: [D27277835](https://our.internmc.facebook.com/intern/diff/D27277835/)",pytorch
54763,rohan-varma,pr,2021-03-26T08:15:26Z,[c10d] s/torch::autograd::variable/at::Tensor/g,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54764 [DDP] Mark a few variables as const in reducer
* **#54763 [c10d] s/torch::autograd::variable/at::Tensor/g**

Replaces deprecated torch::autograd::variable with at::Tensor.
torch::autograd::variable is defined as equal to at::Tensor now so this should
be a noop, but follows convention of using tensor instead of Variable.

There are also a few uses in distributed autograd c++, can remove those in a separate diff

Differential Revision: [D27356450](https://our.internmc.facebook.com/intern/diff/D27356450/)",pytorch
54764,rohan-varma,pr,2021-03-26T08:31:48Z,[DDP] Mark a few variables as const in reducer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54764 [DDP] Mark a few variables as const in reducer**
* #54763 [c10d] s/torch::autograd::variable/at::Tensor/g

We mark a few vars as const in Reducer, also do this for replicas_ and
process_group_ as they should not be changed by Reducer during training.

Differential Revision: [D27357132](https://our.internmc.facebook.com/intern/diff/D27357132/)",pytorch
54768,lithuak,pr,2021-03-26T12:14:56Z,Fix minor style/typos problems in comment_device_type.py,"One typo, one example correction and capitalization for a couple of comment lines.

@ailzhang ",pytorch
54798,lithuak,pr,2021-03-26T21:16:48Z,Port torch.outer method_tests() to OpInfos,"An attempt to make an OpInfo-based test for torch.outer (aka toch.ger).

As a part of #54261 effort.

@mruberry 
",pytorch
54840,ngimel,pr,2021-03-28T05:27:28Z,Simplify convolution double backward gradInput formulas,"Currently in convolution double backward grad of input is computed as `convT(ggW, gO.T)`. Notice how first argument is, in fact, of the size that convolution weight has, and second is of the size of gradOutput, which is an inverse order compared to how convolutions are regularly called, and sizes are far from what cudnn heuristics is trained for and what cudnn is guaranteed to have efficient kernels for. This takes cudnn 8 to some dark places, calling  kernels that take 20-100 s. But, luckily for us, convT is a commutative operation (unlike conv), so convT(ggW, gO) is actually the same as  convT(gO, ggW), modulo some transposes because of conventions around the weight size, so we  can use convT(gO, ggW). As an added bonus, we don't need a special branch for groups with this formulation. 
For the following pretty standard convolution, 
 - cudnn 7.6+old formulation takes 7.5 ms for double backward,
 - cudnn 8 + old formulation takes ~40 s, 
 - cudnn 8 + new formulation is 1.8 ms with benchmark enabled, 
 - cudnn 8 + new formulation is 4 ms with benchmark disabled, 
 benchmarking script is below:
```
import torch
import time

#torch.backends.cudnn.benchmark=True

def ggI(conv, inp):
    out = conv(inp)
    grads = torch.autograd.grad(out, conv.weight, torch.rand_like(out), create_graph=True, retain_graph=True)
    torch.cuda.synchronize()
    start = time.time()
    grads[0].backward(torch.rand_like(grads[0]))
    torch.cuda.synchronize()
    print(""db time: "", time.time()-start)
    return inp.grad

conv = torch.nn.Conv2d(512,256,kernel_size=3, padding=1, groups=2).cuda()
inp = torch.randn(1,512,128,128, device=""cuda"", requires_grad=True)
for _ in range(20):
    ggI(conv, inp)
torch.cuda.synchronize()
```
",pytorch
54859,supriyar,pr,2021-03-29T03:47:33Z,[quant][fx][pyper] Get first linear use of quantize_per_tensor for FQN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54860 [quant][fx] Add pass in convert to fold quant-dequant sequence
* **#54859 [quant][fx][pyper] Get first linear use of quantize_per_tensor for FQN**

Summary:
This is applicable to the case when a call_function linear op is one of the users of quantize op
In order to be able to map the qparams of quantize_per_tensor to the qparams of the linear operator
that consumes it, we need to use the FQN of the module with linear op for the qparmas of quantize_per_tensor.

Test Plan:
python test/test_quantization.py test_qparams_fqn
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D27390505](https://our.internmc.facebook.com/intern/diff/D27390505)",pytorch
54860,supriyar,pr,2021-03-29T03:47:38Z,[quant][fx] Add pass in convert to fold quant-dequant sequence,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54860 [quant][fx] Add pass in convert to fold quant-dequant sequence**
* #54859 [quant][fx][pyper] Get first linear use of quantize_per_tensor for FQN

Summary:
Currently we insert a quantize_per_tensor op when we encounter the quantizable input,
so if it has multiple uses and not all are quantizable then we need to add a dequantize op
before these ops.

In this pass - For a sequence of quantize_per_tensor - dequantize, we combine them
since it is a no-op.

Test Plan:
python test/test_quantization.py test_fold_quant_dequant

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D27390506](https://our.internmc.facebook.com/intern/diff/D27390506)",pytorch
54919,rohan-varma,pr,2021-03-29T22:16:08Z,[DDP Logging] Log use of uneven inputs API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54919 [DDP Logging] Log use of uneven inputs API**

Log the use of uneven inputs API for better tracking and use case
detection.

Differential Revision: [D27410764](https://our.internmc.facebook.com/intern/diff/D27410764/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D27410764/)!",pytorch
54955,peterjc123,pr,2021-03-30T13:52:44Z,Fix CUDA 11.2 jobs for Windows,Fixes https://github.com/pytorch/pytorch/pull/54589#issuecomment-810255467,pytorch
54984,eqy,pr,2021-03-30T19:53:56Z,fix typo in ReduceMinMaxKernel,,pytorch
54991,rohan-varma,pr,2021-03-30T21:42:40Z,[NCCL] warn when barrier guesses device to use,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54991 [NCCL] warn when barrier guesses device to use**

Actual proposed fix is in
https://github.com/pytorch/pytorch/pull/53934, in the meantime, would be useful
to include this LOG when barrier does not know what devices to use, and suggest
the workaround of passing in device_ids into barrier().

Differential Revision: [D27444917](https://our.internmc.facebook.com/intern/diff/D27444917/)",pytorch
55009,rohan-varma,pr,2021-03-31T00:37:06Z,[c10d] Enforce order of waited ranks in monitored barrier.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55265 [c10d] Log API usage of monitored barrier
* #55197 [c10d] monitored_barrier: ensure all ranks pass or none do
* #55010 [c10d] Monitored barrier: option to collect all failed ranks
* **#55009 [c10d] Enforce order of waited ranks in monitored barrier.**

Changes monitoredBarrier so that we await acknowledgemenet from ranks
in a consistent order (from least to greatest). This will reduce confusion
around the order the ranks are awaited. We are still planning to add support
for awaiting all ranks in follow up changes.

Differential Revision: [D27405417](https://our.internmc.facebook.com/intern/diff/D27405417/)",pytorch
55010,rohan-varma,pr,2021-03-31T00:37:13Z,[c10d] Monitored barrier: option to collect all failed ranks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55265 [c10d] Log API usage of monitored barrier
* #55197 [c10d] monitored_barrier: ensure all ranks pass or none do
* **#55010 [c10d] Monitored barrier: option to collect all failed ranks**
* #55009 [c10d] Enforce order of waited ranks in monitored barrier.

Follow up change to add a flag to provide an option for monitored barrier to collect all the failed ranks and then throw instead of just throwing on the first one. This is useful as now monitored barrier will be able to pick up on all hanging ranks instead of just one.

This is done by passing in a flag `wait_all_ranks=True`.

Differential Revision: [D27447787](https://our.internmc.facebook.com/intern/diff/D27447787/)",pytorch
55026,ngimel,pr,2021-03-31T05:51:52Z,various overhead improvements to cuda addmm,"Add fast common case to `prepare_matrix_for_cublas`, use index size instead of size(), move some checks where they belong so they are not triggered where they are guaranteed to be true. 
",pytorch
55049,lezcano,pr,2021-03-31T13:59:04Z,More stable and faster implementation of the gradient of torch.linalg.eigh ,"This PR:
- Renames symeig_backward to eigh_backward
- Improves the stability and speed of the gradient computation by doing `V(A + B)Vh` instead of `VAVh + VBVh`  when both the gradients of the eigenvectors and eigenvalues are defined.
- Updates the comments of the function to make them arguably clearer",pytorch
55074,rohan-varma,pr,2021-03-31T18:39:25Z,[DDP] Call ensure_prior_reduction_finished within lock,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55075 [DDP] Param to name mapping in Reducer
* **#55074 [DDP] Call ensure_prior_reduction_finished within lock**

This function accesses member variables that can be modified by
different threads (i.e. autograd engine threads), so call it within lock scope.

Differential Revision: [D27474526](https://our.internmc.facebook.com/intern/diff/D27474526/)",pytorch
55075,rohan-varma,pr,2021-03-31T18:39:33Z,[DDP] Param to name mapping in Reducer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55075 [DDP] Param to name mapping in Reducer**

Constructs and passes in a mapping with parameter names to Reducer to log information about unused parameters in error messages about unused parameters/not all parameters getting gradient.

Use case:
1) User runs DDP forward + bwd, and it has some unused parameters that will result in ddp error in next iteration
2) Next forward pass calls `Reducer::ensure_prior_reduction_finished()` where we check all params got gradient from the previous bwd pass. DDP would throw here in this case.
3) Reducer maintains mapping and tracks used parameters, and computes which parameters did not get gradient and logs this as part of the error.

Implementation details:
0) The following is only enabled for debug modes of INFO or DETAIL.
1) To save memory, we don't map param -> param name so that we don't have to copy the entire tensor, instead we map param_index -> param_name and use the existing concept of variable_index in Reducer to look up parameter names.
2) DDP constructs param index -> param name mapping. The name is the fully qualified name: f""{module_name}:{param_name}"" and passes it into Reducer
3) Reducer maintains per-iteration std::set<int> of variable indices that have had `mark_variable_ready` called.
4) When some params go unused, we take a set difference to detect the unused params.
5) Unittests to test the logged unused params, as well as for nested modules, are added

Differential Revision: [D27356394](https://our.internmc.facebook.com/intern/diff/D27356394/)",pytorch
55085,lezcano,pr,2021-03-31T19:00:21Z,symeig supports complex backward,"Fixes https://github.com/pytorch/pytorch/issues/53651
I did not put much effort in improving the docs, as I will go over all these docs in future PRs
cc @anjali411 ",pytorch
55092,lezcano,pr,2021-03-31T19:29:40Z,Write OpInfo for dist,"Fixes https://github.com/pytorch/pytorch/issues/53516
cc @anjali411 ",pytorch
55100,r-barnes,pr,2021-03-31T20:23:36Z,Use irange in a few places,"Differential Revision: D27447708

",pytorch
55107,ngimel,pr,2021-03-31T21:31:46Z,construct only necessary elements in OffsetCalculator,"Per title. Elements beyond `dim` are never accessed because https://github.com/pytorch/pytorch/blob/646510f7028f12e8b1f3a9d3b63b8519ed80e391/aten/src/ATen/cuda/detail/OffsetCalculator.cuh#L49-L51. 
On `addmm` instruction count per 30 repetitions 1467813 -> 1452261
`add`  651522 -> 633462
`add_` 529331 -> 511271

add benchmarking snippet:
```
 timer = Timer(""m1.add_(b);"", setup=""at::Tensor m1=torch::empty({2,2},device(at::kCUDA) ); at::Tensor b = torch::empty({2}, device(at::kCUDA));"", language=""c++"", timer=timeit.default_timer)
 stats=timer.collect_callgrind(number=30)
 print(stats.as_standardized().stats(inclusive=False))
```

 
",pytorch
55141,lezcano,pr,2021-04-01T11:49:04Z,"Correct many OpInfos ""test_out"" skips.","Partially solves https://github.com/pytorch/pytorch/issues/54061

This PR solves many of the ""easy to solve"" problems with `out=` not notifying when it resizes a tensor. It also reports the cause of some fails of the `out=` operation in the tests. Hopefully this way we will be able to catch some errors that do not come simply from not using `resize_output`.
cc @mruberry @anjali411 ",pytorch
55148,r-barnes,pr,2021-04-01T15:06:11Z,irange on int64_t,"Differential Revision: D27447811

",pytorch
55163,r-barnes,pr,2021-04-01T17:23:54Z,irange for size_t,"Differential Revision: D27448156

",pytorch
55184,r-barnes,pr,2021-04-01T22:45:27Z,Add some missing types to torch,"Differential Revision: D27515470

",pytorch
55186,r-barnes,pr,2021-04-01T22:48:46Z,"Add some missing types to training_toolkit/torch, make a test name consistent","Differential Revision: D27514839

",pytorch
55197,rohan-varma,pr,2021-04-02T00:47:15Z,[c10d] monitored_barrier: make other ranks fail when one rank doesn't reach the barrier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55265 [c10d] Log API usage of monitored barrier
* **#55197 [c10d] monitored_barrier: make other ranks fail when one rank doesn't reach the barrier**

From initial user feedback, one unexpected difference between monitored_barrier impl and barrier is the ""all or nothing"" semantics.

In barrier, all ranks pass or they all fail. With monitored barrier however, if rank 1 is healthy, it will respond to both send and recv from rank 0, but rank 0 can later fail because rank 2 is stuck. In this case, rank 1 will move forward out of the barrier.

This change makes it so that if a rank fails in monitored barrier, all other ranks in monitored barrier will also fail. It does so by the following process, similar to acknowledgements:

Nonzero ranks call send()
Nonzero ranks call recv()

Rank 0 calls recv(), if this succeeds, rank 0 has acknowledged rank N as healthy
Once all ranks are acknowledged as healthy:
Rank 0 calls send() to all nonzero ranks to unblock them

Modified unittests to ensure the all or nothing failure behavior

Differential Revision: [D27523060](https://our.internmc.facebook.com/intern/diff/D27523060/)",pytorch
55204,rohan-varma,pr,2021-04-02T02:24:30Z,[Dist profiling] Fix ProcessGroupNCCL collective profiling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #56840 Clang format ProcessGroupNCCL.cpp
* #56427 [c10d] Profiler support for nccl p2p collectives
* #56216 [Dist profiling] Enable tests for dist profiling with torch.profiler
* **#55204 [Dist profiling] Fix ProcessGroupNCCL collective profiling**
**
closes https://github.com/pytorch/pytorch/issues/48987
Implements a fix discussed offline with @pritamdamia87 to run end callbacks after CUDAFuture's wrapCallback has ensured appropriate synchronization. Also enables the relevant distributed profiling tests that were previously disabled for ProcessGroupNCCL.
Note that the profiling infrastructure has moved to primarily encourage the use of torch.profiler and CUPTI to trace CUDA kernels, support for distributed collectives for that will require further discussion with @ilia-cher. However, this PR improves the usability of torch.autograd.profiler with respect to distributed collectives.
Differential Revision: [D27491711](https://our.internmc.facebook.com/intern/diff/D27491711/)",pytorch
55233,r-barnes,pr,2021-04-02T16:32:45Z,Add some missing HIP error types,"Differential Revision: D27536444

",pytorch
55265,rohan-varma,pr,2021-04-03T00:01:21Z,[c10d] Log API usage of monitored barrier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55265 [c10d] Log API usage of monitored barrier**
* #55197 [c10d] monitored_barrier: make other ranks fail when one rank doesn't reach the barrier

Logs API usage of monitored barrier for better tracking and use case
understanding.

Differential Revision: [D27548433](https://our.internmc.facebook.com/intern/diff/D27548433/)",pytorch
55275,peterjc123,pr,2021-04-03T11:00:59Z,Fix ordered_dict.h for CUDA on Windows,"Fixes https://github.com/pytorch/pytorch/issues/55266
",pytorch
55292,ngimel,pr,2021-04-04T23:23:03Z,Wrap cub in its own namespace,"Tentative fix for #55027.
Wraps cub import in its name space so that static variables used by cub and thrust don't conflict if they end up in the different libraries when torch is built with BUILD_SPLIT_CUDA. cub variables end up in their own namespace, thrust variables are unwrapped, so they don't clash. 
This also allows extensions to use cub without wrapping it (thrust will still be problematic). The solution to allowing extensions to use thrust is to stop using thrust in pytorch completely. 
Now importing cub and importing thrust cannot coexist, so I had to move nonzero to its own file, and remove reliance on thrust functions for it. Nonzero now uses cub only. 
Also, we cannot selectively import just some of cub headers, we are forced to import `cub/cub.cuh`, which is not great. 
Caffe2 ops using cub are not touched (there are too many), so mixing caffe2 and torch will (can) still result in the same bug. We are moving towards disabling c2 ops, so I think this is fine. 
Still, even with that compiler (correctly) warns about redefinition of `CUB_NS_PREFIX` because including `ATen/ATen.h` transitively includes `thrust/complex.h` and that in turn includes original (empty) definition of `CUB_NS_PREFIX`. We probably can just ignore this warning. Here's an example warning:
```
In file included from /data/users/ngimel/pytorch/aten/src/ATen/native/cuda/Nonzero.cu:9:
/data/users/ngimel/pytorch/aten/src/ATen/cuda/CubUtils.cuh:4: warning: ""CUB_NS_PREFIX"" redefined
 #define CUB_NS_PREFIX namespace at{ namespace native{
 
In file included from /home/ngimel/local/cuda/include/thrust/system/cuda/config.h:76,
                 from /home/ngimel/local/cuda/include/thrust/system/cuda/detail/execution_policy.h:33,
                 from /home/ngimel/local/cuda/include/thrust/iterator/detail/device_system_tag.h:23,
                 from /home/ngimel/local/cuda/include/thrust/iterator/iterator_traits.h:111,
                 from /home/ngimel/local/cuda/include/thrust/detail/type_traits/pointer_traits.h:23,
                 from /home/ngimel/local/cuda/include/thrust/type_traits/is_contiguous_iterator.h:27,
                 from /home/ngimel/local/cuda/include/thrust/type_traits/is_trivially_relocatable.h:19,
                 from /home/ngimel/local/cuda/include/thrust/detail/complex/complex.inl:20,
                 from /home/ngimel/local/cuda/include/thrust/complex.h:1031,
                 from /data/users/ngimel/pytorch/c10/util/complex.h:9,
                 from /data/users/ngimel/pytorch/c10/core/ScalarType.h:4,
                 from /data/users/ngimel/pytorch/c10/core/Scalar.h:10,
                 from /data/users/ngimel/pytorch/build/aten/src/ATen/core/TensorBody.h:8,
                 from /data/users/ngimel/pytorch/aten/src/ATen/Tensor.h:3,
                 from /data/users/ngimel/pytorch/aten/src/ATen/Context.h:4,
                 from /data/users/ngimel/pytorch/aten/src/ATen/ATen.h:9,
                 from /data/users/ngimel/pytorch/aten/src/ATen/native/cuda/Nonzero.cu:1:
/home/ngimel/local/cuda/include/cub/util_namespace.cuh:43: note: this is the location of the previous definition
 #define CUB_NS_PREFIX
 
```
We will need a lint rule to prevent people from including `cub/cub.cuh`, because this will lead to #55027 reappearing again for some sequence of operations (and will lead to errors with cub code in extensions). 
Also, for this to work reliably we'll need to make sure that everything calling cub ends up in only one of libtorch_cuda_cu or libtorch_cuda_cpp, otherwise even namespace won't help (there still will be same symbols in 2 libraries). 

Upd: libtorch_cuda_cpp and libtorch_cuda_cu still contain the same symbols, which means that there exists a sequence of operations that will cause cache bug to reappear, so this is not a solution, we need to adjust file lists for BUILD_SPLITC_CUDA:
```
(pytorch) [ngimel@ ~/local/pytorch/build/lib] nm libtorch_cuda_cu.so | grep PerDeviceAttributeCache | c++filt
000000000c6bf808 u guard variable for at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache
000000000c600830 u guard variable for cub::GetPerDeviceAttributeCache<cub::PtxVersionCacheTag>()::cache
00000000018625e0 t at::native::cub::PerDeviceAttributeCache::DevicePayload at::native::cub::PerDeviceAttributeCache::operator()<at::native::cub::PtxVersion(int&)::{lambda(int&)#1}>(at::native::cub::PtxVersion(int&)::{lambda(int&)#1}&&, int)
00000000009ce630 t cub::PerDeviceAttributeCache::DevicePayload cub::PerDeviceAttributeCache::operator()<cub::PtxVersion(int&)::{lambda(int&)#1}>(cub::PtxVersion(int&)::{lambda(int&)#1}&&, int)
000000000c6bf820 u at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache
000000000c600840 u cub::GetPerDeviceAttributeCache<cub::PtxVersionCacheTag>()::cache
(pytorch) [ngimel@ ~/local/pytorch/build/lib] nm libtorch_cuda_cpp.so | grep PerDeviceAttributeCache | c++filt
0000000000ad2d98 u guard variable for at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache
0000000000ad2da0 u at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache
```
Upd2:
Moved TensorFactories.cu to torch_cuda_cu sources (see a change to caffe2/CMakeLists.txt), so now cub-related symbols are only in libtorch_cuda_cu. We'd need a test for that, any suggestions on how best to test it?
cc @zasdfgbnm @malfet 
",pytorch
55319,rohan-varma,pr,2021-04-05T19:44:12Z,[c10d] sequence number in process group,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55718 [c10d] Increment sequence numbers on collectives.
* **#55319 [c10d] sequence number in process group**

Adds a sequence number class as well as integration with ProcessGroup (nccl and gloo) as part of better debugability.

The main use case is that each ProcessGroup instantiated will have a sequence number initially set by rank 0, and broadcasted to all others. We will increment the number on each collective, thus allowing us to match the numbers appropriately when checking for desynchronization.

This PR just adds the bare-bones integration and verifies sequence numbers are set appropriately at the beginning. The next diff in this stack will increment the sequence number appropriately on collectives.

Note that we initialize sequence numbers in python in `new_group` and `init_process_group` APIs for NCCL and Gloo backends only. The reason we do this instead of the constructor is because the pg constructor in C++ is not a synchronization point, and can result in timeout issues.

Differential Revision: [D27562769](https://our.internmc.facebook.com/intern/diff/D27562769/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D27562769/)!",pytorch
55320,r-barnes,pr,2021-04-05T19:47:49Z,irange for size_t,"Differential Revision: D27572577

",pytorch
55325,r-barnes,pr,2021-04-05T20:55:47Z,Use irange in a few places,"Differential Revision: D27573006

",pytorch
55338,r-barnes,pr,2021-04-05T22:28:07Z,"Add some missing types to training_toolkit/torch, make a test name consistent 2","Differential Revision: D27575367

",pytorch
55392,eqy,pr,2021-04-06T17:49:50Z,"Port `topk` from THC to ATen, migrate most of sort as well","Fixes #24648
Fixes #28871

The large tensor codepath is ported, but there is a legacy codepath that depends on an inplace sort in THC that is not callable from `at::`. At first glance, THC `topk` seems to be the only function that uses this `sortKeyValueInplace`.
Is the correct change to wrap `sortKeyValueInplace` in legacy functions for visibility in the `at::` namespace?",pytorch
55444,rohan-varma,pr,2021-04-07T05:20:17Z,[NCCL] Join work clean up thread before aborting communicators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55444 [NCCL] Join work clean up thread before aborting communicators**

Changes ~ProcessGroupNCCL so that we join work cleanup thread before aborting nccl communicators. This is because if we abort nccl communicators first on destruction, outstanding work objects in `workMetaList` can have exceptions set on them. Right now this doesn't trigger errors in nccl async error handling due to the terminated check, but it seems a bit cleaner to just join this thread first.

The main motivation is also to reduce log spam since we added some logging when an exception is set on `WorkNCCL`, but this unexpectedly resulted in a lot of false-positive errors being logged even after pg shutdown. An example is below:

```
I0406 18:30:27.361981 1567104 ProcessGroupNCCL.cpp:527] [Rank 1] NCCL watchdog thread terminated normally
I0406 18:30:27.364675 1567105 ProcessGroupNCCL.cpp:265] [Rank 1] found async exception when checking for NCCL errors: NCCL error: unhandled system error, NCCL version 2.
7.3
```

With this change, we no longer see these false positive logs.

Differential Revision: [D27613035](https://our.internmc.facebook.com/intern/diff/D27613035/)",pytorch
55456,lezcano,pr,2021-04-07T12:11:36Z,Fix a problem when removing parametrizations,"There was an error when removing a parametrization with `leave_parametrized=True`. It had escaped the previous tests. This PR should fix that.
**Edit.**
I also took this chance to fix a few mistakes that the documentation had, and to also write the `set_original_` in a more compact way.
",pytorch
55527,ngimel,pr,2021-04-07T19:13:42Z,adds data_ptr checks to in-place OpInfo variant tests and out OpInfo tests,"Fixes #55088. 
Unfortunately, this test wouldn't have caught index_add_ breakage (because index_add_ breakage would appear only in a particular type promotion situation). 
",pytorch
55556,r-barnes,pr,2021-04-07T20:24:20Z,Use irange in torch/csrc utils,"Differential Revision: D27625936

",pytorch
55659,rohan-varma,pr,2021-04-09T05:39:12Z,[BE] Speed up runtime of test_ddp_model_diff_across_ranks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55659 [BE] Speed up runtime of test_ddp_model_diff_across_ranks**

As per https://github.com/pytorch/pytorch/issues/55583, this is the most expensive distributed test.

Instead of waiting for process 0 in this test to be taken down by
nccl_async_error_handling, just remove the barrier and let the process exit
when the backend is NCCL.

A slight downside here is that the test no longer verifies that the process
would be brought down by nccl_async_error_handling, but
nccl_async_error_handling is already well tested in other tests. If we feel we
need to ensure this for this test, then we can pass in a process group with a
smaller timeout as an alternative solution.

The test now runs in 8-9s as opposed to 70. Ran the test 1000 times to verify
no flakiness

Differential Revision: [D27672161](https://our.internmc.facebook.com/intern/diff/D27672161/)",pytorch
55660,rohan-varma,pr,2021-04-09T05:39:20Z,[BE][Docs] Improve dist.new_group doc,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55660 [BE][Docs] Improve dist.new_group doc**

Noticed this doc was missing clarification on nccl env vars that
init_process_group docs have. Also, specify default behavior when backend=None
is passed in.

Differential Revision: [D27672208](https://our.internmc.facebook.com/intern/diff/D27672208/)",pytorch
55716,r-barnes,pr,2021-04-09T22:41:42Z,Use irange in torch/csrc/jit,"Differential Revision: D27690245

",pytorch
55718,rohan-varma,pr,2021-04-09T22:57:26Z,[c10d] Increment sequence numbers on collectives.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55718 [c10d] Increment sequence numbers on collectives.**

Increments sequence numbers when ProcessGroupGloo::enqueue or
ProcessGroupNCCL::collective is run, which is a common call all collectives
make. The next step will be to log these along with other collective info in
debug mode as well as integrating them with the process group wrapper.

Differential Revision: [D27690690](https://our.internmc.facebook.com/intern/diff/D27690690/)",pytorch
55746,ngimel,pr,2021-04-11T04:15:03Z,"`addmv`: port to structured kernels, improve error checks","Per title
I've revamped size checks a bit to provide better error message if `self` is of the wrong size, also added check that inplace variant has correct `self` size

Ref: #55070 
",pytorch
55848,r-barnes,pr,2021-04-12T22:30:26Z,Add stricter typing to caffe2/torch/distributed/elastic/multiprocessing/errors/__init__.py,"Differential Revision: D27714781

",pytorch
55855,r-barnes,pr,2021-04-12T23:52:54Z,Additional annotations in fbcode/caffe2/torch/_jit_internal.py,"Differential Revision: D27715202

",pytorch
55856,r-barnes,pr,2021-04-12T23:53:37Z,Better types in fbcode/caffe2/torch/jit/_script.py,"Differential Revision: D27715495

",pytorch
55861,rohan-varma,pr,2021-04-13T00:52:20Z,"[c10d] Remove deprecated use of torch.LongTensor, torch.ByteTensor","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55861 [c10d] Remove deprecated use of torch.LongTensor, torch.ByteTensor**

APIs such as torch.LongTensor and torch.ByteTensor are deprecated and
the recommended API is torch.tensor(args, dtype=...). Use this API in
distributed_c10d.

Differential Revision: [D27726600](https://our.internmc.facebook.com/intern/diff/D27726600/)",pytorch
55880,ngimel,pr,2021-04-13T05:31:40Z,allow tests to run locally without setting environment variables,"Fixes breakage caused by #55753
",pytorch
55913,nSircombe,pr,2021-04-13T15:55:20Z,Enables builds with Compute Library backend for oneDNN,"Since v1.7, oneDNN (MKL-DNN) has supported the use of Compute Library
for the Arm architeture to provide optimised convolution primitives
on AArch64.

This change enables the use of Compute Library in the PyTorch build.
Following the approach used to enable the use of CBLAS in MKLDNN,
It is enabled by setting the env vars USE_MKLDNN and USE_MKLDNN_ACL.
The location of the Compute Library build must be set useing `ACL_ROOT_DIR`.

This is an extension of the work in https://github.com/pytorch/pytorch/pull/50400
which added support for the oneDNN/MKL-DNN backend on AArch64.

_Note: this assumes that Compute Library has been built and installed at
ACL_ROOT_DIR. Compute library can be downloaded here:
`https://github.com/ARM-software/ComputeLibrary`_

Fixes #{issue number}
",pytorch
55974,r-barnes,pr,2021-04-13T23:39:17Z,Annotate functions in _jit_internal.py,"Differential Revision: D27752786

",pytorch
55979,rohan-varma,pr,2021-04-14T00:57:52Z,Fix tensorpipe test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55979 Fix tensorpipe test**

Fix name used for this test

Differential Revision: [D27755320](https://our.internmc.facebook.com/intern/diff/D27755320/)",pytorch
55989,rohan-varma,pr,2021-04-14T04:51:10Z,[reland][c10d] Log API usage of monitored barrier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55990 [reland][c10d] monitored_barrier: ensure all ranks pass or none do
* **#55989 [reland][c10d] Log API usage of monitored barrier**

Reland of https://github.com/pytorch/pytorch/pull/55197, which fails windows test that was only run on master.

Differential Revision: [D27758425](https://our.internmc.facebook.com/intern/diff/D27758425/)",pytorch
55990,rohan-varma,pr,2021-04-14T04:51:17Z,[reland][c10d] monitored_barrier: ensure all ranks pass or none do,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55990 [reland][c10d] monitored_barrier: ensure all ranks pass or none do**
* #55989 [reland][c10d] Log API usage of monitored barrier

Reland of https://github.com/pytorch/pytorch/pull/55197, which fails windows test that was only run on master.

Disabled these tests for windows, similar to they are disabled on MacOS. The reason for disabling as that they use libuv transport which does not have as robust error handling as tcp on linux. The result is that non-zero ranks that were healthy don't throw immediately (like they do on linux) but they throw on timeout. Waiting for timeout will make test run for an unreasonably long time. The error handling still occurs as expected on rank 0 for all platforms.

Ci-all PR https://github.com/pytorch/pytorch/pull/55991

Differential Revision: [D27758424](https://our.internmc.facebook.com/intern/diff/D27758424/)",pytorch
55991,rohan-varma,pr,2021-04-14T04:54:45Z,Ci-all for https://github.com/pytorch/pytorch/pull/55990,"Fixes #{issue number}
",pytorch
55994,eqy,pr,2021-04-14T05:32:14Z,Add `USE_MAGMA` build flag,"Many model pipelines/workflows don't use MAGMA even though it is included in the build by default. Leaving MAGMA kernels out of the build can save 60+MB of GPU memory when loading `libtorch_cuda.so` (tested on V100, current upstream master).

A current sharp corner of this flag is that toggling it when rebuilding requires `torch/include/THC/THCGeneral.h` to be *manually* deleted by the user, as even running `make clean` or `setup.py` with `--cmake` does not properly regenerate it with the appropriate substitution for `#cmakedefine USE_MAGMA`. Is there a way to force the regeneration of the header during a rebuild?

CC @malfet @ptrblck ",pytorch
56005,rohan-varma,pr,2021-04-14T07:51:23Z,Ci all/rvarm1/param name ddp,Creating this CI all for 55075 as it is a large PR and it would be good to test it on multigpu.,pytorch
56054,rohan-varma,pr,2021-04-14T19:07:44Z,[wip] enhance DDPSink to work for general outputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56054 [wip] enhance DDPSink to work for general outputs**
* #55248 [WIP] enable static graph training in DDP
* #54995 static graph api skeleton
* #54981 refactor autograd_hook
* #54977 refactor prepare_for_backward

Enhances use of DDPSink to work for all output types DDP supports as per https://github.com/pytorch/pytorch/issues/55876.

TODO: Add additional testing for tuple, list, dict return types

Differential Revision: [D27756985](https://our.internmc.facebook.com/intern/diff/D27756985/)",pytorch
56101,ngimel,pr,2021-04-15T00:38:16Z,cleanup unused implicit argument of expand function,"Per title
",pytorch
56139,qingyunqu,pr,2021-04-15T12:02:28Z,Migrate acos to structured kernel,"Fixes #55070
Migrate acos to structured kernel

",pytorch
56216,rohan-varma,pr,2021-04-15T23:57:47Z,[Dist profiling] Enable tests for dist profiling with torch.profiler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #56840 Clang format ProcessGroupNCCL.cpp
* #56427 [c10d] Profiler support for nccl p2p collectives
* **#56216 [Dist profiling] Enable tests for dist profiling with torch.profiler**
* #55204 [Dist profiling] Fix ProcessGroupNCCL collective profiling

Verifies that the newly added distributed profiling works as expected for torch.profiler.

Example trace from `test_ddp_profiling`:

```
1	
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                               cudaEventCreateWithFlags         0.00%      90.000us         0.00%      90.000us       1.429us       0.000us         0.00%       0.000us       0.000us            63  
                                        cudaEventRecord         0.01%     276.000us         0.01%     276.000us       4.059us       0.000us         0.00%       0.000us       0.000us            68  
                                               aten::to         0.01%     236.000us         0.01%     236.000us      39.333us       0.000us         0.00%       0.000us       0.000us             6  
                                           aten::linear         0.02%     363.000us        31.04%     603.343ms     100.557ms       0.000us         0.00%      10.000us       1.667us             6  
                                                aten::t         0.10%       1.851ms         0.23%       4.483ms     186.792us       1.000us         1.30%       3.000us       0.125us            24  
                                        aten::transpose         0.09%       1.672ms         0.14%       2.632ms     109.667us       1.000us         1.30%       2.000us       0.083us            24  
                                       aten::as_strided         0.07%       1.344ms         0.07%       1.344ms      36.324us       1.000us         1.30%       1.000us       0.027us            37  
                                           aten::matmul         0.01%     235.000us        30.96%     601.909ms     100.318ms       0.000us         0.00%      10.000us       1.667us             6  
                                               aten::mm         1.68%      32.731ms        31.70%     616.231ms      51.353ms      40.000us        51.95%      44.000us       3.667us            12  
                                            aten::empty         0.07%       1.327ms         0.07%       1.337ms      46.103us       2.000us         2.60%       2.000us       0.069us            29  
                                               cudaFree        29.31%     569.816ms        29.31%     569.816ms      94.969ms       0.000us         0.00%       0.000us       0.000us             6  
                                         cudaEventQuery         0.00%      18.000us         0.00%      18.000us       2.250us       0.000us         0.00%       0.000us       0.000us             8  
                                       cudaEventDestroy         0.07%       1.425ms         0.07%       1.425ms      95.000us       0.000us         0.00%       0.000us       0.000us            15  
                                 cudaDeviceGetAttribute         0.00%       1.000us         0.00%       1.000us       0.042us       0.000us         0.00%       0.000us       0.000us            24  
                                             cudaMalloc         0.55%      10.703ms         0.55%      10.703ms       1.338ms       0.000us         0.00%       0.000us       0.000us             8  
                                             cudaMemcpy         0.00%      57.000us         0.00%      57.000us      28.500us       0.000us         0.00%       0.000us       0.000us             2  
                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         2.60%       2.000us       1.000us             2  
                                       cudaLaunchKernel        67.50%        1.312s        67.50%        1.312s      23.857ms       0.000us         0.00%       0.000us       0.000us            55  
void gemmSN_TN_kernel_64addr<float, 128, 16, 2, 4, 4...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us        11.69%       9.000us       9.000us             1  
                                              aten::sum         0.05%       1.049ms        67.51%        1.312s     218.719ms       9.000us        11.69%       9.000us       1.500us             6  
void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us        11.69%       9.000us       9.000us             1  
                                        aten::ones_like         0.02%     391.000us         0.07%       1.373ms     228.833us       0.000us         0.00%       2.000us       0.333us             6  
                                       aten::empty_like         0.04%     755.000us         0.07%       1.359ms     113.250us       0.000us         0.00%       1.000us       0.083us            12  
                                    aten::empty_strided         0.02%     292.000us         0.02%     292.000us      48.667us       0.000us         0.00%       0.000us       0.000us             6  
                                            aten::fill_         0.01%     277.000us         0.02%     392.000us      65.333us       2.000us         2.60%       2.000us       0.333us             6  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         2.60%       2.000us       2.000us             1  
                                           SumBackward0         0.02%     404.000us         0.07%       1.284ms     214.000us       0.000us         0.00%       0.000us       0.000us             6  
                                           aten::expand         0.03%     662.000us         0.05%     880.000us     146.667us       0.000us         0.00%       0.000us       0.000us             6  
                                             MmBackward         0.05%     969.000us         0.91%      17.652ms       2.942ms       0.000us         0.00%      36.000us       6.000us             6  
                                             aten::conj         0.01%     103.000us         0.01%     103.000us      17.167us       0.000us         0.00%       0.000us       0.000us             6  
                                            aten::clone         0.03%     640.000us         0.10%       2.005ms     334.167us       0.000us         0.00%       3.000us       0.500us             6  
                                            aten::copy_         0.03%     538.000us         0.04%     854.000us      85.400us       2.000us         2.60%       2.000us       0.200us            10  
                                      sgemm_32x32x32_NN         0.00%       0.000us         0.00%       0.000us       0.000us      29.000us        37.66%      29.000us       9.667us             3  
                                              TBackward         0.01%     225.000us         0.08%       1.614ms     269.000us       0.000us         0.00%       1.000us       0.167us             6  
                        torch::autograd::AccumulateGrad         0.02%     376.000us         0.05%     914.000us     152.333us       0.000us         0.00%       3.000us       0.500us             6  
                                           aten::detach         0.00%      51.000us         0.00%      88.000us      88.000us       2.000us         2.60%       2.000us       2.000us             1  
                                                 detach         0.00%      37.000us         0.00%      37.000us      37.000us       0.000us         0.00%       0.000us       0.000us             1  
                                              aten::mul         0.03%     527.000us         0.04%     685.000us     114.167us       2.000us         2.60%       2.000us       0.333us             6  
                                    cudaStreamWaitEvent         0.00%      44.000us         0.00%      44.000us       1.833us       0.000us         0.00%       0.000us       0.000us            24  
                                        nccl:all_reduce         0.05%     909.000us         0.06%       1.130ms     188.333us      13.000us        16.88%      13.000us       2.167us             6  
                                        cudaMemcpyAsync         0.02%     302.000us         0.02%     302.000us      30.200us       0.000us         0.00%       0.000us       0.000us            10  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       7.000us         9.09%       7.000us       1.167us             6  
                                              aten::add         0.02%     400.000us         0.03%     533.000us      88.833us       1.000us         1.30%       1.000us       0.167us             6  
                                   cudaEventSynchronize         0.00%      55.000us         0.00%      55.000us       2.200us       0.000us         0.00%       0.000us       0.000us            25  
                                   cudaEventElapsedTime         0.00%      47.000us         0.00%      47.000us       2.350us       0.000us         0.00%       0.000us       0.000us            20  
                                         nccl:broadcast         0.01%     195.000us         0.01%     245.000us     122.500us       0.000us         0.00%       0.000us       0.000us             2  
                                  cudaStreamSynchronize         0.00%       7.000us         0.00%       7.000us       3.500us       0.000us         0.00%       0.000us       0.000us             2  
                                             aten::add_         0.02%     326.000us         0.02%     450.000us      90.000us       1.000us         1.30%       1.000us       0.200us             5  
void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         2.60%       2.000us       2.000us             1  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         2.60%       2.000us       1.000us             2  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         2.60%       2.000us       2.000us             1  
            ncclAllReduceRingLLKernel_sum_f32(ncclColl)         0.00%       0.000us         0.00%       0.000us       0.000us      13.000us        16.88%      13.000us      13.000us             1  
                                  cudaDeviceSynchronize         0.00%       8.000us         0.00%       8.000us       8.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 1.944s
Self CUDA time total: 77.000us

```

Note that tests are disabled internally due to an unrelated hang issue but run in OSS.

Differential Revision: [D27645105](https://our.internmc.facebook.com/intern/diff/D27645105/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D27645105/)!",pytorch
56265,lezcano,pr,2021-04-16T14:59:37Z,Improved docs for `torch.linalg`,"This PR tries to make the docs of `torch.linalg` have/be:
- More uniform notation and structure for every function.
- More uniform use of back-quotes and the `:attr:` directive
- More readable for a non-specialised audience through explanations of the form that factorisations take and when would it be beneficial to use what arguments in some solvers.
- More connected among the different functions through the use of  the `.. seealso::` directive.
- More information on when do gradients explode / when is a function silently returning a wrong result / when things do not work in general

I tried to follow the structure of ""one short description and then the rest"" to be able to format the docs like those of `torch.` or `torch.nn`. I did not do that yet, as I am waiting for the green light on this idea:
https://github.com/pytorch/pytorch/issues/54878#issuecomment-816636171

What this PR does not do:
- Clean the documentation of other functions that are not in the `linalg` module (although I started doing this for `torch.svd`, but then I realised that this PR would touch way too many functions).

Fixes https://github.com/pytorch/pytorch/issues/54878

cc @mruberry @IvanYashchuk ",pytorch
56293,ngimel,pr,2021-04-16T20:12:11Z,clean up unused reduction functions in THC,Per title,pytorch
56403,eqy,pr,2021-04-19T19:44:52Z,fix misaligned access #56325,"CC @ngimel @ptrblck
ref: #56325",pytorch
56412,rohan-varma,pr,2021-04-19T20:58:16Z,Separate profiling tests from p2p tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56412 Separate profiling tests from p2p tests**

We are investigating some flaky profiiling tests such as https://github.com/pytorch/pytorch/issues/56146. One issue is that the profiling tests are tightly coupled to these send/recv tests, hence if this test is disabled, we lose signal round send/recv collectives tests.

To mitigate this, separate the tests into ones that only test send/recv, and ones that test it with profiling. This way flakiness should not result in the send/recv only tests being disabled.

Differential Revision: [D27864845](https://our.internmc.facebook.com/intern/diff/D27864845/)",pytorch
56427,rohan-varma,pr,2021-04-19T22:04:02Z,[c10d] Profiler support for nccl p2p collectives,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #56840 Clang format ProcessGroupNCCL.cpp
* **#56427 [c10d] Profiler support for nccl p2p collectives**

This PR enables support for nccl send/recv profiling similar to how we have it for MPI and Gloo.

The process to do so is similar to the NCCL collectives where we create the `recordingFunction` in `initWork` and then add a callback that runs the profiler end callbacks. Tests are added similar to send/recv tests with gloo/MPI.

We also test with both autograd profiler and torch.profiler.

Profiler output:

```
----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                cudaFree         8.95%     134.888ms         8.95%     134.888ms     134.888ms       0.000us         0.00%       0.000us       0.000us             1  
            cudaStreamCreateWithPriority         0.07%       1.060ms         0.07%       1.060ms      16.562us       0.000us         0.00%       0.000us       0.000us            64  
                cudaEventCreateWithFlags         0.00%      22.000us         0.00%      22.000us       3.667us       0.000us         0.00%       0.000us       0.000us             6  
                   cudaDeviceGetPCIBusId         0.00%      21.000us         0.00%      21.000us       1.909us       0.000us         0.00%       0.000us       0.000us            11  
                           cudaHostAlloc         0.05%     710.000us         0.05%     710.000us     355.000us       0.000us         0.00%       0.000us       0.000us             2  
                  cudaDeviceGetAttribute         0.00%       1.000us         0.00%       1.000us       0.250us       0.000us         0.00%       0.000us       0.000us             4  
                              cudaMalloc         2.41%      36.381ms         2.41%      36.381ms       6.064ms       0.000us         0.00%       0.000us       0.000us             6  
                              cudaMemset        87.30%        1.316s        87.30%        1.316s     219.251ms       0.000us         0.00%       0.000us       0.000us             6  
                         Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      70.000us         1.44%      70.000us      11.667us             6  
                      cudaGetDeviceCount         0.00%       1.000us         0.00%       1.000us       0.167us       0.000us         0.00%       0.000us       0.000us             6  
                 cudaDeviceCanAccessPeer         0.00%       5.000us         0.00%       5.000us       2.500us       0.000us         0.00%       0.000us       0.000us             2  
                     cudaIpcGetMemHandle         0.00%      45.000us         0.00%      45.000us      22.500us       0.000us         0.00%       0.000us       0.000us             2  
                    cudaIpcOpenMemHandle         0.03%     427.000us         0.03%     427.000us     213.500us       0.000us         0.00%       0.000us       0.000us             2  
                              cudaMemcpy         0.00%      74.000us         0.00%      74.000us      14.800us       0.000us         0.00%       0.000us       0.000us             5  
        Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.12%       6.000us       1.000us             6  
                         cudaEventRecord         0.00%      11.000us         0.00%      11.000us       1.375us       0.000us         0.00%       0.000us       0.000us             8  
                     cudaStreamWaitEvent         0.00%      10.000us         0.00%      10.000us       1.429us       0.000us         0.00%       0.000us       0.000us             7  
                               nccl:send         0.08%       1.272ms         0.54%       8.150ms       8.150ms      19.000us         0.39%      19.000us      19.000us             1  
                        cudaLaunchKernel         0.46%       6.886ms         0.46%       6.886ms       3.443ms       0.000us         0.00%       0.000us       0.000us             2  
    ncclSendRecvKernel_copy_i8(ncclColl)         0.00%       0.000us         0.00%       0.000us       0.000us       4.772ms        98.41%       4.772ms       2.386ms             2  
                      record_param_comms         0.00%      46.000us         0.00%      51.000us      25.500us       0.000us         0.00%       0.000us       0.000us             2  
                        cudaEventDestroy         0.00%       3.000us         0.00%       3.000us       0.750us       0.000us         0.00%       0.000us       0.000us             4  
                             aten::empty         0.03%     389.000us         0.03%     389.000us      32.417us       0.000us         0.00%       0.000us       0.000us            12  
                             aten::fill_         0.01%     148.000us         0.01%     148.000us      49.333us       0.000us         0.00%       0.000us       0.000us             3  
                                aten::to         0.04%     574.000us         0.34%       5.157ms     644.625us       0.000us         0.00%       2.000us       0.250us             8  
                     aten::empty_strided         0.01%     205.000us         0.01%     205.000us      41.000us       0.000us         0.00%       0.000us       0.000us             5  
                             aten::copy_         0.02%     228.000us         0.29%       4.378ms     875.600us       2.000us         0.04%       2.000us       0.400us             5  
                         cudaMemcpyAsync         0.27%       4.137ms         0.27%       4.137ms       2.068ms       0.000us         0.00%       0.000us       0.000us             2  
                   cudaStreamSynchronize         0.00%      13.000us         0.00%      13.000us       6.500us       0.000us         0.00%       0.000us       0.000us             2  
                               nccl:recv         0.01%     116.000us         0.01%     138.000us     138.000us       4.753ms        98.02%       4.753ms       4.753ms             1  
        Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.02%       1.000us       1.000us             1  
                     aten::promote_types         0.00%      23.000us         0.00%      23.000us      23.000us       0.000us         0.00%       0.000us       0.000us             1  
                          aten::allclose         0.01%     134.000us         0.29%       4.443ms       4.443ms       0.000us         0.00%       0.000us       0.000us             1  
                           aten::isclose         0.03%     499.000us         0.26%       3.934ms       3.934ms       0.000us         0.00%       0.000us       0.000us             1  
                                aten::eq         0.02%     304.000us         0.03%     446.000us     111.500us       0.000us         0.00%       0.000us       0.000us             4  
                                aten::ne         0.04%     532.000us         0.08%       1.272ms     212.000us       0.000us         0.00%       0.000us       0.000us             6  
                          aten::__iand__         0.01%      89.000us         0.02%     287.000us     143.500us       0.000us         0.00%       0.000us       0.000us             2  
                      aten::bitwise_and_         0.01%      91.000us         0.01%     198.000us      99.000us       0.000us         0.00%       0.000us       0.000us             2  
                       aten::bitwise_and         0.01%     107.000us         0.01%     107.000us      53.500us       0.000us         0.00%       0.000us       0.000us             2  
                           aten::__ior__         0.01%      80.000us         0.02%     287.000us     143.500us       0.000us         0.00%       0.000us       0.000us             2  
                       aten::bitwise_or_         0.01%     111.000us         0.01%     207.000us     103.500us       0.000us         0.00%       0.000us       0.000us             2  
                        aten::bitwise_or         0.01%      96.000us         0.01%      96.000us      48.000us       0.000us         0.00%       0.000us       0.000us             2  
                               aten::mul         0.02%     241.000us         0.03%     395.000us     197.500us       0.000us         0.00%       0.000us       0.000us             2  
                               aten::abs         0.03%     419.000us         0.04%     645.000us     107.500us       0.000us         0.00%       0.000us       0.000us             6  
                               aten::add         0.01%     141.000us         0.02%     310.000us     310.000us       0.000us         0.00%       0.000us       0.000us             1  
                               aten::sub         0.00%      74.000us         0.00%      74.000us      74.000us       0.000us         0.00%       0.000us       0.000us             1  
                          aten::isfinite         0.01%     160.000us         0.07%     985.000us     985.000us       0.000us         0.00%       0.000us       0.000us             1  
                              aten::item         0.00%      66.000us         0.01%     123.000us      61.500us       0.000us         0.00%       0.000us       0.000us             2  
               aten::_local_scalar_dense         0.00%      57.000us         0.00%      57.000us      28.500us       0.000us         0.00%       0.000us       0.000us             2  
                                aten::le         0.01%     169.000us         0.02%     238.000us     119.000us       0.000us         0.00%       0.000us       0.000us             2  
                               aten::all         0.01%     226.000us         0.02%     310.000us     310.000us       0.000us         0.00%       0.000us       0.000us             1  
                        aten::as_strided         0.00%      36.000us         0.00%      36.000us      36.000us       0.000us         0.00%       0.000us       0.000us             1  
----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 1.507s
Self CUDA time total: 4.849ms
```

Differential Revision: [D27866600](https://our.internmc.facebook.com/intern/diff/D27866600/)",pytorch
56450,r-barnes,pr,2021-04-20T04:57:53Z,Types for torch.jit.script and friends,"Differential Revision: D27872158

",pytorch
56459,qingyunqu,pr,2021-04-20T08:18:33Z,`max_pool2d_with_indices`: port to structured kernel,"Port max_pool2d_with_indices to structured kernel
#55070
also clean code for #56320",pytorch
56530,rohan-varma,pr,2021-04-20T20:58:46Z,[c10d] Add debug level field in ProcessGroup,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56530 [c10d] Add debug level field in ProcessGroup**

For upcoming diffs, ProcessGroup will need to know about debug level
for e.g. logging collective operations.

Differential Revision: [D27849839](https://our.internmc.facebook.com/intern/diff/D27849839/)",pytorch
56754,rohan-varma,pr,2021-04-23T01:29:41Z,[DDP] remove backend constraints on uneven input tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #56755 [DDP] Uneven inputs: option to throw early
* **#56754 [DDP] remove backend constraints on uneven input tests**

Previously these tests would not run on CI envs that did not have NCCL, but these tests should be okay to run when there is only Gloo.

Differential Revision: [D27954174](https://our.internmc.facebook.com/intern/diff/D27954174/)",pytorch
56755,rohan-varma,pr,2021-04-23T01:29:48Z,[DDP] Uneven inputs: option to throw early,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56755 [DDP] Uneven inputs: option to throw early**

Rehash of https://github.com/pytorch/pytorch/pull/47488

Adds a flag to ddp join() context manager that enables throwing an error across all ranks when this flag is specified.

To do this, we implement the design in #47250. When running with this flag, we schedule an additional allreduce in the case that a joined rank needs to throw a StopIteration. In non-joined ranks forward pass, we match this allreduce and if at least one rank tells us to throw, we raise an error.

Tested by modifying existing tests, as well as adding additional tests validating that this works with SyncBatchNorm models and a model with custom collectives in the forward pass.

Due to the extra blocking allreduce introduced, there is an expected 2-3% perf gap when running with this flag. Benchmark results are available on internal diff. We will only recommend it for models with custom collective communication in the forward pass and consider performance improvements based on OSS user feedback.

Differential Revision: [D27958369](https://our.internmc.facebook.com/intern/diff/D27958369/)",pytorch
56840,rohan-varma,pr,2021-04-23T23:26:21Z,Clang format ProcessGroupNCCL.cpp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56840 Clang format ProcessGroupNCCL.cpp**
* #56427 [c10d] Profiler support for nccl p2p collectives

Per comments in https://github.com/pytorch/pytorch/pull/56427/files

Differential Revision: [D27980768](https://our.internmc.facebook.com/intern/diff/D27980768/)",pytorch
56877,bharatr21,pr,2021-04-25T05:15:12Z,DOC: Add note to mutating methods,"Fixes #56243 by adding a note to mutating functions not following the trailing `_` convention in `torch/nn/modules/module.py`

I can also raise separate PRs for other files, if needed
",pytorch
56880,rohan-varma,pr,2021-04-25T06:46:51Z,[Not for review] Debug dist profiling flakiness ,"Fixes #{issue number}
",pytorch
56955,ngimel,pr,2021-04-26T20:58:03Z,[WIP] Generating aliases,"Here are minimum changes to codegen that would allow us to define aliases in native_functions.yaml. 
This avoids most of the codegen for the new function, only substituting a call to the base function at the python bindings codegen level. An advantage of this approach is that there's minimum effect on build time and binary size, and autograd works automatically (it only ever sees base function which is presumably knows how to handle). Alias symbols don't get their own schema, they are short-lived and disappear as soon as we leave python. 
Issues:
1) There's an unsolved problem with jit here, because the new symbol is not added to interned strings, so jit script does not work. I could not find a convenient place in jit to do string substitution of the new symbol for the base symbol. 
2) aliases in c++ are not supported - do we need them? I don't thinks so. If we decide that we do, changes to codegeining CPUFunctions.h/CUDAFunctions.h/Functions.cpp and friends will be necessary, but not to Register**, those should work automatically (the only functions called wheh dispatching to backends should be base functions)
3) only full aliases (for all overloads) are supported, so things like `max` where we want to alias only some overloads can't be easily supported with this approach
4) for functions in the submodules (such as torch.linalg, torch.special etc) python exposure is done with `_add_docstr` mechanism in `__init__.py` file, so literally no codegen is necessary, just add `_add_docstr` linking to base function in the `__init__.py` so this is not handled in this PR
5) Docstring is not updated automatically, maybe we can see how we can do that (that'll also possibly address submodule `_add_docstr` issue).

An alternative is to define aliases like any other composite op within ""composite ops in python"" project. The drawback of this is that (depending on implementation) likely this will result in extra dispatch overhead (that's pretty much what's happening with aliases today, when they are implemented in ATen/native).

Another alternative is to use python language constructs to say `torch.foo = torch.bar`, but that doesn't extend well to supporting methods and script. 
",pytorch
56963,rohan-varma,pr,2021-04-26T22:34:26Z,[Dist profiling] Fix flaky tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56963 [Dist profiling] Fix flaky tests**

Tests such as https://github.com/pytorch/pytorch/issues/50840 and https://github.com/pytorch/pytorch/issues/56690 have been flaky due to the profiling part.

What was happening is that in these tests, we occasionally had inaccurate (off by one) counts of the expected no. of distributed collectives events expected to be profiled.

It was challenging to find the root cause of this as the issue was extremely rare. Digging into profiler's `parse_legacy_records`, I verified the correct no. of events were alway returned. However, when constructing `EventList` after parsing records, sometimes distributed collective events were getting filtered out leading to a reduction in their count.

The reason they were getting filtered out is because `_remove_dup_nodes` filters out parent/child events that appear to be the same event based on some heuristics. However these distributed collectives were incorrectly getting picked up as parent/child events since their times can overlap due to their async nature. To fix, add a blocklist and bypass the filtering if it is a distributed collective.

Testplan:

To reproduce:
1) SSH into circleCI instance that runs distributed_test.py with 3 GPUs
2) Modify `test_send_recv_autograd_profiler` to run 100+ send/recv calls. Without this patch the issue reproduces, verify that this patch fixes the issue. 
Was able to reproduce the issue more often by doing 100+ send/recv calls in _test_send_recv. Verified this fix on CI machine by running the test thousands of times.

An alternative fix is to mark these distributed collectives as async via the `is_async` flag (because work/wait() paradigm is basically async). This might be a bit cleaner as it also bypasses the parent/child heuristics. 

Differential Revision: [D28013540](https://our.internmc.facebook.com/intern/diff/D28013540/)",pytorch
56967,eqy,pr,2021-04-26T23:13:15Z,fix #56822,"Fixes #56822

There was an off by one in CPU randperm when checking the limits of the requested range. Also shows up in the ""CUDA"" version as it will fallback to CPU for small input sizes.

CC @zasdfgbnm 
",pytorch
56982,rohan-varma,pr,2021-04-27T03:40:14Z,[BE][SyncBN] Avoid sync stats in eval mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#56982 [BE][SyncBN] Avoid sync stats in eval mode**

SyncBatchNorm should behave as a regular BN layer in eval model, this
change ensures that this is the case.

In particular, the bug was when `track_running_stats=False`, `bn_training` would be set to True in eval mode, but this would trigger a collective sync in syncBN.

When `track_running_stats=True`, this is not an issue as in inference mode, `(self.running_mean is None) and (self.running_var is None)` would evaluate to False and we would not sync either way.

However, in eval mode syncBN should behave like a regular BN layer and not do this sync.

Closes https://github.com/pytorch/pytorch/issues/48988

Ensured with unittest that when used for inference on a single rank, stats sync is not triggered.

Differential Revision: [D27579297](https://our.internmc.facebook.com/intern/diff/D27579297/)",pytorch
56993,crcrpar,pr,2021-04-27T05:44:51Z,[PyTorch] Reduce errors of `foreach` functions,"This is based on  #48224. 

To make `foreach` more flexible, this PR pushes unsupported cases to slow path.
Also, this adds some tests to verify that
- `foreach` functions work with tensors of different dtypes and/or memory layouts in https://github.com/pytorch/pytorch/commit/7bd4b2c89fad23c17a58969623ea7145833548a1
- `foreach` functions work with tensors on different devices in a list, but are on the same device if the indices are the same: https://github.com/pytorch/pytorch/commit/def4b9b5a19c325bb7f82ef6d69ca28fa2927131

Future plans:
1. Improve the coverage of unittests using `ops` decorator & updating `foreach_unary_op_db` and creating `foreach_(binary|pointwise|minmax)_db`.
2. Support broadcasting in slow path. Ref:  https://github.com/pytorch/pytorch/pull/52448
3. Support type promotion in fast path. Ref https://github.com/pytorch/pytorch/pull/52449

CC: @ngimel @mcarilli  @ptrblck ",pytorch
57043,ssnl,pr,2021-04-27T18:54:16Z,Fix unflatten example doc rendering,"doc doesn't seem to detect the end of the code block well, this should fix it.

Fixes #{issue number}
",pytorch
57073,rohan-varma,pr,2021-04-28T00:19:03Z,enhance DDPSink to work for general outputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #57081 [DDP] Support not all outputs used in loss calculation
* **#57073 enhance DDPSink to work for general outputs**

Enhances use of DDPSink to work for all output types DDP supports as per https://github.com/pytorch/pytorch/issues/55876.

TODO: Add additional testing for tuple, list, dict return types

Differential Revision: [D27756985](https://our.internmc.facebook.com/intern/diff/D27756985/)",pytorch
57081,rohan-varma,pr,2021-04-28T01:03:30Z,[DDP] Support not all outputs used in loss calculation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57081 [DDP] Support not all outputs used in loss calculation**

Changes in this diff:
1. Enable passthrough autograd function when find_unused_parameters=True.
2. With above, move `prepare_for_backward` which does unused parameter checking logic to beginning of backwards pass, only when `find_unused_parameters=True`. 
3. Enhance process of unused parameter checking to account for outputs not being used in loss.

The way (3) is implemented is by triggering the autograd hook corresponding to parameters that did not participate in loss computation. Since they did not participate, the autograd hook is triggered with a gradient of None, and the reducer handles this appropriately to ensure that the gradient is not touched.

Tested by ensuring that when a model output is not used in loss, the corresponding grad is not modified. Also verified that the grads are the same in local vs DDP training case. Also verified that gradients are not touched in this case, i.e. if grad is originally `None`, it stays as `None`, not zero, after.

Note that in this diff we are not enabling the pass through autograd function for regular case find_unused_parameters=False because that has a much bigger blast radius and needs additional careful analysis especially with regard to the performance. 


Differential Revision: [D28048628](https://our.internmc.facebook.com/intern/diff/D28048628/)",pytorch
57177,ngimel,pr,2021-04-28T19:30:52Z,cat: support fast path for discontiguous tensors only for contiguous output format,"Fixes #57122
",pytorch
57205,rohan-varma,pr,2021-04-28T22:26:09Z,[Dist profiling] Fix flaky tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57205 [Dist profiling] Fix flaky tests**

Tests such as https://github.com/pytorch/pytorch/issues/50840 and https://github.com/pytorch/pytorch/issues/56690 have been flaky due to the profiling part.

What was happening is that in these tests, we occasionally had inaccurate (off by one) counts of the expected no. of distributed collectives events expected to be profiled.

It was challenging to find the root cause of this as the issue was extremely rare. Digging into profiler's `parse_legacy_records`, I verified the correct no. of events were alway returned. However, when constructing `EventList` after parsing records, sometimes distributed collective events were getting filtered out leading to a reduction in their count.

The reason they were getting filtered out is because `_remove_dup_nodes` filters out parent/child events that appear to be the same event based on some heuristics. However these distributed collectives were incorrectly getting picked up as parent/child events since their times can overlap due to their async nature. To fix, add a blocklist and bypass the filtering if it is a distributed collective.

Testplan:
Was able to reproduce the issue more often by doing 100+ send/recv calls in `_test_send_recv`. Verified this fix on CI machine by running the test thousands of times.

Differential Revision: [D28013540](https://our.internmc.facebook.com/intern/diff/D28013540/)",pytorch
57253,rohan-varma,pr,2021-04-29T06:18:16Z,[Dist profiling] Add is_async field,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57253 [Dist profiling] Add is_async field**

This PR:

1. Adds is_async getter/setter to RecordFunction
2. Adds is_async field to LegacyEvent and KinetoEvent, read from RecordFunction
3. Modifies python profiler code to check is_async via this flag (and keeps the old thread check as well)
4. Sets profiling of c10d collectives as async in ProcessGroup.cpp
5. Modifies tests to ensure is_async is set

This also fixes flaky tests such as #50840 and #56690 which have been flaky due to the profiling part (https://github.com/pytorch/pytorch/pull/56963 tried to do so as well but this is a better approach).

Differential Revision: [D28086719](https://our.internmc.facebook.com/intern/diff/D28086719/)",pytorch
57276,lezcano,pr,2021-04-29T12:19:53Z,linalg.eig backwards and linalg.eigvals,This PR adds backwards support for `eig` and `eigvals`.,pytorch
57299,ngimel,pr,2021-04-29T20:46:11Z,"Revert ""Remove sync for randperm on small tensors. (#54113)""","This reverts commit e8c268746b297efa988e03abc61ff22203bf3980.
It occasionally produces wrong results.

",pytorch
57318,eqy,pr,2021-04-29T22:35:18Z,fix comments in ATenNVRTC.h,"Adding a function in ATenNVRTC.h also requires changing Lazy NVRTC.cpp, but this was missing in the comments.
Also fix a typo.

CC @jjsjann123
",pytorch
57331,peterjc123,pr,2021-04-30T02:27:01Z,Fix error code checking for Windows build scripts,"The variable `%errorlevel%` is evaluated before the whole line of command starts, so it is useless when used in a if-block. Also, let's prevent using `%errorlevel%` because it may be set by the users accidentally.

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm",pytorch
57376,rohan-varma,pr,2021-04-30T19:18:30Z,[DDP] Profile search_unused_parameters,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57376 [DDP] Profile search_unused_parameters**

Having this in profiler/trace outputs will be useful when
investigating performance overhead of find_unused_parameters for certain
workloads, to determine whether it is a bottleneck or not.

Differential Revision: [D28126233](https://our.internmc.facebook.com/intern/diff/D28126233/)",pytorch
57448,rohan-varma,pr,2021-05-02T23:26:08Z,Fix document around DDP uneven inputs,Typo fix and additional clarifications on the API. ,pytorch
57479,r-barnes,pr,2021-05-03T18:24:42Z,irange for Indexing.cu,"Differential Revision: D28135714

",pytorch
57517,rohan-varma,pr,2021-05-03T23:50:46Z,[Flaky tests] Fix flaky rpc profiling tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57517 [Flaky tests] Fix flaky rpc profiling tests**

Closes https://github.com/pytorch/pytorch/issues/45145
Closes https://github.com/pytorch/pytorch/issues/45067

Fixes the flaky tests https://github.com/pytorch/pytorch/issues/45145
and https://github.com/pytorch/pytorch/issues/45067.

The root cause is that it is not the case that all remote events will be
children of the record function remote event, as other events can sometimes be
profiled under the hood such as the issue described in
https://github.com/pytorch/pytorch/issues/43868.

We fix this issue by verifying that the set of events that are children on the
remote end and children on the local end are the same, without necessarily
enforcing specific events to be logged.

Tested by running the test 1000+ times and verifying it passed. Will also test on CI box before landing

Differential Revision: [D28166602](https://our.internmc.facebook.com/intern/diff/D28166602/)",pytorch
57520,micmelesse,pr,2021-05-04T01:14:49Z,[ROCm] enable test_cufft_plan_cache test,"This pr enables the test_cufft_plan_cache in test_spectral suite. 
",pytorch
57549,lezcano,pr,2021-05-04T15:49:27Z,[Draft] Deprecation / Aliases for torch.linalg,"Adds deprecation notes and aliases to the documentation and code of `torch.` methods pointing to their equivalents in `torch.linalg`.

It also fixes the function `torch.chain_matmul`, which was not using the `out=` parameter when passed.

<details>
  <summary>Script to check the warnings</summary>
  
```python
import torch
from functools import partial

#DEVICE = ""cpu""
DEVICE = ""cuda""

def square():
    A = torch.rand(3, 3, device=DEVICE)
    A = A.T @ A  + torch.eye(3, device=DEVICE)
    return A

def warn(fun, n_args, has_out=True):
    args = [square() for _ in range(n_args)]
    print(40*""-"" + ""  NORMAL {}  "".format(fun.__name__ if hasattr(fun, ""__name__"") else """") + 40*""-"")
    out = fun(*args)
    if has_out:
        print(40*""-"" + ""  OUT {}  "".format(fun.__name__ if hasattr(fun, ""__name__"") else """") + 40*""-"")
        fun(*args, out=out)
    input()
    print()
    print()

# Shows one by one the warnings (press enter)
# The script should be run once with each of the possibilities for the global variable DEVICE 
warn(torch.cholesky, 1)
warn(torch.eig, 1, has_out=False)     # Warn in common
warn(torch.symeig, 1, has_out=False)  # Warn in common
warn(torch.svd, 1)
warn(torch.qr, 1)
warn(torch.matrix_rank, 1, has_out=False)
warn(partial(torch.matrix_rank, tol=1e-4), 1, has_out=False)  # Has a different path
warn(torch.chain_matmul, 2)
warn(torch.solve, 2)
warn(torch.lstsq, 2)
```
</details>

<details>
  <summary>Script to check that the suggested replacements are correct </summary>

```python
import torch
from functools import partial
from itertools import product

def square(device):
    for size in ((2, 3, 3), (3, 3)):
        A = torch.rand(*size, device=device)
        yield A.transpose(-2, -1) @ A  + torch.eye(3, device=DEVICE)

def non_square(device):
    for size in ((2, 3, 4), (2, 4, 3), (3, 4), (4, 3), (2, 3, 3), (3, 3)):
        yield torch.rand(*size, device=device)

def assert_eq(f1, f2, *args):
    X1 = f1(*args)
    X2 = f2(*args)
    if isinstance(X1, tuple):
        for t1, t2 in zip(X1, X2):
            if not torch.allclose(t1, t2, atol=1e-3, rtol=1e-4):
                print(t1, t2)
                raise RuntimeError(f""{f1.__name__}\n{f2.__name__}\n{args}"")
    else:
        if not torch.allclose(X1, X2, atol=1e-3, rtol=1e-4):
            print(X1, X2)
            raise RuntimeError(f""{f1.__name__}\n{f2.__name__}\n{args}"")


def cholesky(device):
    def my_cholesky(A, upper):
        if upper:
            return torch.linalg.cholesky(A.transpose(-2, -1)).transpose(-2, -1)
        else:
            return torch.linalg.cholesky(A)

    for t, upper in product(square(device), [True, False]):
        assert_eq(torch.cholesky, my_cholesky, t, upper)


def symeig(device):
    def my_symeig(A, eigenvectors):
        if eigenvectors:
            L, V = torch.linalg.eigh(A)
            return L, V.abs()
        else:
            return torch.linalg.eigvalsh(A)

    def torch_symeig(A, eigenvectors):
        if eigenvectors:
            L, V = torch.symeig(A, eigenvectors)
            return L, V.abs()
        else:
            return torch.symeig(A, eigenvectors)[0]

    for t, eigenvectors in product(square(device), [True, False]):
        assert_eq(torch_symeig, my_symeig, t, eigenvectors)

def svd(device):
    def my_svd(A, some, compute_uv):
        if compute_uv:
            U, S, Vh = torch.linalg.svd(A, full_matrices=not some)
            return U.abs(), S, Vh.transpose(-2, -1).conj().abs()
        else:
            S = torch.linalg.svdvals(A)
            return S

    def torch_svd(A, some, compute_uv):
        if compute_uv:
            U, S, V = torch.svd(A, some, True)
            return U.abs(), S, V.abs()
        else:
            _, S, _ = torch.svd(A, some, True)
            return S

    for t, some, compute_uv in product(non_square(device), [True, False], [True, False]):
        assert_eq(torch_svd, my_svd, t, some, compute_uv)

def qr(device):
    def my_qr(A, some):
        return torch.linalg.qr(A, ""reduced"" if some else ""complete"")

    for t, some in product(non_square(device), [True, False]):
        assert_eq(torch.qr, my_qr, t, some)


def matrix_rank(device):
    def my_matrix_rank(A, symmetric):
        return torch.linalg.matrix_rank(A, hermitian=symmetric)

    for t, symmetric in product(square(device), [True, False]):
        assert_eq(torch.matrix_rank, my_matrix_rank, t, symmetric)

def chain_matmul(device):
    def my_multi_dot(*tensors):
        return torch.linalg.multi_dot(tensors)

    make_t = partial(torch.rand, device=device)
    assert_eq(torch.chain_matmul, my_multi_dot, make_t(3,2), make_t(2,3))
    assert_eq(torch.chain_matmul, my_multi_dot, make_t(3,3), make_t(3,3))

def solve(device):
    def my_solve(B, A):
        return torch.linalg.solve(A, B)

    def torch_solve(B, A):
        return torch.solve(B, A).solution

    for t in square(device):
        assert_eq(torch_solve, my_solve, t, t.clone())


def lstsq(device):
    def my_lstsq(B, A):
        return torch.linalg.lstsq(A, B).solution

    def torch_lstsq(B, A):
        return torch.lstsq(B, A).solution[:A.size(1)]

    for t in non_square(device):
        if t.ndim == 3: # torch.lstsq cuda does not support batches
            continue
        if DEVICE == ""cuda"" and t.size(-2) < t.size(-1): # Not implemented
            continue
        assert_eq(torch_lstsq, my_lstsq, t, t.clone())


# If none of them throws, we're good

# eig is too different, we will not compare it
for device in [""cpu"", ""cuda""]:
    cholesky(device)
    symeig(device)
    svd(device)
    qr(device)
    matrix_rank(device)
    chain_matmul(device)
    solve(device)
    lstsq(device)
```
</details>

",pytorch
57711,rohan-varma,pr,2021-05-06T06:08:01Z,[c10d] Log when store based barrier succeeds,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57711 [c10d] Log when store based barrier succeeds**

Seeing some hangs/issues around store based barrier internally, would
be good to have this log to indicate whether store based barrier has completed
successfully or not for a particular rank to debug further.

Differential Revision: [D28249087](https://our.internmc.facebook.com/intern/diff/D28249087/)",pytorch
57713,qingyunqu,pr,2021-05-06T06:56:44Z,Port addmm to structured,"Related #55070
Port addmm to structured kernel
",pytorch
57725,lezcano,pr,2021-05-06T12:16:27Z,Deprecate torch.cholesky,"**BC-breaking note:**

This PR deprecates torch.cholesky in favor of torch.linalg.cholesky. A upgrade guide is added to the documentation for torch.cholesky.

Note this PR DOES NOT remove torch.cholesky. ",pytorch
57727,lezcano,pr,2021-05-06T12:35:33Z,Deprecate torch.eig,"**BC-Breaking Note:**

This deprecates the torch.eig function in favor of the torch.linalg.eig function, which always returns two complex tensors instead of two real-valued tensors. See the documentation for torch.eig for details on how to migrate to torch.linalg.eig.

It DOES NOT remove torch.eig. But torch.eig will throw a warning (once) when called, and direct users to use torch.linalg.eig.",pytorch
57732,lezcano,pr,2021-05-06T14:03:06Z,Deprecate symeig,This one had a tricky usage of `torch.symeig` that had to be replaced. I tested the replacement locally though.,pytorch
57734,lezcano,pr,2021-05-06T14:17:09Z,Deprecate matrix_rank,"This one's straightforward


**BC-breaking Note**

This PR deprecates matrix_rank in favor of linalg.matrix_rank. An upgrade guide from matrix_rank to linalg.matrix_rank is provided in the documentation of matrix_rank.

It DOES NOT remove matrix_rank.",pytorch
57735,lezcano,pr,2021-05-06T14:24:48Z,Deprecate chain_matmul,"This one's easy. I also included a bugfix.


**BC-Breaking Note**

This PR extends the deprecation of chain_matmul to throw a warning when it's called. It also adds an upgrade guide from chain_matmul to multi_dot. 

It DOES NOT remove torch.chain_matmul.",pytorch
57741,lezcano,pr,2021-05-06T15:13:35Z,Deprecate torch.solve,"Deprecate deprecate deprecate.

**BC-breaking Note:**

This PR deprecates torch.solve. It includes an upgrade guide in the documentation for torch.solve describing how to use torch.linalg.solve or torch.lu (or both) instead.

It DOES NOT remove torch.solve.",pytorch
57743,lezcano,pr,2021-05-06T15:28:19Z,Deprecate torch.lstsq,"**BC-breaking note:**

This PR deprecates torch.lstsq; it adds an upgrade guide for how to use torch.linalg.lstsq instead.

It DOES NOT remove torch.lstsq, but warns once when it's called",pytorch
57745,lezcano,pr,2021-05-06T16:01:17Z,Deprecate QR,"**BC-breaking note:**

This PR deprecates the torch.qr function in favor of torch.linalg.qr. An upgarde guide is added to the documentation for torch.qr.

Note that it DOES NOT remove torch.qr, but torch.qr will be removed in a future PyTorch release.",pytorch
57762,r-barnes,pr,2021-05-06T18:48:46Z,Typeshed for torch.nn.named_modules(),"Test Plan: Sandcastle

Reviewed By: arthaud

Differential Revision: D27945412

",pytorch
57771,rohan-varma,pr,2021-05-06T20:35:15Z,[DDP] fix param to name mapping,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57771 [DDP] fix param to name mapping**

This mapping didn't work properly when certain parameters didn't
require grad. Fixed that and added a test.

Differential Revision: [D28265636](https://our.internmc.facebook.com/intern/diff/D28265636/)",pytorch
57790,eqy,pr,2021-05-07T01:26:36Z,`topk`: port to structured,"#55070

There are a few places where `const_cast` is used with utility functions shared with unstructured operators.
The RFC says that assigning to the `out` tensor doesn't work, but that seems to be what e.g., `_allocate_or_resize_output_with_indices` seems to do. Does assignment ""work"" when the tensors are not allocated?",pytorch
57797,qingyunqu,pr,2021-05-07T04:16:09Z,`max_pool2d_with_indices_backward`: port to structured,"Realted #55070
Port max_pool2d_with_indices_backward to structure kernel",pytorch
57810,qingyunqu,pr,2021-05-07T07:29:44Z,`threshold`: port to structured,"Related #55070
Port threshold and threshold_backward to structure

",pytorch
57821,lezcano,pr,2021-05-07T14:43:07Z,"Alias det, slogdet, matrix_power, inverse, pinverse","When doing this, I realised that `torch.linalg.pinv` did not have a note on the problems of its derivative (`torch.pinverse` did have it), so I added that.

As I was at it, I made a bit more explicit the recommendation for some functions in `torch.linalg`  to prefer other functions. I also changed the mentions of ""stable"" to ""numerically stable"" as discussed with @IvanYashchuk and @mruberry

If it seems like too much, I'm happy to move the recommendations part of `torch.linalg` to a different PR, but it was such a small thing that I figured it wouldn't be that big a deal if it was here.",pytorch
57868,eqy,pr,2021-05-07T22:50:11Z,Add `pad_sequence` as a native function,"#56229
",pytorch
57902,ngimel,pr,2021-05-08T21:12:14Z,remove syncs in one_hot,"Fixes #55579
Now on cuda one-hot relies on device-side asserts thrown by scatter
",pytorch
57924,ngimel,pr,2021-05-09T21:02:20Z,tweak sync note wording for linalg docs,"Per title
",pytorch
57959,rohan-varma,pr,2021-05-10T17:18:51Z,[Not for review] CI lint testing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57959 [Not for review] CI lint testing**

Differential Revision: [D28327778](https://our.internmc.facebook.com/intern/diff/D28327778/)",pytorch
57974,rohan-varma,pr,2021-05-10T19:24:51Z,[nccl] log rank when communicator is aborted,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57974 [nccl] log rank when communicator is aborted**

We see this error quite a bit in internal workflows, would be useful
to have this additional logging information here.

Differential Revision: [D28331693](https://our.internmc.facebook.com/intern/diff/D28331693/)",pytorch
58006,eqy,pr,2021-05-11T00:45:20Z,move `flatten_dense_tensors` and `unflatten_dense_tensors` to Native,"#55240

CC @ngimel ",pytorch
58012,r-barnes,pr,2021-05-11T04:34:28Z,Some types for remote_module,"Differential Revision: D28334611

",pytorch
58095,rohan-varma,pr,2021-05-11T22:55:32Z,Remove printout from distributed tests,"These were added to help debug a flaky test, the flaky test has since been resolved. ",pytorch
58105,rohan-varma,pr,2021-05-12T01:21:20Z,[DDP] Don't find tensors if static graph,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58105 [DDP] Don't find tensors if static graph**
* #57073 enhance DDPSink to work for general outputs

When find_unused_parameters=True but static_graph is also set, static graph handles unused parameter accounting, so this code path is not needed

Differential Revision: [D28371954](https://our.internmc.facebook.com/intern/diff/D28371954/)",pytorch
58110,rohan-varma,pr,2021-05-12T02:20:52Z,[DDP] Tests for DDP supporting multiple backwards,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58110 [DDP] Tests for DDP supporting multiple backwards**
* #57081 [DDP] Support not all outputs used in loss calculation
* #57073 enhance DDPSink to work for general outputs

Now that prepare_for_backward is moved to the backwards() of the
_DDPSink and not at the end of the forward, we should seamlessly support
multiple bwd with retain_graph=True.

Also includes a fix for issue described in https://github.com/pytorch/pytorch/issues/58111; we now use the # of backwards call to determine `static_graph_first_iteration()` and `static_graph_after_first_iteration()`. 

Differential Revision: [D28370999](https://our.internmc.facebook.com/intern/diff/D28370999/)",pytorch
58125,qingyunqu,pr,2021-05-12T06:55:57Z,Add multi-thread CUDAStreamGuard test,"Add testcase for multi-thread CUDAStreamGuard.
",pytorch
58137,lezcano,pr,2021-05-12T14:05:43Z,cfloat and cdouble functions,"This adds the methods `Tensor.cfloat()` and `Tensor.cdouble()`.

I was not able to find the tests for `.float()` functions. I'd be happy to add similar tests for these functions  once someone points me to them.

Fixes https://github.com/pytorch/pytorch/issues/56014",pytorch
58189,eqy,pr,2021-05-12T21:28:56Z,port `square` to structured,"#55070
There is a `const_cast` as `pow_out` still takes a non-`const` reference.
",pytorch
58197,eqy,pr,2021-05-13T00:34:30Z,port `sgn` to structured,"#55070
",pytorch
58200,eqy,pr,2021-05-13T00:45:44Z,fix lint?,Getting CI errors about `Tuple` not being used in `package_exporter.py`,pytorch
58209,crcrpar,pr,2021-05-13T02:28:32Z,Add `#pragma once` to CUDA foreach headers,"Per the title, adding `#pragma once` to cuda headers related to foreach functions.

cc: @ptrblck
",pytorch
58214,ngimel,pr,2021-05-13T04:37:47Z,Make norm and vector_norm use the same kernels. ,"Fixes a few problems with `torch.norm` (incorrect behavior for empty inputs and negative p, #52783, and incorrect imaginary part for complex). 
Most importantly, makes linalg_norm and vector_norm use the same kernels, reducing compile time and binary size. 
",pytorch
58224,rohan-varma,pr,2021-05-13T08:14:35Z,[c10d] Introduce ProcessGroupWrapper,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58224 [c10d] Introduce ProcessGroupWrapper**

Adds C++ implementation of ProcessGroupWrapper. It wraps
an underlying ProcessGroup and does debug checks before dispatching the
collective to the underlying pg. The design mostly follows https://github.com/pytorch/pytorch/issues/22071.

Concretely, on each collective, we:
1. Verify op type consistency. This can help catch mismatched ops in the user application (i.e. allreduce on one rank and allgather on another)
2. Verify tensor shapes. This can help catch bugs where the tensor inputs are malformed, whereas normally in NCCL this would just lead to a hang. The shapes verification for allgather/allreduce_coalesced is omitted because they actually accept different shape tensors and don't error out.

This is done through an abstraction called `CollectiveFingerPrint` which uses a helper process group to do the above verification. Concretely, we gather the data we need for each of the above checks into tensors, and allgather them, and verify their equivalence.

Once all of this passes we simply dispatch the collective to the underlying pg.

Added `ProcessGroupWrapperTest` in python to comprehensively test these changes.

Differential Revision: [D28023981](https://our.internmc.facebook.com/intern/diff/D28023981/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D28023981/)!",pytorch
58259,eqy,pr,2021-05-13T21:20:11Z,port square to structured ,"#55070
",pytorch
58266,eqy,pr,2021-05-13T22:16:09Z,port `square` to structured,"#55070

`square` really just calls `pow` so it seems a separate `TensorIterator` doesn't need to be built for it",pytorch
58281,rohan-varma,pr,2021-05-14T00:13:29Z,[c10d] Use pg wrapper in detailed debug mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58281 [c10d] Use pg wrapper in detailed debug mode**

When TORCH_DISTRIBUTED_DEBUG=DETAIL is enabled, this PR causes process groups created by `new_group` and `init_process_group` that are nccl or gloo to be wrapped in `ProcessGroupWrapper`.

As a result, the user will get back a `ProcessGroupWrapper` that they can use in the exact same way as a regular nccl/gloo pg, but will be more helpful in terms of debugging desync/hangs.

Besides doing collective desync checks, which should be transparent if there are indeed no issues in the user application, there are no semantic differences in using the wrapper pg. Note that there is a performance implication here but that is a tradeoff we are making when DETAIL debug mode is enabled.

Open to suggestions on how to test better. Currently I verified locally that enabling TORCH_DISTRIBUTED_DEBUG=detail creates the wrapper and all tests still pass, but that doesn't run in CI. On the other hand testing everything with debug=detail and the regular tests might be too much, so we have only added it to a few tests for now. We also do have tests in the below diff.

Differential Revision: [D28402301](https://our.internmc.facebook.com/intern/diff/D28402301/)",pytorch
58286,rohan-varma,pr,2021-05-14T02:06:53Z,Mention distributed profiling in documentation,"Added a simple section indicating distributed profiling is expected to work similar to other torch operators, and is supported for all communication backends out of the box.
",pytorch
58318,eqy,pr,2021-05-14T19:58:58Z,Fix incorrect inplace sort in `topk` (#58314),"Fixes #58314

#55392 introduced a bug by not allocating a separate value tensor for sorting
CC @ngimel @zasdfgbnm 
",pytorch
58322,rohan-varma,pr,2021-05-14T20:29:41Z,Document monitored barrier,"Will not land before the release, but would be good to have this function documented in master for its use in distributed debugability. ",pytorch
58325,eqy,pr,2021-05-14T21:03:10Z,test for `topk` #58318,"Separate PR for tests to go along with #58318 (to verify breakage in current CI). Should pass after rebasing on top of #58318.

CC @ngimel @zasdfgbnm ",pytorch
58327,eqy,pr,2021-05-14T21:19:06Z,Check memory overlap in sort for large input sizes,"The downstream cub sort doesn't support inplace sorting; this PR adds a check to bail out to allocating a new tensor instead of silently corrupting the returned indices.

CC @ngimel @zasdfgbnm",pytorch
58362,ngimel,pr,2021-05-16T05:51:36Z,reduce number of randperm template instantiations,"Per title, benchmarks in #54113 don't regress, size of torch_cuda_cu_generated_Randperm.cu.o goes 8562152 -> 2585792 for a single architecture, compilation time decreases also. 
",pytorch
58432,ngimel,pr,2021-05-17T20:56:22Z,add kernel launch checks after each kernel launch to silence the check,"T90898552
",pytorch
58468,ngimel,pr,2021-05-18T05:51:45Z,fix nonzero perf regression,"#55292 introduced perf regression for nonzero cuda, this fixes it. nvcc is still pretty bad about unrolling loops with boundaries that are not known at compile time, this makes `write_indices` kernels ~5x slower than it should be. 
",pytorch
58488,lezcano,pr,2021-05-18T16:17:28Z,Parametrizations depending on several inputs,"Makes possible that the first register parametrization depends on a number of parameters rather than just one. Examples of these types of parametrizations are `torch.nn.utils.weight_norm` and low rank parametrizations via the multiplication of a `n x k`  tensor by a `k x m` tensor with `k <= m, n`.

Follows the plan outlined in https://github.com/pytorch/pytorch/pull/33344#issuecomment-768574924. A short summary of the idea is: we call `right_inverse` when registering a parametrization to generate the tensors that we are going to save. If `right_inverse` returns a sequence of tensors, then we save them as `original0`, `original1`...  If it returns a `Tensor` or a sequence of length 1, we save it as `original`.

We only allow to have many-to-one parametrizations in the first parametrization registered. The next parametrizations would need to be one-to-one.

There were a number of choices in the implementation:

If the `right_inverse` returns a sequence of parameters, then we unpack it in the forward. This is to allow to write code as:
```python
class Sum(nn.Module):
  def forward(self, X, Y):
    return X + Y
  def right_inverse(Z):
    return Z, torch.zeros_like(Z)
```
rather than having to unpack manually a list or a tuple within the `forward` function.

At the moment the errors are a bit all over the place. This is to avoid having to check some properties of `forward` and `right_inverse` when they are registered. I left this like this for now, but I believe it'd be better to call these functions when they are registered to make sure the invariants hold and throw errors as soon as possible.

The invariants are the following:
1. The following code should be well-formed
```python
X = module.weight
Y = param.right_inverse(X)
assert isinstance(Y, Tensor) or isinstance(Y, collections.Sequence)
Z = param(Y) if isisntance(Y, Tensor) else param(*Y)
```
in other words, if `Y` is a `Sequence` of `Tensor`s (we check also that the elements of the sequence are Tensors), then it is of the same length as the number parameters `param.forward` accepts.

2. Always: `X.dtype == Z.dtype and X.shape == Z.shape`. This is to protect the user from shooting themselves in the foot, as it's too odd for a parametrization to change the metadata of a tensor.
3. If it's one-to-one: `X.dtype == Y.dtype`. This is to be able to do `X.set_(Y)` so that if a user first instantiates the optimiser and then puts the parametrisation, then we reuse `X` and the user does not need to add a new parameter to the optimiser. Alas, this is not possible when the parametrisation is many-to-one. The current implementation of `spectral_norm` and `weight_norm` does not seem to care about this, so this would not be a regression. I left a warning in the documentation though, as this case is a bit tricky.

I'm still missing to go over the formatting of the documentation, I'll do that tomorrow.

",pytorch
58524,rohan-varma,pr,2021-05-19T01:34:34Z,[Reducer] Remove some unused variables,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58524 [Reducer] Remove some unused variables**

Per title

Differential Revision: [D28528223](https://our.internmc.facebook.com/intern/diff/D28528223/)",pytorch
58526,ngimel,pr,2021-05-19T02:13:04Z,"move code to Blas.cpp, clean up THC magma","To improve compilation times
",pytorch
58592,rohan-varma,pr,2021-05-19T19:39:24Z,[Reducer] Completely remove VariableIndex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58603 [Reducer] Remove replica size == 1 checks
* #58595 [DDP] Remove train call to module copies
* #58594 [Reducer] move comment to the right place
* #58593 Format reducer.cpp, hpp
* **#58592 [Reducer] Completely remove VariableIndex**

Completely removes VariableIndex from reducer code, as it is not
needed. replica_index is always 0 so simplify the code to only use the
parameter index. Next, we should also remove all of the nested data structures
that were needed when num_replicas > 1 was possible.

Differential Revision: [D28528440](https://our.internmc.facebook.com/intern/diff/D28528440/)",pytorch
58593,rohan-varma,pr,2021-05-19T19:39:31Z,"Format reducer.cpp, hpp","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58603 [Reducer] Remove replica size == 1 checks
* #58595 [DDP] Remove train call to module copies
* #58594 [Reducer] move comment to the right place
* **#58593 Format reducer.cpp, hpp**
* #58592 [Reducer] Completely remove VariableIndex

Per title

Differential Revision: [D28528465](https://our.internmc.facebook.com/intern/diff/D28528465/)",pytorch
58594,rohan-varma,pr,2021-05-19T19:46:55Z,[Reducer] move comment to the right place,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58603 [Reducer] Remove replica size == 1 checks
* #58595 [DDP] Remove train call to module copies
* **#58594 [Reducer] move comment to the right place**
* #58593 Format reducer.cpp, hpp
* #58592 [Reducer] Completely remove VariableIndex

This comment was misplaced after some changes, move it to the right
place.

Differential Revision: [D28548100](https://our.internmc.facebook.com/intern/diff/D28548100/)",pytorch
58595,rohan-varma,pr,2021-05-19T19:55:22Z,[DDP] Remove train call to module copies,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58603 [Reducer] Remove replica size == 1 checks
* **#58595 [DDP] Remove train call to module copies**
* #58594 [Reducer] move comment to the right place
* #58593 Format reducer.cpp, hpp
* #58592 [Reducer] Completely remove VariableIndex

No longer needed since this list is always of size 1.

Differential Revision: [D28548426](https://our.internmc.facebook.com/intern/diff/D28548426/)",pytorch
58603,rohan-varma,pr,2021-05-19T20:37:28Z,[Reducer] Remove replica size == 1 checks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#58603 [Reducer] Remove replica size == 1 checks**
* #58595 [DDP] Remove train call to module copies
* #58594 [Reducer] move comment to the right place
* #58593 Format reducer.cpp, hpp
* #58592 [Reducer] Completely remove VariableIndex

No longer need these checks

Differential Revision: [D28549893](https://our.internmc.facebook.com/intern/diff/D28549893/)",pytorch
58702,rohan-varma,pr,2021-05-20T21:18:54Z,[c10d] Fix monitored_barrier with wait_all_ranks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58224 [c10d] Introduce ProcessGroupWrapper
* **#58702 [c10d] Fix monitored_barrier with wait_all_ranks**

Off by one error when determining if some ranks failed or not with
`wait_all_ranks=True`. This wasn't caught by tests because the tests only
tested failure scenarios, not success scenarios with `wait_all_ranks=True`.

Differential Revision: [D28583235](https://our.internmc.facebook.com/intern/diff/D28583235/)",pytorch
58714,ngimel,pr,2021-05-20T22:49:16Z,Fix nonzero perf regressions,1.9 version of #58468,pytorch
58721,eqy,pr,2021-05-21T00:48:57Z,[cuDNN v8] Conv2D Backward/Transpose port to cuDNN v8 API,"Just a copy-paste pattern match to see if things work initially. Many parts of the code (such as the run() lambda) can likely be refactored to reduce duplication.
[will tag issues/people later]

#58858",pytorch
58746,nSircombe,pr,2021-05-21T11:08:53Z,[Release/1.9] Enables builds with Compute Library backend for oneDNN (#55913),"Summary:
Since v1.7, oneDNN (MKL-DNN) has supported the use of Compute Library
for the Arm architeture to provide optimised convolution primitives
on AArch64.

This change enables the use of Compute Library in the PyTorch build.
Following the approach used to enable the use of CBLAS in MKLDNN,
It is enabled by setting the env vars USE_MKLDNN and USE_MKLDNN_ACL.
The location of the Compute Library build must be set useing `ACL_ROOT_DIR`.

This is an extension of the work in https://github.com/pytorch/pytorch/pull/50400
which added support for the oneDNN/MKL-DNN backend on AArch64.

_Note: this assumes that Compute Library has been built and installed at
ACL_ROOT_DIR. Compute library can be downloaded here:
`https://github.com/ARM-software/ComputeLibrary`_

Cherry pick of  https://github.com/pytorch/pytorch/pull/55913 into release/1.9

Reviewed By: ailzhang

Differential Revision: D28559516

Pulled By: malfet",pytorch
58747,lezcano,pr,2021-05-21T12:53:47Z,torch.flip via TI,"Implements an idea by @ngimel to improve the performance of `torch.flip` via a clever hack into TI to bypass the fact that TI is not designed to work with negative indices.

Something that might be added is vectorisation support on CPU, given how simple the implementation is now.

Some low-hanging fruits that I did not implement:
- Write it as a structured kernel
- Migrate the tests to opinfos
- Have a look at `cumsum_backward` and `cumprod_backward`,  as I think that they could be implemented faster with `flip`, now that `flip` is fast.

**Edit**
This operation already has OpInfos and it cannot be migrated to a structured kernel because it implements quantisation

Summary of the PR:
- x1.5-3 performance boost on CPU
- x1.5-2 performance boost on CUDA
- Comparable performance across dimensions, regardless of the strides (thanks TI)
- Simpler code


<details>
<summary>
Test Script
</summary>

```python
from itertools import product

import torch
from torch.utils.benchmark import Compare, Timer


def get_timer(size, dims, num_threads, device):
    x = torch.rand(*size, device=device)

    timer = Timer(
        ""torch.flip(x, dims=dims)"",
        globals={""x"": x, ""dims"": dims},
        label=f""Flip {device}"",
        description=f""dims: {dims}"",
        sub_label=f""size: {size}"",
        num_threads=num_threads,
    )

    return timer.blocked_autorange(min_run_time=5)


def get_params():
    sizes = ((1000,)*2, (1000,)*3, (10000,)*2)
    for size, device in product(sizes, (""cpu"", ""cuda"")):
        threads = (1, 2, 4) if device == ""cpu"" else (1,)
        list_dims = [(0,), (1,), (0, 1)]
        if len(size) == 3:
            list_dims.append((0, 2))
        for num_threads, dims in product(threads, list_dims):
            yield size, dims, num_threads, device


def compare():
    compare = Compare([get_timer(*params) for params in get_params()])
    compare.trim_significant_figures()
    compare.colorize()
    compare.print()


compare()
```
</details>

<details>
<summary>
Benchmark PR
</summary>

![image](https://user-images.githubusercontent.com/3291265/119139954-81e46d80-ba3b-11eb-9aad-e825e515d41b.png)

</details>

<details>
<summary>
Benchmark master
</summary>

![image](https://user-images.githubusercontent.com/3291265/119139915-76914200-ba3b-11eb-9aa8-84b3ca220c93.png)

</details>

Fixes #16424",pytorch
58793,rohan-varma,pr,2021-05-22T02:01:22Z,[not for review] Ci all/rohan/pg wrapper,ci all for https://github.com/pytorch/pytorch/pull/58224,pytorch
58795,rohan-varma,pr,2021-05-22T02:19:05Z,[not for review] Ci all/rohan/pg wrapper ci 2,"Fixes #{issue number}
",pytorch
58810,ngimel,pr,2021-05-22T22:08:06Z,various TensorIterator speed improvements,"1) remove pushing back to strides vector for 1D tensors, those strides are never used in the loop anyway
2) avoid calling get_data_ptrs unless necessary
3) don't call into assert_no_partial_overlap if tensorImpls are the same (assert_no_partial_overlap has this comparison too, but after a couple of nested function calls)
4) is_non_overlapping_and_dense instead of is_contiguous in memory overlap (which, for some reason, is faster than is_contiguous, though I hoped after is_contiguous is non-virtualized, it should be the same). 

Altogether, brings instruction count down from ~110K to 102735 for the following binary inplace benchmark:
```
In [2]:  timer = Timer(""m1.add_(b);"", setup=""at::Tensor m1=torch::empty({1}); at::Tensor b = torch::empty({1});"", language=""c++"", timer=timeit.default_timer)
   ...:  stats=timer.collect_callgrind(number=30, repeats=3)
   ...:  print(stats[1].as_standardized().stats(inclusive=False))
```
similar improvements for unary inplace. 

Upd: returned stride packing for now, counts is now 104295, so packing is worth ~ 52 instructions, we should think about how to remove it  safely. ",pytorch
58830,ngimel,pr,2021-05-23T23:24:30Z,simplify cpu_kernel to not have contiguous special case,"Per title
 `unroll_contiguous_scalar_checks` tries to verify that all arguments (including outputs) are contiguous except maybe 1 scalar (with stride 0). Then it calls the passed lambda with index of the scalar arg if this verification succeeded, or 0 if args were not contiguous/there was no scalar. Depending on the value of this index (with 0=not found) a different function can be called (in vectorized kernels itâ€™s vectorized loop if args are contiguous + scalar, and basic loop if not). It makes sense for vectorized kernel (vectorized loop can still be used in some broadcasted cases), but all other (cpu_kernel, serial_cpu_kernel, cpu_kernel_multiple_outputs) donâ€™t even use idx argument in lambda, so regardless of what `unroll_contiguous_scalar_checks` does, they'll do the same thing. No point in calling `unroll_contiguous_scalar_checks` then. ",pytorch
58927,supriyar,pr,2021-05-25T17:35:32Z,"[quant][refactor tests] Split test_quantize into test_quantize_eager_ptq, test_quantize_eager_qat and test_fusion","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module
* #58943 [quant][refactor tests] Move test_quantized_module into test_quantize_eager_ptq
* #58929 [quant][refactor tests] Move test_quantized_functional.py into test_quantize_eager_ptq.py
* #58928 [quant][refactor tests] Move test_qat_module into test_quantize_eager_qat
* **#58927 [quant][refactor tests] Split test_quantize into test_quantize_eager_ptq, test_quantize_eager_qat and test_fusion**

Summary:
Part of larger re-factor of quantization tests to make it clearer as to which test belongs where.

proposed folder structure
```
test/quantization
         - bc/
            - test_backward_compatibility.py
         - core/
            - test_quantized_kernels.py
            - test_quantized_workflow_ops.py
            - test_quantized_tensor.py
            - test_workflow_module.py
         - eager/
            - test_quantize_eager_ptq.py
            - test_quantize_eager_qat.py
            - test_fusion.py
         - equalization/
            - test_equalize_eager.py
            - test_bias_correction_eager.py
         - fx/
           - test_quantize_fx.py
         - jit/
            - test_quantize_jit.py
            - test_fusion_passes.py
         - numeric_suite/
            - test_numeric_suite_fx.py
            - test_numeric_suite_eager.py
```
Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28683926](https://our.internmc.facebook.com/intern/diff/D28683926)",pytorch
58928,supriyar,pr,2021-05-25T17:48:51Z,[quant][refactor tests] Move test_qat_module into test_quantize_eager_qat,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module
* #58943 [quant][refactor tests] Move test_quantized_module into test_quantize_eager_ptq
* #58929 [quant][refactor tests] Move test_quantized_functional.py into test_quantize_eager_ptq.py
* **#58928 [quant][refactor tests] Move test_qat_module into test_quantize_eager_qat**
* #58927 [quant][refactor tests] Split test_quantize into test_quantize_eager_ptq, test_quantize_eager_qat and test_fusion

Summary:

Test Plan:
python test/test_quantization.py TestConvBNQATModule

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28683925](https://our.internmc.facebook.com/intern/diff/D28683925)",pytorch
58929,supriyar,pr,2021-05-25T17:54:09Z,[quant][refactor tests] Move test_quantized_functional.py into test_quantize_eager_ptq.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59007 [quant][refactor tests] Move quantization tests into subfolders
* #59000 [quant][refactor tests] Move TestModelNumerics to a separate file
* #58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module
* #58943 [quant][refactor tests] Move test_quantized_module into test_quantize_eager_ptq
* **#58929 [quant][refactor tests] Move test_quantized_functional.py into test_quantize_eager_ptq.py**

Summary:

Test Plan:
python test/test_quantization.py TestQuantizedFunctionalOps

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28683924](https://our.internmc.facebook.com/intern/diff/D28683924)",pytorch
58943,supriyar,pr,2021-05-25T20:35:36Z,[quant][refactor tests] Move test_quantized_module into test_quantize_eager_ptq,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59007 [quant][refactor tests] Move quantization tests into subfolders
* #59000 [quant][refactor tests] Move TestModelNumerics to a separate file
* #58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module
* **#58943 [quant][refactor tests] Move test_quantized_module into test_quantize_eager_ptq**
* #58929 [quant][refactor tests] Move test_quantized_functional.py into test_quantize_eager_ptq.py

Summary:

Test Plan:
python test/test_quantization.py TestStaticQuantizedModule
python test/test_quantization.py TestDynamicQuantizedModule
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28696598](https://our.internmc.facebook.com/intern/diff/D28696598)",pytorch
58953,eqy,pr,2021-05-25T21:59:24Z,Move remaining \*Sort\* in `THC` to `ATen`,"#24637

CC @zasdfgbnm @ngimel",pytorch
58960,crcrpar,pr,2021-05-25T22:59:18Z,Refactor Foreach Tests: Unary Functions,"Related issue: #58833

__changes__
- slowpath tests: pass every dtype&device tensors and compare the behavior with regular functions including inplace
- check of #cudaLaunchKernel
- rename `ForeachUnaryFuncInfo` -> `ForeachFuncInfo`: This change is mainly for the future binary/pointwise test refactors

cc: @ngimel @ptrblck @mcarilli ",pytorch
58963,supriyar,pr,2021-05-26T00:06:32Z,[quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59007 [quant][refactor tests] Move quantization tests into subfolders
* #59000 [quant][refactor tests] Move TestModelNumerics to a separate file
* #58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests
* **#58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module**

Summary:
some tests are used to check the op level numerics of the fake quantize operations

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28696599](https://our.internmc.facebook.com/intern/diff/D28696599)",pytorch
58999,supriyar,pr,2021-05-26T16:52:45Z,[quant][refactor tests] Rename test_numeric_suite and equalization tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59007 [quant][refactor tests] Move quantization tests into subfolders
* #59000 [quant][refactor tests] Move TestModelNumerics to a separate file
* **#58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests**
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module

Summary:
Rename the test files to be more explicit that they are for eager mode

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28713909](https://our.internmc.facebook.com/intern/diff/D28713909)",pytorch
59000,supriyar,pr,2021-05-26T16:52:50Z,[quant][refactor tests] Move TestModelNumerics to a separate file,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59007 [quant][refactor tests] Move quantization tests into subfolders
* **#59000 [quant][refactor tests] Move TestModelNumerics to a separate file**
* #58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module

Summary:
These tests span both QAT and PTQ APIs so factor them out

Test Plan:
python test/test_quantization.py TestModelNumericsEager

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28713910](https://our.internmc.facebook.com/intern/diff/D28713910)",pytorch
59003,ngimel,pr,2021-05-26T17:22:33Z,fix unique for discontiguous inputs,"Fixes #58959
",pytorch
59007,supriyar,pr,2021-05-26T18:09:08Z,[quant][refactor tests] Move quantization tests into subfolders,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59007 [quant][refactor tests] Move quantization tests into subfolders**
* #59000 [quant][refactor tests] Move TestModelNumerics to a separate file
* #58999 [quant][refactor tests] Rename test_numeric_suite and equalization tests
* #58963 [quant][refactor tests] split test_workflow_module into test_workflow_ops and test_workflow_module

Summary:
Create folders for each test category and move the tests.
Will follow-up with a cleanup of test_quantization.py

```
test/quantization
         - bc/
            - test_backward_compatibility.py
         - core/
            - test_quantized_kernels.py
            - test_quantized_workflow_ops.py
            - test_quantized_tensor.py
            - test_workflow_module.py
         - eager/
            - test_quantize_eager_ptq.py
            - test_quantize_eager_qat.py
            - test_fusion.py
            - test_model_numerics.py
            - test_numeric_suite_eager.py
            - test_equalize_eager.py
            - test_bias_correction_eager.py
         - fx/
           - test_quantize_fx.py
           - test_numeric_suite_fx.py
         - jit/
            - test_quantize_jit.py
            - test_fusion_passes.py      
     
```

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28718742](https://our.internmc.facebook.com/intern/diff/D28718742)",pytorch
59048,ngimel,pr,2021-05-26T22:19:35Z,[PyTorch] Remove device check from a few indexing methods (#58800),"Summary:
1.9 version of #58800

",pytorch
59055,ngimel,pr,2021-05-27T00:12:20Z,fix unique for discontiguous inputs,"Per title
",pytorch
59059,ngimel,pr,2021-05-27T02:03:57Z,don't copy indices to the self device in dispatch_index,"Let index/index_put implementation in aten take care of moving the indices to the correct device, don't make python wrapper do that. 
",pytorch
59065,rohan-varma,pr,2021-05-27T04:59:57Z,[DDP] use work.result() in _check_global_requires_backward_grad_sync,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59066 [DDP] Remove unused initialize_buckets
* **#59065 [DDP] use work.result() in _check_global_requires_backward_grad_sync**

Cleaner to use work.result() instead of sending back the tensor from
this function.

Differential Revision: [D28551203](https://our.internmc.facebook.com/intern/diff/D28551203/)",pytorch
59066,rohan-varma,pr,2021-05-27T05:00:04Z,[DDP] Remove unused initialize_buckets,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59066 [DDP] Remove unused initialize_buckets**
* #59065 [DDP] use work.result() in _check_global_requires_backward_grad_sync

Per title

Differential Revision: [D28734666](https://our.internmc.facebook.com/intern/diff/D28734666/)",pytorch
59070,rohan-varma,pr,2021-05-27T06:39:38Z,[c10d] Remove verbose log,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59070 [c10d] Remove verbose log**

This log is too verbose, especially in the case we call monitored
barrier before every collective as we do in ProcessGroupWrapper.

Differential Revision: [D28738189](https://our.internmc.facebook.com/intern/diff/D28738189/)",pytorch
59088,supriyar,pr,2021-05-27T16:38:49Z,[quant][refactor tests] Clean up test_quantization.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59089 [quant][refactor tests] Move qtensor serialization tests from test_deprecated_jit
* **#59088 [quant][refactor tests] Clean up test_quantization.py**

Summary:
Clean up comments and organize the tests better

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28750064](https://our.internmc.facebook.com/intern/diff/D28750064)",pytorch
59089,supriyar,pr,2021-05-27T16:38:54Z,[quant][refactor tests] Move qtensor serialization tests from test_deprecated_jit,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59089 [quant][refactor tests] Move qtensor serialization tests from test_deprecated_jit**
* #59088 [quant][refactor tests] Clean up test_quantization.py

Summary:
Move these tests into test_quantized_tensor

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D28750065](https://our.internmc.facebook.com/intern/diff/D28750065)",pytorch
59134,ngimel,pr,2021-05-28T04:42:02Z,avoid explicitly casting low precision inputs to fp32 in norm,"Per title. Now `norm` with fp16/bfloat16 inputs and fp32 outputs on cuda won't do explicit cast
",pytorch
59135,ngimel,pr,2021-05-28T05:18:28Z,make vector_norm backward call norm_backward,"Per title. Remove duplicated code.
",pytorch
59266,rohan-varma,pr,2021-06-01T21:58:00Z,[v1.9] [c10d] Fix monitored_barrier with wait_all_ranks ,"PR https://github.com/pytorch/pytorch/pull/58702 has been merged into master, this PR is to merge the bugfix into release branch

Original commit description: 

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58702

Off by one error when determining if some ranks failed or not with
`wait_all_ranks=True`. This wasn't caught by tests because the tests only
tested failure scenarios, not success scenarios with `wait_all_ranks=True`.
ghstack-source-id: 129559840

Test Plan: CI

Reviewed By: zhaojuanmao

Differential Revision: D28583235

fbshipit-source-id: a8f376efb13a3f36c788667acab86543c80aff59

Fixes #{issue number}
",pytorch
59278,rohan-varma,pr,2021-06-02T00:35:41Z,"[reland] [pytorch][PR] Fix DistributedSampler mem usage on large datasets""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59278 [reland] [pytorch][PR] Fix DistributedSampler mem usage on large datasets""**

Reland of https://github.com/pytorch/pytorch/pull/51841 which did not pass lint.

Differential Revision: [D28816940](https://our.internmc.facebook.com/intern/diff/D28816940/)",pytorch
59281,rohan-varma,pr,2021-06-02T02:57:26Z,[DDP] Log when errors happen,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59351 [DDP] log usage of torch_distributed_debug
* #59284 [DDP] Log some python-side errors
* **#59281 [DDP] Log when errors happen**

Adds ability to log when reducer/ddp encounters an error. We add fields ""has_error"" and ""error"" to indicate that an error has
occured in this iteration, and the other fields (performance stats) are not
guaranteed to be updated.

Errors encountered in python-side DDP will be added in the next diff.

Differential Revision: [D28652717](https://our.internmc.facebook.com/intern/diff/D28652717/)",pytorch
59284,rohan-varma,pr,2021-06-02T04:59:54Z,[DDP] Log some python-side errors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59351 [DDP] log usage of torch_distributed_debug
* **#59284 [DDP] Log some python-side errors**
* #59281 [DDP] Log when errors happen

Logs a few python-side errors to DDP logging.

TODO: Most python errors actually have to do with user input correctness, so they throw before reducer is constructed and thus there is no logger. For this case, should we allow `logger` to be created optionally without a reducer, just for the purpose of logging errors, so that we can gain insight into these errors in scuba?

Differential Revision: [D28820290](https://our.internmc.facebook.com/intern/diff/D28820290/)",pytorch
59313,wangkuiyi,pr,2021-06-02T16:21:56Z,Fix fecher continue next after StopIterator,"Fixes #59312

cc @VitalyFedyunin @dzhulgakov ",pytorch
59351,rohan-varma,pr,2021-06-03T00:07:50Z,[DDP] log usage of torch_distributed_debug,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59351 [DDP] log usage of torch_distributed_debug**
* #59284 [DDP] Log some python-side errors
* #59281 [DDP] Log when errors happen

Logging PT distributed debug level to track usage internally.

Differential Revision: [D28854914](https://our.internmc.facebook.com/intern/diff/D28854914/)",pytorch
59359,rohan-varma,pr,2021-06-03T02:59:15Z,[DDP] Support for multiple backwards,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59359 [DDP] Support for multiple backwards**

Move `prepare_for_backward` into `_DDPSink` backward instead of calling it in DDP forward pass so that we can run multiple backwards in DDP with `retain_graph=True`. 

ci-all version of this PR: https://github.com/pytorch/pytorch/pull/59931

Differential Revision: [D28855226](https://our.internmc.facebook.com/intern/diff/D28855226/)",pytorch
59399,r-barnes,pr,2021-06-03T18:46:39Z,Change a number of PyTorch loops to use irange,"Summary: Uses D28874212 to codemod PyTorch to use the `c10::irange` example

Differential Revision: D28876623

",pytorch
59431,ngimel,pr,2021-06-04T03:42:13Z,remove THCReduce.cuh,"Per title
",pytorch
59480,r-barnes,pr,2021-06-04T20:54:10Z,irange for PyTorch jit,"Differential Revision: D28909698

",pytorch
59481,r-barnes,pr,2021-06-04T20:54:12Z,irange for PyTorch sans jit,"Differential Revision: D28909681

",pytorch
59509,ngimel,pr,2021-06-05T06:31:47Z,flip via TI,Resubmit of #58747,pytorch
59564,lezcano,pr,2021-06-07T15:26:37Z,Simplify parametrizations.SpectralNorm and improve its initialization,"Implements a number of changes discussed with @soulitzer offline.
In particular:
- Initialise `u`, `v` in `__init__` rather than in `_update_vectors`
- Initialise `u`, `v` to some reasonable vectors by doing 15 power iterations at the start
- Simplify the code of `_reshape_weight_to_matrix` (and make it faster) by using `flatten`",pytorch
59598,r-barnes,pr,2021-06-07T22:15:26Z,Modernize for-loops in aten,"Differential Revision: D28946826

",pytorch
59604,rohan-varma,pr,2021-06-08T00:01:29Z,Document debugability features in torch.distributed,"Adds comprehensive documentation around debugability features added to `torch.distributed` recently, including the `monitored_barrier` and TORCH_DISTRIBUTED_DEBUG env variable.

![dist_one](https://user-images.githubusercontent.com/8039770/121102672-0f052180-c7b3-11eb-974c-81dbbe102cb6.png)
![dist_two](https://user-images.githubusercontent.com/8039770/121102734-39ef7580-c7b3-11eb-94f7-c75469351440.png)
",pytorch
59615,ngimel,pr,2021-06-08T05:35:04Z,Renorm fix,"Fixes #59584
@albanD, @soulitzer, `renorm` grad was completely busted. Fast gradcheck is definitely not doing its job. 
",pytorch
59643,r-barnes,pr,2021-06-08T18:37:29Z,Fix some compiler warnings,"Differential Revision: D28916206

",pytorch
59656,eqy,pr,2021-06-08T20:28:28Z,[DO NOT REVIEW/MERGE] [CUDNN v8 API] cuDNN benchmark,"#58859, also includes the changes from #58721
We now cache ""plans"" instead of engine configs.

The current workspace size is a placeholder; it will be updated based on the maximum workspace requirements of the plans or available device memory. 
Includes a fix to properly initialize `CacheKey` in order to make `ParamsHash` work safely.

CC @zasdfgbnm @ptrblck ",pytorch
59666,rohan-varma,pr,2021-06-08T21:01:59Z,[DDP] Test inference works with eval() and no_grad(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59666 [DDP] Test inference works with eval() and no_grad()**

Tests that inference with DDP model won't hang when user sets eval()
or no_grad(). Note that if the model has a syncBN layer, they need both eval()
and no_grad() as eval() makes SyncBN work like a regular BN layer.

Differential Revision: [D28974146](https://our.internmc.facebook.com/intern/diff/D28974146/)",pytorch
59667,rohan-varma,pr,2021-06-08T21:12:32Z,[c10d] Use TORCH_CHECK for monitored barrier error,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59684 torch/lib/c10d: Use torch_check instead of throwing runtime_error
* #59683 Replace throw std::runtime_error with torch_check in torch/csrc/distributed
* **#59667 [c10d] Use TORCH_CHECK for monitored barrier error**

Use torch_check over throw std::runtime_error in monitored barrier so
that it works with torch_cpp_show_stacktraces to reveal the entire callstack
where the monitored barrier failed, which can help determine where the
particular rank encountered an issue.

We should eventually replace all uses of throwing runtime_error with torch_check in distributed C++ code as the latter can provide cpp stack traces. 

Differential Revision: [D28974510](https://our.internmc.facebook.com/intern/diff/D28974510/)",pytorch
59668,r-barnes,pr,2021-06-08T21:13:40Z,Construct a -Wall around Torch,"Differential Revision: D28974453

",pytorch
59683,rohan-varma,pr,2021-06-09T00:39:19Z,Replace throw std::runtime_error with torch_check in torch/csrc/distributed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59684 torch/lib/c10d: Use torch_check instead of throwing runtime_error
* **#59683 Replace throw std::runtime_error with torch_check in torch/csrc/distributed**

Replaces usages of throw std::runtime_error(""foo"") with the better
torch_check(false, ""foo"") which allows C++ stacktraces to show up when
TORCH_SHOW_CPP_STACKTRACES=1. This will hopefully provide much better debugging
information when debugging crashes/flaky tests.

Differential Revision: [D28981327](https://our.internmc.facebook.com/intern/diff/D28981327/)",pytorch
59684,rohan-varma,pr,2021-06-09T00:39:26Z,torch/lib/c10d: Use torch_check instead of throwing runtime_error,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59684 torch/lib/c10d: Use torch_check instead of throwing runtime_error**
* #59683 Replace throw std::runtime_error with torch_check in torch/csrc/distributed

Same reasoning as in the below diff.

Differential Revision: [D28981326](https://our.internmc.facebook.com/intern/diff/D28981326/)",pytorch
59726,rohan-varma,pr,2021-06-09T18:58:40Z,[reland] Document debugability features in torch.distributed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59726 [reland] Document debugability features in torch.distributed**

Reland of https://github.com/pytorch/pytorch/pull/59604 with indentation fix

Differential Revision: [D29001923](https://our.internmc.facebook.com/intern/diff/D29001923/)",pytorch
59753,rohan-varma,pr,2021-06-09T22:11:05Z,Ci all/rvarm1/ddp multi,CI all for https://github.com/pytorch/pytorch/pull/59359,pytorch
59767,ngimel,pr,2021-06-10T01:23:04Z,update kineto submodule,Another try for #59755,pytorch
59784,lezcano,pr,2021-06-10T11:06:01Z,[docs] Correct errata in linalg.eigh and add a bit more information,"Add extra information about the returned elements of the spectral
decompositions

Resolves https://github.com/pytorch/pytorch/issues/59718
",pytorch
59819,r-barnes,pr,2021-06-10T19:53:02Z,Fix const correctness and loop variable type in CUDACachingAllocator,"Differential Revision: D29034650

",pytorch
59830,eqy,pr,2021-06-10T22:11:57Z,Disable cuDNN persistent RNN on A30,"#59829

cherry-picked from @ptrblck 's change CC @ngimel @xwang233 ",pytorch
59840,rohan-varma,pr,2021-06-10T23:36:09Z,[c10d] Move pg wrapper tests to their own file.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59840 [c10d] Move pg wrapper tests to their own file.**

moving these tests to their own standalone file. No meaningful code changes.

Differential Revision: [D29012664](https://our.internmc.facebook.com/intern/diff/D29012664/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29012664/)!",pytorch
59853,r-barnes,pr,2021-06-11T01:06:52Z,Build an -Wextra around c10,"Differential Revision: D29016682

",pytorch
59882,supriyar,pr,2021-06-11T17:30:10Z,[quant][fx] Remove extra q-dq for weight bias in normalization ops,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59882 [quant][fx] Remove extra q-dq for weight bias in normalization ops**

Summary:
Currently for normalization ops, the weight and bias arguments are treated as activationn inputs which require observers.
This results in adding extra quant-dequant ops for the weight and bias inputs.

This PR adds support to skip observing weight/bias inputs of norm operators, thus removing the redundant q-dq ops

Quantized graph with F.layer_norm
Before this PR
```
def forward(self, x):
    _input_scale_0 = self._input_scale_0
    _input_zero_point_0 = self._input_zero_point_0
    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None
    scale = self.scale
    _input_scale_1 = self._input_scale_1
    _input_zero_point_1 = self._input_zero_point_1
    quantize_per_tensor_1 = torch.quantize_per_tensor(scale, _input_scale_1, _input_zero_point_1, torch.quint8);  scale = _input_scale_1 = _input_zero_point_1 = None
    bias = self.bias
    _input_scale_2 = self._input_scale_2
    _input_zero_point_2 = self._input_zero_point_2
    quantize_per_tensor_2 = torch.quantize_per_tensor(bias, _input_scale_2, _input_zero_point_2, torch.quint8);  bias = _input_scale_2 = _input_zero_point_2 = None
    _scale_0 = self._scale_0
    _zero_point_0 = self._zero_point_0
    dequantize = quantize_per_tensor_1.dequantize();  quantize_per_tensor_1 = None
    dequantize_1 = quantize_per_tensor_2.dequantize();  quantize_per_tensor_2 = None
    layer_norm = torch.ops.quantized.layer_norm(quantize_per_tensor, [2, 5, 5], weight = dequantize, bias = dequantize_1, eps = 1e-05, output_scale = _scale_0, output_zero_point = _zero_point_0);  quantize_per_tensor = dequantize = dequantize_1 = _scale_0 = _zero_point_0 = None
    dequantize_2 = layer_norm.dequantize();  layer_norm = None
    return dequantize_2
```
After
```
def forward(self, x):
    _input_scale_0 = self._input_scale_0
    _input_zero_point_0 = self._input_zero_point_0
    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None
    scale = self.scale
    bias = self.bias
    _scale_0 = self._scale_0
    _zero_point_0 = self._zero_point_0
    layer_norm = torch.ops.quantized.layer_norm(quantize_per_tensor, [2, 5, 5], weight = scale, bias = bias, eps = 1e-05, output_scale = _scale_0, output_zero_point = _zero_point_0);  quantize_per_tensor = scale = bias = _scale_0 = _zero_point_0 = None
    dequantize = layer_norm.dequantize();  layer_norm = None
    return dequantize
```

Test Plan:
python test/test_quantization.py TestQuantizeFxOps.test_norm_weight_bias
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29068203](https://our.internmc.facebook.com/intern/diff/D29068203)",pytorch
59897,r-barnes,pr,2021-06-11T22:58:01Z,Fix a number of lint perf and safety issues in torch,"Differential Revision: D29037012

",pytorch
59903,r-barnes,pr,2021-06-12T00:43:33Z,Fix bad change in a CUDACachingAllocator loop,"Summary: D29034650 (https://github.com/pytorch/pytorch/commit/cf0c4ac25811cf93e51b4be6eb58bbdb95963b3b) probably breaks something because it changes a for loop from `[size,max)` to `[0,max)`. This fixes that

Test Plan: Sandcastle

Differential Revision: D29081688

",pytorch
59907,crcrpar,pr,2021-06-12T03:22:38Z,Foreach Binary Test Refactor,"Related: https://github.com/pytorch/pytorch/issues/58833

## Changes I'm a bit concerned
- binary ops with one tensorlist and one scalarlist support complex dtypes. To realize this, I added a specialization of [`TensorListScalarListMetadata<c10::complex<double>, 1>` ](https://github.com/pytorch/pytorch/pull/59907/files#diff-131eb9b310905b15b3528da6a23e542a3a3aa952bc88f7423c98a23a8a28cca1R49). This might be out of the scope of this pull request.


cc @ptrblck @ngimel @mcarilli ",pytorch
59909,r-barnes,pr,2021-06-12T04:31:45Z,Fix some items identified as problematic by Wextra and other clean-up,"Differential Revision: D29073150

",pytorch
59918,rohan-varma,pr,2021-06-13T08:28:26Z,[reland] torch/lib/c10d: Use torch_check instead of throwing runtime_error,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59918 [reland] torch/lib/c10d: Use torch_check instead of throwing runtime_error**

Reland of https://github.com/pytorch/pytorch/pull/59684
Fix is to update `ProcessGroupNCCLErrorsTest` appropriately. CI all is in https://github.com/pytorch/pytorch/pull/59919

Differential Revision: [D29081452](https://our.internmc.facebook.com/intern/diff/D29081452/)",pytorch
59919,rohan-varma,pr,2021-06-13T09:12:36Z,[ci-all] torch/lib/c10d: Use torch_check instead of throwing runtime error,"ci all for https://github.com/pytorch/pytorch/pull/59918/files
â€¦error

Reland of https://github.com/pytorch/pytorch/pull/59684

Differential Revision: [D29081452](https://our.internmc.facebook.com/intern/diff/D29081452/)

[ghstack-poisoned]

Fixes #{issue number}
",pytorch
59931,rohan-varma,pr,2021-06-14T07:30:17Z,Ci all/rvarm1/ddp bwd,Ci-all for https://github.com/pytorch/pytorch/pull/59359,pytorch
59944,fritzo,pr,2021-06-14T15:59:17Z,Implement a generic Distribution.expand() and use it in a few places,"This aims to make it easier to implement new distributions by providing a best-effort default implementation of `Distribution.expand()`, which is possible with the new `Constraint.event_dim` metadata provided by #50581 and #51369.  While this simple default implementation does not work in all cases, this PR aims to support the following common cases of new distributions (only some of which currently work; discussion needed):
- [x] All the distribution's data is stored as instance attributes described by `.arg_constraints` (e.g. Normal, Cauchy, Poisson);
- [ ] The distribution `.__init__()` simply constructs a `base_dist` and `tansforms` and passes those to `TransformedDistribution.__init__()`;
- [ ] The distribution inherits from another distribution and the `.__init__()` method simply changes parameters (e.g. Chi2)

To make this work, I've refactored some instance attributes such as `._param` and `._event_ndim` to be properties or lazy properties.

## Questions for reviewers
- [ ] Should we weaken the check in [Distribution._get_checked_instance()](https://github.com/pytorch/pytorch/blob/68d690ffbd64d0fb697dc3da1635216366649787/torch/distributions/distribution.py#L279)?
- [ ] How should we document and check whether the default `.expand()` works correctly?

cc @neerajprad @fehiepsi ",pytorch
59952,r-barnes,pr,2021-06-14T17:07:26Z,Fix some typing issues,"Differential Revision: D29083423

",pytorch
59957,r-barnes,pr,2021-06-14T17:33:09Z,Some fixes to vec256_bfloat16.h,"Test Plan: Sandcastle

Reviewed By: VitalyFedyunin

Differential Revision: D29073913

",pytorch
59977,eqy,pr,2021-06-14T20:11:13Z,`topk` on CUDA supports `bfloat16`,"Fixes #56176 via #58196

CC @zasdfgbnm @ngimel @ptrblck ",pytorch
59997,rohan-varma,pr,2021-06-14T23:59:48Z,test PR ,"Fixes #{issue number}
",pytorch
60118,rohan-varma,pr,2021-06-16T18:48:37Z,"Fix Pipe + DDP for unused parameters, static graph","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59359 [DDP] Support for multiple backwards
* #60123 Correct backend in pipe_with_ddp_test
* **#60118 Fix Pipe + DDP for unused parameters, static graph**

Pipe + DDP has a few issues:

1) with static graph, does not synchronize gradients on first backward pass (i.e. delay allreduce is not run). does not work since https://github.com/pytorch/pytorch/pull/55248
2) when find_unused_parameters=True, also does not result in gradient synchronization. does not work since https://github.com/pytorch/pytorch/pull/57081


The reason for both cases is that calling `DDPSink.apply(output_tensor)` does not call the custom `backward` of `DDPSink` when the `output_tensor` is actually an `OwnerRRef`, which is the case when running DDP in `Pipe`. This is because we do `backward` on the `rref.local_value()` which does not have this autograd recording.

To fix, we unwrap the RRef and reconstruct it as needed, similar to the fix in https://github.com/pytorch/pytorch/pull/49908.

to test:
All tests in pipe_with_ddp_test pass.
The reason these tests did not catch the errors earlier is because all ranks received the same model inputs. So if gradient synchronization did not occur, then grads would still be the same because the model is the same on all ranks (guaranteed by ddp). Fixed the tests to use different inputs across ranks.

Differential Revision: [D29167283](https://our.internmc.facebook.com/intern/diff/D29167283/)",pytorch
60123,rohan-varma,pr,2021-06-16T19:07:03Z,Correct backend in pipe_with_ddp_test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59359 [DDP] Support for multiple backwards
* **#60123 Correct backend in pipe_with_ddp_test**
* #60118 Fix Pipe + DDP for unused parameters, static graph

All of the tests would run with nccl, but some tests specify a
different backend param which we should respect.

Differential Revision: [D29171549](https://our.internmc.facebook.com/intern/diff/D29171549/)",pytorch
60209,eqy,pr,2021-06-17T20:50:40Z,TF32 threshold twiddling for tests,"Following #59624 I observed some straggling failing tests on Ampere due to TF32 thresholds. This PR just twiddles some more thresholds to fix the (6) failing tests I saw on A100.

CC @Flamefire @ptrblck @ngimel ",pytorch
60224,rohan-varma,pr,2021-06-17T23:33:13Z,Ci all/rvarm1/ci all ddp mult bwd,ci-all for https://github.com/pytorch/pytorch/pull/59359,pytorch
60227,crcrpar,pr,2021-06-17T23:52:57Z,[add/sub] Cast `alpha` to `acc_type` ,"This PR lets `torch.add` & `torch.sub` CUDA kernels cast `alpha` to `acc_type`, not `scalar_t`.
I do not remove `cast`s from `test/test_foreach.py` because I'll do this in #59907 or follow-up for it.

Current upstream `torch._foreach_add` & `torch._foreach_sub` upcast `alpha` parameter to `acc_type<scalar_t>` while `torch.add` & `torch.sub` not. This is kind of problematic because outputs of `torch.add` and `torch.sub` are different from `torch._foreach_add` and `torch._foreach_sub`, respectively if the dtype of input tensors is either `torch.half` or `torch.bfloat16`. The discrepancy is proportional-ish to `abs(alpha)` except when `alpha` is representable with 16 bits.

ref:
- `torch._foreach_add` & `torch._foreach_sub` cast `alpha`: https://github.com/pytorch/pytorch/blob/6d0fb85a623f5ef3f3f1a2afc3660cb71fa70511/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu#L21-L28, `BinaryOpListAlphaFunctor` is defined here: https://github.com/pytorch/pytorch/blob/6d0fb85a623f5ef3f3f1a2afc3660cb71fa70511/aten/src/ATen/native/cuda/ForeachFunctors.cuh#L202

related: https://github.com/pytorch/pytorch/issues/58833, https://github.com/pytorch/pytorch/pull/59907

cc @ngimel @ptrblck @mcarilli ",pytorch
60236,crcrpar,pr,2021-06-18T05:10:00Z,Replace TensorRT's deprecated API in `caffe2/python/trt/test_pt_onnx_trt.py`,"TensorRT v8 is going to remove some functions/methods that used in test.

ref:
- getMaxWorkspaceSize deprecation: https://github.com/NVIDIA/TensorRT/blob/b2d60b6e1003f973983903de154a274b569006b8/include/NvInfer.h#L6984-L6993
- buildCudaEngine deprecation: https://github.com/NVIDIA/TensorRT/blob/b2d60b6e1003f973983903de154a274b569006b8/include/NvInfer.h#L7079-L7087

cc @ptrblck ",pytorch
60237,rohan-varma,pr,2021-06-18T05:32:44Z,Enhance ProcessGroupWrapper with additional checks + refactor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60237**

Closes https://github.com/pytorch/pytorch/issues/58711

This diff refactors the collective consistency checking in `ProcessGroupWrapper` as described in the above issue. In particular, we no longer run separate verification checks (`all_gather`s) for shapes, op type, etc. Instead, we implement a function `serialize_fingerprint` to serialize all this data into a single tensor and only verify that.

This has the benefit of being a lot more extensible, the developer does not need to add separate `all_gather` calls in order to verify additional data in the future. We can also provide some sort of mechanism where we allow data that needs to be verified to be ""registered"" in the `CollectiveFingerPrint` struct and make it even easier to add additional data, we can consider doing this if there are significant additions to `process group wrapper`.

We now also begin to check tensor `dtypes` and device types for consistency as well. Tests are refactored/added accordingly.

Differential Revision: [D28597287](https://our.internmc.facebook.com/intern/diff/D28597287/)",pytorch
60275,rohan-varma,pr,2021-06-18T17:00:14Z,Update DDP documentation to mention outputs not used in loss is supported,"We recently landed a change to ensure that when running under ``find_unused_parameters=True``, not all module outputs have to be used in loss computation and DDP will work as expected. Mention this update in the documentation and add some additional clarification. ",pytorch
60301,rohan-varma,pr,2021-06-18T21:39:47Z,[DDP] Remove python GradBucket construction,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60301 [DDP] Remove python GradBucket construction**

`GradBucket` is not meant to be constructed by Python user, only
consumed as part of comm. hook

Differential Revision: [D29239320](https://our.internmc.facebook.com/intern/diff/D29239320/)",pytorch
60313,micmelesse,pr,2021-06-18T23:41:15Z,[ROCM] enable fft tests,"This PR enables fft tests on ROCM. It contains a function that generates a valid input for fft tests that call hipfftExecC2R or hipfftExecZ2D. With this helper function we are able to fix a number of fft tests. This brings a close to the series of fft PRs enabling fft tests on ROCM.
",pytorch
60386,supriyar,pr,2021-06-21T19:02:13Z,[quant] avoid resize calls in observer/fake_quant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60386 [quant] avoid resize calls in observer/fake_quant**

Summary:
During QAT we sometimes encounter errors with scripted models
`RuntimeError: cannot resize variables that require grad`

For per-tensor cases we don't need to resize some buffers so this PR removes the extra resize ops where applicable
Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29271905](https://our.internmc.facebook.com/intern/diff/D29271905)",pytorch
60411,r-barnes,pr,2021-06-21T22:24:45Z,Remove some unused variables,"Test Plan: Sandcastle

Differential Revision: D29221207

",pytorch
60412,r-barnes,pr,2021-06-21T22:25:35Z,Make some downcast issues explicit,"Test Plan: Sandcastle

Differential Revision: D29243195

",pytorch
60414,r-barnes,pr,2021-06-21T22:33:17Z,Fix some variable types,"Differential Revision: D29221183

",pytorch
60415,r-barnes,pr,2021-06-21T22:34:00Z,Fix some loop types,"Differential Revision: D29221174

",pytorch
60432,cloudhan,pr,2021-06-22T03:06:37Z,approx 100x acceleration for parse_kineto_results,"Fixes https://github.com/pytorch/kineto/issues/308, #58983 maybe related",pytorch
60454,crcrpar,pr,2021-06-22T10:54:20Z,Use `accscalar_t` for CUDA add/sub with Tensor and Scalar,"Follow up of #60227, related to #59907 & #58833 

With this pull request, `torch.add` & `torch.sub` use `acc_type` for `Scalar` if either of two arguments is `Scalar`.
This mimics the behavior of [`torch.mul`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L18), `torch._foreach_(add|sub).Scalar` and `torch._foreach_(add|sub).ScalarList`.

---

**reference**
- torch.mul CUDA kernel: https://github.com/pytorch/pytorch/blob/b0c9762e2d1dfcde549344628ad6be063378ef6a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L17-L25
- `torch._foreach_(add|sub).Scalar`: cast scalar https://github.com/pytorch/pytorch/blob/b0c9762e2d1dfcde549344628ad6be063378ef6a/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu#L27
- `torch._foreach_(add|sub).ScalarList`: `BinaryOpScalarListFunctor` https://github.com/pytorch/pytorch/blob/b0c9762e2d1dfcde549344628ad6be063378ef6a/aten/src/ATen/native/cuda/ForeachFunctors.cuh#L180-L182 and multi_tensor_apply handles `scalar_t` and computes `opmath_t` (almost equivalent `accscalar_t`)  https://github.com/pytorch/pytorch/blob/b0c9762e2d1dfcde549344628ad6be063378ef6a/aten/src/ATen/native/cuda/MultiTensorApply.cuh#L60-L68. BinaryOpScalarListFunctor 
is used https://github.com/pytorch/pytorch/blob/b0c9762e2d1dfcde549344628ad6be063378ef6a/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu#L24

cc @ngimel @ptrblck @mcarilli ",pytorch
60474,lezcano,pr,2021-06-22T17:37:41Z,[docs] Fix backticks in docs,"There is a very common error when writing docs: One forgets to write a matching `` ` ``, and something like ``:attr:`x`` is rendered in the docs. This PR fixes most (all?) of these errors (and a few others).

I found these running ``grep -r "">[^#<][^<]*\`""`` on the `docs/build/html/generated` folder. The regex finds an HTML tag that does not start with `#` (as python comments in example code may contain backticks) and that contains a backtick in the rendered HTML.

This regex has not given any false positive in the current codebase, so I am inclined to suggest that we should add this check to the CI. Would this be possible / reasonable / easy to do @malfet ? 
",pytorch
60504,r-barnes,pr,2021-06-23T01:48:13Z,Fix loop types,"Differential Revision: D29313197

",pytorch
60505,r-barnes,pr,2021-06-23T01:49:30Z,Make some comparisons explicit,"Differential Revision: D29313240

",pytorch
60529,lezcano,pr,2021-06-23T10:41:27Z,Prefer linalg::qr over qr in the C++ API,"Fixes #60060

Also adds `torch::linalg::qr` to the C++ API, as it was missing.",pytorch
60530,lezcano,pr,2021-06-23T10:54:38Z,Parametrizations depending on several inputs,"Resubmit of https://github.com/pytorch/pytorch/pull/58488

There was a line that had been changed in `test_nn.py` as caught in https://github.com/pytorch/pytorch/pull/58488#discussion_r651267668

I reverted that line, which should never have been changed. I reckon that should solve the issue.",pytorch
60555,supriyar,pr,2021-06-23T17:23:56Z,[quant][fx][fix] QAT with object_type in qconfig,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60555 [quant][fx][fix] QAT with object_type in qconfig**
* #60386 [quant] avoid resize calls in observer/fake_quant

Summary:
When we do QAT, we swap the FP32 modules with the corresponding quantized modules counterpart by calling `qat_swap_modules` in prepare.
However when we try to look up using the swapped module type in qconfig_dict, we cannot find a match anymore since the qconfig dict contains the original
module type.

In this PR we update the qconfig_dict to include the modules swapped for QATT

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_qconfig_qat_module_type

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29337036](https://our.internmc.facebook.com/intern/diff/D29337036)",pytorch
60562,r-barnes,pr,2021-06-23T18:36:16Z,Add exclusion list to _check_kernel_launches.py,"Test Plan:
```
buck test //caffe2/test:kernel_launch_checks
```

Reviewed By: ngimel

Differential Revision: D29336561

",pytorch
60630,ngimel,pr,2021-06-24T06:18:34Z,tentative fix for adaptiveavgpool gradient computation,"Fixes #60524
",pytorch
60634,r-barnes,pr,2021-06-24T08:13:40Z,Improve kernel launch checks using recursive regex to match parens,"Differential Revision: D29340676

",pytorch
60635,r-barnes,pr,2021-06-24T08:13:45Z,Add missing kernel checks,"Differential Revision: D29355747

",pytorch
60642,lezcano,pr,2021-06-24T11:56:17Z,Faster cumsum and cumprod backwards,"Piggybacking on https://github.com/pytorch/pytorch/pull/58747, now we can implement the backwards of `cumsum` and `cumprod` without tricks. This minimises the number of kernels that are launched in GPU, so we see a reasonable speed-up on GPU. We should also get a better stability for ill-conditioned inputs, as we do not perform any numerical tricks to get the result.

Note that the benchmarks test forward + backward, so the true speed-up on the backward should be even faster. Even more so in `cumsum`, as it requires less operations than the backward of `cumprod`.

<details>
<summary>
Test Script
</summary>

```python
from itertools import product

import torch
from torch.utils.benchmark import Compare, Timer



def get_timer(ndims, prod_dim, dim, num_threads, device):
    size = [500]*ndims
    size[dim] = prod_dim

    x = torch.rand(*size, device=device, requires_grad=True)
    # Make sure there are no zeros as the formula for the backward
    # that we are testing is for when the backward has no zeros
    with torch.no_grad():
        x.add_(1e-3)
    grad = torch.ones_like(x)

    timer = Timer(
        ""torch.autograd.grad([x.cumprod(dim)], [x], grad_outputs=[grad])"",
        globals={""x"": x, ""dim"": dim, ""grad"": grad},
        label=f""Cumprod + Backwards {device}"",
        description=f""dim: {dim}"",
        sub_label=f""prod_dim: {prod_dim}"",
        num_threads=num_threads,
    )

    return timer.blocked_autorange(min_run_time=5)


def get_params():
    ndims = 3
    dims = range(ndims)
    prod_dims = [10, 100, 500]
    for dim, prod_dim, device in product(dims, prod_dims, (""cpu"", ""cuda"")):
        threads = (1, 2, 4) if device == ""cpu"" else (1,)
        for num_threads in threads:
            yield ndims, prod_dim, dim, num_threads, device


compare = Compare([get_timer(*params) for params in get_params()])
compare.trim_significant_figures()
compare.print()
```

</details>

<details>
<summary>
Benchmark PR
</summary>

```
[------------ Cumprod + Backwards cpu -------------]
                     |  dim: 0  |  dim: 1  |  dim: 2
1 threads: -----------------------------------------
      prod_dim: 10   |     11   |     14   |     12
      prod_dim: 100  |    260   |    270   |    260
      prod_dim: 500  |   1400   |   1550   |   1360
2 threads: -----------------------------------------
      prod_dim: 10   |      6   |      6   |      6
      prod_dim: 100  |    170   |    166   |    167
      prod_dim: 500  |    902   |    950   |    858
4 threads: -----------------------------------------
      prod_dim: 10   |      4   |      3   |      3
      prod_dim: 100  |    110   |    108   |    106
      prod_dim: 500  |    576   |    590   |    547

Times are in milliseconds (ms).

[------------ Cumprod + Backwards cuda ------------]
                     |  dim: 0  |  dim: 1  |  dim: 2
1 threads: -----------------------------------------
      prod_dim: 10   |    562   |    566   |   1075
      prod_dim: 100  |   5388   |   5394   |   6697
      prod_dim: 500  |  28170   |  27580   |  30740

Times are in microseconds (us).
```

</details>

<details>
<summary>
Benchmark master
</summary>

```
[------------ Cumprod + Backwards cpu -------------]
                     |  dim: 0  |  dim: 1  |  dim: 2
1 threads: -----------------------------------------
      prod_dim: 10   |     11   |     13   |     12
      prod_dim: 100  |    270   |    270   |    256
      prod_dim: 500  |   1500   |   1590   |   1300
2 threads: -----------------------------------------
      prod_dim: 10   |      6   |      6   |      6
      prod_dim: 100  |    170   |    170   |    164
      prod_dim: 500  |    911   |    940   |    840
4 threads: -----------------------------------------
      prod_dim: 10   |      4   |      4   |      4
      prod_dim: 100  |    111   |    109   |    105
      prod_dim: 500  |    570   |    590   |    536

Times are in milliseconds (ms).

[------------ Cumprod + Backwards cuda ------------]
                     |  dim: 0  |  dim: 1  |  dim: 2
1 threads: -----------------------------------------
      prod_dim: 10   |    616   |    597   |   1109
      prod_dim: 100  |   5976   |   5723   |   7017
      prod_dim: 500  |  31110   |  29160   |  32320

Times are in microseconds (us).
```

</details>",pytorch
60647,qingyunqu,pr,2021-06-24T14:00:50Z,Port addbmm to structured kernels,"Related #55070
",pytorch
60662,rohan-varma,pr,2021-06-24T16:53:39Z,[c10d] Fix test_collective_hang flakiness,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60662 [c10d] Fix test_collective_hang flakiness**

Fixes this flaky test. Basically, sometimes a rank can exit the test
early before rank 0 calls into allreduce. In this case Gloo will throw
connection reset error on all other ranks.

Closes https://github.com/pytorch/pytorch/issues/60652

Differential Revision: [D29364806](https://our.internmc.facebook.com/intern/diff/D29364806/)",pytorch
60715,crcrpar,pr,2021-06-25T03:47:47Z,CUDA `addcmul` and `addcdiv` do math in float for 16 bits I/O,"Currently foreach `addcmul` and `addcdiv` cast scalar to float so that actual math is done in FP32 when tensor dtype is Float16/BFloat16 while regular `addcmul` and `addcdiv`, not.

### Reproducible steps to see the behavioral difference
```ipython
In [1]: import torch; torch.__version__
Out[1]: '1.9.0'

In [2]: a, b, c = torch.tensor([60000.0], device='cuda', dtype=torch.half), torch.tensor([60000.0], device='cuda', dtype=torch.half), torch.tensor([-1.0], device='cuda', dtype=torch.half)

In [4]: torch.addcmul(a, b, c, value=2)
Out[4]: tensor([-inf], device='cuda:0', dtype=torch.float16)

In [5]: torch._foreach_addcmul([a], [b], [c], value=2)[0]
Out[5]: tensor([-60000.], device='cuda:0', dtype=torch.float16)
```

### How foreach casts?
Foreach addcmul and addcdiv cast scalar to `opmath_t` (almost equivalent to acc_type) here: https://github.com/pytorch/pytorch/blob/42c8439b6eaccf175cceaa820452583e2459a521/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu#L30 and cast inputs and results here:
https://github.com/pytorch/pytorch/blob/42c8439b6eaccf175cceaa820452583e2459a521/aten/src/ATen/native/cuda/ForeachFunctors.cuh#L133-L135

Related to #58833 #60227 #60454
cc @ptrblck @mcarilli @ngimel ",pytorch
60755,eqy,pr,2021-06-25T18:05:15Z,"[cuDNN v8 API] cuDNN benchmark, convolution bwd / transposed convolution fwd, `bfloat16`, conv-bias-activation fusion","#58414, #58859, #58858 #58860 #58861

We're currently testing performance with both ""find"" and ""get"" with this PR. 

CC @zasdfgbnm @ptrblck @ngimel @puririshi98

In addition to the `USE_EXPERIMENTAL_CUDNN_V8_API` build flag, we've added a `CUDNN_V8_API_ENABLED` runtime feature flag.
`USE_EXPERIMENTAL_CUDNN_V8_API=1` will build with v8 API support while keeping all v7 functionality, with v8 usage disabled by default.
`CUDNN_V8_API_ENABLED=1` at runtime on a `USE_EXPERIMENTAL_CUDNN_V8_API=1` build uses the v8 API.
A debug flag `CUDNN_V8_API_DEBUG=1` can be used to verify which API is used when dispatching convolutions.

Note that in v7, `bfloat16` convolutions will dispatch to a native PyTorch implementation, but a fully v8 enabled build will dispatch to cuDNN implementations.",pytorch
60778,r-barnes,pr,2021-06-25T20:20:55Z,Paren-matching kernel launch check without external deps,"Differential Revision: D29401624

",pytorch
60788,rohan-varma,pr,2021-06-25T21:14:11Z,[not for land] Prototype fix for https://github.com/pytorch/pytorch/issues/60733,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60788**

Differential Revision: [D29403687](https://our.internmc.facebook.com/intern/diff/D29403687/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29403687/)!",pytorch
60818,bharatr21,pr,2021-06-26T06:52:30Z,[DOC] Fix Optimizer state_dict docs,"Fixes #60121 by correcting the `state_dict` docs
Tagging @mruberry @vincentqb @brianjo for review since they were originally mentioned in the issue",pytorch
60837,ngimel,pr,2021-06-27T21:23:27Z,Don't reference reflection_pad3d in functional.py,"To work around FC issue
",pytorch
60872,r-barnes,pr,2021-06-28T15:45:47Z,Remove for-loop for getting number of elements in favour of abstraction,"Differential Revision: D29406199

",pytorch
60873,r-barnes,pr,2021-06-28T15:45:47Z,Remove another for-loop in SoftMax,"Differential Revision: D29406429

",pytorch
60874,r-barnes,pr,2021-06-28T15:46:21Z,Loop transformation,"Differential Revision: D29406474

",pytorch
60875,r-barnes,pr,2021-06-28T15:48:02Z,Fix some comparison warnings,"Differential Revision: D29406593

",pytorch
60876,r-barnes,pr,2021-06-28T16:13:21Z,More loop transforms,"Differential Revision: D29410111

",pytorch
60882,rohan-varma,pr,2021-06-28T16:40:39Z,[DDP] Fix case where new tensors with no grad_fn are returned in DDP forward.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60882**

Fixes https://github.com/pytorch/pytorch/issues/60733, which
identified an issue with a previous PR that resulted in DDP no longer
supporting cases where newly created tensors are returned that don't have a
grad_fn. The result of this is the grad_fn is set to that of the `DDPSink`
custom backward which results in errors during the backwards pass.

This PR fixes the issue by ensuring we don't touch the `grad_fn` of the tensors
if it is `None`. Added relevant tests as well.

Differential Revision: [D29423822](https://our.internmc.facebook.com/intern/diff/D29423822/)",pytorch
60894,r-barnes,pr,2021-06-28T20:01:45Z,Fix some integer comparisons,"Differential Revision: D29431512

",pytorch
60895,r-barnes,pr,2021-06-28T20:02:23Z,Fix some more loops,"Differential Revision: D29431572

",pytorch
60896,r-barnes,pr,2021-06-28T20:04:41Z,Fix a variable initialization,"Differential Revision: D29431625

",pytorch
60901,rohan-varma,pr,2021-06-28T20:45:45Z,[ProcessGroupNCCL] change WARNING to INFO,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60901**

Short-term fix to address
https://github.com/pytorch/pytorch/issues/60752 . Longer-term fix is tracked here:
https://github.com/pytorch/pytorch/issues/53658 and will involve detecting
whether the user has called `torch.cuda.set_device` in their script and
respecting that device if so, otherwise falling back to our current approach.

Differential Revision: [D29439322](https://our.internmc.facebook.com/intern/diff/D29439322/)",pytorch
60921,rohan-varma,pr,2021-06-29T01:57:05Z,[DDP] Disable reducer hooks from running outside of DDP backwards.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60921**

Sometimes local modules can fire hooks (such as when user calls
backward after using `ddp_module.module` explicitly). This isn't supported
behavior and can cause issues with various state and gradient reduction we run
in DDP, so it's best to disable this entirely.

Differential Revision: [D29435737](https://our.internmc.facebook.com/intern/diff/D29435737/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29435737/)!",pytorch
60984,rohan-varma,pr,2021-06-29T20:24:23Z,Ci all/rvarm1/ddp local fix,"Fixes #{issue number}
",pytorch
61010,zhuzilin,pr,2021-06-30T03:32:00Z,[DataLoader] Introduce ConcatMapDataPipe functional datapipe,"As part of #57031, this PR adds the ConcatMapDataPipe functional datapipe for the MapDataPipe class.

We may need to discuss how to treat the datapipes with no valid length. For now, I just use them as if they have infinite length and the `__getitem__` could not go pass them.

Thank you for your time on reviewing this~

cc @ejguan ",pytorch
61017,rohan-varma,pr,2021-06-30T05:51:42Z,[DDP] Remove SPMD from get_bucket_tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61020
* #61019
* #61018
* **#61017**

Removes SPMD nested vector logic from this codepath. This is mostly in preparation for the next diffs in this stack which enable support for join with comm. hook.

Differential Revision: [D29477360](https://our.internmc.facebook.com/intern/diff/D29477360/)",pytorch
61018,rohan-varma,pr,2021-06-30T05:51:49Z,[DDP] Refactor hook running logic,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61020
* #61019
* **#61018**
* #61017

Extract logic of hook running to a function `run_reduction_hook` that takes in a `GradBucket` and runs the hook/allreduce. This is mainly to prepare for join to support comm. hook in follow up diffs.

Differential Revision: [D29477143](https://our.internmc.facebook.com/intern/diff/D29477143/)",pytorch
61019,rohan-varma,pr,2021-06-30T05:51:56Z,[DDP] Refactor uneven inputs to take GradBucket,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61020
* **#61019**

Changes uneven input logic of running allreduce to using `GradBucket` structure. This is to enable support for comm. hook with join in the next diff.

Differential Revision: [D29480027](https://our.internmc.facebook.com/intern/diff/D29480027/)",pytorch
61020,rohan-varma,pr,2021-06-30T05:52:03Z,[DDP] Make uneven inputs work with comm. hook,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61020**
* #61019

Closes https://github.com/pytorch/pytorch/issues/60611
Makes uneven input support with `join` context manager work with
custom communication hooks. This will ensure that the two features can work
well together. Added relevant unittests to test allreduce and powerSGD hooks.

Instead of calling `allreduce`, the join manager now calls into `_run_reduction_hook` which will automatically run whatever hook is installed.

Differential Revision: [D29480028](https://our.internmc.facebook.com/intern/diff/D29480028/)",pytorch
61056,fritzo,pr,2021-06-30T18:10:15Z,Improve error message on invalid values to Distribution methods,Fixes #18133,pytorch
61065,eqy,pr,2021-06-30T19:34:27Z,Fix deterministic index put on CUDA,"#61032
The issue was caused by the fill value not being explicitly ""broadcasted"" to match the number of elements specified by the indices.

CC @ptrblck ",pytorch
61073,micmelesse,pr,2021-06-30T20:54:15Z,[ROCM] fix bug in #60313,"This PR fixes a bug in #60313. Where the tensors generated by _generate_valid_rocfft_input are on the cpu instead of the gpu. This was due to using numpy to generate tensors and converting it to pytorch using torch.from_numpy. This leads to the generated tensors staying on the cpu. We now generate the tensors using pytorch itself which carries over the device type of the input tensors to the generated tensor.
",pytorch
61132,supriyar,pr,2021-07-01T18:24:39Z,[quant][fx][perf] improve runtime of prepare step for large models,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61132**

Summary:
For large models, the insert_observers_for_model function was taking a long time, especially for the case where not all the nodes are being quantized

For example for a model with 21000 nodes of which only ~50 are being quantized the breakdown of prepare_fx vs convert fx was

prepare_fx 979 seconds
convert_fx 9 seconds

The main reason was because we were doing some unnecessary computation for all nodes in this function, this PR just moves them to where they are actually used

After this PR
prepare_fx 26 seconds
convert_fx 9 seconds

Test Plan:
Existing tests

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29522303](https://our.internmc.facebook.com/intern/diff/D29522303)",pytorch
61163,crcrpar,pr,2021-07-02T03:18:48Z,Add `expecttest` to CONTRIBUTING.md,"Now expecttest is an independent library but `CONTRIBUTING.md` and `requirements.txt` do not mention the need of the library.

Related: https://github.com/pytorch/pytorch/pull/60658 ",pytorch
61240,ngimel,pr,2021-07-05T02:14:53Z,clean up pow instantiations,"This reduces the PowKernel.cu.o file size from 8 something MB (for sm_70) to 3.4 MB. Compile time, however, barely budges (from 9 min to 8 min). Most of the remaining compile time is in line https://github.com/pytorch/pytorch/compare/ngimel/pow?expand=1#diff-2358cc6e15a09767aa56fd090789c73676ba9e77f7b49d5be2e8988b88daf358R98, commenting it out reduces compile time from 8 minutes to 1.5m. and I don't understand how it's different from the instantiation 2 lines below, or similar complex instantiation in line 141.
",pytorch
61282,cloudhan,pr,2021-07-06T10:48:46Z,Report more information for memory profiling,"Report pointed memory size, total allocated memory, total reserved size all in one report.

`ptr` and `alloc_size` will be used for associating with op trace.
`allocated_size`, `reserved_size` will be used for memory trace. 
",pytorch
61314,supriyar,pr,2021-07-06T22:48:46Z,[quant] Add tensor_qparam variant to fake_quantize_per_tensor,"Summary:
Add an overload to fake_quantize_per_tensor that accepts scale/zero_point as input. The reasons to do this are

* required for fused observer + fake_quant operator on GPU where the scale/zero_point will be calculated by the observer on device. Passing tensor inputs enables us to directly access the scale/zero-point value in the cuda kernel to avoid extra copies/malloc
* enables us to pass in float as scale dtype and int32 as zero_point dtype (which is consistent with what the quantize call actually uses) https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/affine_quantizer_base.cpp#L52-L53
* overload consistent with `quantizer_per_tensor.tensor_qparams`

Test Plan:
buck test mode/dev-nosan caffe2/test/:quantization -- test_backward_per_tensor_cachemask
buck test mode/dev-nosan caffe2/test/:quantization -- test_forward_per_tensor_cachemask

Differential Revision: D29552727

",pytorch
61315,supriyar,pr,2021-07-06T22:49:30Z,[quant] update FakeQuant modules to use tensor qparams,"Depends on https://github.com/pytorch/pytorch/pull/61314

Summary:
Remove the `float()` and `int()` calls in the forward function so that we can directly use the tensor qparams in the fake_quantize operator.

Calling `float()/int()` internally calls `item()` which can trigger a gpu-> cpu copy if the original tensors reside on GPU.
Local benchmark P427668213

Before this change
```
                                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                     aten::_aminmax         2.57%       1.507ms         3.10%       1.819ms      36.371us       2.872ms         4.81%       2.872ms      57.446us            50
              aten::fake_quantize_per_tensor_affine         1.04%     610.915us         3.60%       2.114ms      42.276us     472.896us         0.79%       2.698ms      53.962us            50
    aten::fake_quantize_per_tensor_affine_cachemask         1.69%     993.626us         2.56%       1.503ms      30.058us       2.225ms         3.73%       2.225ms      44.504us            50
                                   aten::is_nonzero         3.85%       2.258ms        19.68%      11.540ms      46.161us       2.168ms         3.63%      11.084ms      44.336us           250
                                   aten::zeros_like         1.82%       1.064ms         6.65%       3.901ms      39.007us       1.531ms         2.57%       3.905ms      39.045us           100
                                           aten::eq        13.80%       8.093ms        25.90%      15.189ms      37.972us       9.580ms        16.05%      15.566ms      38.914us           400
                                         aten::item         5.67%       3.323ms        21.50%      12.607ms      36.019us       3.233ms         5.42%      12.167ms      34.762us           350
                                        aten::zeros         0.94%     549.208us         2.93%       1.717ms      34.343us     688.928us         1.15%       1.695ms      33.894us            50
                                           aten::le         2.52%       1.478ms         4.50%       2.641ms      26.411us       1.753ms         2.94%       2.845ms      28.448us           100
                                         aten::rsub         1.04%     608.715us         2.44%       1.433ms      28.667us     532.000us         0.89%       1.418ms      28.353us            50
                                          aten::max         1.54%     905.401us         4.62%       2.711ms      27.106us     847.488us         1.42%       2.697ms      26.969us           100
                                         aten::ones         0.92%     542.159us         2.16%       1.266ms      25.324us     661.856us         1.11%       1.301ms      26.017us            50
                                          aten::min         0.82%     479.167us         2.15%       1.258ms      25.160us     407.808us         0.68%       1.276ms      25.530us            50
                          aten::_local_scalar_dense        15.83%       9.284ms        15.83%       9.284ms      26.526us       8.934ms        14.97%       8.934ms      25.524us           350
                                        aten::clamp         2.35%       1.378ms         4.21%       2.467ms      24.669us       1.546ms         2.59%       2.461ms      24.612us           100
                                        aten::zero_         2.53%       1.482ms         5.65%       3.316ms      22.108us       1.326ms         2.22%       3.380ms      22.531us           150
                                      aten::maximum         3.08%       1.805ms         3.08%       1.805ms      18.052us       1.849ms         3.10%       1.849ms      18.494us           100
                                      aten::minimum         1.33%     778.854us         1.33%     778.854us      15.577us     868.672us         1.46%     868.672us      17.373us            50
                                        aten::round         1.36%     799.910us         1.36%     799.910us      15.998us     809.568us         1.36%     809.568us      16.191us            50
                                        aten::copy_         6.61%       3.878ms         6.61%       3.878ms      15.513us       4.036ms         6.76%       4.036ms      16.143us           250
                                          aten::div         2.53%       1.483ms         2.53%       1.483ms      14.833us       1.535ms         2.57%       1.535ms      15.353us           100
                                          aten::mul         2.44%       1.431ms         2.44%       1.431ms      14.314us       1.478ms         2.48%       1.478ms      14.782us           100
                                       aten::detach         1.46%     855.670us         2.41%       1.411ms      14.110us     832.448us         1.39%       1.395ms      13.949us           100
                                          aten::add         2.22%       1.301ms         2.22%       1.301ms      13.008us       1.383ms         2.32%       1.383ms      13.828us           100
                                        aten::fill_         4.18%       2.452ms         4.18%       2.452ms      12.262us       2.693ms         4.51%       2.693ms      13.463us           200
                                          aten::sub         5.06%       2.967ms         5.06%       2.967ms      14.837us       2.675ms         4.48%       2.675ms      13.374us           200
                                           aten::to         2.10%       1.230ms         3.65%       2.140ms      10.701us       1.310ms         2.20%       2.062ms      10.310us           200
                                       aten::select         1.28%     749.144us         1.49%     874.227us       8.742us     863.232us         1.45%     863.232us       8.632us           100
                                             detach         0.95%     555.326us         0.95%     555.326us       5.553us     562.496us         0.94%     562.496us       5.625us           100
                                   aten::as_strided         0.40%     232.289us         0.40%     232.289us       1.161us       0.000us         0.00%       0.000us       0.000us           200
                                        aten::empty         2.93%       1.720ms         2.93%       1.720ms       3.439us       0.000us         0.00%       0.000us       0.000us           500
                                      aten::resize_         1.04%     611.313us         1.04%     611.313us       2.038us       0.000us         0.00%       0.000us       0.000us           300
                                   aten::empty_like         0.75%     438.585us         1.77%       1.036ms       5.180us       0.000us         0.00%       0.000us       0.000us           200
                                aten::empty_strided         1.36%     799.442us         1.36%     799.442us       3.198us       0.000us         0.00%       0.000us       0.000us           250
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 58.645ms
Self CUDA time total: 59.674ms
```

After this change
```

test_fake_quant_profiler (scripts.supriyar.benchmark.module_bench.ProfilerBench) ... -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::fake_quantize_per_tensor_affine         0.98%     505.210us         4.38%       2.259ms      45.187us     419.424us         0.78%       3.218ms      64.367us            50
                                         aten::_aminmax         2.78%       1.434ms         3.42%       1.766ms      35.321us       2.825ms         5.27%       2.825ms      56.505us            50
aten::fake_quantize_per_tensor_affine_cachemask_tens...         2.38%       1.229ms         3.40%       1.754ms      35.083us       2.799ms         5.22%       2.799ms      55.979us            50
                                             aten::rsub         0.94%     485.040us         5.02%       2.590ms      51.793us     458.976us         0.86%       2.587ms      51.747us            50
                                       aten::is_nonzero         3.78%       1.952ms        23.64%      12.196ms      48.786us       2.055ms         3.83%      11.986ms      47.944us           250
                                             aten::item         6.92%       3.572ms        19.86%      10.244ms      40.977us       3.670ms         6.85%       9.931ms      39.724us           250
                                       aten::zeros_like         1.65%     848.874us         6.64%       3.426ms      34.260us       1.397ms         2.61%       3.572ms      35.717us           100
                                            aten::zeros         0.85%     436.691us         3.00%       1.549ms      30.984us     551.936us         1.03%       1.576ms      31.516us            50
                                               aten::eq        10.60%       5.467ms        20.26%      10.452ms      26.130us       7.018ms        13.09%      10.832ms      27.079us           400
                                               aten::le         2.58%       1.332ms         4.67%       2.407ms      24.074us       1.580ms         2.95%       2.614ms      26.144us           100
                              aten::_local_scalar_dense        12.93%       6.673ms        12.93%       6.673ms      26.691us       6.261ms        11.68%       6.261ms      25.046us           250
                                            aten::clamp         2.43%       1.253ms         4.37%       2.256ms      22.560us       1.431ms         2.67%       2.273ms      22.725us           100
                                             aten::ones         0.89%     460.133us         2.18%       1.123ms      22.467us     570.496us         1.06%       1.128ms      22.551us            50
                                              aten::min         0.74%     383.132us         2.06%       1.065ms      21.296us     377.536us         0.70%       1.091ms      21.824us            50
                                            aten::zero_         2.36%       1.219ms         5.87%       3.029ms      20.194us       1.261ms         2.35%       3.199ms      21.327us           150
                                              aten::max         1.51%     779.081us         4.06%       2.096ms      20.960us     791.680us         1.48%       2.130ms      21.295us           100
                                              aten::sub         7.97%       4.111ms         7.97%       4.111ms      20.556us       3.847ms         7.18%       3.847ms      19.234us           200
                                              aten::div         2.94%       1.516ms         2.94%       1.516ms      15.158us       1.580ms         2.95%       1.580ms      15.798us           100
                                            aten::round         1.45%     750.445us         1.45%     750.445us      15.009us     756.064us         1.41%     756.064us      15.121us            50
                                            aten::copy_         6.88%       3.548ms         6.88%       3.548ms      14.190us       3.701ms         6.90%       3.701ms      14.803us           250
                                          aten::minimum         1.32%     681.654us         1.32%     681.654us      13.633us     713.664us         1.33%     713.664us      14.273us            50
                                          aten::maximum         2.55%       1.317ms         2.55%       1.317ms      13.169us       1.338ms         2.50%       1.338ms      13.378us           100
                                              aten::mul         2.63%       1.358ms         2.63%       1.358ms      13.581us       1.328ms         2.48%       1.328ms      13.283us           100
                                           aten::detach         1.34%     688.820us         2.35%       1.211ms      12.110us     772.800us         1.44%       1.278ms      12.779us           100
                                            aten::fill_         4.53%       2.338ms         4.53%       2.338ms      11.692us       2.495ms         4.65%       2.495ms      12.473us           200
                                              aten::add         2.32%       1.197ms         2.32%       1.197ms      11.968us       1.240ms         2.31%       1.240ms      12.405us           100
                                               aten::to         2.07%       1.069ms         3.66%       1.889ms       9.443us       1.224ms         2.28%       1.975ms       9.874us           200
                                           aten::select         1.44%     743.042us         1.64%     848.207us       8.482us     641.600us         1.20%     641.600us       6.416us           100
                                                 detach         1.01%     522.155us         1.01%     522.155us       5.222us     505.088us         0.94%     505.088us       5.051us           100
                                       aten::as_strided         0.44%     227.884us         0.44%     227.884us       1.139us       0.000us         0.00%       0.000us       0.000us           200
                                            aten::empty         3.20%       1.652ms         3.20%       1.652ms       3.304us       0.000us         0.00%       0.000us       0.000us           500
                                          aten::resize_         1.25%     646.711us         1.25%     646.711us       2.156us       0.000us         0.00%       0.000us       0.000us           300
                                       aten::empty_like         0.79%     407.768us         2.07%       1.067ms       5.334us       0.000us         0.00%       0.000us       0.000us           200
                                    aten::empty_strided         1.52%     785.788us         1.52%     785.788us       3.143us       0.000us         0.00%       0.000us       0.000us           250
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 51.590ms
Self CUDA time total: 53.609ms
```

Test Plan: buck test mode/dev-nosan caffe2/test/:quantization

Differential Revision: D29566512

",pytorch
61317,supriyar,pr,2021-07-06T23:20:30Z,[quant] Add tensor_qparam variant to fake_quantize_per_tensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61318
* **#61317**

Add an overload to fake_quantize_per_tensor that accepts scale/zero_point as input. The reasons to do this are

* required for fused observer + fake_quant operator on GPU where the scale/zero_point will be calculated by the observer on device. Passing tensor inputs enables us to directly access the scale/zero-point value in the cuda kernel to avoid extra copies/malloc
* enables us to pass in float as scale dtype and int32 as zero_point dtype (which is consistent with what the quantize call actually uses) https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/affine_quantizer_base.cpp#L52-L53
* overload consistent with `quantizer_per_tensor.tensor_qparams`

Differential Revision: [D29552727](https://our.internmc.facebook.com/intern/diff/D29552727/)",pytorch
61318,supriyar,pr,2021-07-06T23:20:37Z,[quant] update FakeQuant modules to use tensor qparams,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61318**
* #61317

Remove the `float()` and `int()` calls in the forward function so that we can directly use the tensor qparams in the fake_quantize operator.

Calling `float()/int()` internally calls `item()` which can trigger a gpu-> cpu copy if the original tensors reside on GPU.
Local benchmark P427668213

Before this change
```
                                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                     aten::_aminmax         2.57%       1.507ms         3.10%       1.819ms      36.371us       2.872ms         4.81%       2.872ms      57.446us            50
              aten::fake_quantize_per_tensor_affine         1.04%     610.915us         3.60%       2.114ms      42.276us     472.896us         0.79%       2.698ms      53.962us            50
    aten::fake_quantize_per_tensor_affine_cachemask         1.69%     993.626us         2.56%       1.503ms      30.058us       2.225ms         3.73%       2.225ms      44.504us            50
                                   aten::is_nonzero         3.85%       2.258ms        19.68%      11.540ms      46.161us       2.168ms         3.63%      11.084ms      44.336us           250
                                   aten::zeros_like         1.82%       1.064ms         6.65%       3.901ms      39.007us       1.531ms         2.57%       3.905ms      39.045us           100
                                           aten::eq        13.80%       8.093ms        25.90%      15.189ms      37.972us       9.580ms        16.05%      15.566ms      38.914us           400
                                         aten::item         5.67%       3.323ms        21.50%      12.607ms      36.019us       3.233ms         5.42%      12.167ms      34.762us           350
                                        aten::zeros         0.94%     549.208us         2.93%       1.717ms      34.343us     688.928us         1.15%       1.695ms      33.894us            50
                                           aten::le         2.52%       1.478ms         4.50%       2.641ms      26.411us       1.753ms         2.94%       2.845ms      28.448us           100
                                         aten::rsub         1.04%     608.715us         2.44%       1.433ms      28.667us     532.000us         0.89%       1.418ms      28.353us            50
                                          aten::max         1.54%     905.401us         4.62%       2.711ms      27.106us     847.488us         1.42%       2.697ms      26.969us           100
                                         aten::ones         0.92%     542.159us         2.16%       1.266ms      25.324us     661.856us         1.11%       1.301ms      26.017us            50
                                          aten::min         0.82%     479.167us         2.15%       1.258ms      25.160us     407.808us         0.68%       1.276ms      25.530us            50
                          aten::_local_scalar_dense        15.83%       9.284ms        15.83%       9.284ms      26.526us       8.934ms        14.97%       8.934ms      25.524us           350
                                        aten::clamp         2.35%       1.378ms         4.21%       2.467ms      24.669us       1.546ms         2.59%       2.461ms      24.612us           100
                                        aten::zero_         2.53%       1.482ms         5.65%       3.316ms      22.108us       1.326ms         2.22%       3.380ms      22.531us           150
                                      aten::maximum         3.08%       1.805ms         3.08%       1.805ms      18.052us       1.849ms         3.10%       1.849ms      18.494us           100
                                      aten::minimum         1.33%     778.854us         1.33%     778.854us      15.577us     868.672us         1.46%     868.672us      17.373us            50
                                        aten::round         1.36%     799.910us         1.36%     799.910us      15.998us     809.568us         1.36%     809.568us      16.191us            50
                                        aten::copy_         6.61%       3.878ms         6.61%       3.878ms      15.513us       4.036ms         6.76%       4.036ms      16.143us           250
                                          aten::div         2.53%       1.483ms         2.53%       1.483ms      14.833us       1.535ms         2.57%       1.535ms      15.353us           100
                                          aten::mul         2.44%       1.431ms         2.44%       1.431ms      14.314us       1.478ms         2.48%       1.478ms      14.782us           100
                                       aten::detach         1.46%     855.670us         2.41%       1.411ms      14.110us     832.448us         1.39%       1.395ms      13.949us           100
                                          aten::add         2.22%       1.301ms         2.22%       1.301ms      13.008us       1.383ms         2.32%       1.383ms      13.828us           100
                                        aten::fill_         4.18%       2.452ms         4.18%       2.452ms      12.262us       2.693ms         4.51%       2.693ms      13.463us           200
                                          aten::sub         5.06%       2.967ms         5.06%       2.967ms      14.837us       2.675ms         4.48%       2.675ms      13.374us           200
                                           aten::to         2.10%       1.230ms         3.65%       2.140ms      10.701us       1.310ms         2.20%       2.062ms      10.310us           200
                                       aten::select         1.28%     749.144us         1.49%     874.227us       8.742us     863.232us         1.45%     863.232us       8.632us           100
                                             detach         0.95%     555.326us         0.95%     555.326us       5.553us     562.496us         0.94%     562.496us       5.625us           100
                                   aten::as_strided         0.40%     232.289us         0.40%     232.289us       1.161us       0.000us         0.00%       0.000us       0.000us           200
                                        aten::empty         2.93%       1.720ms         2.93%       1.720ms       3.439us       0.000us         0.00%       0.000us       0.000us           500
                                      aten::resize_         1.04%     611.313us         1.04%     611.313us       2.038us       0.000us         0.00%       0.000us       0.000us           300
                                   aten::empty_like         0.75%     438.585us         1.77%       1.036ms       5.180us       0.000us         0.00%       0.000us       0.000us           200
                                aten::empty_strided         1.36%     799.442us         1.36%     799.442us       3.198us       0.000us         0.00%       0.000us       0.000us           250
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 58.645ms
Self CUDA time total: 59.674ms
```

After this change
```

test_fake_quant_profiler (scripts.supriyar.benchmark.module_bench.ProfilerBench) ... -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::fake_quantize_per_tensor_affine         0.98%     505.210us         4.38%       2.259ms      45.187us     419.424us         0.78%       3.218ms      64.367us            50
                                         aten::_aminmax         2.78%       1.434ms         3.42%       1.766ms      35.321us       2.825ms         5.27%       2.825ms      56.505us            50
aten::fake_quantize_per_tensor_affine_cachemask_tens...         2.38%       1.229ms         3.40%       1.754ms      35.083us       2.799ms         5.22%       2.799ms      55.979us            50
                                             aten::rsub         0.94%     485.040us         5.02%       2.590ms      51.793us     458.976us         0.86%       2.587ms      51.747us            50
                                       aten::is_nonzero         3.78%       1.952ms        23.64%      12.196ms      48.786us       2.055ms         3.83%      11.986ms      47.944us           250
                                             aten::item         6.92%       3.572ms        19.86%      10.244ms      40.977us       3.670ms         6.85%       9.931ms      39.724us           250
                                       aten::zeros_like         1.65%     848.874us         6.64%       3.426ms      34.260us       1.397ms         2.61%       3.572ms      35.717us           100
                                            aten::zeros         0.85%     436.691us         3.00%       1.549ms      30.984us     551.936us         1.03%       1.576ms      31.516us            50
                                               aten::eq        10.60%       5.467ms        20.26%      10.452ms      26.130us       7.018ms        13.09%      10.832ms      27.079us           400
                                               aten::le         2.58%       1.332ms         4.67%       2.407ms      24.074us       1.580ms         2.95%       2.614ms      26.144us           100
                              aten::_local_scalar_dense        12.93%       6.673ms        12.93%       6.673ms      26.691us       6.261ms        11.68%       6.261ms      25.046us           250
                                            aten::clamp         2.43%       1.253ms         4.37%       2.256ms      22.560us       1.431ms         2.67%       2.273ms      22.725us           100
                                             aten::ones         0.89%     460.133us         2.18%       1.123ms      22.467us     570.496us         1.06%       1.128ms      22.551us            50
                                              aten::min         0.74%     383.132us         2.06%       1.065ms      21.296us     377.536us         0.70%       1.091ms      21.824us            50
                                            aten::zero_         2.36%       1.219ms         5.87%       3.029ms      20.194us       1.261ms         2.35%       3.199ms      21.327us           150
                                              aten::max         1.51%     779.081us         4.06%       2.096ms      20.960us     791.680us         1.48%       2.130ms      21.295us           100
                                              aten::sub         7.97%       4.111ms         7.97%       4.111ms      20.556us       3.847ms         7.18%       3.847ms      19.234us           200
                                              aten::div         2.94%       1.516ms         2.94%       1.516ms      15.158us       1.580ms         2.95%       1.580ms      15.798us           100
                                            aten::round         1.45%     750.445us         1.45%     750.445us      15.009us     756.064us         1.41%     756.064us      15.121us            50
                                            aten::copy_         6.88%       3.548ms         6.88%       3.548ms      14.190us       3.701ms         6.90%       3.701ms      14.803us           250
                                          aten::minimum         1.32%     681.654us         1.32%     681.654us      13.633us     713.664us         1.33%     713.664us      14.273us            50
                                          aten::maximum         2.55%       1.317ms         2.55%       1.317ms      13.169us       1.338ms         2.50%       1.338ms      13.378us           100
                                              aten::mul         2.63%       1.358ms         2.63%       1.358ms      13.581us       1.328ms         2.48%       1.328ms      13.283us           100
                                           aten::detach         1.34%     688.820us         2.35%       1.211ms      12.110us     772.800us         1.44%       1.278ms      12.779us           100
                                            aten::fill_         4.53%       2.338ms         4.53%       2.338ms      11.692us       2.495ms         4.65%       2.495ms      12.473us           200
                                              aten::add         2.32%       1.197ms         2.32%       1.197ms      11.968us       1.240ms         2.31%       1.240ms      12.405us           100
                                               aten::to         2.07%       1.069ms         3.66%       1.889ms       9.443us       1.224ms         2.28%       1.975ms       9.874us           200
                                           aten::select         1.44%     743.042us         1.64%     848.207us       8.482us     641.600us         1.20%     641.600us       6.416us           100
                                                 detach         1.01%     522.155us         1.01%     522.155us       5.222us     505.088us         0.94%     505.088us       5.051us           100
                                       aten::as_strided         0.44%     227.884us         0.44%     227.884us       1.139us       0.000us         0.00%       0.000us       0.000us           200
                                            aten::empty         3.20%       1.652ms         3.20%       1.652ms       3.304us       0.000us         0.00%       0.000us       0.000us           500
                                          aten::resize_         1.25%     646.711us         1.25%     646.711us       2.156us       0.000us         0.00%       0.000us       0.000us           300
                                       aten::empty_like         0.79%     407.768us         2.07%       1.067ms       5.334us       0.000us         0.00%       0.000us       0.000us           200
                                    aten::empty_strided         1.52%     785.788us         1.52%     785.788us       3.143us       0.000us         0.00%       0.000us       0.000us           250
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 51.590ms
Self CUDA time total: 53.609ms

Differential Revision: [D29566512](https://our.internmc.facebook.com/intern/diff/D29566512/)",pytorch
61327,crcrpar,pr,2021-07-07T01:22:10Z,"Foreach Test Refactor: Pointwise, Min/Max-imum","- rewrite pointwise unittests using `ops` decorator
- rewrite minimum&maximum unittests using `ops` decorator
- enable minimum/maximum fastpath for BFloat16
- remove _test_data method

#58833 

cc: @ptrblck @ngimel ",pytorch
61363,r-barnes,pr,2021-07-07T18:21:50Z,Add a type for test fixture world_size,"Test Plan: Sandcastle

Differential Revision: D29561360

",pytorch
61390,rohan-varma,pr,2021-07-07T23:53:06Z,[TCPStore] enhance connect timeout error message,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61390**

Enhances this error message for better debugability.

Differential Revision: [D29601528](https://our.internmc.facebook.com/intern/diff/D29601528/)",pytorch
61396,rohan-varma,pr,2021-07-08T02:17:15Z,[BE] Fix flaky ProcessGroupGloo tests,"A hypothesis as to why tests such as https://github.com/pytorch/pytorch/issues/57469 may be flaky is due to `c10d = ProcessGroupGloo(...)` is not actually guaranteed to be a synchronization point, so some ranks may create the PG, run all the error checking (which does not actually call into gloo APIs so doesn't require synchronization), and then exit, all before other ranks have created the gloo pg.

This can result in the following error:
```
File ""distributed/test_c10d_gloo.py"", line 1037, in test_reduce_checks
May 03 06:42:34     pg = c10d.ProcessGroupGloo(store, self.rank, self.world_size, self.opts())
May 03 06:42:34 RuntimeError: [/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [127.0.0.1]:35521
```

which indicates that the remote end has hung up. Furthermore all the flaky tests in this file only do error checking and don't call into the gloo APIs, further indicating that this issue may be the root cause. Not 100% sure this PR will fix it because I haven't been able to actually repro the issue even after 10000+ runs, but it happens regularly in CI.

To fix this, we add a `dist.barrier(group=pg)` call after creating the pg to enforce a synchronization. Would be good to land this and observe whether it helps with the flakiness.",pytorch
61399,rohan-varma,pr,2021-07-08T05:38:37Z,"Back out ""[DDP] Disable reducer hooks from running outside of DDP backwards.""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61497
* #61401
* **#61399**

Reverts https://github.com/pytorch/pytorch/pull/60921
Original commit changeset: fef76a0dd295

Differential Revision: [D29594262](https://our.internmc.facebook.com/intern/diff/D29594262/)",pytorch
61401,rohan-varma,pr,2021-07-08T06:31:25Z,Revert [DDP] Support for multiple backwards,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61497
* **#61401**
* #61399

Reverts https://github.com/pytorch/pytorch/pull/59359, which is causing a few internal issues in DDP training. We will evaluate the internal use cases and reland it after reconsidering the design.

Differential Revision: [D29608948](https://our.internmc.facebook.com/intern/diff/D29608948/)",pytorch
61424,bryant1410,pr,2021-07-08T18:21:20Z,Save some little memory in `default_collate`,It can be a non-little save if there are many workers and a large batch size.,pytorch
61440,crcrpar,pr,2021-07-09T01:21:34Z,Use `fastAtomicAdd` in `torch.scatter_add` CUDA kernel,"`torch.scatter_add(input, dim, index, src)` FP16 CUDA kernel is much slower than FP32 kernel when `index` is explicitly broadcasted. 
`fastAtomicAdd` accesses this performance drop.

# Benchmark Results

In my benchmark, `input`, `index`, and `src` tensors have the shape of `[n, d]`.
`half_pr` and `fp32_pr` columns are the results of `fastAtomicAdd` kernel and `half_1.9.0` and `fp32_1.9.0` are the results of `gpuAtomicAdd` kernl. By comparing `half/fp32_pr` with `half/fp32_1.9.0`, I inferred that the FP16 performance drop is mitigated thanks to `fastAtomicAdd` even though still I'm seeing that FP16 kernel is sometimes slower than FP32 kernel.

`index` tensor is prepared as follows:
1. sample a tensor from integer uniform distribution whose size is `[1, d]` if dim of 0 and `[n, 1]` otherwise.
2. Then `expand_as(src)` is called on the sampled tensor
3. Finally, call `contiguous` if `broadcast == False` (the current `torch.scatter_add` is expected to be much slower than FP32 kernel when `boradcast == False` in following 4 tables).

## Broadcast: False
As written above, `index` tensor is explicitly broadcasted. `contiguous` is not called on `index` tensor.
So, the current FP16 kernel is expected to be much slower.

### Dim: 0

|      n |    d |   half/fp32_pr |   half/fp32_1.9.0 |     half_pr |     fp32_pr |   half_1.9.0 |   fp32_1.9.0 |
|-------:|-----:|---------------:|------------------:|------------:|------------:|-------------:|-------------:|
|    128 |   64 |       1.76267  |           3.22732 | 2.47192e-05 | 1.40238e-05 |  4.529e-05   |  1.40333e-05 |
|    128 |  139 |       1.69238  |           3.43899 | 2.51055e-05 | 1.48344e-05 |  4.74405e-05 |  1.37949e-05 |
|    128 | 2048 |       2.76676  |          16.2946  | 4.07267e-05 | 1.472e-05   |  0.000226569 |  1.39046e-05 |
|   1025 |   64 |       0.796432 |          25.5055  | 1.19209e-05 | 1.49679e-05 |  0.000354643 |  1.39046e-05 |
|   1025 |  139 |      16.5262   |          48.3141  | 0.000189757 | 1.14822e-05 |  0.000675473 |  1.39809e-05 |
|   1025 | 2048 |       2.80191  |          18.13    | 0.000215826 | 7.70283e-05 |  0.00149145  |  8.22639e-05 |
| 636576 |   64 |       0.832687 |           2.22949 | 0.00290289  | 0.00348617  |  0.00701689  |  0.00314731  |
| 636576 |  139 |       0.653549 |           1.14264 | 0.00435508  | 0.00666375  |  0.00746663  |  0.00653456  |

### Dim: 1

|      n |    d |   half/fp32_pr |   half/fp32_1.9.0 |     half_pr |     fp32_pr |   half_1.9.0 |   fp32_1.9.0 |
|-------:|-----:|---------------:|------------------:|------------:|------------:|-------------:|-------------:|
|    128 |   64 |       1.02458  |           2.53059 | 1.2126e-05  | 1.18351e-05 |  3.55005e-05 |  1.40285e-05 |
|    128 |  139 |       2.69701  |           3.32535 | 3.05176e-05 | 1.13153e-05 |  4.56667e-05 |  1.37329e-05 |
|    128 | 2048 |       0.859864 |          84.4759  | 1.50681e-05 | 1.75238e-05 |  0.00147711  |  1.74856e-05 |
|   1025 |   64 |       1.03551  |           3.3478  | 1.18208e-05 | 1.14155e-05 |  4.63581e-05 |  1.38474e-05 |
|   1025 |  139 |       3.27292  |           7.36438 | 3.72839e-05 | 1.13916e-05 |  0.000102539 |  1.39236e-05 |
|   1025 | 2048 |       0.845071 |          90.4612  | 9.07469e-05 | 0.000107384 |  0.00947941  |  0.00010479  |
| 636576 |   64 |       0.89044  |           8.25655 | 0.00168624  | 0.00189372  |  0.0156397   |  0.00189421  |
| 636576 |  139 |       1.28612  |          12.1468  | 0.00467188  | 0.00363255  |  0.0442828   |  0.00364564  |

## Broadcast: True
`contiguous` is called on explicitly broadcasted `index` tensor. So, index is contiguous. 

### Dim: 0

|      n |    d |   half/fp32_pr |   half/fp32_1.9.0 |     half_pr |     fp32_pr |   half_1.9.0 |   fp32_1.9.0 |
|-------:|-----:|---------------:|------------------:|------------:|------------:|-------------:|-------------:|
|    128 |   64 |       2.45968  |           3.68162 | 2.76327e-05 | 1.12343e-05 |  5.2824e-05  |  1.4348e-05  |
|    128 |  139 |       2.47458  |           3.68563 | 2.85435e-05 | 1.15347e-05 |  5.09834e-05 |  1.3833e-05  |
|    128 | 2048 |       3.58455  |          15.3973  | 5.26619e-05 | 1.46914e-05 |  0.000233035 |  1.51348e-05 |
|   1025 |   64 |      14.9526   |          28.9071  | 0.000173044 | 1.15728e-05 |  0.000434608 |  1.50347e-05 |
|   1025 |  139 |       1.01888  |          35.1498  | 1.31226e-05 | 1.28794e-05 |  0.000502486 |  1.42956e-05 |
|   1025 | 2048 |       9.32589  |          11.4646  | 0.000762649 | 8.17776e-05 |  0.000978713 |  8.53682e-05 |
| 636576 |   64 |       1.00502  |           2.41401 | 0.00322514  | 0.00320902  |  0.00660583  |  0.00273646  |
| 636576 |  139 |       0.764873 |           1.13628 | 0.00455863  | 0.00595998  |  0.00730052  |  0.00642495  |

### Dim: 1

|      n |    d |   half/fp32_pr |   half/fp32_1.9.0 |     half_pr |     fp32_pr |   half_1.9.0 |   fp32_1.9.0 |
|-------:|-----:|---------------:|------------------:|------------:|------------:|-------------:|-------------:|
|    128 |   64 |       1.03767  |           2.45095 | 1.18208e-05 | 1.13916e-05 |  3.43132e-05 |  1.39999e-05 |
|    128 |  139 |       2.58868  |           3.53828 | 2.96497e-05 | 1.14536e-05 |  4.87089e-05 |  1.37663e-05 |
|    128 | 2048 |       0.927128 |         101.681   | 1.44386e-05 | 1.55735e-05 |  0.00164849  |  1.62125e-05 |
|   1025 |   64 |       1.03851  |           3.30365 | 1.19591e-05 | 1.15156e-05 |  4.529e-05   |  1.37091e-05 |
|   1025 |  139 |       3.00958  |           7.15997 | 3.44419e-05 | 1.14441e-05 |  9.94539e-05 |  1.38903e-05 |
|   1025 | 2048 |       0.849038 |          87.8004  | 8.77762e-05 | 0.000103383 |  0.00956022  |  0.000108886 |
| 636576 |   64 |       0.907441 |           9.07507 | 0.00161881  | 0.00178393  |  0.0162693   |  0.00179275  |
| 636576 |  139 |       1.32399  |          12.8253  | 0.00452948  | 0.00342109  |  0.0439241   |  0.0034248   |


cc: @ptrblck @mcarilli @ngimel ",pytorch
61497,rohan-varma,pr,2021-07-10T00:36:00Z,Revert [DDP] Support not all outputs used in loss calculation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61497**
* #61401
* #61399

Reverts [DDP] Support not all outputs used in loss calculation

Differential Revision: [D29642892](https://our.internmc.facebook.com/intern/diff/D29642892/)",pytorch
61511,tillahoffmann,pr,2021-07-10T20:49:38Z,Poisson zero rate,"This PR fixes #53485 by allowing zero rates for the Poisson distribution. This implementation is consistent with `scipy.stats.poisson` which admits zero rates. In addition to addressing the aforementioned issue, this PR makes two supporting changes:

1. add a `nonnegative` constraint to enforce non-negative rates for the Poisson distribution.
2. adjust the evaluation of the gradient of `xlogy` such that it is well defined for `x == 0 and y == 0`.",pytorch
61546,rohan-varma,pr,2021-07-12T18:04:10Z,[BE] Fix flaky test_ddp_model_diff_across_ranks test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61546**

Closes https://github.com/pytorch/pytorch/issues/60661
Fixes this flaky test by using blocking wait instead of async error handling, and performs a gloo-based barrier with higher timeout at the end of test which avoids issues with Barrier.sync. This also allows us to remove this test from the `skip_return_code_checks` list.

Differential Revision: [D29663884](https://our.internmc.facebook.com/intern/diff/D29663884/)",pytorch
61570,supriyar,pr,2021-07-13T02:51:52Z,[quant] Add a new fused MovingAvg Obs + FakeQuant operator(CPU),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61921
* #61691
* #61589
* __->__ #61570

Summary:
Fused operator that computes moving average min/max values (in-place) of the input tensor and fake-quantizes it.
It expects the qmin/qmax values to reflect the range of the quantized tensor (instead of reduce_range)

Motivation for adding this operator is for performance reasons, since moving the computation from python to C++/CUDA can increase the performance of QAT.

Test Plan:
python test/test_quantization.py TestFusedObsFakeQuant

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29682762](https://our.internmc.facebook.com/intern/diff/D29682762)",pytorch
61578,lezcano,pr,2021-07-13T11:33:14Z,[Docs] Bundle of errata and small corrections / improvements for torch.linalg docs,"This PR bundles a number of errata detected in the linalg docs over the last few weeks.

- Simpler Cholesky deprecation rule
- Remove repeated consecutive words
- Correct cond with rcond in lstsq
- Correct examples of lstsq
- More concise examples
- Use the names of the inputs / outputs in the variables of the examples",pytorch
61589,supriyar,pr,2021-07-13T18:09:09Z,[quant] Add a new fused MovingAvg Obs + FakeQuant operator (GPU),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61921
* #61691
* __->__ #61589
* #61570

Summary:
Custom GPU implementation that does the observer + calculate qparams calculation on GPU.
It calls the aten fake_quant_per_tensor/channel functions to perform the fake quant step.

Test Plan:
python test/test_quantization.py TestFusedObsFakeQuant

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29682761](https://our.internmc.facebook.com/intern/diff/D29682761)",pytorch
61612,eqy,pr,2021-07-13T23:20:53Z,[bc-breaking] Dispatch index_put with boolean mask argument to masked_fill,"#57515

Based on @ngimel 's branch, with a few tweaks to determine when to copy value tensors to device memory/additional tests.
bc-breaking note: Previously, if in `x[index]=value` `value` was a 0-d tensor with device different from `x`'s device, it resulted in a RuntimeError. Now this case is handled by copying `value` to the correct device. ",pytorch
61618,r-barnes,pr,2021-07-14T00:21:17Z,Fix some sign comparisons,"Differential Revision: D29688193

",pytorch
61637,rohan-varma,pr,2021-07-14T07:16:29Z,[DDP] Add API to get model parameters in hook,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61637**

To support running optimizer as a communication hook, add API to
retrieve the model parameters.

The API returns a `List[tensor]` that contains model parameters for the bucket. It can be used as follows:
```
per_param_grad_tensors = bucket.get_per_parameter_tensors()
model_params_for_bucket = bucket.get_model_params_for_bucket()
# Now gradient per_param_grad_tensors[i] is the gradient tensor for model_params_for_bucket[i].
```

This provides a way for comm. hook developer to retrieve model parameters within a hook. In the next diffs, we will use this to run optimizer as a DDP comm. hook.

Differential Revision: [D29691418](https://our.internmc.facebook.com/intern/diff/D29691418/)",pytorch
61640,rohan-varma,pr,2021-07-14T08:11:02Z,[DDP] Enhance register_comm_hook docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61640**
* #61637

1) Specify return type more clearly, 2) Misc fixes

Differential Revision: [D29692120](https://our.internmc.facebook.com/intern/diff/D29692120/)",pytorch
61663,r-barnes,pr,2021-07-14T16:55:46Z,Fix some sign comparisons and a loop,"Differential Revision: D29696766

",pytorch
61666,rohan-varma,pr,2021-07-14T18:34:28Z,[DDP] fix test_ddp_inference,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61666**

Closes https://github.com/pytorch/pytorch/issues/61481. Fixes this
test by removing section that uses only torch.no_grad() and doesn't call
model.eval(). For SyncBN, need to call model.eval() otherwise SyncBN will
assume it is in training mode, which does collective calls in the forward pass
and does not work for inference.

Differential Revision: [D29699444](https://our.internmc.facebook.com/intern/diff/D29699444/)",pytorch
61677,rohan-varma,pr,2021-07-14T21:13:56Z,[DDP] Enhance comm hook docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61677**

Specify return type more clearly, 2) Misc fixes

Differential Revision: [D29701384](https://our.internmc.facebook.com/intern/diff/D29701384/)",pytorch
61678,rohan-varma,pr,2021-07-14T21:14:05Z,[DDP] Implement a hook which performs FunctionalSGD step.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61678

This diff makes the following changes:
- Add `step_param` method to `_FunctionalSGD` class which is written similar to `step` but for a single param
- Implement a communication hook wrapper that runs a given comm. hook and then applies functional SGD step
- Verifies that this is equal to regular allreduce + SGD optimizer

Differential Revision: [D29701447](https://our.internmc.facebook.com/intern/diff/D29701447/)",pytorch
61691,supriyar,pr,2021-07-15T02:54:51Z,[quant] Create FusedMovingAvgObsFakeQuantize for QAT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #61921
* __->__ #61691
* #61589
* #61570

Summary:
Create a new module for QAT that does a Fused MovingAvgMinMaxObserver and FakeQuantize operation
The module currently only supports per-tensor quantization (affine/symmetric). Follow-up PR will add support for per-channel

Results on running QAT with MobileNetV2 (Obs enabled/fake_quant enabled)
Original FQ module
PyTorchObserver {""type"": ""_"", ""metric"": ""qnnpack_fp_latency_ms"", ""unit"": ""ms"", ""value"": ""242.80261993408203""}
PyTorchObserver {""type"": ""_"", ""metric"": ""qnnpack_qat0_latency_ms"", ""unit"": ""ms"", ""value"": ""505.7964324951172""}
PyTorchObserver {""type"": ""_"", ""metric"": ""fbgemm_fp_latency_ms"", ""unit"": ""ms"", ""value"": ""235.80145835876465""}
PyTorchObserver {""type"": ""_"", ""metric"": ""fbgemm_qat0_latency_ms"", ""unit"": ""ms"", ""value"": ""543.8144207000732""}

Fused FakeQuant module (~50% improvement in latency)
PyTorchObserver {""type"": ""_"", ""metric"": ""qnnpack_fp_latency_ms"", ""unit"": ""ms"", ""value"": ""232.1624755859375""}
PyTorchObserver {""type"": ""_"", ""metric"": ""qnnpack_qat0_latency_ms"", ""unit"": ""ms"", ""value"": ""263.8866901397705""}
PyTorchObserver {""type"": ""_"", ""metric"": ""fbgemm_fp_latency_ms"", ""unit"": ""ms"", ""value"": ""236.9832992553711""}
PyTorchObserver {""type"": ""_"", ""metric"": ""fbgemm_qat0_latency_ms"", ""unit"": ""ms"", ""value"": ""292.1590805053711""}

Individual module benchmark result (>5x improvement in latency)
===> Baseline FakeQuantize module
```
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
              aten::fake_quantize_per_tensor_affine         0.77%       1.210ms         4.92%       7.730ms     154.596us     718.528us         0.45%       9.543ms     190.862us            50
    aten::fake_quantize_per_tensor_affine_cachemask         2.41%       3.792ms         4.15%       6.520ms     130.402us       8.825ms         5.58%       8.825ms     176.492us            50
                                     aten::_aminmax         3.25%       5.105ms         4.43%       6.955ms     139.102us       8.193ms         5.18%       8.193ms     163.868us            50
                                   aten::zeros_like         1.87%       2.939ms         6.95%      10.922ms     109.218us       5.992ms         3.79%      10.844ms     108.442us           100
                                        aten::zeros         0.97%       1.527ms         3.11%       4.885ms      97.702us       2.383ms         1.51%       4.800ms      96.010us            50
                                         aten::rsub         1.34%       2.106ms         2.94%       4.614ms      92.277us       2.063ms         1.30%       4.559ms      91.173us            50
                                        aten::clamp         2.79%       4.381ms         5.42%       8.519ms      85.190us       5.385ms         3.41%       8.438ms      84.381us           100
                                           aten::eq        11.70%      18.384ms        21.31%      33.479ms      83.280us      22.465ms        14.21%      33.310ms      82.861us           402
                                         aten::ones         1.05%       1.656ms         2.57%       4.038ms      80.751us       2.494ms         1.58%       3.951ms      79.028us            50
                                           aten::le         2.52%       3.955ms         4.84%       7.607ms      76.071us       4.998ms         3.16%       7.702ms      77.016us           100
                                          aten::min         0.69%       1.087ms         2.32%       3.641ms      72.827us       1.017ms         0.64%       3.603ms      72.055us            50
                                          aten::max         1.40%       2.195ms         4.62%       7.260ms      72.597us       2.008ms         1.27%       7.140ms      71.404us           100
                                   aten::is_nonzero         2.68%       4.207ms        11.35%      17.829ms      71.033us       4.062ms         2.57%      17.225ms      68.625us           251
                                       aten::detach         1.17%       1.831ms         3.65%       5.736ms      57.360us       1.680ms         1.06%       5.634ms      56.340us           100
                                          aten::mul         3.36%       5.278ms         3.36%       5.278ms      53.862us       5.215ms         3.30%       5.215ms      53.216us            98
                                          aten::div         3.42%       5.376ms         3.42%       5.376ms      53.759us       5.320ms         3.36%       5.320ms      53.196us           100
                                          aten::sub         6.79%      10.672ms         6.79%      10.672ms      53.901us      10.504ms         6.64%      10.504ms      53.050us           198
                                         aten::item         4.06%       6.380ms        12.02%      18.883ms      53.798us       6.127ms         3.87%      18.322ms      52.198us           351
                                          aten::add         3.28%       5.147ms         3.28%       5.147ms      52.518us       5.113ms         3.23%       5.113ms      52.171us            98
                                      aten::minimum         1.63%       2.555ms         1.63%       2.555ms      51.092us       2.585ms         1.64%       2.585ms      51.708us            50
                                      aten::maximum         3.22%       5.065ms         3.22%       5.065ms      50.646us       5.133ms         3.25%       5.133ms      51.329us           100
                                        aten::round         1.61%       2.529ms         1.61%       2.529ms      50.578us       2.528ms         1.60%       2.528ms      50.552us            50
                                        aten::zero_         1.99%       3.125ms         4.72%       7.422ms      49.481us       2.835ms         1.79%       7.269ms      48.462us           150
                                        aten::copy_         6.62%      10.394ms         6.62%      10.394ms      41.576us      10.252ms         6.48%      10.252ms      41.010us           250
                                             detach         2.49%       3.905ms         2.49%       3.905ms      39.049us       3.954ms         2.50%       3.954ms      39.539us           100
                                       aten::select         2.01%       3.154ms         2.47%       3.876ms      38.759us       3.866ms         2.44%       3.866ms      38.658us           100
                          aten::_local_scalar_dense         7.96%      12.503ms         7.96%      12.503ms      35.621us      12.195ms         7.71%      12.195ms      34.743us           351
                                           aten::to         2.31%       3.625ms         4.16%       6.530ms      32.650us       4.320ms         2.73%       6.270ms      31.348us           200
                                        aten::fill_         3.70%       5.808ms         3.70%       5.808ms      29.039us       5.892ms         3.73%       5.892ms      29.459us           200
                                   aten::as_strided         0.79%       1.244ms         0.79%       1.244ms       6.221us       0.000us         0.00%       0.000us       0.000us           200
                                        aten::empty         3.55%       5.579ms         3.55%       5.579ms      11.137us       0.000us         0.00%       0.000us       0.000us           501
                                      aten::resize_         2.36%       3.712ms         2.36%       3.712ms      12.332us       0.000us         0.00%       0.000us       0.000us           301
                                   aten::empty_like         1.45%       2.284ms         3.68%       5.776ms      28.878us       0.000us         0.00%       0.000us       0.000us           200
                                aten::empty_strided         2.80%       4.398ms         2.80%       4.398ms      17.592us       0.000us         0.00%       0.000us       0.000us           250
---------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 157.108ms
Self CUDA time total: 158.122ms
```

===> FusedFakeQuant
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                   fb::fused_fake_quant        23.42%       6.408ms       100.00%      27.361ms     547.215us       7.887ms        27.20%      28.996ms     579.925us            50
                  aten::fake_quantize_per_tensor_affine         4.25%       1.162ms        27.65%       7.565ms     151.298us     686.176us         2.37%      10.217ms     204.336us            50
aten::_fake_quantize_per_tensor_affine_cachemask_ten...        14.11%       3.860ms        23.40%       6.403ms     128.068us       9.531ms        32.87%       9.531ms     190.612us            50
                                         aten::_aminmax        20.57%       5.628ms        27.47%       7.515ms     150.305us       8.218ms        28.34%       8.218ms     164.367us            50
                                             aten::item         3.65%     999.522us        10.27%       2.810ms      56.202us     931.904us         3.21%       2.674ms      53.481us            50
                              aten::_local_scalar_dense         6.62%       1.811ms         6.62%       1.811ms      36.212us       1.742ms         6.01%       1.742ms      34.843us            50
                                            aten::empty        10.85%       2.969ms        10.85%       2.969ms      14.843us       0.000us         0.00%       0.000us       0.000us           200
                                       aten::as_strided         1.92%     524.365us         1.92%     524.365us       5.244us       0.000us         0.00%       0.000us       0.000us           100
                                       aten::empty_like         6.48%       1.774ms        14.62%       4.000ms      26.670us       0.000us         0.00%       0.000us       0.000us           150
                                    aten::empty_strided         8.14%       2.226ms         8.14%       2.226ms      14.842us       0.000us         0.00%       0.000us       0.000us           150
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 27.361ms
Self CUDA time total: 28.996ms
```

Test Plan:
python test/test_quantization.py TestFusedObsFakeQuantModule

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29706889](https://our.internmc.facebook.com/intern/diff/D29706889)",pytorch
61722,r-barnes,pr,2021-07-15T18:48:28Z,Upgrade CPUFallback for loops,"Differential Revision: D29715862

",pytorch
61735,r-barnes,pr,2021-07-15T23:28:49Z,Modernize to default constructor and nullptr in torch,"Differential Revision: D29716659

",pytorch
61736,r-barnes,pr,2021-07-15T23:30:33Z,Modernize fix deprecated header,"Differential Revision: D29716965

",pytorch
61737,r-barnes,pr,2021-07-15T23:30:36Z,Modernize some loops in torch,"Differential Revision: D29716813

",pytorch
61739,r-barnes,pr,2021-07-15T23:32:18Z,Modernize use make_unique,"Differential Revision: D29717133

",pytorch
61740,r-barnes,pr,2021-07-15T23:32:50Z,Modernize avoid a C array,"Differential Revision: D29717118

",pytorch
61741,r-barnes,pr,2021-07-15T23:34:00Z,Modernize make pointers,"Differential Revision: D29717385

",pytorch
61742,r-barnes,pr,2021-07-15T23:34:21Z,Modernize replace,"Differential Revision: D29717433

",pytorch
61744,r-barnes,pr,2021-07-15T23:53:08Z,Modernize override,"Differential Revision: D29717320

",pytorch
61745,r-barnes,pr,2021-07-15T23:53:12Z,Fix missing braces,"Differential Revision: D29717538

",pytorch
61753,rohan-varma,pr,2021-07-16T01:58:53Z,[Reland][DDP] Support not all outputs used in loss calculation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61753

Reland of https://github.com/pytorch/pytorch/pull/57081.
Main difference is that the former diff moved `prepare_for_backward` check into `DDPSink` backward, but that resulted in issues due to potential autograd engine races. The original diff moved `prepare_for_backward` into `DDPSink` as part of a long-term plan to always call it within `DDPSink`.

In particular this doesn't work because `prepare_for_backward` sets `expect_autograd_hooks=true` which enables autograd hooks to fire, but there were several use cases internally where autograd hooks were called before DDPSink called `prepare_for_backward`, resulting in errors/regression.

We instead keep the call to `prepare_for_backward` in the forward pass, but still run outputs through `DDPSink` when find_unused_parameters=True. As a result, outputs that are not used when computing loss have `None` gradients and we don't touch them if they are globally `None`. Note that the hooks still fire with a undefined gradient which is how we avoid the Reducer erroring out with the message that some hooks did not fire.

Added the unittests that were part of the reverted diff.

Differential Revision: [D29726179](https://our.internmc.facebook.com/intern/diff/D29726179/)",pytorch
61756,rohan-varma,pr,2021-07-16T03:54:16Z,Parity tests for functional optimizer step_param,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62079
* #62078
* __->__ #61756

DDP will support running optimizer as communication hook with
optimizers that support a per-parameter/gradient step function `step_param`.
Add parity tests as we implement more optimizers that support step_param to
ensure parity with regular optimizers.

Differential Revision: [D29727549](https://our.internmc.facebook.com/intern/diff/D29727549/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29727549/)!",pytorch
61799,eqy,pr,2021-07-17T00:52:23Z,Enable `bfloat16` in NCCL,"NCCL upstream 2.10.1-3 recently enabled `bfloat16` support. Trying it out initially to see if anything breaks.

CC @ptrblck ",pytorch
61833,lezcano,pr,2021-07-19T09:55:45Z,[docs] Correct torch.permute,"Noted while reviewing https://github.com/pytorch/pytorch/issues/61830
",pytorch
61849,r-barnes,pr,2021-07-19T16:27:26Z,Fix some sign comparisons,"Differential Revision: D29736180

",pytorch
61871,rohan-varma,pr,2021-07-19T21:16:13Z,[DDP] Log if graph is static at end of training,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61871

When set_static_graph=False, the only type of dynamism we really
support in DDP is dynamic set of unused parameters which must be explicitly
enabled with find_unused_parameters=True. Although, some workflows have static
set of unused parameters, would be good to detect and add this to logging to
identify workflows that are candidates for static graph optimization.

Differential Revision: [D29773962](https://our.internmc.facebook.com/intern/diff/D29773962/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D29773962/)!",pytorch
61889,ngimel,pr,2021-07-20T04:44:45Z,wrap cudaStreamSynchronize calls,This is a first step towards creating context manager that errors out on synchronizing calls. ,pytorch
61921,supriyar,pr,2021-07-20T17:25:58Z,[quant] Remove calls to .item() for fake_quant_on,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61921
* #61691
* #61589
* #61570

Summary:
For GPU training, the fake_quant_on tensors are present on the GPU and the .item() calls incur a GPU->CPU copy to access the tensor element.
This call can prove expensive and hurt the performance during training as the `item()` and `local_scalar_dense()` calls take up 11% of the total CPU execution time.
The solution here is to access the tensor on the GPU without a copy.

Individual op benchmarks show a 33% speedup just by removing the `.item()` calls

Profiler Before
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::fused_moving_avg_obs_fake_quant         5.61%       1.538ms       100.00%      27.421ms     548.425us     978.208us         3.42%      28.575ms     571.501us            50
                  aten::_fused_moving_avg_obs_fq_helper        27.63%       7.576ms        94.39%      25.883ms     517.668us       6.536ms        22.87%      27.597ms     551.937us            50
aten::_fake_quantize_per_tensor_affine_cachemask_ten...        11.07%       3.037ms        21.54%       5.905ms     118.103us       9.549ms        33.42%       9.549ms     190.978us            50
                                         aten::_aminmax        19.39%       5.317ms        27.44%       7.524ms     150.484us       8.683ms        30.38%       8.683ms     173.651us            50
                                             aten::item         4.49%       1.232ms        11.12%       3.051ms      61.011us       1.058ms         3.70%       2.829ms      56.579us            50
                              aten::_local_scalar_dense         6.63%       1.818ms         6.63%       1.818ms      36.363us       1.771ms         6.20%       1.771ms      35.419us            50
                                            aten::empty         5.76%       1.579ms         5.76%       1.579ms      15.792us       0.000us         0.00%       0.000us       0.000us           100
                                       aten::as_strided         2.29%     628.399us         2.29%     628.399us       6.284us       0.000us         0.00%       0.000us       0.000us           100
                                       aten::empty_like         7.56%       2.073ms        17.13%       4.696ms      31.310us       0.000us         0.00%       0.000us       0.000us           150
                                    aten::empty_strided         9.57%       2.623ms         9.57%       2.623ms      17.489us       0.000us         0.00%       0.000us       0.000us           150
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 27.421ms
Self CUDA time total: 28.575ms
```
After
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                  aten::fused_moving_avg_obs_fake_quant         6.59%       1.240ms       100.00%      18.820ms     376.396us     490.272us         2.36%      20.745ms     414.901us            50
                  aten::_fused_moving_avg_obs_fq_helper        26.12%       4.916ms        93.41%      17.580ms     351.597us       2.033ms         9.80%      20.255ms     405.096us            50
aten::_fake_quantize_per_tensor_affine_cachemask_ten...        14.55%       2.738ms        31.09%       5.850ms     117.005us       9.968ms        48.05%       9.968ms     199.363us            50
                                         aten::_aminmax        25.28%       4.758ms        36.21%       6.814ms     136.278us       8.253ms        39.79%       8.253ms     165.069us            50
                                            aten::empty         7.94%       1.494ms         7.94%       1.494ms      14.944us       0.000us         0.00%       0.000us       0.000us           100
                                       aten::as_strided         2.99%     561.785us         2.99%     561.785us       5.618us       0.000us         0.00%       0.000us       0.000us           100
                                       aten::empty_like         8.36%       1.573ms        16.53%       3.112ms      31.118us       0.000us         0.00%       0.000us       0.000us           100
                                    aten::empty_strided         8.17%       1.538ms         8.17%       1.538ms      15.384us       0.000us         0.00%       0.000us       0.000us           100
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 18.820ms
Self CUDA time total: 20.745ms
```
Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29796533](https://our.internmc.facebook.com/intern/diff/D29796533)",pytorch
61961,rohan-varma,pr,2021-07-21T15:57:32Z,"[WIP, not for review] Remove copy_bucket_to_grad when optimizer runs as hook","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61961
* #61756
* #61678

Per title

Differential Revision: [D29809942](https://our.internmc.facebook.com/intern/diff/D29809942/)",pytorch
61986,supriyar,pr,2021-07-21T20:55:23Z,[quant] Add from_blob_quantized_per_tensor API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62049
* __->__ #61986

Summary:
Adds a new function that accepts qint data blobs as input and creates a quantized tensor using the pre-allocated data and the provided scale and zero_point inputs
Addresses issue https://github.com/pytorch/pytorch/issues/61777

Test Plan:

./build/bin/quantized_test --gtest_filter='TestQTensor.FromBlobQuantizedPerTensor'

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29831135](https://our.internmc.facebook.com/intern/diff/D29831135)",pytorch
61992,rohan-varma,pr,2021-07-21T21:59:24Z,[DDP] Run test_ddp_new_tensor_in_fwd with static graph,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #61992

This test previously was not enabled for static graph but to ensure
this feature is supported with DDPSink, enable it for static graph which
currently passes outputs to DDPSink.

Differential Revision: [D29830887](https://our.internmc.facebook.com/intern/diff/D29830887/)",pytorch
62045,r-barnes,pr,2021-07-22T17:59:09Z,irange-ify,"Summary:
Auto-generated with D28874212

Per D25969148, the number of modernizable loops is fairly small

Differential Revision: D29834756

",pytorch
62049,supriyar,pr,2021-07-22T19:01:47Z,[quant] Add from_blob_quantized_per_channel API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62049
* #61986

Summary:
Adds a new function that accepts qint data blobs as input and creates a per-channel quantized tensor using the pre-allocated data and the provided scale and zero_point inputs
Addresses issue #61777

Test Plan:
./build/bin/quantized_test --gtest_filter='TestQTensor.FromBlobQuantizedPerChannel'

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29854136](https://our.internmc.facebook.com/intern/diff/D29854136)",pytorch
62078,rohan-varma,pr,2021-07-23T05:49:46Z,[Functional Optim] Test kwargs parity for SGD,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62079
* __->__ #62078
* #61756

Ensure that kwarg arguments such as momentum and weight decay maintain
parity between optimizer.step and step_param.

Differential Revision: [D29837942](https://our.internmc.facebook.com/intern/diff/D29837942/)",pytorch
62079,rohan-varma,pr,2021-07-23T05:49:55Z,[DDP/Functional Optim] Support kwarg arguments,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62079
* #62078
* #61756

Adds support for kwarg arguments into functional optimizer running as
hook.

Differential Revision: [D29838127](https://our.internmc.facebook.com/intern/diff/D29838127/)",pytorch
62086,lezcano,pr,2021-07-23T09:30:54Z,Create linalg and parametrizations codeowners,"Added myself @nikitaved  and @IvanYashchuk 

",pytorch
62089,lezcano,pr,2021-07-23T12:49:23Z,Implements the orthogonal parametrization,"Implements an orthogonal / unitary parametrisation.

It does passes the tests and I have trained a couple models with this implementation, so I believe it should be somewhat correct. Now, the implementation is very subtle. I'm tagging @nikitaved  and @IvanYashchuk as reviewers in case they have comments / they see some room for optimisation of the code, in particular of the `forward` function.

Fixes https://github.com/pytorch/pytorch/issues/42243
",pytorch
62092,ngimel,pr,2021-07-23T16:00:15Z,enable warnings on cuda synchronization ,"This creates `torch.cuda.set_warn_on_synchronization()` function that would warn or error when synchronizing operation is performed. We could wrap it in a context manager for ease of use, but it would be a lie, because it sets global, and not thread-local state. Since it's intended for debugging, maybe that's ok though.  
As all `torch.cuda.*` functions, it's going through CPython, not pybind, so the argument is converted to long before being passed to c10 function. I'll make python argument a python enum class, but without pybind it'll still have to go thourgh long conversion. 

For a test script
```
import torch
torch.cuda.set_sync_debug_mode(1)
x=torch.randn(10, device=""cuda"")
x.nonzero()
y=torch.randn((), device=""cuda"")

if y:
    print(""something"")
torch.multinomial(x.abs(), 10, replacement=False)
torch.randperm(20000, device=""cuda"")
ind = torch.randint(10, (3,), device=""cuda"")
mask = torch.randint(2, (10,), device=""cuda"", dtype=torch.bool)
val = torch.randn((), device=""cuda"")
x[mask]=1.
x[mask] = val
torch.cuda.synchronize()
```
the output is 
```
/../playground/sync_warn_test.py:4: UserWarning: called a synchronizing operation (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:145.)
  x.nonzero()
/../playground/sync_warn_test.py:7: UserWarning: called a synchronizing operation (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:145.)
  if y:
something
/../playground/sync_warn_test.py:9: UserWarning: called a synchronizing operation (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:145.)
  torch.multinomial(x.abs(), 10, replacement=False)
/../playground/sync_warn_test.py:15: UserWarning: called a synchronizing operation (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:145.)
  x[mask] = val
```
",pytorch
62100,r-barnes,pr,2021-07-23T17:25:02Z,Add some missing cuda guards,"Test Plan: Sandcastle

Differential Revision: D29880330

",pytorch
62108,rohan-varma,pr,2021-07-23T19:29:35Z,[Not for review/repro] Repro for https://github.com/pytorch/pytorch/issues/61982,"Summary:
Repro for https://github.com/pytorch/pytorch/issues/61982. Training
with static graph does not work if in-place operation is done on view tensor
that is returned by DDPSink.

We can fix this with `clone` of the tensors in DDPSink forward pass, but that
might incur a nontrivial perf hit. On the other hand, user can also clone the
output tensor in their training loop to fix the issue, or not use inplace
modification.

Test Plan: CI

Differential Revision: D29884341

",pytorch
62112,r-barnes,pr,2021-07-23T19:47:51Z,irange-ify 3,"Differential Revision: D29879513

",pytorch
62113,r-barnes,pr,2021-07-23T19:47:56Z,irange-ify 2,"Differential Revision: D29879507

",pytorch
62114,r-barnes,pr,2021-07-23T19:48:51Z,irange-ify 5,"Differential Revision: D29879534

",pytorch
62115,r-barnes,pr,2021-07-23T19:48:56Z,irange-ify 6,"Differential Revision: D29879576

",pytorch
62116,r-barnes,pr,2021-07-23T19:49:04Z,irange-ify 4,"Differential Revision: D29879522

",pytorch
62117,r-barnes,pr,2021-07-23T19:49:06Z,irange-ify 7,"Differential Revision: D29879640

",pytorch
62118,r-barnes,pr,2021-07-23T19:49:11Z,irange-ify 9,"Differential Revision: D29879670

",pytorch
62119,r-barnes,pr,2021-07-23T19:50:13Z,irange-ify 14,"Differential Revision: D29879749

",pytorch
62120,r-barnes,pr,2021-07-23T19:50:24Z,irange-ify 12,"Differential Revision: D29879713

",pytorch
62121,r-barnes,pr,2021-07-23T19:50:44Z,irange-ify 11,"Differential Revision: D29879701

",pytorch
62122,r-barnes,pr,2021-07-23T19:51:08Z,irange-ify 10,"Differential Revision: D29879694

",pytorch
62123,r-barnes,pr,2021-07-23T19:51:35Z,irange-ify 15,"Differential Revision: D29879765

",pytorch
62167,crcrpar,pr,2021-07-25T22:42:28Z,[Foreach] support implicit broadcasting in slow path,"This PR has foreach functions support implicit broadcasting via slow path.

rel: https://github.com/pytorch/pytorch/issues/58833

cc: @ptrblck  @ngimel ",pytorch
62175,cloudhan,pr,2021-07-26T03:44:12Z,Enable CUPTI for kineto by default on windows.,"It fix nothing.

For tracking this PR, please refers to https://github.com/pytorch/kineto/issues/356
",pytorch
62177,rohan-varma,pr,2021-07-26T07:09:20Z,[Reland] [DDP] Implement a hook which performs FunctionalSGD step.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62079
* #62078
* #61756
* __->__ #62177

Reland of https://github.com/pytorch/pytorch/pull/61678
Fix CI failure by gating including torchvision model on whether torchvision is available or not.

Differential Revision: [D29904101](https://our.internmc.facebook.com/intern/diff/D29904101/)",pytorch
62181,kashif,pr,2021-07-26T11:53:04Z,[NNAPI] added elu and hardswish operators,Added `ELU` and `HARD_SWISH` NNAPI operators,pytorch
62193,r-barnes,pr,2021-07-26T15:41:05Z,irange-ify 1,"Differential Revision: D29879504

",pytorch
62194,r-barnes,pr,2021-07-26T16:15:39Z,Fix sign comparison,"Differential Revision: D29885396

",pytorch
62195,r-barnes,pr,2021-07-26T16:24:21Z,irange-ify 8b,"Differential Revision: D29887946

",pytorch
62197,supriyar,pr,2021-07-26T16:53:25Z,[quant][fix] Update quantization c++ tests to not run if CPU_STATIC_DISPATCH is specified,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62197

Summary:
For build configs with ATEN_CPU_STATIC_DISPATCH defined, quantization tests will fail since they
require QuantizedCPU dispatch to be enabled.
This will fix some internal test failures like https://www.internalfb.com/intern/test/844424941811803?ref_report_id=0 which are run under the `caffe2_aten_cpu_inference` project

Test Plan:
buck test @mode/dev //caffe2/aten:quantized_test

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D29912742](https://our.internmc.facebook.com/intern/diff/D29912742)",pytorch
62209,rohan-varma,pr,2021-07-26T19:00:54Z,[DDP] Log unused param names under DETAIL debug mode.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62209

When `TORCH_DISTRIBUTED_DEBUG=DETAIL` is set, log names and indices of unused parameters when searching for them.

Motivation is that we have seen a couple of issues occasionally when there are errors related to parameter possibly being marked as unused when it shouldn't, this can help narrow down the root cause by explicitly logging param names that are marked as unused.

Differential Revision: [D29916085](https://our.internmc.facebook.com/intern/diff/D29916085/)",pytorch
62229,rohan-varma,pr,2021-07-26T23:34:26Z,[DDP] Save bucket size limits,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62279
* #62232
* #62231
* #62230
* __->__ #62229
* #62209

First of a stack of diffs to save and log the bucket size limits to help debug/discover discrepancies and analyze impact of bucket size tuning

Differential Revision: [D29918629](https://our.internmc.facebook.com/intern/diff/D29918629/)",pytorch
62230,rohan-varma,pr,2021-07-26T23:34:33Z,[DDP][ez] Remove misleading comment,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62279
* #62232
* #62231
* __->__ #62230
* #62229
* #62209

We don't iterate over model replicas anymore.

Differential Revision: [D29918760](https://our.internmc.facebook.com/intern/diff/D29918760/)",pytorch
62231,rohan-varma,pr,2021-07-26T23:34:40Z,[DDP] Make compute_bucket_assignment_by_size return per bucket sizes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62279
* #62232
* __->__ #62231
* #62230
* #62229
* #62209

`compute_bucket_assignment_by_size` is responsible for setting per-bucket size limits, return this information from the function so that we are aware of size limits for each bucket.

This is currently not being consumed, but will be in the next diff when we log bucket size limits to DDP logging. This will help us run experiments under different bucket size configs and analyze the impact.

Differential Revision: [D29919056](https://our.internmc.facebook.com/intern/diff/D29919056/)",pytorch
62232,rohan-varma,pr,2021-07-26T23:34:47Z,[DDP] log bucket sizes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62232

Logs the bucket sizes in DDP logging so that we know which workflow ran with what bucket size config. Will be used to verify how changing bucket sizes in DDP affects perf.

Based on the test, we can see inconsistency where the ""first"" bucket size actually is (last before rebuild buckets, first after).

Differential Revision: [D29922299](https://our.internmc.facebook.com/intern/diff/D29922299/)",pytorch
62245,rohan-varma,pr,2021-07-27T04:12:38Z,[DDP] Small logging fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62245

When find_unused_parameters, buckets are not actually rebuilt.

Differential Revision: [D29928117](https://our.internmc.facebook.com/intern/diff/D29928117/)",pytorch
62279,rohan-varma,pr,2021-07-27T18:17:51Z,[wip] Move smallest bucket to end after rebuild buckets,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62279

Before rebuild buckets, `kDefaultFirstBucketBytes` is actually misleading because we reverse the parameter indices when initialize reducer so it is actually the size of the last bucket.

Currently rebuild buckets sets this to be the first bucket size, but seeing if keeping it as last can help perf. Added an env variable to toggle this on/off with the default to off to preserve the current behavior. This will allow for quicker experimentation.

Based on experiments, we can either make this the default if perf is improved, or document/improve the process of allowing user to tune this if we determine performance differences on different workloads.

Differential Revision: [D29927931](https://our.internmc.facebook.com/intern/diff/D29927931/)",pytorch
62345,supriyar,pr,2021-07-28T18:58:08Z,[quant] update per-channel observer min/max_val attribute names,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62345

Summary:
This PR updates the attribute names from min_vals to min_val. the motivation for this is to keep the attribute name consistent with per-tensor observers so that dependencies (like FusedMovingAvgObsFakeQuantize) don't need to differentiate between the two observer types to access the attributes.

It also adds some BC tests to make sure that observers saved earlier with min_vals/max_vals can be loaded depending on the state_dict version.
Note: Scriptability of the observers isn't fully supported yet, so we aren't testing for that in this PR.

Test Plan:
python test/test_quantization.py TestSerialization

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30003700](https://our.internmc.facebook.com/intern/diff/D30003700)",pytorch
62346,supriyar,pr,2021-07-28T18:58:14Z,[quant] Support PerChannel quantization in FusedMovingAvgObsFakeQuantize,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62346

Summary:
Update the operator code to resize the min/max tensors if per-channel quant is selected. We need to do this because by default the observer creates empty tensors for min/max and scale/zero_point values when per-channel quantization is enabled

Test Plan:
python test/test_quantization.py test_fused_mod_per_channel

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30003835](https://our.internmc.facebook.com/intern/diff/D30003835)",pytorch
62386,vfdev-5,pr,2021-07-29T08:05:45Z,Restored compatiblity with 1.8.0 for nearest interpolation special cases (#62237),"Fixes #62237

Decription:

- Readded missing code for nearest upsampling op for cases:
  - output size == input size
  - output size == 2 * input size

Context: my PR #54500 (in v1.9.0) has missing code that is present in v1.8.0 : https://github.com/pytorch/pytorch/blob/56b43f4fec1f76953f15a627694d4bba34588969/aten/src/ATen/native/cpu/UpSampleKernel.cpp#L18-L23

- Added tests

TODO:
- fix consistency between CPU and CUDA implementations for these cases
- update upsampling Nearest1d tests on CUDA


",pytorch
62422,r-barnes,pr,2021-07-29T17:16:13Z,irange-ify 8,"Differential Revision: D29879655

",pytorch
62476,r-barnes,pr,2021-07-30T15:19:28Z,irange-ify 13b,"Differential Revision: D30001445

",pytorch
62477,r-barnes,pr,2021-07-30T15:19:44Z,irange-ify 13d,"Differential Revision: D30001499

",pytorch
62478,r-barnes,pr,2021-07-30T15:20:20Z,irange-ify 4c,"Differential Revision: D30001704

",pytorch
62479,r-barnes,pr,2021-07-30T15:21:10Z,irange-ify 4e,"Differential Revision: D30001703

",pytorch
62480,r-barnes,pr,2021-07-30T15:24:45Z,irange-ify 4b,"Differential Revision: D30001702

",pytorch
62483,r-barnes,pr,2021-07-30T15:41:08Z,Fix sign comparison,"Differential Revision: D30015385

",pytorch
62484,r-barnes,pr,2021-07-30T15:54:41Z,irange-ify 12b,"Differential Revision: D30015528

",pytorch
62486,r-barnes,pr,2021-07-30T16:45:25Z,irange-ify 4d,"Differential Revision: D30001705

",pytorch
62505,r-barnes,pr,2021-07-30T21:25:37Z,irange-ify 8d,"Differential Revision: D29971891

",pytorch
62516,bryant1410,pr,2021-07-30T23:49:06Z,Save some memory in scatter,Also removes some redundant parenthesis for clarity.,pytorch
62611,rohan-varma,pr,2021-08-02T22:11:35Z,Enable step_param for Adam functional optimizer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62611

Enables optimizer overlap with backwards in DDP for Adam. Additional optimizers, especially Adagrad will be done in follow up diffs.

1. Implement `step_param` method based on `step` in _FunctionalAdam (perf permitting we can later dedupe `step` to call `step_param`
2. Modify tests to test all current functional optimizers.

Differential Revision: [D29891783](https://our.internmc.facebook.com/intern/diff/D29891783/)",pytorch
62620,rohan-varma,pr,2021-08-02T23:06:23Z,Enable ZeRO optimizer for windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62620
* #62611

moves zero optimizer to torch.distributed.zero namespace so that it doesn't conflict with distributed optimizers that require RRefs, so it is now enabled for windows. Issue is fully described in https://github.com/pytorch/pytorch/issues/62137.

In addition, keep backwards compatibility by still being able to use `from torch.distributed.optim import ZeroRedundancyOptimizer` as the previous import was.
Also adds running of the test internally.

Follow up task is to update docs/tutorials to use this new import path instead of the old one.

Differential Revision: [D30056292](https://our.internmc.facebook.com/intern/diff/D30056292/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30056292/)!",pytorch
62625,rohan-varma,pr,2021-08-02T23:53:13Z,[Reland][DDP] log bucket sizes,"Summary: reland of https://github.com/pytorch/pytorch/pull/62232 which ran into a land race.

Test Plan: ci

Differential Revision: D30058217

",pytorch
62646,crcrpar,pr,2021-08-03T13:16:18Z,[Foreach] Implement L1&L2 norm,"Implement L1 & L2 norm in fast path with the reference of [nvidia/apex](https://github.com/NVIDIA/apex/blob/master/csrc/multi_tensor_l2norm_kernel.cu).
When `ord` is neither 1 nor 2, then slow path is chosen.

Related: #58833 

cc @ptrblck @mcarilli @ngimel ",pytorch
62653,r-barnes,pr,2021-08-03T16:20:37Z,Are tensors on same device?,"Summary:
This consolidates checks determining whether tensors live on the same device into a single line using template parameter packs to unroll the check code.

The advantage of using the new checking syntax is that it makes it easy to use static analysis to determine both if the check is present and whether or not it is comprehensive. D30072495 includes a linter which performs this action.

Note that this is especially useful for PyTorch extensions which don't receive this check automatically from codegen.

Test Plan:
```
buck test //caffe2/torch/fb/sparsenn:gpu_test
buck test //caffe2/torch/fb/sparsenn:test
```

Differential Revision: D29924464

",pytorch
62702,supriyar,pr,2021-08-04T03:44:57Z,[quant] Add default_qat_config_v2 to use the fused observer+fake_quant module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62702
* #62863

Summary:
Expose the qconfig to the user to speed up training by leveraging the fused module.
The module currently supports per-tensor/per-channel moving avg observer and fake-quantize.

For details on perf benefits, refer to https://github.com/pytorch/pytorch/pull/61691

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30093719](https://our.internmc.facebook.com/intern/diff/D30093719)",pytorch
62710,cloudhan,pr,2021-08-04T09:23:07Z,Refactor RecordFunction to use CustomClass ,"Fixes #35026.

There used to be a try in #35596 but is long left behind and then abandoned. This PR redo it on relative new codebase.

And #62390 is related, this PR should remove additional `zeros` and `empty` op from profiling trace. Whether there will be a performance improvement is to be measured.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @ilia-cher @robieta @chaekit @gdankel @bitfort @ngimel @orionr @nbcsm @guotuofeng @guyang3532 @gaoteng-git",pytorch
62715,lezcano,pr,2021-08-04T12:16:08Z,Create linalg.matrix_exp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62734
* #62716
* __->__ #62715

Fixes https://github.com/pytorch/pytorch/issues/61648

Differential Revision: [D31641698](https://our.internmc.facebook.com/intern/diff/D31641698)

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
62716,lezcano,pr,2021-08-04T12:16:13Z,Implement forward AD for linalg.matrix_exp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62734
* __->__ #62716
* #62715



cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D31823231](https://our.internmc.facebook.com/intern/diff/D31823231)",pytorch
62734,lezcano,pr,2021-08-04T17:17:12Z,Better and more consistent error messages in torch.linalg,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62734

Following https://github.com/pytorch/pytorch/pull/62715#discussion_r682610788
- squareCheckInputs takes a string with the name of the function
- We reuse more functions when checking the inputs

The state of the errors in torch.linalg is far from great though. We
leave a more comprehensive clean-up for the future.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D31823230](https://our.internmc.facebook.com/intern/diff/D31823230)",pytorch
62748,rohan-varma,pr,2021-08-04T19:35:14Z,[DDP] Allow tuning of first bucket,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62770
* #62751
* __->__ #62748

Previously after buckets were rebuilt the first bucket size was always
defaulted to 1MB, this diff allows first bucket to be tuned like the rest of
the bucket sizes can.

Setting `dist._DEFAULT_FIRST_BUCKET_BYTES = 1` results in the following logs as
expected:
I0804 12:31:47.592272 246736 reducer.cpp:1694] 3 buckets rebuilt with size
limits: 1, 1048, 1048 bytes.

Differential Revision: [D30110041](https://our.internmc.facebook.com/intern/diff/D30110041/)",pytorch
62751,rohan-varma,pr,2021-08-04T19:58:30Z,[DDP] log gradient ready order and bucket indices,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62770
* __->__ #62751
* #62748

This will help us determine whether gradient ready order and bucket
indices are aligned amongst all the ranks. This should always be true for rank
0 as we determine rebuilt bucket order by the gradient ready order on rank 0,
but would be interested to see this on different workloads for other ranks

Differential Revision: [D30111833](https://our.internmc.facebook.com/intern/diff/D30111833/)",pytorch
62770,rohan-varma,pr,2021-08-04T21:56:36Z,[DDP] Add host-side time to CUDATimer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62770
* #62751
* #62748

Adding timing of forward, backward comp, backward comm, etc will help
detect desynchronization issues.

Differential Revision: [D30115585](https://our.internmc.facebook.com/intern/diff/D30115585/)",pytorch
62773,eqy,pr,2021-08-04T22:13:19Z,Preserve memory layout when aten batchnorm is used,"#62594 

CC @cpuhrsch",pytorch
62774,rohan-varma,pr,2021-08-04T22:28:09Z,[WIP] Gate DistributedOptimizers on RPC availability,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62774

Gates DistributedOptimizer which relies on RRef based on if RPC is available. This should enable ZeRo to work with Windows as Windows should not try to import the DIstributedOptimizer. If this works as expected we can enable the windows tests for functional/local sgd optimizers as well.

Also move `functional_optim_map` to a common place, as zero was previously reading it from DistributedOptimizer class, but we don't want zero to rely on DistributedOptimizer.

Differential Revision: [D30117838](https://our.internmc.facebook.com/intern/diff/D30117838/)",pytorch
62839,eqy,pr,2021-08-05T19:30:41Z,Check contiguous to dispatch to NHWC cuda template,follow up of #62773,pytorch
62863,supriyar,pr,2021-08-05T23:34:22Z,[quant] add reduce_range option to FusedMovingAvgFakeQuantize module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62702
* __->__ #62863

Summary:
To make this consistent with other observers, add reduce_range option that can be used to update quant_min/max.

Updated the observer code to move the calculation of qparams to the init function so that it is done once at observer initialization and not during calculate_qparams that gets called during convert.

Test Plan:
python test/test_quantization.py test_fused_mod_reduce_range

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30146602](https://our.internmc.facebook.com/intern/diff/D30146602)",pytorch
62912,ngimel,pr,2021-08-06T19:06:58Z,Allow broadcasting along non-reduction dimension for cosine similarity,"Checks introduced by #58559 are too strict and disable correctly working cases that people were relying on. 
",pytorch
62916,eqy,pr,2021-08-06T19:31:32Z,change nccl version reporting,"#62295

Previously the packing and unpacking of the NCCL version ""integer"" was done to have parity with the upstream NCCL version encoding. However, there doesn't seem to be any place where this integer is directly compared with a version integer sourced from upstream NCCL, and syncing the encoding seems to be error-prone (e.g., a recent change where a special case was added for minor versions >= 10 https://github.com/NVIDIA/nccl/blob/7e515921295adaab72adf56ea71a0fafb0ecb5f3/src/nccl.h.in#L22).

This patch changes the reporting to return a tuple of version numbers instead (to preserve ease-of-use for comparisons) and tweaks the passing between C/Python to avoid the digit overflow problem.

CC @ngimel @mcarilli ",pytorch
62937,rohan-varma,pr,2021-08-07T08:58:12Z,[reland] Gate DistributedOptimizers on RPC availability,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62939
* __->__ #62937

reland due to windows + cuda failure, fix by running it on gloo on windows even with cuda.

Differential Revision: [D30177734](https://our.internmc.facebook.com/intern/diff/D30177734/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30177734/)!",pytorch
62938,rohan-varma,pr,2021-08-07T09:08:23Z,CI all for https://github.com/pytorch/pytorch/pull/62937,"reland due to windows + cuda failure, fix by running it on gloo on windows even with cuda.

Differential Revision: [D30177734](https://our.internmc.facebook.com/intern/diff/D30177734/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30177734/)!

[ghstack-poisoned]

Fixes #{issue number}
",pytorch
62939,rohan-varma,pr,2021-08-07T09:19:29Z,Remove unnecessary windows skips,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62939
* #62937

Now that Zero, functional, and post local sgd optimizer can be used
without RPC dependency we can remove these.

Differential Revision: [D30177784](https://our.internmc.facebook.com/intern/diff/D30177784/)",pytorch
62948,peterjc123,pr,2021-08-08T05:44:57Z,Enable rebuilds for Ninja on Windows,Fixes https://github.com/pytorch/pytorch/issues/59859.,pytorch
62949,peterjc123,pr,2021-08-08T08:22:00Z,Disable RDYNAMIC check with MSVC,"When testing with clang-cl, the flag is added though it is unsupported and that generates a few warnings. Tried a few alternatives like https://cmake.org/cmake/help/latest/module/CheckLinkerFlag.html, but they just don't work.

",pytorch
62952,peterjc123,pr,2021-08-08T16:25:47Z, Stop exporting symbols in anonymous namespaces,"The cases are found out by compiling against clang on Windows.
Those functions will still be exported under this case, which is a waste of space in the symbol table.",pytorch
62953,peterjc123,pr,2021-08-08T16:44:10Z,minor fixes in c10d for Windows,"Found out by triggering builds against clang on Windows.
",pytorch
62986,r-barnes,pr,2021-08-09T19:38:06Z,Fix issue with skipping elements during removal,"Summary: Diff in response to malfet's comment [here](https://www.internalfb.com/diff/D29971891?dst_version_fbid=591173775596567&transaction_fbid=260674192298671).

Test Plan: Sandcastle

Differential Revision: D30199367

",pytorch
62990,r-barnes,pr,2021-08-09T19:59:13Z,Use `const auto` with irange,"Test Plan: Sandcastle

Differential Revision: D30199748

",pytorch
62993,rohan-varma,pr,2021-08-09T20:27:05Z,Ci all https://github.com/pytorch/pytorch/pull/61753,"Fixes #{issue number}
",pytorch
62996,rohan-varma,pr,2021-08-09T21:14:27Z,[DDP] Dont set thread local state in reducer autograd hook.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62996

No need to set this because autograd engine already propagates TLS
states.

Differential Revision: [D30202078](https://our.internmc.facebook.com/intern/diff/D30202078/)",pytorch
63007,r-barnes,pr,2021-08-09T23:39:44Z,Improve typing for torch autograd,"Differential Revision: D30200526

",pytorch
63029,ngimel,pr,2021-08-10T16:31:03Z,fix sort and topk with discontiguous out,"Fixes #62645 and #62940. The root cause of those bugs is in the bad interaction between `collapseDims` and setting the size of sorting/topK dimension to 1. If all other dimensions happen to be 1, `collapseDims` thinks that that `1` dimension is collapsible (even though it was specifically marked to be preserved) and loses its stride information. If dimension was really of size 1, the stride information would be unimportant, but since in reality that dimension is not 1 and was set to 1 for convenience, the loss of stride information results in incorrect outputs. 
",pytorch
63031,ngimel,pr,2021-08-10T16:56:04Z,remove dead code,,pytorch
63043,supriyar,pr,2021-08-10T20:32:03Z,[quant] Make version 1 the default for get_default_qat_qconfig,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63043

Summary:
In version 1 we use the fused module/operator during QAT. Making this the default for all QAT runs going forward.

Older models saved after prepare_qat_fx can still load their state_dict into a model prepared using version 1.
The state_dict will still have the same attribute for the observer/fake_quant modules.

There may be some numerics difference between the old observer code in observer.py and the new fused module that was
re-written in C++/CUDA to perform observe + fake_quantize.

This PR also updates the test to check for the new module instead of the default FakeQuantize module.
Note: there are also some changes to make the operator work for multi-dim per-channel quantization + updated the test for that.

Test Plan:
python test/test_quantization.py TestSerialization.test_default_qat_qconfig

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30232222](https://our.internmc.facebook.com/intern/diff/D30232222)",pytorch
63071,rohan-varma,pr,2021-08-11T05:01:55Z,[WIP] Pre backward hooks for nn.Module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63072
* __->__ #63071

Adds a pre-backward hook for `nn.Module` as described in https://github.com/pytorch/pytorch/issues/62998. The main use case for now is to enable profiling of module-level backwards pass.

Differential Revision: [D30240465](https://our.internmc.facebook.com/intern/diff/D30240465/)",pytorch
63072,rohan-varma,pr,2021-08-11T05:02:00Z,[WIP] Enable module profiling in backwards pass,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63072
* #63071

Adds an API to enable backwards pass of `nn.Module` to be profiled used record_function. We can easily extend this to profiling forward pass automatically as well using forward hooks.

Differential Revision: [D30240464](https://our.internmc.facebook.com/intern/diff/D30240464/)",pytorch
63087,rohan-varma,pr,2021-08-11T16:26:02Z,Skip zero test on windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63087

Test failed on windows unexpectedly see
https://github.com/pytorch/pytorch/issues/63086. Skip for now while we
investigate

Differential Revision: [D30251300](https://our.internmc.facebook.com/intern/diff/D30251300/)",pytorch
63151,eqy,pr,2021-08-12T16:43:45Z,preserve memory format in `cudnn_convolution_add_relu_out`,"Basically identical to #62482, just for the ""add"" variant. CC @cpuhrsch 

Will add a test case (e.g., a channels-last conv2d provided by @cpuhrsch once it is fully unbroken).",pytorch
63155,eqy,pr,2021-08-12T17:39:44Z,sync c10d NCCL version parsing with formatting,"related to #62295
NCCL broke compatibility with previous version format reporting in 2.9.x, and as we use the NCCL function here rather than the compile-time macros (as related to #62916), this function needs to match the new version format:
https://github.com/NVIDIA/nccl/blob/7e515921295adaab72adf56ea71a0fafb0ecb5f3/src/nccl.h.in#L22

Note that the new format doesn't actually support major versions >= 3 in upstream, but we anticipate that this format change will also be applied to those versions and account for that in this PR.

CC @ptrblck

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
63165,supriyar,pr,2021-08-12T18:51:42Z,[docs][ao] update quantize_per_tensor to mention overloads,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63165

Summary:
Add details about the overloads for
* list of tensors input
* supporting tensor scale/zero-point inputs

Test Plan:
CI

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30291045](https://our.internmc.facebook.com/intern/diff/D30291045)",pytorch
63199,ngimel,pr,2021-08-13T00:35:25Z,[hackathon] fix benchmarking script in CONTRIBUTING,"[skip ci]
Per title
",pytorch
63232,r-barnes,pr,2021-08-13T16:32:45Z,String annotations error,"Differential Revision: D30305182

",pytorch
63240,supriyar,pr,2021-08-13T17:37:23Z,[docs][ao] Add missing documentation for torch.quantized_batch_norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63263
* #63258
* #63242
* __->__ #63240

Summary:
Op is exposed via torch.quantized_batch_norm to the end user without any existing documentation

Test Plan:
CI
Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30316431](https://our.internmc.facebook.com/intern/diff/D30316431)",pytorch
63242,supriyar,pr,2021-08-13T17:53:50Z,[docs][ao] Add missing docstrings for quantized_max_pool1d and quantized_max_pool2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63263
* #63258
* __->__ #63242
* #63240

Summary:
These functions are part of the native functions namespace as well as the quantized namespace

Test Plan:
CI

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30316430](https://our.internmc.facebook.com/intern/diff/D30316430)",pytorch
63258,supriyar,pr,2021-08-13T21:39:23Z,[docs][ao] Add overload information for fake_quantize_per_tensor_affine,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63263
* __->__ #63258
* #63242
* #63240

Summary:
This function supports scalar and tensor qparams

Test Plan:
CI

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30316432](https://our.internmc.facebook.com/intern/diff/D30316432)",pytorch
63263,supriyar,pr,2021-08-13T23:29:25Z,[docs][ao] add missing torch.choose_qparams_optimized documentation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63263
* #63258
* #63242
* #63240

Summary:
The op is part of the torch.namespace and uses greedy approach to find the optimal min/max values to minimize the quantization error

Test Plan:
CI

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30318456](https://our.internmc.facebook.com/intern/diff/D30318456)",pytorch
63288,peterjc123,pr,2021-08-15T03:01:14Z,Test clang build on Windows,"Fixes #{issue number}
",pytorch
63298,crcrpar,pr,2021-08-15T23:53:37Z,"Use `fastAtomicAdd` in EmbeddingBag (mode ""max"") backward","Rel: https://github.com/pytorch/pytorch/issues/62695

### This PR
|   n_tokens |   num_embeddings |   embedding_dim | mode   |    bwd_fp32 |    bwd_fp16 |
|-----------:|-----------------:|----------------:|:-------|------------:|------------:|
|       4096 |             4096 |            4096 | max    | 0.000326228 | 0.000181448 |
|       4096 |             4096 |           16384 | max    | 0.00102805  | 0.000618136 |
|       4096 |            16384 |            4096 | max    | 0.000907326 | 0.000530422 |
|       4096 |            16384 |           16384 | max    | 0.00334988  | 0.00264645  |
|      16384 |             4096 |            4096 | max    | 0.000366449 | 0.000320232 |
|      16384 |             4096 |           16384 | max    | 0.00126421  | 0.00104183  |
|      16384 |            16384 |            4096 | max    | 0.00087738  | 0.00065068  |
|      16384 |            16384 |           16384 | max    | 0.00379229  | 0.00298201  |

### Original
|   n_tokens |   num_embeddings |   embedding_dim | mode   |    bwd_fp32 |    bwd_fp16 |
|-----------:|-----------------:|----------------:|:-------|------------:|------------:|
|       4096 |             4096 |            4096 | max    | 0.00032407  | 0.000188231 |
|       4096 |             4096 |           16384 | max    | 0.00104356  | 0.000624001 |
|       4096 |            16384 |            4096 | max    | 0.000902069 | 0.000527382 |
|       4096 |            16384 |           16384 | max    | 0.00302202  | 0.00255153  |
|      16384 |             4096 |            4096 | max    | 0.000384343 | 0.000403249 |
|      16384 |             4096 |           16384 | max    | 0.00126445  | 0.00135069  |
|      16384 |            16384 |            4096 | max    | 0.000880814 | 0.000825679 |
|      16384 |            16384 |           16384 | max    | 0.00337611  | 0.00319515  |

cc @xwang233 @ptrblck @ngimel ",pytorch
63336,r-barnes,pr,2021-08-16T16:30:48Z,"""(object)"" is extraneous in Python 3","Differential Revision: D30341531

",pytorch
63343,supriyar,pr,2021-08-16T18:25:19Z,[quant][fx] Ensure qconfig works for QAT with multiple modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63343

Summary:
The previous implementation had a bug where we were trying to modify an ordered dict value while iterating through it.
This fixes it by creating a copy before modifying it.

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_qconfig_qat_module_type

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30346116](https://our.internmc.facebook.com/intern/diff/D30346116)",pytorch
63349,ngimel,pr,2021-08-16T19:59:11Z,Generating aliases,"New version of #56955, now with jit support (for some reason, automatic push to #56955 didn't work)
",pytorch
63375,rohan-varma,pr,2021-08-17T00:16:12Z,[DDP][Grad compression] Fix fp16 cpp hook,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63379
* __->__ #63375

I think tensor.copy_(tensor.to(torch::kFloat16)); will keep it as
float32.

Tested by add the following line:

```
LOG(INFO) << ""Type is: "" << compressed_tensor.scalar_type();
```

before:

```
I0816 17:03:09.823688 364141 default_comm_hooks.cpp:21] Type is: Float
```
after:

```
I0816 17:01:16.779052 353924 default_comm_hooks.cpp:21] Type is: Half
```

Differential Revision: [D30356256](https://our.internmc.facebook.com/intern/diff/D30356256/)",pytorch
63379,rohan-varma,pr,2021-08-17T01:10:24Z,[DDP] Add a debug check in cpp fp16 compress,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63379
* #63375

this codepath has been prone to bugs as seen in the below diff, this
will help ensure against changes/refactors that touch this, as a basic sanity
check. Enabled it in debug-only builds to not affect the perf.

Differential Revision: [D30358440](https://our.internmc.facebook.com/intern/diff/D30358440/)",pytorch
63382,rohan-varma,pr,2021-08-17T01:46:45Z,[DDP] Support step_param for AdamW,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63383
* __->__ #63382

Adds `step_param` support to AdamW functional optimizer so that it can be ran as a part of DDP optimizer as hook.

Differential Revision: [D30255446](https://our.internmc.facebook.com/intern/diff/D30255446/)",pytorch
63383,rohan-varma,pr,2021-08-17T01:46:49Z,[BE] remove _SUPPORTED_OPTIM_MAP from tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63383
* #63382

Remove this duplicated mapping from tests and use `torch.distributed.optim.functional_optim_map`. 

Differential Revision: [D30358921](https://our.internmc.facebook.com/intern/diff/D30358921/)",pytorch
63387,crcrpar,pr,2021-08-17T04:26:52Z,`F.avg_pool3` CUDA backward: gpuAtomicAddNoReturn -> fastAtomicAdd,"Rel: #62695 

In the following two tables, I set `kernel_size` to 3 and `stride` to 2.
In benchmark, input tensors have the shape of (N, C, n_features, n_features, n_features).
Tested on RTX3080 w/ CUDA11.4 Update 1.

## This PR

|   N |   C |   n_features | dtype         |        time |
|----:|----:|-------------:|:--------------|------------:|
|  32 |   3 |            8 | torch.float16 | 7.46846e-05 |
|  32 |   3 |            8 | torch.float32 | 8.18968e-05 |
|  32 |   3 |           32 | torch.float16 | 0.000156748 |
|  32 |   3 |           32 | torch.float32 | 0.000165236 |
|  32 |   3 |          128 | torch.float16 | 0.00549854  |
|  32 |   3 |          128 | torch.float32 | 0.008926    |

## master (6acd87f)

|   N |   C |   n_features | dtype         |        time |
|----:|----:|-------------:|:--------------|------------:|
|  32 |   3 |            8 | torch.float16 | 7.60436e-05 |
|  32 |   3 |            8 | torch.float32 | 7.55072e-05 |
|  32 |   3 |           32 | torch.float16 | 0.000189292 |
|  32 |   3 |           32 | torch.float32 | 0.000168645 |
|  32 |   3 |          128 | torch.float16 | 0.00699538  |
|  32 |   3 |          128 | torch.float32 | 0.00890226  |

master's time divided by PR's time is as follows:

| N | C | n_features | master / PR |
|---:|---:|---------------:|----------------:|
| 32 | 3 | 8 | 1.018 |
| 32 | 3 | 32 | 1.208 | 
| 32 | 3 | 128 | 1.272|


cc: @xwang233 @ptrblck @ngimel ",pytorch
63451,ngimel,pr,2021-08-18T01:29:12Z,Revert embedding thrust->cub migration,"Fixes #63427
",pytorch
63462,rohan-varma,pr,2021-08-18T05:26:33Z,[BE] Enable functional optim tests for windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63463
* __->__ #63462

Now that `torch.distributed.optim` gates DistributedOptimizer on RPC availability, these tests can be run on windows.

Differential Revision: [D30358923](https://our.internmc.facebook.com/intern/diff/D30358923/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
63463,rohan-varma,pr,2021-08-18T05:26:38Z,[BE] Enable PostLocalSGD tests on windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63463
* #63462

Now that `torch.distributed.optim` gates DistributedOptimizer on RPC availability, local sgd optimizer can be used on windows.

Differential Revision: [D30358922](https://our.internmc.facebook.com/intern/diff/D30358922/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
63561,lezcano,pr,2021-08-19T09:56:54Z,Use trsm for triangular_solve in CPU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63562
* __->__ #63561

This PR also exposes the `side` argument of this function which is used
in the second PR of this stack to optimise the number copies one needs to make
when preparing the arguments to be sent to the backends.

The docs are fixed in the next PR of this stack.

Fixes https://github.com/pytorch/pytorch/issues/56326

The current implementation called trtrs for CPU and trsm for CUDA.
See
https://github.com/pytorch/pytorch/issues/56326#issuecomment-825496115
for a discussion of the differences between these two functions and why
we prefer trsm vs trtrs on CUDA.

On top of that, this is the first of a stack of PRs that aim to
improve the performance of triangular_solve. trsm has an extra parameter
(`side`), which allows to ellide the copy of the triangular matrix.",pytorch
63562,lezcano,pr,2021-08-19T09:56:59Z,Add linalg.triangular_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63562
* #63561

This PR adds the first solver with structure to `linalg`. This solver
has an API compatible with that of `linalg.solve` preparing these for a
possible future merge of the APIs. The new API:
- Just returns the solution, rather than the solution and a copy of `A`
- Removes the confusing `transpose` argument and replaces it by a
correct handling of conj and strides within the call
- Adds a `left=True` kwarg. This can be achieved via transposes of the
inputs and the result, but it's exposed for convenience.

This PR also implements a dataflow that minimises the number of copies
needed before calling LAPACK / MAGMA / cuBLAS and takes advantage of the
conjugate and neg bits.

This algorithm is implemented for `triangular_solve` (which, for this, is
the most complex of all the solvers due to the `upper` parameters).
Once more solvers are added, we will factor out this calling algorithm,
so that all of them can take advantage of it.

Given the complexity of this algorithm, we implement some thorough
testing. We also added tests for all the backends, which was not done
before.

We also add forward AD support for `triangular_solve` and improve the
docs of `linalg.triangular_solve`. We also fix a few issues with those of
`torch.triangular_solve`.

Fixes https://github.com/pytorch/pytorch/issues/54258
Fixes https://github.com/pytorch/pytorch/issues/56327",pytorch
63563,lezcano,pr,2021-08-19T10:47:59Z,Use trsm for triangular_solve in CPU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63564
* __->__ #63563

This PR also exposes the `side` argument of this function which is used
in the second PR of this stack to optimise the number copies one needs to make
when preparing the arguments to be sent to the backends.

The docs are fixed in the next PR of this stack.

Fixes https://github.com/pytorch/pytorch/issues/56326

The current implementation called trtrs for CPU and trsm for CUDA.
See
https://github.com/pytorch/pytorch/issues/56326#issuecomment-825496115
for a discussion of the differences between these two functions and why
we prefer trsm vs trtrs on CUDA.

On top of that, this is the first of a stack of PRs that aim to
improve the performance of triangular_solve. trsm has an extra parameter
(`side`), which allows to ellide the copy of the triangular matrix.",pytorch
63564,lezcano,pr,2021-08-19T10:48:04Z,Add linalg.triangular_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63564
* #63563

This PR adds the first solver with structure to `linalg`. This solver
has an API compatible with that of `linalg.solve` preparing these for a
possible future merge of the APIs. The new API:
- Just returns the solution, rather than the solution and a copy of `A`
- Removes the confusing `transpose` argument and replaces it by a
correct handling of conj and strides within the call
- Adds a `left=True` kwarg. This can be achieved via transposes of the
inputs and the result, but it's exposed for convenience.

This PR also implements a dataflow that minimises the number of copies
needed before calling LAPACK / MAGMA / cuBLAS and takes advantage of the
conjugate and neg bits.

This algorithm is implemented for `triangular_solve` (which, for this, is
the most complex of all the solvers due to the `upper` parameters).
Once more solvers are added, we will factor out this calling algorithm,
so that all of them can take advantage of it.

Given the complexity of this algorithm, we implement some thorough
testing. We also added tests for all the backends, which was not done
before.

We also add forward AD support for `triangular_solve` and improve the
docs of `linalg.triangular_solve`. We also fix a few issues with those of
`torch.triangular_solve`.

Fixes https://github.com/pytorch/pytorch/issues/54258
Fixes https://github.com/pytorch/pytorch/issues/56327",pytorch
63567,lezcano,pr,2021-08-19T13:44:10Z,Use trsm for triangular_solve on CPU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63570
* #63569
* #63568
* __->__ #63567

The current implementation called trtrs for CPU and trsm for CUDA.
See https://github.com/pytorch/pytorch/issues/56326#issuecomment-825496115 for a discussion on the differences between
these two functions and why we prefer trsm vs trtrs on CUDA.

This PR also exposes the `side` argument of this function which is used
in the second PR of this stack to optimise the number copies one needs to make
when preparing the arguments to be sent to the backends.

It also changes the use of `bool`s to a common enum type to represent
whether a matrix is transposed / conj transposed, etc. This makes the API
consistent, as before, the behaviour of these functions with `transpose=True`
and `conjugate_transpose=True` it was not well defined.
Functions to transform this type into the specific types / chars for the different
libraries are provided under the names `to_blas`, `to_lapack`, `to_magma`, etc.

This is the first of a stack of PRs that aim to improve the performance of 
`linalg.solve_triangular`. `trsm` has an extra parameter (`side`), which allows to 
ellide the copy of the triangular matrix in many cases.

Fixes https://github.com/pytorch/pytorch/issues/56326

Differential Revision: [D30566479](https://our.internmc.facebook.com/intern/diff/D30566479)

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
63568,lezcano,pr,2021-08-19T13:44:15Z,Add linalg.solve_triangular,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63570
* #63569
* __->__ #63568

This PR adds the first solver with structure to `linalg`. This solver
has an API compatible with that of `linalg.solve` preparing these for a
possible future merge of the APIs. The new API:
- Just returns the solution, rather than the solution and a copy of `A`
- Removes the confusing `transpose` argument and replaces it by a
correct handling of conj and strides within the call
- Adds a `left=True` kwarg. This can be achieved via transposes of the
inputs and the result, but it's exposed for convenience.

This PR also implements a dataflow that minimises the number of copies
needed before calling LAPACK / MAGMA / cuBLAS and takes advantage of the
conjugate and neg bits.

This algorithm is implemented for `solve_triangular` (which, for this, is
the most complex of all the solvers due to the `upper` parameters).
Once more solvers are added, we will factor out this calling algorithm,
so that all of them can take advantage of it.

Given the complexity of this algorithm, we implement some thorough
testing. We also added tests for all the backends, which was not done
before.

We also add forward AD support for `linalg.solve_triangular` and improve the
docs of `linalg.solve_triangular`. We also fix a few issues with those of
`torch.triangular_solve`.

Resolves https://github.com/pytorch/pytorch/issues/54258
Resolves https://github.com/pytorch/pytorch/issues/56327
Resolves https://github.com/pytorch/pytorch/issues/45734

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32588230](https://our.internmc.facebook.com/intern/diff/D32588230)",pytorch
63569,lezcano,pr,2021-08-19T13:44:21Z,Modify LU_backward and lu_solve_backward to use linalg_solve_triangular,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63570
* __->__ #63569

This PR also rewrites `lu_solve_backward` from scratch going from
solving 5 systems of equations to just 2.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32618014](https://our.internmc.facebook.com/intern/diff/D32618014)",pytorch
63570,lezcano,pr,2021-08-19T13:44:26Z,Deprecate torch.triangular_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63570

There is a use of `at::triangular_solve_out` in the file
`torch/csrc/jit/tensorexpr/external_functions.cpp` that I have not dared
to move to `at::linalg_solve_triangular_out`.

**Deprecation note:**

This PR deprecates the `torch.triangular_solve` function in favor of
`torch.linalg.solve_triangular`. An upgrade guide is added to the
documentation for `torch.triangular_solve`.

Note that it DOES NOT remove `torch.triangular_solve`, but
`torch.triangular_solve` will be removed in a future PyTorch release.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32618035](https://our.internmc.facebook.com/intern/diff/D32618035)",pytorch
63600,r-barnes,pr,2021-08-19T19:48:44Z,Sort out hpc work returns,"Differential Revision: D30343574

",pytorch
63619,rohan-varma,pr,2021-08-19T23:09:20Z,Add a record scope around autograd::engine::evaluate_function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63619

Adds a RECORD_FUNCTION with the function that is being valuate as part
of backwards execution. This has been useful in picking up some operations
in the backwards pass that otherwise would not show up, for example custom cpp
functions that use custom C++ code.

Differential Revision: [D30439492](https://our.internmc.facebook.com/intern/diff/D30439492/)",pytorch
63743,cloudhan,pr,2021-08-23T03:21:38Z,Rename profiler metadata key ,rename metadata key to be the same with variable name,pytorch
63756,peterjc123,pr,2021-08-23T13:59:24Z,Subprocess encoding fixes for cpp extension,"Fixes https://github.com/pytorch/pytorch/issues/63584
",pytorch
63799,supriyar,pr,2021-08-23T20:03:35Z,[quant][fx] Add support for dynamic linear + relu fusion (INT8),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63826
* #63824
* #63820
* __->__ #63799

Summary:
Add a new module that can be used for module swap with the nni.LinearReLU module in convert function.
Supports INT8 currently (since FP16 op doesn't have relu fusion yet).

Fixes #55393

Test Plan:
python test/test_quantization.py test_dynamic_fusion

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30502812](https://our.internmc.facebook.com/intern/diff/D30502812)",pytorch
63820,supriyar,pr,2021-08-23T23:53:34Z,[quant] support linear_relu_dynamic for qnnpack backend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63826
* #63824
* __->__ #63820
* #63799

Summary:
Adds support in the operator directly to call relu operator if relu fusion is enabled.
Once QNNPACK natively supports relu fusion in the linear_dynamic this can be removed

Test Plan:
python test/test_quantization.py TestDynamicQuantizedLinear.test_qlinear

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30502813](https://our.internmc.facebook.com/intern/diff/D30502813)",pytorch
63824,supriyar,pr,2021-08-24T01:07:57Z,[quant] Add op support for linear_relu_dynamic_fp16,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63826
* __->__ #63824
* #63820
* #63799

Summary:
Add a fused operator implementation that will work with the quantization fusion APIs.
Once FBGEMM FP16 kernel supports relu fusion natively we can remove the addition from the PT operator.

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30503514](https://our.internmc.facebook.com/intern/diff/D30503514)",pytorch
63826,supriyar,pr,2021-08-24T01:18:33Z,[quant] Add support for linear_relu fusion for FP16 dynamic quant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63826
* #63824
* #63820
* #63799

Summary:
Support the conversion of the intrinsic linearRelu module to the quantized dynamic LinearReLU module
Verify the support works for both linear module and functional linear fusion

Test Plan:
python test/test_quantization.py test_dynamic_with_fusion

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30503513](https://our.internmc.facebook.com/intern/diff/D30503513)",pytorch
63831,rohan-varma,pr,2021-08-24T04:23:11Z,Fix issue  re: DDP and create_graph=True,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63831

Closes https://github.com/pytorch/pytorch/issues/63812

`at::mul_out` is not supported when `grad` itself requires grad, which is useful for computing higher order derivatives.

In this case, fall back to a mul + copy instead of mul_out.

Note that DDP doesn't really work well with higher order grads. Will add appropriate warning messaging.

Differential Revision: [D30505573](https://our.internmc.facebook.com/intern/diff/D30505573/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30505573/)!

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
63885,ngimel,pr,2021-08-24T20:26:52Z,compute reduction intermediate buffer size in elements,"Fixes #63869
`iter` strides are in bytes, and we are additionally multiplying size computed using those strides by `sizeof(arg_t)`. Computing `output_memory_size` in elements should be enough. 
This doesn't fix the still real problem of allocating large intermediate tensor, but it makes this tensor smaller by typically a factor of 4.",pytorch
63928,rohan-varma,pr,2021-08-25T05:27:16Z,[c10d] Prefer use of torch_check,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63928

throw std::invalid_argument results in not getting stacktraces with
TORCH_SHOW_CPP_STACKTRACES=1, so instead prefer torch_check here.

Differential Revision: [D30533955](https://our.internmc.facebook.com/intern/diff/D30533955/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
63969,ngimel,pr,2021-08-25T18:02:45Z,Compute cuda reduction buffer size in elements,"Resubmit of #63885

",pytorch
64035,rohan-varma,pr,2021-08-26T17:19:11Z,[DDP] Add more logging iterations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Adding more logging iterations to get additional data.

Differential Revision: [D30579367](https://our.internmc.facebook.com/intern/diff/D30579367/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64071,rohan-varma,pr,2021-08-27T01:23:54Z,[DDP] Add more logging iterations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64073
* #64072
* __->__ #64071

Adding more logging iterations to get additional data.

Differential Revision: [D30579367](https://our.internmc.facebook.com/intern/diff/D30579367/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64072,rohan-varma,pr,2021-08-27T01:23:58Z,[DDP] Log num threads,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64073
* __->__ #64072
* #64071

Log gloo threads to DDP logging.

Differential Revision: [D30596083](https://our.internmc.facebook.com/intern/diff/D30596083/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64073,rohan-varma,pr,2021-08-27T01:57:58Z,Enable NCCL_BLOCKING_WAIT if TORCH_DIST_DEBUG=detail,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64073

Enables NCCL_BLOCKING_WAIT if TORCH_DISTRIBUTED_DEBUG is set to detail
since this is the most verbose debug option and we'd like to use all debug
tools/flags available to get as much information/stacktraces as possible.

Differential Revision: [D30596590](https://our.internmc.facebook.com/intern/diff/D30596590/)",pytorch
64074,rohan-varma,pr,2021-08-27T01:59:07Z,Fix incorrect DDP test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64074

Previous PR https://github.com/pytorch/pytorch/pull/63831 did not actually test the error in https://github.com/pytorch/pytorch/issues/63812. Introduce a test
directly from the repro that simulates it.

Differential Revision: [D30569719](https://our.internmc.facebook.com/intern/diff/D30569719/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64111,bryant1410,pr,2021-08-27T20:56:32Z,Avoid an unnecessary list creation in `DataChunk`,,pytorch
64114,bryant1410,pr,2021-08-27T21:13:56Z,Make datasets in `ConcatDataset` not need to be sized,"`datasets` needs to be iterable, but also sized because the length is checked. But immediately after it's converted to a list. By changing the order of these 2 lines, it doesn't need to be sized anymore.",pytorch
64179,lezcano,pr,2021-08-30T14:31:25Z,"Add tensor.{adjoint(),H,mT,mH} methods and properties","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64181
* #64180
* __->__ #64179

This PR follows the discussion in https://github.com/pytorch/pytorch/issues/45063#issuecomment-904431478

Fixes https://github.com/pytorch/pytorch/issues/45063

cc @ezyang @anjali411 @dylanbespalko @mruberry @Lezcano @nikitaved @rgommers @pmeier @asmeurer @leofang @AnirudhDagar @asi1024 @emcastillo @kmaehashi @heitorschueroff

Differential Revision: [D30730483](https://our.internmc.facebook.com/intern/diff/D30730483)",pytorch
64180,lezcano,pr,2021-08-30T14:31:30Z,Deprecate x.T on tensors of dimension other than 0 or 2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64181
* __->__ #64180
* #64179

**BC-breaking note:**

This PR deprecates the `Tensor.T` are not matrices. An upgrade guide is added to the
documentation for `Tensor.T`.

This PR DOES NOT make this attribute to throw an error when called on a tensor of `dim != 2`,
but this will be its behavior in a future PyTorch release.

cc @mruberry @rgommers @pmeier @asmeurer @leofang @AnirudhDagar @asi1024 @emcastillo @kmaehashi @heitorschueroff

Differential Revision: [D31610611](https://our.internmc.facebook.com/intern/diff/D31610611)",pytorch
64181,lezcano,pr,2021-08-30T14:31:35Z,"Prefer mT and mH over transpose(-2, -1) and transpose(-2, -1).conj()","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64181

This PR replaces all the calls to:
- `transpose(-2, -1)` or `transpose(-1, -2)` by `mT()` in C++ and `mT` in Python
- `conj().transpose(-2, -1)` or `transpose(-2, -1).conj()` or `conj().transpose(-1, -2)` or `transpose(-1, -2).conj()` by `mH()` in C++ and `mH` in Python.

It also simplifies two pieces of code, and fixes one bug where a pair
of parentheses were missing in the function `make_symmetric_matrices`.

Differential Revision: [D31692896](https://our.internmc.facebook.com/intern/diff/D31692896)",pytorch
64194,ngimel,pr,2021-08-30T16:42:05Z,"Optimize reduction configs, avoid invalid configuration arguments we have for theâ€¦","â€¦ reduce kernels

Previously, when reduction included fastest dimension, and that dimension was small, and number of outputs was also small, it resulted in a very small threadblock (e.g. (1,2)). This led to bad performance, and, as tensor became bigger, could trigger launching too many blocks and ""invalid configuration argument"".

Fixes #48573

It also greatly improves performance of the cases similar to #48573 reproducer that were passing before:
```
import torch
dummy = torch.randn(1, 512, 1024, 4, device='cuda:0')*1000
inp = dummy[..., :3]
out=torch.mean(inp)
```
before this PR on V100  - 2 ms, after this PR - 22 us
",pytorch
64197,rohan-varma,pr,2021-08-30T17:14:25Z,Remove ref to test_distributed_fork,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64253
* __->__ #64197

Removes this line as test is gone.

Differential Revision: [D30642929](https://our.internmc.facebook.com/intern/diff/D30642929/)",pytorch
64241,rohan-varma,pr,2021-08-31T02:33:36Z,[c10d] Provide failure reason from ProcessGroup when aborting NCCL comm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64241

When things go wrong PG NCCL aborts nccl communicators via `ncclCommAbort`, but one issues is that often the error can be set to `ncclSystemError` (see  https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/NCCLUtils.hpp#L176) when that might not be the true cause of the issue and the actual issue is that some prior work timed out, communicator was aborted on other rank, etc.

This results in a lot of confusion when debugging jobs with a large no. of processes as the current message for ncclSystemError is not very informative: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/NCCLUtils.hpp#L22

The fix here is to pass in a string exception message from PG NCCL down to `NCCLUtils` which will aim to raise that as the actual issue and not the confusing `ncclSystemError` message.

Error before this diff when a comm was aborted:
```
NCCL communicator was aborted on rank 1.
```

With this diff:
```
NCCL communicator was aborted on rank 1. Original reason for failure was: Work: WorkNCCL(OpType=ALLREDUCE, TensorShape=[1], Timeout(ms)=2000) timed out in call to wait().
```

Differential Revision: [D30658855](https://our.internmc.facebook.com/intern/diff/D30658855/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30658855/)!

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64242,rohan-varma,pr,2021-08-31T02:33:41Z,[c10d] Bring down PG NCCL on failure in abortTimedOutCollectives,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64242
* #64241

It appears that this is only run in async error handling mode, so
there is no need to wait for the work cleanup thread to run and handle this
error if we know the work object encountered an error. We might as well bring
the process down in this case which is what the work cleanup loop would do
anyways. The purpose of doing it here is because occasionally other collectives
may run before the work cleanup loop does, resulting in confusing NCCL error
messages/crashes when we actually just want to bring down the process.

Differential Revision: [D30659941](https://our.internmc.facebook.com/intern/diff/D30659941/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64253,rohan-varma,pr,2021-08-31T07:03:27Z,[Dist CI] Move rest of distributed tests to their own CI job,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64413
* __->__ #64253

Follow up to D30496178 to move the rest of distributed tests to their own jobs for Linux GHA.

Differential Revision: [D30662999](https://our.internmc.facebook.com/intern/diff/D30662999/)",pytorch
64275,lezcano,pr,2021-08-31T16:01:19Z,Fix nll_backward for negative weights.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64275

Fixes https://github.com/pytorch/pytorch/issues/64256
It adds a test with negative weights as well.

This PR also cleans the code of `nll_backward` which contained a number
of inconsistencies between the different branches of the code: some of
them would check that an index was correct while others didn't. Now the
code is more homogeneous and hopefully easier to follow.",pytorch
64387,lezcano,pr,2021-09-01T17:55:26Z,Micro-optimisations for matmul,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64387

A number of these include:

- Always prefer `DimVector` over `std::vector` when handling dimensions.
- Make the code `const` correct.
- Create `DimVector`'s more efficiently (e.g. prefer `append` over
`insert`).
- Access sizes of the tensors via `sizes().front()` / `sizes().back()`
  / `sizes().end()[-2]`
- Do not create intermediary tensors / vectors when it can be avoided.
- Dispatch to `mv` rather than `mm` in the case `(n,) x (n, m)`.
- Call `reshape` rather than `expect_contiguous`  + `view`

On top of this, while fixing the CI and after further discussions, the following features were added. 
- [Fix CI] Add forward AD to `mv`, `matmul`, `__rmatmul__`
- [Fix CI] Add support for Half for a number of operations
- [Fix CI] Change some calls that called directly into the matmul implementation. Now they call into `_out`.
- Remove the uses of `set_`. (requested by @ezyang )
- Solve the resize bug in `matmul_out`.

Looking into the future, further optimisations could include dispatching to lower level functions
when one of the inputs has shape `(n, 1)` or `(1, n)`.

Fixes https://github.com/pytorch/pytorch/issues/67767

cc @VitalyFedyunin @ngimel @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @heitorschueroff",pytorch
64411,rohan-varma,pr,2021-09-02T01:19:08Z,[DDP] Fix logging iterations,"Summary:
These are not actually the training iterations, but are offset by how
frequently DDP stats collection actually runs (default being
kDDPRuntimeLoggingSampleRate = 100). So with this change, they are actually
logged to scuba every:
10, 10 * 100, 40 * 100, etc iterations.

Test Plan: CI

Differential Revision: D30718274



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64413,rohan-varma,pr,2021-09-02T03:54:50Z,"[WIP] OSS CI for test_ddp_hooks, distributed quantization","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64413

Differential Revision: [D30720449](https://our.internmc.facebook.com/intern/diff/D30720449/)",pytorch
64466,rohan-varma,pr,2021-09-03T00:01:50Z,Update distributed debug doc,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64466
* #64073

Document that we set nccl_blocking_wait when torch_distributed_debug=detail, and also recommend to use cuda_launch_blocking.

Differential Revision: [D30744757](https://our.internmc.facebook.com/intern/diff/D30744757/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64472,rohan-varma,pr,2021-09-03T01:09:53Z,[DDP] Fix when buffers are reassigned in module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* #64474
* #64473
* __->__ #64472

Closes: #63916

Sometimes, user module can reassign tensor buffer, as in:

```
self.buffer = torch.randn(1, 2) # in init
self.buffer += 1 # in forward
```

in this case, `self.modules_buffers` will become outdated and we should
repopulate self.modules_buffers if we need to sync module buffers.

See https://github.com/pytorch/pytorch/issues/63916 for full description of the
issue.

Differential Revision: [D30745921](https://our.internmc.facebook.com/intern/diff/D30745921/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64473,rohan-varma,pr,2021-09-03T01:09:57Z,[DDP] Remove self.modules_params,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* #64474
* __->__ #64473
* #64472

Unused after SPMD deprecated.

Differential Revision: [D30745961](https://our.internmc.facebook.com/intern/diff/D30745961/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64474,rohan-varma,pr,2021-09-03T01:10:02Z,[DDP] Remove SPMD from self.modules_buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* __->__ #64474
* #64473
* #64472

No need for a nested list here.

Differential Revision: [D30745960](https://our.internmc.facebook.com/intern/diff/D30745960/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64501,vfdev-5,pr,2021-09-03T22:53:15Z,Added nearest-exact interpolation mode,"Added ""nearest-exact"" interpolation mode to fix the issues: #34808 and #62237.

Description:

As we can not fix ""nearest"" mode without large impact on already trained model [it was suggested](https://github.com/pytorch/pytorch/pull/64501#pullrequestreview-749771815) to introduce new mode instead of fixing exising ""nearest"" mode. 

- New mode ""nearest-exact"" performs index computation for nearest interpolation to match scikit-image, pillow, TF2 and while ""nearest"" mode still match opencv INTER_NEAREST, which appears to be buggy, see https://ppwwyyxx.com/blog/2021/Where-are-Pixels/#Libraries.

""nearest"":
```
input_index_f32 = output_index * scale
input_index = floor(input_index_f32)
```

""nearest-exact""
```
input_index_f32 = (output_index + 0.5) * scale - 0.5
input_index = round(input_index_f32)
```


Comparisions with other libs: https://gist.github.com/vfdev-5/a5bd5b1477b1c82a87a0f9e25c727664

PyTorch version | 1.9.0 ""nearest"" | this PR ""nearest"" | this PR ""nearest-exact""
---|---|---|---
Resize option: | | 
OpenCV INTER_NEAREST result mismatches | 0 | 0 | 10
OpenCV INTER_NEAREST_EXACT result mismatches | 9 | 9 | 9
Scikit-Image result mismatches | 10 | 10 | 0 
Pillow result mismatches | 10 | 10 | 7
TensorFlow result mismatches | 10 | 10 | 0
Rescale option: | | |
size mismatches (https://github.com/pytorch/pytorch/issues/62396) | 10 | 10 | 10
OpenCV INTER_NEAREST result mismatches | 3 | 3| 5
OpenCV INTER_NEAREST_EXACT result mismatches | 3 | 3| 4
Scikit-Image result mismatches | 4 | 4 | 0
Scipy result mismatches | 4 | 4 | 0
TensorFlow: no such option | - |  -

Versions:
```
skimage: 0.19.0.dev0
opencv: 4.5.4-dev
scipy: 1.7.2
Pillow: 8.4.0
TensorFlow: 2.7.0
```

Implementations in other libs:
 
- Pillow: 
  - https://github.com/python-pillow/Pillow/blob/ee079ae67e7e24ec789d3cc7d180820a70d32fe6/src/libImaging/Geometry.c#L889-L899
  - https://github.com/python-pillow/Pillow/blob/ee079ae67e7e24ec789d3cc7d180820a70d32fe6/src/libImaging/Geometry.c#L11
  - `a[2] == 0`

- Scikit-Image : 
  - dev v0.19.0 uses scipy ndi.zoom:
    - https://github.com/scikit-image/scikit-image/blob/38fae50c3ffc19e78b3adc848eb9bd0916e13dc8/skimage/transform/_warps.py#L180-L188
    - https://github.com/scipy/scipy/blob/47bb6febaa10658c72962b9615d5d5aa2513fa3a/scipy/ndimage/src/ni_interpolation.c#L775-L779
    - https://github.com/scipy/scipy/blob/47bb6febaa10658c72962b9615d5d5aa2513fa3a/scipy/ndimage/src/ni_interpolation.c#L479


Additionally:
- Updated upsampling tests


cc @ezyang @gchanan @albanD @mruberry @jbschlosser @walterddr @fmassa @heitorschueroff @ppwwyyxx
",pytorch
64513,rohan-varma,pr,2021-09-05T01:06:28Z,[DDP] Custom buffer reduction,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* __->__ #64513

Proposal: https://github.com/pytorch/pytorch/issues/63041
Support custom buffer reduction in DDP via hook. This is required for an internal training use case and could be more broadly applicable to OSS scenarios. 

Differential Revision: [D30751152](https://our.internmc.facebook.com/intern/diff/D30751152/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64514,rohan-varma,pr,2021-09-05T01:06:33Z,[DDP] Refactor and remove sync_params,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* __->__ #64514
* #64513

sync_params is a misnomer since we don't actually synchroniz
parameters. While removing this I realized
`self._check_and_sync_module_buffers` does almost everything we need it to, so
just refactored that and made DDP forward call into it.

Differential Revision: [D30751231](https://our.internmc.facebook.com/intern/diff/D30751231/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64515,rohan-varma,pr,2021-09-05T01:06:37Z,[DDP] Allow await of custom buffer reduction in backward,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64515

For performance reasons, we would like to ensure that we can await
user collectives as part of custom buffer reduction in parallel to other work.
As a result, add support to return futures from custom buffer hooks and await
those futures at end of backwards pass.

Also added some docs to clarify how to use these APIs.

Differential Revision: [D30757761](https://our.internmc.facebook.com/intern/diff/D30757761/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23",pytorch
64559,ngimel,pr,2021-09-06T23:00:49Z,Remove dead code from THC (THCApply.cuh),"cc @peterbell10 
",pytorch
64572,lezcano,pr,2021-09-07T14:27:12Z,Fix nll_backward for negative weights,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64572

Fixes https://github.com/pytorch/pytorch/issues/64256
It also fixes an inconsistent treatment of the case `reduction = ""mean""`
when the whole target is equal to `ignore_index`. It now returns `NaN`
in this case, consistently with what it returns when computing the mean
over an empty tensor.

We add tests for all these cases.

Differential Revision: [D31116297](https://our.internmc.facebook.com/intern/diff/D31116297)

cc @ezyang @gchanan",pytorch
64695,r-barnes,pr,2021-09-08T20:21:56Z,Fix a shadowed variable,"Summary:
Resolves this warning:
```
caffe2/aten/src/ATen/ParallelOpenMP.h:109:63: warning: declaration of 'int64_t begin' shadows a parameter [-Wshadow=compatible-local]
  109 |   internal::invoke_parallel(begin, end, grain_size, [&](int64_t begin, int64_t end) {
      |                                                       ~~~~~~~~^~~~~
caffe2/aten/src/ATen/ParallelOpenMP.h:86:1: note: shadowed declaration is here
   85 | inline scalar_t parallel_reduce(
      |                 ~~~~~~~~~~~~~~~~
   86 |     const int64_t begin,
      | ^   ~
```

Differential Revision: D30816128

",pytorch
64721,rohan-varma,pr,2021-09-09T02:29:31Z,[Dist/CI] Remove dist from target determinator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64721

There are a couple PRs where distributed CI did not run and we expect
it to. Examples:

https://github.com/pytorch/pytorch/pull/64513/checks?check_run_id=3539190960,
https://github.com/pytorch/pytorch/pull/64113. All distributed tests should've
been run on these PRs, but we can see they were not:

```
Determination is skipping distributed/test_c10d_common
Determination is skipping distributed/test_c10d_gloo
Determination is skipping distributed/test_c10d_nccl
Determination is skipping distributed/test_c10d_spawn_gloo
Determination is skipping distributed/test_c10d_spawn_nccl
Running distributed/test_data_parallel without determination
Determination is skipping distributed/test_distributed_spawn
Determination is skipping distributed/test_jit_c10d
```

Since it is important to run distributed tests on PRs that touch distributed,
exclude distributed from target_det_list for now.

Differential Revision: [D30830455](https://our.internmc.facebook.com/intern/diff/D30830455/)",pytorch
64776,rohan-varma,pr,2021-09-09T20:07:39Z,"Back out ""Revert D30745921: [DDP] Fix when buffers are reassigned in module""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* #64778
* #64777
* __->__ #64776

Original commit changeset: 343ead86bf1e

Differential Revision: [D30849444](https://our.internmc.facebook.com/intern/diff/D30849444/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @cbalioglu @gcramer23",pytorch
64777,rohan-varma,pr,2021-09-09T20:07:44Z,"Back out ""Revert D30745961: [DDP] Remove self.modules_params""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* #64778
* __->__ #64777
* #64776

Original commit changeset: 59f7cc50d369

Differential Revision: [D30849442](https://our.internmc.facebook.com/intern/diff/D30849442/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
64778,rohan-varma,pr,2021-09-09T20:07:49Z,"Back out ""Revert D30745960: [DDP] Remove SPMD from self.modules_buffers""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64515
* #64514
* #64513
* __->__ #64778
* #64777
* #64776

Original commit changeset: d3f3fb813c45

Differential Revision: [D30849443](https://our.internmc.facebook.com/intern/diff/D30849443/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
64829,supriyar,pr,2021-09-10T17:24:18Z,[quant] handle empty input in fused_moving_avg_obs_fake_quant op,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64829

Summary:
If an empty input is passed in, the aminmax operator fails with a runtime error like
```
RuntimeError: aminmax(): cannot compute aminmax over an empty dimension as the operation has no identity.
```

To avoid this during training we just return the input if we find it to be empty

Test Plan:
python test/test_quantization.py TestFusedObsFakeQuant

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D30870879](https://our.internmc.facebook.com/intern/diff/D30870879)",pytorch
64860,supriyar,pr,2021-09-10T22:45:21Z,torch.ao migration: quantize_jit.py phase1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64860
* #64861

Differential Revision: [D30880574](https://our.internmc.facebook.com/intern/diff/D30880574/)",pytorch
64861,supriyar,pr,2021-09-10T22:47:47Z,torch.ao migration: stubs.py phase 1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64860
* __->__ #64861

1. move the file
  ```
  hg mv caffe2/torch/quantization/stubs.py caffe2/torch/ao/quantization/
  ```

  2. create a new file in the old location and copy the imports
  3. fix all call sites inside `torch`

Differential Revision: [D30879678](https://our.internmc.facebook.com/intern/diff/D30879678/)",pytorch
64878,ngimel,pr,2021-09-11T03:58:04Z,kill SkipInfo,"Per offline discussion, replaces SkipInfo with DecorateInfo. SkipInfo class itself is not removed yet to give functorch time to replace its SkipInfos. 
cc @zou3519 ",pytorch
64892,nkreeger,pr,2021-09-12T12:05:11Z,Drop incremental linking on Windows with REL_WITH_DEB_INFO=1.,"The library will no longer link properly on VS 2019 (14.29.30133). To
ensure that engineers building on Windows can use and debug with this
build type, incremental linking needs to be turned off for this build
flag.

Verified that this build type successfully builds, links, and provides
debuggable Python modules on Windows.
",pytorch
64965,ngimel,pr,2021-09-13T23:24:56Z,[THC] remove TensorTypeUtils and TensorInfo,"per title
",pytorch
64972,ngimel,pr,2021-09-14T04:13:22Z,remove SkipInfo class,"per title
",pytorch
64983,vfdev-5,pr,2021-09-14T10:38:59Z,[BC-breaking] Fixed interpolation output size to match opencv if scale factor is specified,"Fixes #62396

Dscription:
- Fixed interpolation output size to match opencv, scikit-image, scipy if scale factor is specified


cc @heitorschueroff , @fmassa 


",pytorch
65032,rohan-varma,pr,2021-09-15T02:01:03Z,[Not for land] Chunked version of all_gather,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65032
* #64776

Chunks a large tensor into num_chunks partitions, and all_gathers them
in parallel to improve throughput.

Differential Revision: [D30951753](https://our.internmc.facebook.com/intern/diff/D30951753/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30951753/)!

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
65142,vfdev-5,pr,2021-09-16T12:00:30Z,"Added antialias flag to interpolate (CPU only, bilinear)","Description:
- Added antialias flag to interpolate (CPU only)
  - forward and backward for bilinear mode
  - added tests

### Benchmarks

<details>
<summary>
Forward pass, CPU. PTH interpolation vs PIL
</summary>

Cases:
- PTH RGB 3 Channels, float32 vs PIL RGB uint8 (apples vs pears)
- PTH 1 Channel, float32 vs PIL 1 Channel Float

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```
# OMP_NUM_THREADS=1 python bench_interp_aa_vs_pillow.py

Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_75,code=sm_75
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, 

Num threads: 1
[------------------------ Downsampling: torch.Size([1, 3, 906, 438]) -> (320, 196) ------------------------]
                                                  |  Reference, PIL 8.3.2, mode: RGB  |  1.10.0a0+git1e87d91
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                2.9                |          3.1        
      channels_last non-contiguous torch.float32  |                2.6                |          3.6        

Times are in milliseconds (ms).

[------------------------ Downsampling: torch.Size([1, 3, 906, 438]) -> (460, 220) ------------------------]
                                                  |  Reference, PIL 8.3.2, mode: RGB  |  1.10.0a0+git1e87d91
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                3.4                |          4.0        
      channels_last non-contiguous torch.float32  |                3.4                |          4.8        

Times are in milliseconds (ms).

[------------------------ Downsampling: torch.Size([1, 3, 906, 438]) -> (120, 96) -------------------------]
                                                  |  Reference, PIL 8.3.2, mode: RGB  |  1.10.0a0+git1e87d91
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                1.6                |          1.8        
      channels_last non-contiguous torch.float32  |                1.6                |          1.9        

Times are in milliseconds (ms).

[----------------------- Downsampling: torch.Size([1, 3, 906, 438]) -> (1200, 196) ------------------------]
                                                  |  Reference, PIL 8.3.2, mode: RGB  |  1.10.0a0+git1e87d91
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                9.0                |          11.3       
      channels_last non-contiguous torch.float32  |                8.9                |          12.5       

Times are in milliseconds (ms).

[----------------------- Downsampling: torch.Size([1, 3, 906, 438]) -> (120, 1200) ------------------------]
                                                  |  Reference, PIL 8.3.2, mode: RGB  |  1.10.0a0+git1e87d91
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                2.1                |          1.8        
      channels_last non-contiguous torch.float32  |                2.1                |          3.4        

Times are in milliseconds (ms).

[--------------- Downsampling: torch.Size([1, 1, 906, 438]) -> (320, 196) --------------]
                                 |  Reference, PIL 8.3.2, mode: F  |  1.10.0a0+git1e87d91
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               1.2               |          1.0        

Times are in milliseconds (ms).

[--------------- Downsampling: torch.Size([1, 1, 906, 438]) -> (460, 220) --------------]
                                 |  Reference, PIL 8.3.2, mode: F  |  1.10.0a0+git1e87d91
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               1.4               |          1.3        

Times are in milliseconds (ms).

[--------------- Downsampling: torch.Size([1, 1, 906, 438]) -> (120, 96) ---------------]
                                 |  Reference, PIL 8.3.2, mode: F  |  1.10.0a0+git1e87d91
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |              719.9              |         599.9       

Times are in microseconds (us).

[-------------- Downsampling: torch.Size([1, 1, 906, 438]) -> (1200, 196) --------------]
                                 |  Reference, PIL 8.3.2, mode: F  |  1.10.0a0+git1e87d91
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               3.7               |          3.5        

Times are in milliseconds (ms).

[-------------- Downsampling: torch.Size([1, 1, 906, 438]) -> (120, 1200) --------------]
                                 |  Reference, PIL 8.3.2, mode: F  |  1.10.0a0+git1e87d91
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |              834.4              |         605.7       

Times are in microseconds (us).



```

</details>


Code is moved from torchvision: https://github.com/pytorch/vision/pull/3761, https://github.com/pytorch/vision/pull/3810 and https://github.com/pytorch/vision/pull/4208

",pytorch
65173,rohan-varma,pr,2021-09-16T22:59:29Z,[NCCL] Init dummy NCCL comms in constructor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65173

Initializes dummy NCCL communicators in constructor for a basic health
check that communicators can be initialized prior to launching the first
collective.

After successful init, we immediately use `ncclCommAbort` to destroy these
communicators to ensure they don't interfere with regular communicator creation
during collectives.

For failures, the reason is mostly hangs when initializing the NCCL comm. To detect this it is done in a separate thread, and the main thread does a timed wait for a successful return and throws if it encounters a timeout. Other errors will just terminate the thread resulting in an application crash. 

Differential Revision: [D31005792](https://our.internmc.facebook.com/intern/diff/D31005792/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31005792/)!",pytorch
65184,ngimel,pr,2021-09-17T00:06:22Z,remove utils.cpp,"Dead code
",pytorch
65189,rohan-varma,pr,2021-09-17T01:01:55Z,Rohan fix,"Fixes #{issue number}


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @cbalioglu @gcramer23",pytorch
65262,ngimel,pr,2021-09-18T00:16:13Z,"Revert ""Revert D30558877: Ported std/var to ReductionOpInfo","Reland of #63978
",pytorch
65369,ngimel,pr,2021-09-20T20:35:58Z,THCTensor cleanup,,pytorch
65385,rohan-varma,pr,2021-09-21T00:59:34Z,[BE] Enable ZeRO test on windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65519
* __->__ #65385

Enables the ZeRO tests to run on windows. Closes
https://github.com/pytorch/pytorch/issues/63086.

Backend == NCCL was used as a proxy to see if we were running under CUDA, but Windows GPU tests uses Gloo. In this case use Gloo on GPU.

For some reason these tests don't seem to test Gloo on GPU with ZeRO in general (picks NCCL backend when GPU is available), so kept that behavior for now.

Differential Revision: [D31071181](https://our.internmc.facebook.com/intern/diff/D31071181/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65420,supriyar,pr,2021-09-21T18:15:37Z,[quant] change observer FQNs generated in prepare step,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65420

Summary:
Context: In some FB use cases we have a need to map observer stats from train model checkpoint to inference model. We observerd that some buffer names are different becuase the intermediate activation tensors
are generated differently across train and inference model.

Currently, for each observer (activation_post_process), the FQN of the module inserted is determined based on the FQN of the input tensor it is observing.

In this change we change the observer FQN to include the FQN of the op/module it is observing rather than tensor/intermediate op names along with the â€œinputâ€/â€œoutputâ€ detail.

Before
```
def forward(self, x):
    x_activation_post_process_0 = self.x_activation_post_process_0(x);  x = None
    mods1_w = self.mods1.w
    mods1_w_activation_post_process_0 = self.mods1_w_activation_post_process_0(mods1_w);  mods1_w = None
    mods1_b = self.mods1.b
    linear = torch.nn.functional.linear(x_activation_post_process_0, mods1_w_activation_post_process_0, bias = mods1_b);  x_activation_post_process_0 = mods1_w_activation_post_process_0 = mods1_b = None
    linear_activation_post_process_0 = self.linear_activation_post_process_0(linear);  linear = None
    return linear_activation_post_process_0
```

After
```
def forward(self, x):
    mods1_input_activation_post_process_0 = self.mods1_input_activation_post_process_0(x);  x = None
    mods1_w = self.mods1.w
    mods1_w_activation_post_process_0 = self.mods1_w_activation_post_process_0(mods1_w);  mods1_w = None
    mods1_b = self.mods1.b
    linear = torch.nn.functional.linear(mods1_input_activation_post_process_0, mods1_w_activation_post_process_0, bias = mods1_b);  x_activation_post_process_0 = mods1_w_activation_post_process_0 = mods1_b = None
    mods1_output_activation_post_process_0 = self.mods1_output_activation_post_process_0(linear);  linear = None
    return mods1_output_activation_post_process_0
```

Test Plan:
python test/test_quantization.py test_observer_fqn

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31088652](https://our.internmc.facebook.com/intern/diff/D31088652)",pytorch
65511,crcrpar,pr,2021-09-23T02:05:58Z,fix typo in _sharded_tensor,"per title

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65519,rohan-varma,pr,2021-09-23T04:58:15Z,[BE] Run Zero test internally,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65519
* #65385

Adds buck target so we can run this internally.

Differential Revision: [D31072784](https://our.internmc.facebook.com/intern/diff/D31072784/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31072784/)!

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65529,nkreeger,pr,2021-09-23T12:12:24Z,Auto-gen the 'mm' Op in the new lazy tensor core.,"This looks to be a straight-forward Op to port. The existing tooling auto-gens and everything seems to be working.

Are there any existing unit tests I should add/update here? I have a sample test file that I've verified with locally:

```py
from lazy_tensor_core import debug
import torch
import lazy_tensor_core
import lazy_tensor_core.debug.metrics as metrics

lazy_tensor_core._LAZYC._ltc_init_ts_backend()

torch.manual_seed(42)

device = 'lazy'
dtype = torch.float32

x = torch.randn(3, 3, device=device, dtype=dtype)
y = torch.randn(3, 3, device=device, dtype=dtype)

print(torch.mm(x, y))
print(metrics.metrics_report())
```

The diff from running the same code with the legacy op vs. the auto-generated one looks like this:
```diff
â–¶ diff -u legacy-output.txt new-output.txt
--- legacy-output.txt   2021-09-23 07:10:12.431555000 -0500
+++ new-output.txt      2021-09-23 07:11:00.441555000 -0500
@@ -3,21 +3,21 @@
         [ 0.3243,  2.8696,  2.7954]], device='lazy:0')
 Metric: DeviceLockWait
   TotalSamples: 21
-  Accumulator: 010.400us
-  ValueRate: 251.506us / second
-  Rate: 507.849 / second
-  Percentiles: 1%=000.300us; 5%=000.400us; 10%=000.400us; 20%=000.400us; 50%=000.500us; 80%=000.600us; 90%=000.700us; 95%=000.700us; 99%=000.700us
+  Accumulator: 011.800us
+  ValueRate: 279.624us / second
+  Rate: 497.635 / second
+  Percentiles: 1%=000.400us; 5%=000.400us; 10%=000.400us; 20%=000.500us; 50%=000.500us; 80%=000.600us; 90%=000.700us; 95%=000.900us; 99%=001.000us
 Metric: IrValueTensorToDataHandle
   TotalSamples: 4
-  Accumulator: 016.500us
-  ValueRate: 483.657us / second
-  Rate: 117.25 / second
-  Percentiles: 1%=001.400us; 5%=001.400us; 10%=001.400us; 20%=001.400us; 50%=003.400us; 80%=010.000us; 90%=010.000us; 95%=010.000us; 99%=010.000us
+  Accumulator: 016.200us
+  ValueRate: 472.753us / second
+  Rate: 116.729 / second
+  Percentiles: 1%=001.600us; 5%=001.600us; 10%=001.600us; 20%=001.600us; 50%=003.700us; 80%=009.200us; 90%=009.200us; 95%=009.200us; 99%=009.200us
 Metric: TensorsGraphSize
   TotalSamples: 21
   Accumulator: 81.00
-  ValueRate: 2126.05 / second
-  Rate: 551.197 / second
+  ValueRate: 2078.44 / second
+  Rate: 538.855 / second
   Percentiles: 1%=1.00; 5%=3.00; 10%=3.00; 20%=3.00; 50%=3.00; 80%=5.00; 90%=5.00; 95%=6.00; 99%=9.00
 Counter: CachedCompile
   Value: 1
@@ -57,8 +57,6 @@
   Value: 2
 Counter: lazy::lt
   Value: 1
-Counter: lazy::mm
-  Value: 1
 Counter: lazy::mul
   Value: 1
 Counter: lazy::ne
```
",pytorch
65548,r-barnes,pr,2021-09-23T17:58:53Z,Fix nullptr addition,"Summary:
Fixes
caffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)

Test Plan: Sandcastle

Differential Revision: D31148405

",pytorch
65608,cloudhan,pr,2021-09-24T09:58:58Z,Enable CUPTI for kineto by default on windows,"Retry of https://github.com/pytorch/pytorch/pull/62175

See https://github.com/pytorch/pytorch/pull/62175#issuecomment-926411151 for more information.

@malfet @gdankel ",pytorch
65621,supriyar,pr,2021-09-24T18:34:51Z,[quant] update fused_obs_fake_quant op to accept output_fake_quant argument,"Summary:
Add a new attribute to the FusedMovingAvgObsFakeQuantize that controls if the Fake Quant operation should be applied at the output of a particular layer. The motivation is to give the users additional control to control the numerics of the fake_quant operators during training. It defaults to always fake quant the output (True).

Note: We will still observer the tensors as before (only the fake_quant operation is controlled using this flag)

For example
```
input model
x -> fc1 -> fc2 -> non_quantizable_op -> fc3

After fake_quant
x -> fake_quant(x) -> fc1 -> fake_quant(fc1) -> fc2 -> fake_quant(fc2) -> non_quantizable_op -> fake_quant() -> fc3 -> fake_quantize(fc3)

With output_fake_quant disabled at the output of fc2 and fc3 (since their outputs are non-quantizable)
x -> fake_quant(x) -> fc1 -> fake_quant(fc1) -> fc2 -> non_quantizable_op -> fake_quant() -> fc3
```

Test Plan: ./buck-out/gen/caffe2/test/quantization_fx\#binary.par -r test_disable_output_fake_quant

Differential Revision: D31174526

",pytorch
65633,nkreeger,pr,2021-09-24T21:30:11Z,[LTC] Auto-gen the 'mv' and 'mv.out' Op in the new lazy tensor core.,"Moves over the ""mv"" and the ""mv_out"" op to codegen. Also cleans up the `ir::ops::Dot()` reference from https://github.com/pytorch/pytorch/pull/65529.

Verified with the following scratch code:
```py
from lazy_tensor_core import debug
import torch
import lazy_tensor_core
import lazy_tensor_core.debug.metrics as metrics

lazy_tensor_core._LAZYC._ltc_init_ts_backend()

device = 'lazy'
dtype = torch.float32

mat = torch.randn(2, 3, device=device, dtype=dtype)
vec = torch.randn(3, device=device, dtype=dtype)
print(torch.mv(mat, vec))

print(metrics.metrics_report())
```

Output diff from metrics_report:
```diff
--- old-output.txt      2021-09-24 16:23:19.188545600 -0500
+++ new-output.txt      2021-09-24 16:22:27.418545600 -0500
@@ -1,32 +1,32 @@
-tensor([0.6081, 0.8017], device='lazy:0')
+tensor([-2.8513,  2.2631], device='lazy:0')
 Metric: DeviceLockWait
   TotalSamples: 12
-  Accumulator: 006.500us
-  ValueRate: 174.291us / second
-  Rate: 321.768 / second
-  Percentiles: 1%=000.400us; 5%=000.400us; 10%=000.400us; 20%=000.400us; 50%=000.500us; 80%=000.600us; 90%=000.700us; 95%=000.900us; 99%=000.900us
+  Accumulator: 007.000us
+  ValueRate: 184.251us / second
+  Rate: 315.859 / second
+  Percentiles: 1%=000.400us; 5%=000.400us; 10%=000.400us; 20%=000.400us; 50%=000.500us; 80%=000.700us; 90%=000.800us; 95%=001.000us; 99%=001.000us
 Metric: IrValueTensorToDataHandle
-  TotalSamples: 3
-  Accumulator: 013.200us
-  ValueRate: 398.869us / second
-  Rate: 90.652 / second
-  Percentiles: 1%=001.200us; 5%=001.200us; 10%=001.200us; 20%=001.200us; 50%=003.300us; 80%=008.700us; 90%=008.700us; 95%=008.700us; 99%=008.700us
+  TotalSamples: 4
+  Accumulator: 015.300us
+  ValueRate: 453.124us / second
+  Rate: 118.464 / second
+  Percentiles: 1%=001.600us; 5%=001.600us; 10%=001.600us; 20%=001.600us; 50%=003.500us; 80%=008.600us; 90%=008.600us; 95%=008.600us; 99%=008.600us
 Metric: TensorsGraphSize
   TotalSamples: 12
-  Accumulator: 40.00
-  ValueRate: 1176.04 / second
-  Rate: 352.813 / second
-  Percentiles: 1%=1.00; 5%=1.00; 10%=1.00; 20%=2.00; 50%=3.00; 80%=5.00; 90%=5.00; 95%=7.00; 99%=7.00
+  Accumulator: 48.00
+  ValueRate: 1384.53 / second
+  Rate: 346.133 / second
+  Percentiles: 1%=1.00; 5%=1.00; 10%=3.00; 20%=3.00; 50%=3.00; 80%=5.00; 90%=6.00; 95%=9.00; 99%=9.00
 Counter: CachedCompile
-  Value: 2
+  Value: 1
 Counter: CreateLtcTensor
   Value: 29
 Counter: DestroyLtcTensor
   Value: 27
 Counter: DeviceDataCacheMiss
-  Value: 6
+  Value: 7
 Counter: UncachedCompile
-  Value: 10
+  Value: 11
 Counter: aten::_local_scalar_dense
   Value: 6
 Counter: aten::abs.out
@@ -41,12 +41,10 @@
   Value: 1
 Counter: aten::min
   Value: 1
-Counter: aten::mv
-  Value: 1
 Counter: aten::normal_
   Value: 2
 Counter: lazy::_copy_from
-  Value: 29
+  Value: 26
 Counter: lazy::_copy_from_and_resize
   Value: 6
 Counter: lazy::div
@@ -59,6 +57,8 @@
   Value: 1
 Counter: lazy::mul
   Value: 1
+Counter: lazy::mv
+  Value: 1
 Counter: lazy::ne
   Value: 3
 Counter: lazy::select
```",pytorch
65674,supriyar,pr,2021-09-27T04:24:16Z,[quant] Add support for quantization of Embedding{Bag} in dynamic quant APIs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65674

Summary:
Before this PR user had to use the eager mode static quantization APIs to quantize Embedding/EmbeddingBag modules.
With this PR they can use either the static or dynamic quantization APIs for Embedding quantization

The only qconfig supported for embedding quantization is float_qparams_weight_only_qconfig whcih is currently enforced in the from_float
method of the quantized Embedding/Embedding modules.

To combine embedding quantization with Linear dynamic quantization, user can use the qconfig_dict to specify different qconfig for each module type.

The prepare/convert APIs can still be used to quantize Embeddings, with the caveat that user need to ensure input to Embedding ops are FP32.

Addresses Issue #65185

Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31211199](https://our.internmc.facebook.com/intern/diff/D31211199)",pytorch
65675,Randl,pr,2021-09-27T05:20:51Z,Fix env variable docs for torch.distributed.run,"Accurding to https://github.com/pytorch/pytorch/blob/master/torch/distributed/argparse_util.py#L12 the env variable for pytorch elastic have prefix `PET` and not `TORCH_ELASTIC`

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65677,lezcano,pr,2021-09-27T07:46:26Z,Make empty* and *_like factory functions respect tensor subclasses,"Fixes https://github.com/pytorch/pytorch/issues/65243


cc @albanD",pytorch
65695,peterjc123,pr,2021-09-27T15:13:30Z,Add error reporting when debugging on Windows,"Fixes #47900 
",pytorch
65751,ngimel,pr,2021-09-28T17:07:42Z,don't check 0 elements for cat symbolic diff,"Summary: Fixes symbolic script grad formula for cat to correctly handle empty tensors

Test Plan: Existing tests

Differential Revision: D31208364

",pytorch
65769,rohan-varma,pr,2021-09-28T19:58:22Z,[DDP][Instrumentation] Profiling range for bucket copy,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65772
* #65771
* #65770
* __->__ #65769

Seeing some bottlenecks when copying bucket to grad, help make it more
clear here.

Differential Revision: [D31217340](https://our.internmc.facebook.com/intern/diff/D31217340/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65770,rohan-varma,pr,2021-09-28T19:58:26Z,[DDP] Log iteration in debug mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65772
* #65771
* __->__ #65770
* #65769

This logging info is printed out in debug mode, make it log the
iteration as well for clarity.

Differential Revision: [D31222132](https://our.internmc.facebook.com/intern/diff/D31222132/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65771,rohan-varma,pr,2021-09-28T19:58:31Z,[MonitoredBarrier] Fix some logging,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65772
* __->__ #65771

Fixes some logging around monitored_barrier to make it cleaner.

Differential Revision: [D31222881](https://our.internmc.facebook.com/intern/diff/D31222881/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65772,rohan-varma,pr,2021-09-28T19:58:36Z,[DDP Logging] Add iteration in error reporting,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65772
* #65771

Looking at some workloads and it would be useful to have this info.

Differential Revision: [D31224417](https://our.internmc.facebook.com/intern/diff/D31224417/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @gcramer23",pytorch
65781,r-barnes,pr,2021-09-28T21:56:15Z,LLVM-12 fix for shm_mutex,"Summary:
Fixes
```
stderr: In file included from caffe2/caffe2/contrib/shm_mutex/shm_mutex.cc:1:
caffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:28: error: anonymous non-C-compatible type given name for linkage purposes by alias declaration; add a tag name here [-Werror,-Wnon-c-typedef-for-linkage]
using TicketStruct = struct : ShmBaseHeader {
                           ^
                            TicketStruct
caffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:31: note: type is not C-compatible due to this base class
using TicketStruct = struct : ShmBaseHeader {
                              ^~~~~~~~~~~~~
caffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:7: note: type is given name 'TicketStruct' for linkage purposes by this alias declaration
using TicketStruct = struct : ShmBaseHeader {
      ^
1 error generated.
Cannot execute a rule out of process. On RE worker. Thread: Thread[main,5,main]
Command failed with exit code 1.
```

Test Plan: Sandcastle

Differential Revision: D31248938

",pytorch
65785,r-barnes,pr,2021-09-28T22:39:49Z,LLVM-12 fix for tensor_new.cpp,"Summary: Fixes offset to nullptr at fbcode/caffe2/torch/csrc/utils/tensor_new.cpp:206

Test Plan: Sandcastle

Differential Revision: D31250995

",pytorch
65795,nkreeger,pr,2021-09-29T03:43:54Z,[LTC] Auto-gen _log_softmax for TorchScript backend in LTC,"This PR introduces some changes needed to the generated code for PyTorch ops that begin with `_`. This first change introduces the `_log_softmax` op with proposed changes. All unit tests inside `test/cpp/build/test_ptltc` continue to pass.

It wasn't clear what the best naming and handling convention is with operators that have a high-level definition, but a `_` prefix. I noticed that the tensor.h/tensor_methods.cpp exposes a ""log_softmax"" without the prefix. 

Happy to make any changes that aren't wanted - just looking for some feedback if I'm making the right change here :-)",pytorch
65880,powderluv,pr,2021-09-30T05:51:51Z,Add torch-mlir MLIR Exporter to PyTorch build,"Add torch-mlir (https://github.com/llvm/torch-mlir) MLIR exporter to the PyTorch build guarded by the
USE_MLIR_EXPORTER CMake flag.

TEST=Build with and without USE_MLIR_EXPORTER and verify exporting models to MLIR
",pytorch
65883,rohan-varma,pr,2021-09-30T06:06:37Z,"Back out ""Revert D31005792: [NCCL] Init dummy NCCL comms in constructor""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65883

Original commit changeset: d8e962b8aab6

Differential Revision: [D31299350](https://our.internmc.facebook.com/intern/diff/D31299350/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
65891,cloudhan,pr,2021-09-30T07:23:15Z,Enable CUPTI for kineto by default on windows,Retry of #62175,pytorch
65937,eqy,pr,2021-09-30T17:40:51Z,"Allow scalars for aliased binary ops {`multiply`, `subtract`, `divide`}","#65868 pointed out that the ""long-form"" versions of some binary ops like `mul`, `sub`, and `div` don't match their alias's behavior when it comes to handling scalar inputs. This PR adds the missing registration in `python_arg_parser.cpp` to resolve this.

CC @ptrblck @ngimel ",pytorch
65947,ngimel,pr,2021-09-30T20:06:46Z,add a note on numerical accuracy,"Per title
Fixes #54437",pytorch
66009,r-barnes,pr,2021-10-01T17:18:11Z,Fix LLVM-12 UB in generate_proposals_op.cc,"Summary:
Fixes
```
test_trace_c10_ops (jit.test_tracer.TestTracer) ... third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374:24: runtime error: applying non-zero offset 4 to null pointer
    #0 0x7f5228f72227 in Eigen::internal::BlockImpl_dense<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, true>::BlockImpl_dense(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374
    #1 0x7f5228f7212c in Eigen::BlockImpl<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, Eigen::Dense>::BlockImpl(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:166
    #2 0x7f5228f720dc in Eigen::Block<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false>::Block(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:142
    #3 0x7f5229b0e059 in Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::FixedBlockXpr<internal::get_fixed_value<int>::value, internal::get_fixed_value<long>::value>::Type Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::block<int, long>(long, long, int, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/../plugins/BlockMethods.h:98
    #4 0x7f5229b0c5ca in caffe2::GenerateProposalsOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/generate_proposals_op.cc:348
```

Test Plan: Sandcastle

Differential Revision: D31343046

",pytorch
66015,rohan-varma,pr,2021-10-01T18:24:19Z,Fix https://github.com/pytorch/pytorch/issues/61982,"Summary:
Fixes https://github.com/pytorch/pytorch/issues/61982 by clone of
tensors in DDPSink. Only applies once for static_graph and generally for unused
params which already has overhead, so perf hit should not be an issue. Will
verify with benchmark.

Test Plan: CI

Differential Revision: D31346633



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66038,rohan-varma,pr,2021-10-01T23:29:08Z,[DataParallel] Log API Usage for tracking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66038

Will help track workflows for DP deprecation. Tested via standalone DP
script.

Differential Revision: [D31356975](https://our.internmc.facebook.com/intern/diff/D31356975/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31356975/)!",pytorch
66060,r-barnes,pr,2021-10-03T02:17:38Z,Fix LLVM-12 concat_split_op.h error,"Summary:
Fixes
```
testTumHistoryAdditionalLaser (caffe2.caffe2.fb.layers.tests.tum_history_test.TestTumHistory) ... caffe2/caffe2/operators/concat_split_op.h:363:74: runtime error: applying non-zero offset 8 to null pointer
    #0 0x7f8f39d29795 in caffe2::ConcatOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/concat_split_op.h:363
    #1 0x7f8f39c4978d in caffe2::Operator<caffe2::CPUContext>::Run(int) caffe2/caffe2/core/operator.h:987
    #2 0x7f8f381fe9c9 in caffe2::SimpleNet::Run() caffe2/caffe2/core/net_simple.cc:67
    #3 0x7f8f38ee488e in caffe2::Workspace::RunNet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) caffe2/caffe2/core/workspace.cc:289
```

Test Plan: Sandcastle

Differential Revision: D31366205

",pytorch
66063,supriyar,pr,2021-10-04T05:37:28Z,"Back out ""[quant] update fused_obs_fake_quant op to accept output_fake_quant argument""","Summary: Original commit changeset: bffe776216d0

Differential Revision: D31347042

",pytorch
66113,r-barnes,pr,2021-10-05T05:58:14Z,Speed up DataTypeToTypeMeta,"Summary:
A benchmark in which the lookup items were shuffled and then the items were looked up round-robin fashion 10M times (for a total of 140M lookups) showed:
```
Container           Time (ms)   Multiplier
std::unorderedMap       98129           1x
std::vector            205635         2.1x
folly::f14fastmap      222124         2.3x
std::map               285921      2.9137x
```
In other words, the existing container is the worst choice!

Test Plan: Sandcastle

Differential Revision: D31375117

",pytorch
66118,lezcano,pr,2021-10-05T10:07:11Z,Split matmul into `out` and non-`out` versions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66118
* #64387



cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
66154,rohan-varma,pr,2021-10-05T18:40:40Z,Skip test_multiple_groups on windows,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66154

Skips as the test is flaky:
https://github.com/pytorch/pytorch/issues/66059

Differential Revision: [D31403153](https://our.internmc.facebook.com/intern/diff/D31403153/)",pytorch
66166,rohan-varma,pr,2021-10-05T21:14:35Z,[PG Wrapper][BE] Make some methods private,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66167
* __->__ #66166

These methods should be private.

Differential Revision: [D31353020](https://our.internmc.facebook.com/intern/diff/D31353020/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66167,rohan-varma,pr,2021-10-05T21:14:39Z,"[PG Wrapper][BE] Add collective information when monitored barrier error is
raised","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66167
* #66166

raised

Sometimes due to desync we see PG wrapper monitored barrier fail. In
this case it would be useful to print the info about the collective that was
trying to run along with the actual error.

Differential Revision: [D31353021](https://our.internmc.facebook.com/intern/diff/D31353021/)",pytorch
66168,rohan-varma,pr,2021-10-05T21:23:31Z,[DDP] Flag to control moving inputs to device,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66168

Adds a flag to control whether we want to move inputs to device or
not.

Differential Revision: [D31409239](https://our.internmc.facebook.com/intern/diff/D31409239/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66191,ngimel,pr,2021-10-06T04:37:12Z,fix cosine similarity dimensionality check,"Fixes #66086
",pytorch
66214,ngimel,pr,2021-10-06T18:36:23Z,Fix cosine similarity dim checks ,"1.10 version of #66191

",pytorch
66234,r-barnes,pr,2021-10-06T23:15:27Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit.

Test Plan: `buck run //caffe2/torch/fb/sparsenn:test`

Differential Revision: D30652629



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66241,thuyen,pr,2021-10-07T05:28:15Z,[Bazel] Add CUDA build to CI,"Fixes #35316
On master, bazel cuda build is disabled due to lack of a proper `cu_library` rule. This PR:
- Add `rules_cuda` to the WORKSPACE and forward `cu_library` to `rules_cuda`. 
- Use a simple local cuda and cudnn repositories (adopted from TRTorch) for cuda 11.3.
- Fix current broken cuda build.
- Enable cuda build in CI, not just for `:torch` target but all the test binaries to catch undefined symbols. ",pytorch
66254,r-barnes,pr,2021-10-07T15:20:32Z,Remove apparently unnecessary std::remove_cv_t,"Summary: `std::decay_t` already implies dropping the const

Differential Revision: D31465856

",pytorch
66272,r-barnes,pr,2021-10-07T18:37:39Z,Wextra fix 1 for caffe2,"Differential Revision: D31475543

",pytorch
66279,r-barnes,pr,2021-10-07T19:44:01Z,"Disable ""-Wignored-qualifiers"" for vec256_bfloat16.h","Summary:
This error appears when compiling with ""-Wextra"" and cannot be resolved by fixing the code since the return type of the instrinic being passed to `map` is fixed.

Fixes:
```
caffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]
  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {
                           ^~~~~~
caffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]
  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {
                           ^~~~~~
```

Differential Revision: D31480888

",pytorch
66283,r-barnes,pr,2021-10-07T21:21:51Z,Fix ubsan in concat_split_op.h,"Summary:
Fixes
```
UndefinedBehaviorSanitizer: nullptr-with-nonzero-offset caffe2/caffe2/operators/concat_split_op.h:185:52
```

Test Plan: Sandcastle

Differential Revision: D31486274

",pytorch
66285,r-barnes,pr,2021-10-07T21:27:45Z,Fix ubsan resize in concat_split_op.h,"Summary:
Fixes
```
UndefinedBehaviorSanitizer: nullptr-with-nonzero-offset caffe2/caffe2/operators/concat_split_op.h:361:74 in
```

Test Plan: Sandcastle

Differential Revision: D31486487

",pytorch
66318,r-barnes,pr,2021-10-08T15:36:36Z,More irange,"Differential Revision: D31482900



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66320,r-barnes,pr,2021-10-08T16:16:22Z,Wextra fix for Tensorshape.cpp,"Summary:
Fixes
```
stderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]
    for (size_t offset = 0; offset < numel; offset++) {
                            ~~~~~~ ^ ~~~~~
stderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]
    for (size_t offset = 0; offset < numel; offset++) {
                            ~~~~~~ ^ ~~~~~
```

Test Plan: Sandcastle

Differential Revision: D31505374

",pytorch
66321,r-barnes,pr,2021-10-08T16:16:33Z,Wextra fix for Integration.cpp,"Summary:
Fixes
```
stderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (curr_shape.size() >= target_n_dim)
        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~
stderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (curr_shape.size() >= target_n_dim)
        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~
```

Test Plan: Sandcastle

Differential Revision: D31505347

",pytorch
66323,r-barnes,pr,2021-10-08T16:19:42Z,Wextra fix for CUDAApplyUtils.cuh,"Summary:
Fixes
```
/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:310:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]
  const IndexType bOffset = sizeof...(Offsets) < n ?
                            ~~~~~~~~~~~~~~~~~~ ^ ~
/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:306:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]
  const IndexType aOffset = sizeof...(Offsets) < n ?
                            ~~~~~~~~~~~~~~~~~~ ^ ~
```

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D31505428

",pytorch
66355,rohan-varma,pr,2021-10-09T00:57:41Z,skip test_nccl_timeout,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66355

see https://github.com/pytorch/pytorch/issues/66354

Differential Revision: [D31522773](https://our.internmc.facebook.com/intern/diff/D31522773/)",pytorch
66381,r-barnes,pr,2021-10-10T16:02:47Z,Wextra fix for LossCTC.cpp,"Summary:
Fixes
```
stderr: caffe2/aten/src/ATen/native/cudnn/LossCTC.cpp:83:37: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const long' [-Werror,-Wsign-compare]
  TORCH_CHECK(input_lengths_.size() == batch_size, ""input_lengths needs to have size to match batch_size"");
              ~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~
```

Test Plan: Sandcastle

Differential Revision: D31510217

",pytorch
66393,rohan-varma,pr,2021-10-11T02:16:53Z,"Back out ""Revert D31299350: Back out ""Revert D31005792: [NCCL] Init dummy NCCL comms in constructor""""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66393

Third try!

Fixes:
- test_nccl_timeout can be flaky because of 1s timeout, bump up the timeout to resolve the flakiness. But in general we should not have been relying on time.sleep for this test, filed https://github.com/pytorch/pytorch/issues/66354 to track that.
- ciflow/all did not actually run tests due to a bug causing multigpu tests to not be run. This has since been fixed.

Differential Revision: [D31534735](https://our.internmc.facebook.com/intern/diff/D31534735/)",pytorch
66394,rohan-varma,pr,2021-10-11T02:22:08Z,Skip test_nccl_errors_nonblocking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66393
* __->__ #66394

Skips this test as it currently does not seem to pass after several
internal local runs.

Differential Revision: [D31534806](https://our.internmc.facebook.com/intern/diff/D31534806/)",pytorch
66417,vfdev-5,pr,2021-10-11T12:04:41Z,Updated error message for nn.functional.interpolate,"Description:
- Updated error message for nn.functional.interpolate

Fixes #63845

cc @vadimkantorov ",pytorch
66419,vfdev-5,pr,2021-10-11T13:20:17Z,Exposed `recompute_scale_factor` into nn.Upsample,"Description:
- Exposed recompute_scale_factor into nn.Upsample such that recompute_scale_factor=True option could be used

Context: https://github.com/pytorch/pytorch/pull/64501#discussion_r710205190
",pytorch
66463,ngimel,pr,2021-10-12T00:03:41Z,fix normal with empty std,"Fixes #65709, regression was caused by #52565",pytorch
66524,ngimel,pr,2021-10-12T22:58:39Z,fix normal with empty std,"1.10 version of #66463
",pytorch
66536,silvasean,pr,2021-10-13T03:02:57Z,Add simple `torch.mlir.export` function and demo notebook.,"Please read the demo notebook ([rendered](https://github.com/silvasean/pytorch/blob/torch.mlir.export/torch/mlir/Demo.ipynb)), as it has some further questions for the
PT devs.

This is not ""intended for being commited"". Mainly a discussion point.
",pytorch
66555,ngimel,pr,2021-10-13T16:25:55Z,remove a few unused THCTensor/Storage methods,"Per title
",pytorch
66574,chrisyeh96,pr,2021-10-13T21:04:37Z,[DOC] Improve Transformer documentation,Includes adding some typing annotations to TransformerEncoderLayer and TransformerDecoderLayer,pytorch
66583,chrisyeh96,pr,2021-10-13T22:07:15Z,[DOC] Fix typo in KLDivLoss,Fix simple typo.,pytorch
66643,r-barnes,pr,2021-10-14T18:25:34Z,Fix Wextra issues in Half.h,"Summary:
Fixes:
```
caffe2/c10/util/Half.h:456:14: error: comparison of integers of different signs: 'long' and 'unsigned long' [-Werror,-Wsign-compare]
    return f > limit::max() ||
           ~ ^ ~~~~~~~~~~~~
```

Differential Revision: D31656816

",pytorch
66645,r-barnes,pr,2021-10-14T18:39:09Z,Fix torch.cholesky deprecation warning,"Summary:
Fixes:
```
test_cholesky_solve_batched_broadcasting_cpu_complex128 (__main__.TestLinalgCPU) ... test_linalg.py:3099: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.
```

Test Plan: Sandcastle

Differential Revision: D31635851

",pytorch
66646,r-barnes,pr,2021-10-14T18:44:54Z,Suppress deprecated copy in vec256_qint.h,"Differential Revision: D31660387

",pytorch
66680,rohan-varma,pr,2021-10-15T06:25:16Z,[DDP] Track models with sync bn,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66680

Closes https://github.com/pytorch/pytorch/issues/66215. Tracks models
with sync BN so we can find workflows that use them and target for perf
optimization.

Differential Revision: [D31679477](https://our.internmc.facebook.com/intern/diff/D31679477/)",pytorch
66741,r-barnes,pr,2021-10-15T23:37:45Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705360

",pytorch
66742,r-barnes,pr,2021-10-15T23:38:14Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705366

",pytorch
66743,r-barnes,pr,2021-10-15T23:38:56Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705359

",pytorch
66744,r-barnes,pr,2021-10-15T23:40:10Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705358



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66746,r-barnes,pr,2021-10-15T23:40:58Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705361

",pytorch
66747,r-barnes,pr,2021-10-15T23:40:59Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D31705365

",pytorch
66753,r-barnes,pr,2021-10-16T04:05:47Z,Fix sign warnings in CUDA kernels,"Summary:
Fixes:
```
stderr: caffe2/aten/src/ATen/native/cuda/UnarySignKernels.cu: In lambda function:
caffe2/aten/src/ATen/native/cuda/UnarySignKernels.cu:49:72: error: comparison is always false due to limited range of data type [-Werror=type-limits]
   49 |   AT_DISPATCH_ALL_TYPES_AND2 (https://github.com/pytorch/pytorch/commit/b60050e96a44e6d068bbfa0c3f61000eaf460404)(kBFloat16, ScalarType::Half, iter.input_dtype(), ""signbit_cuda"", [&]() {
      |                                                                      ~~^~~
stderr: caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu: In lambda function:
caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu:99:86: error: comparison is always false due to limited range of data type [-Werror=type-limits]
   99 |     AT_DISPATCH_INTEGRAL_TYPES(dtype, ""div_floor_cuda"", [&]() {
      |                                                                                      ^
caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu:99:97: error: comparison is always false due to limited range of data type [-Werror=type-limits]
   99 |     AT_DISPATCH_INTEGRAL_TYPES(dtype, ""div_floor_cuda"", [&]() {
      |                                                                                                 ^
```
I thought I'd fixed this previously using `std::is_unsigned`, but apparently not.

Differential Revision: D31708173

",pytorch
66761,ngimel,pr,2021-10-17T02:27:21Z,forward fix for #66480,"per title
",pytorch
66767,supriyar,pr,2021-10-17T16:47:57Z,[quant][[fx] update observer_fqn to not depend on node.name,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66878
* __->__ #66767

Summary:
Make observer fqn in prepare step independent of input_node/observed_node name.
This change names the observers as `{input/output}_activation_post_process_{idx}` where idx will be incremented for each new observer instance and is guaranteed to be unique.

Test Plan:
python test/test_quantization.py test_observer_fqn

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31752052](https://our.internmc.facebook.com/intern/diff/D31752052)",pytorch
66776,ngimel,pr,2021-10-18T06:02:18Z,test magma_init,,pytorch
66790,vfdev-5,pr,2021-10-18T13:27:06Z,Fixes CUDA vs CPU consistency for index_put_ when accumulating,"Fixes #39227 
Fixes #66495 (duplicate of 39227)

Description:
- Expands values for CUDA implementation
- Improved shapes checking for CUDA
- Improved error message for CUDA
- Added tests

cc @zou3519 
",pytorch
66812,ngimel,pr,2021-10-18T19:40:03Z,test addr type promotion in a single test,"Fixes #66802
Test time goes from 150s to 15s.",pytorch
66852,cloudhan,pr,2021-10-19T02:49:46Z,Enable kineto integration test on windows with flakiness fix,"Split flakiness fix out, originally in #65891",pytorch
66877,r-barnes,pr,2021-10-19T16:09:39Z,Fix metal issues with irange,"Summary:
Fixes (hopefully):
```
program_source:516:27: error: use of undeclared identifier 'c10'
    for (const auto idx : c10::irange(4)) {
                          ^
program_source:590:27: error: use of undeclared identifier 'c10'
    for (const auto idx : c10::irange(4)) {
                          ^
program_source:810:26: error: use of undeclared identifier 'c10'
    for (const auto iy : c10::irange(roi_bin_grid_h)) {
                         ^
program_source:811:30: error: use of undeclared identifier 'c10'
        for (const auto ix : c10::irange(roi_bin_grid_w)) {
                             ^

DeviceName: AMD Radeon Pro 5500M, LanguageVersion: 131075
Exception raised from -[MetalContext available] at xplat/caffe2/aten/src/ATen/native/metal/MetalContext.mm:66 (most recent call first):
(no backtrace available)
```

Test Plan: Sandcastle

Reviewed By: benb

Differential Revision: D31763270

",pytorch
66878,supriyar,pr,2021-10-19T16:27:10Z,[quant][fx] Add an option in convert_fx to accept qconfig_dict to skip quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66878
* #66767

Summary:
Currently convert_fx quantizes all layers that have been prepared, depending on the prepare qconfig_dict
This PR adds support to accept a variation of qconfig_dict in convert_fx that can be used to specify skip quantizing certain layers

This can help with prepare/observe all operators, quantize a subset of them (based on quantization error), to avoid preparing multiple times.

The qconfig_dict passed to convert_fx can only have the values set to `None`, with the keys being the same as what is allowed in the prepare qconfig_dict

Test Plan:
python test/test_quantization.py TestQuantizeFx.test_convert_qconfig_dict

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31808247](https://our.internmc.facebook.com/intern/diff/D31808247)",pytorch
66897,r-barnes,pr,2021-10-19T21:43:51Z,Wextra fix for NamedTensor.cpp,"Summary:
Fixes:
```
stderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (order_idx >= ellipsis_idx) {
        ~~~~~~~~~ ^  ~~~~~~~~~~~~
stderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]
    if (order_idx >= ellipsis_idx) {
        ~~~~~~~~~ ^  ~~~~~~~~~~~~
```

Test Plan: Sandcastle

Differential Revision: D31774623

",pytorch
66901,r-barnes,pr,2021-10-19T23:10:33Z,Pyre infer dper3_models,"Differential Revision: D31705975

",pytorch
66920,eqy,pr,2021-10-20T01:28:41Z,Do rowwisemoments computation in `float` for `half` `LayerNorm`,#66707,pytorch
66932,lezcano,pr,2021-10-20T11:02:07Z,Remove check_errors from LU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66934
* #66933
* __->__ #66932

This is BC breaking in ATen but not in the PyTorch frontend (at least
not unless one accesses `torch._C`).

Fixes https://github.com/pytorch/pytorch/issues/64014

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
66933,lezcano,pr,2021-10-20T11:02:11Z,Add linalg.lu_factor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* #68184
* #68183
* #67014
* #66934
* #67789
* __->__ #66933

This PR exposes `torch.lu` as `torch.linalg.lu_factor` and
`torch.linalg.lu_factor_ex`.

This PR also adds support for matrices with zero elements both in
the size of the matrix and the batch. Note that this function simply
returns empty tensors of the correct size in this case.

We add a test and an OpInfo for the new function.

This PR also adds documentation for this new function in line of
the documentation in the rest of `torch.linalg`.

Fixes https://github.com/pytorch/pytorch/issues/56590
Fixes https://github.com/pytorch/pytorch/issues/64014

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32834069](https://our.internmc.facebook.com/intern/diff/D32834069)",pytorch
66934,lezcano,pr,2021-10-20T11:02:16Z,Make linalg.lu_factor structured,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* #68184
* #68183
* #67014
* __->__ #66934
* #67789
* #66933



cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32684856](https://our.internmc.facebook.com/intern/diff/D32684856)",pytorch
66956,rohan-varma,pr,2021-10-20T19:27:18Z,[FSDP] Add some comments after reading the code.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66958
* #66957
* __->__ #66956

Adds some comments I found helpful while ramping up on FSDP code.

Differential Revision: [D31780798](https://our.internmc.facebook.com/intern/diff/D31780798/)",pytorch
66957,rohan-varma,pr,2021-10-20T19:27:23Z,[FSDP] No need for list() in _get_shard,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66958
* __->__ #66957
* #66956

chunk appears to return a tuple which is enough given that we just
index to the right chunk and discard the rest.

Differential Revision: [D31780799](https://our.internmc.facebook.com/intern/diff/D31780799/)",pytorch
66958,rohan-varma,pr,2021-10-20T19:27:29Z,[FSDP] Cache _full_param_padded,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66958

It looks like this is always created in the same way during lazy init
for every iteration. But it is pretty much the same tensor every iteration, so
we can cache it.

Differential Revision: [D31783780](https://our.internmc.facebook.com/intern/diff/D31783780/)",pytorch
66959,rohan-varma,pr,2021-10-20T19:27:34Z,"[FSDP] CPU offload, params only","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66959
* #66958
* #66957
* #66956

Implements CPU offload for model parameters in FSDP.


- CPU offload class with only offload_params attribute is created
- If this is specified in FSDP ctor, model parameters are moved back to CPU after sharding in __init__
- In forward pass, model params are moved back to compute device, if necessary
- At end of forward pass, after full params are freed, shard is offloaded to CPU
- At beginning of backward pass (i.e. pre_bwd_hook), params are moved back to compute device
- Note that we do not currently move parameters back to CPU at end of backwards. This is because it will make an issue with optimizer having different device for param and grad. Seems better to address this when implementing gradient offloading.


Regarding tests:
- Makes model on GPU before wrapping with FSDP, previously `.cuda()` was called after wrapping with FSDP but this sort of defeats the purpose of CPU offload. Plus, it is verified that current version of FSDP does not work with CPU since some logic assumes CUDA, and also tests are skipped with < 2 GPUs, so remove some CPU specific code.
- Verifies non-root params are CPU offloaded after forward pass
- Verifies all params are offloaded to CPU after init.

Differential Revision: [D31785340](https://our.internmc.facebook.com/intern/diff/D31785340/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66960,rohan-varma,pr,2021-10-20T19:46:22Z,[easy] add no. of steps to error msg in fsdp_test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66960

Per title

Differential Revision: [D31805740](https://our.internmc.facebook.com/intern/diff/D31805740/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
66961,rohan-varma,pr,2021-10-20T19:48:56Z,[Not for land] Repro FSDP issue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66961

Parity check seems to hit a weird issue when moving param to CPU and then immediately back to GPU. 

Differential Revision: [D31805015](https://our.internmc.facebook.com/intern/diff/D31805015/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67014,lezcano,pr,2021-10-21T10:40:44Z,Remove unnecessary sync in linalg.det,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* #68184
* #68183
* __->__ #67014
* #66934
* #67789
* #66933

LAPACK functions return negative infos when there was an unexpected
input. This happens (for example) when the user does not specify
matrices of the correct size. We already check all this things on the
PyTorch end, so this check that induces a synchronisation is
unnecessary.

I also took this chance to avoid some code repetition in the computation
of the determinant of `P`. I also changed the use of `ExclusivelyOwned<Tensor>`
by regular `Tensors` + moving into the tuple, which should be as efficient or more.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32684851](https://our.internmc.facebook.com/intern/diff/D32684851)",pytorch
67017,vfdev-5,pr,2021-10-21T10:45:12Z,[OpInfo] Additional test case for nll_loss,"Description:
- Added a test case with weight and ignore_index both defined to sample_inputs_nll_loss (trying to keep things without combinatorical explosion)

Context: this sample may be important to functorch tests
cc @zou3519 @Lezcano 

",pytorch
67034,silvasean,pr,2021-10-21T19:15:36Z,[LTC] Fix a few build issues in my environment,"- clang 11 doesn't support `-Wno-error=maybe-uninitialized`. Just
  replace it with `-Wno-error` (I don't think we care that much about
  googletest's warnings -- we could silence all warnings with `-w`
  potentially as well).
- Use `cmake --build` instead of hardcoding `make` in `run_tests.sh`.
  This makes it more portable across cmake generators (I have been using
  ninja)
- Add a separate `cmake --build` invocation for `googletest` to appease
  the CMake Ninja generator. I'm not sure why it requires this.
",pytorch
67046,silvasean,pr,2021-10-21T21:04:16Z,Sort output of *NativeFunctions.h,"This ensures deterministic output, allowing systems like ccache to be
more effective.



cc @ezyang @bhosmer @bdhirsh",pytorch
67047,rohan-varma,pr,2021-10-21T21:09:19Z,[ez] [Docs] Missing import in example for post_local_sgd,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67059
* __->__ #67047

Fix missing import

Differential Revision: [D31841837](https://our.internmc.facebook.com/intern/diff/D31841837/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67048,ngimel,pr,2021-10-21T21:13:22Z,remove accscalar from i0 and i0e,"Removes some of the half math ops to make #64023 possible. 
",pytorch
67049,silvasean,pr,2021-10-21T21:34:05Z,[LTC] Build in C++17 mode.,"We are using it in our code already in
`CreateComputationShapeFromMetaTensors`/`CreateDTypeFromMetaTensors`:

```
lazy_tensor_core/csrc/tensor_util.h:94:50: warning: pack fold expression is a C++17 extension [-Wc++17-extensions]
      ((shape.push_back(tensors.sizes().vec())), ...);
                                                 ^
lazy_tensor_core/csrc/tensor_util.h:103:51: warning: pack fold expression is a C++17 extension [-Wc++17-extensions]
      ((dtypes.push_back(tensors.scalar_type())), ...);
                                                  ^
```

",pytorch
67053,silvasean,pr,2021-10-21T22:17:40Z,[LTC] Code-gen `sort`,"- Code-gen `sort`.
- Teach `gen_lazy_nativefunc_definition` to emit assertions that the
  number of shapes/dtypes computed is as expected.
  - Fix bug in `smooth_l1_loss_backward` uncovered by the assertion.

Test Plan:
```
test/cpp/build/test_ptltc
```
",pytorch
67059,rohan-varma,pr,2021-10-21T23:31:45Z,[DDP] logging improvements,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67059

Debugging some workflows, and sometimes the training does not finish
but I want to know whether the graph was not static. Also, log 0 for unused
parameter size if no unused params were found.

Differential Revision: [D31846669](https://our.internmc.facebook.com/intern/diff/D31846669/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67118,supriyar,pr,2021-10-22T21:35:27Z,[quant][fx] add pass to duplicate dequant nodes with multi use,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67118

Summary:
Fixes a bug in the reference pattern support for nn.Linear when the same quantized input is shared across multiple Linear nodes.

This PR adds a pass to duplicate the dequant nodes for each use so that for a case like
```
x -> quant -> dequant -> linear1 - quant1
                     |
                   linear2 - quant2
```
We duplicate the dequant nodes
```
x -> quant -> dequant1 -> linear1 - quant1
            |
          dequant2-> linear2 - quant2
```
So that we can match each pattern in the loweing step

We also add a pass to remove the extra/duplicate dequant nodes that may be leftover from the above pass if we don't lower them based on pattern match

Test Plan:
python test/test_quantization.py test_ref_pattern_multi_use

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31873511](https://our.internmc.facebook.com/intern/diff/D31873511)",pytorch
67135,rohan-varma,pr,2021-10-23T03:31:45Z,[FSDP] customizable backend in test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67135
* #67249

Add ability to use env var backend for quicker testing (and gloo2 in
the future)

Differential Revision: [D31878285](https://our.internmc.facebook.com/intern/diff/D31878285/)",pytorch
67137,supriyar,pr,2021-10-23T05:40:51Z,fix CI linter,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67137

Summary:
update type ignore and docs

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D31878954](https://our.internmc.facebook.com/intern/diff/D31878954)",pytorch
67149,ngimel,pr,2021-10-23T21:12:01Z,disallow requires_grad=True in make_tensor for integral inputs,"per title
",pytorch
67161,crcrpar,pr,2021-10-24T23:59:37Z,make `TORCH_(CUDABLAS|CUSOLVER)_CHECK` usable in custom extensions,"Make `TORCH_CUDABLAS_CHECK` and `TORCH_CUSOLVER_CHECK` available in custom extensions by exporting the internal functions called by the both macros.

Rel: https://github.com/pytorch/pytorch/issues/67073

cc @xwang233 @ptrblck ",pytorch
67189,vfdev-5,pr,2021-10-25T15:52:06Z,Fixes CUDA vs CPU consistency for index_put_ when accumulating (part 2),"Description:
- Follow up PR to #66790 to fix the tests on functorch, https://github.com/pytorch/functorch/issues/195

In functorch, a null tensor is added to the list of indices for the batch dimension in C++, but I can not find an equivalent of that in python without using `torch.jit.script`. If any other better solutions could be suggested, I'd be happy to replace the current way of testing.


cc @ngimel @zou3519 




",pytorch
67249,rohan-varma,pr,2021-10-26T07:49:50Z,[FSDP] CPU offload resubmit,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67249

Implements CPU offload for model parameters in FSDP. Rehash of https://github.com/pytorch/pytorch/pull/66959.


- CPU offload class with only offload_params attribute is created
- If this is specified in FSDP ctor, model parameters are moved back to CPU after sharding in __init__
- In forward pass, during lazy init, p._local_shard gets set to p.data so it is on CPU. We pin_memory here.
- In forward pass, in _rebuild_full_params, we move p.data back to self.compute_device if necessary. Note that we don't use the device of p._full_param_padded because we don't always have this attr, but when we do its always the same as compute_device.
- The same logic as above applies to the beginning of backwards pass.
- At end of fwd and end of bwd, `_use_param_local_shard` takes care to ensure the parameters are offloaded to CPU again, by pointing it to p._local_shard, which is always on CPU.
- Note that gradient is also offloaded in _post_backward_hook. This is so model param and grad can reside on the same device.


Regarding tests:
- We tests 3 different types of init: 1) CUDA the model before wrapping with FSDP, 2) CUDA the model after wrapping with FSDP, 3) never CUDA the model.
- Case 1 is always supported. Case 2 is not supported with CPU offload and throws an error during fwd pass. Case 3 is only supported with CPU offload at the moment.
- Verifies all params are offloaded to CPU after init.
- Verifies all params are offloaded to CPU after forward and backward.
- Verifies parity with DDP. 

Differential Revision: [D31911085](https://our.internmc.facebook.com/intern/diff/D31911085/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67294,micmelesse,pr,2021-10-26T22:37:12Z,"[ROCM] query warp size for host code, do not use C10_WARP_SIZE","ROCm supports gfx targets with 32 and 64 warp size. Device compilation
correctly handles the C10_WARP_SIZE (aka warpSize) constant. Host
compilation cannot rely on a single hard-coded value, but instead needs
to query device properties at runtime.

worked with @jeffdaily 


cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
67319,r-barnes,pr,2021-10-27T04:55:58Z,Wextra caffe2/,"Test Plan: Sandcastle

Reviewed By: pbelevich

Differential Revision: D31450501

",pytorch
67375,ngimel,pr,2021-10-27T21:15:50Z,initialize only ByteStorage in generic/,"Since storage is untyped, remove previous `generic` codegen structure, and remove cross-type copy methods. This will also make it easier to move storage copy methods from TH/THC.",pytorch
67378,b0noI,pr,2021-10-27T21:48:07Z,[WIP] Initial version of the Android workflow migration,"Not yet ready for the review. Fixes #67301
",pytorch
67405,ngimel,pr,2021-10-28T00:46:48Z,Add adaptive_max_pool OpInfo,"Per title
",pytorch
67422,r-barnes,pr,2021-10-28T05:31:33Z,Fix less_than_lowest warnings,"Differential Revision: D31951939

",pytorch
67443,lezcano,pr,2021-10-28T14:28:51Z,Fix the kl_div docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67443

Fixes https://github.com/pytorch/pytorch/issues/57459

After discussing the linked issue, we resolved that `F.kl_div` computes
the right thing as to be consistent with the rest of the losses in
PyTorch.

To avoid any confusion, these docs add a note discussing how the PyTorch
implementation differs from the mathematical definition and the reasons
for doing so.

These docs also add an example that may further help understanding the
intended use of this loss.

cc @brianjo @mruberry

Differential Revision: [D32136888](https://our.internmc.facebook.com/intern/diff/D32136888)",pytorch
67577,b0noI,pr,2021-10-29T23:49:21Z,pytorch_android_gradle_custom_build_single migrated from Circle to GHA.,"in scope of: #67301. Main changes:
* pytorch_android_gradle_custom_build_single removed from the circle (however template is still there since it is used by another similar workflow: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit, which will be migrated next)
* new GHA workflow added: pytorch_android_gradle_custom_build_single",pytorch
67578,eqy,pr,2021-10-30T00:11:52Z,Disable reduced precision reductions for fp16 GEMMs,"It appears that most NVIDIA architectures (well, at least there haven't been many reports of this issue) don't do reduced precision reductions (e.g., reducing in fp16 given fp16 inputs), but this change attempts to ensure that a reduced precision reduction is never done. The included test case currently fails on Volta but passes on Pascal and Ampere; setting this flag causes the test to pass on all three.

CC @stas00 @ngimel @ptrblck ",pytorch
67633,eqy,pr,2021-11-01T19:22:42Z,"Avoid prematurely casting GEMM parameters `alpha`, `beta` to `scalar_t`","@stas00 uncovered an issue where certain half-precision GEMMs would produce outputs that looked like the result of strange rounding behavior (e.g., `10008.` in place of `10000.`). @ptrblck suspected that this was due to the parameters being downcasted to the input types (which would reproduce the problematic output). Indeed, the GEMM and BGEMM cublas wrappers are currently converting the `alpha` and `beta` parameters to `scalar_t` (which potentially is reduced precision) before converting them back to `float`. This PR changes the ""ARGTYPE"" wrappers to use `acc_t` instead and adds a corresponding test.

CC @ngimel  ",pytorch
67668,rohan-varma,pr,2021-11-02T06:40:32Z,[PG NCCL] Disable NCCL health check,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67668

This adds an env var to enable NCCL health check, which when left unspecified, results in the check not being run. Unit tests that need to test this functionality have the env variable set. Please see internal diff for more details.

Differential Revision: [D32089763](https://our.internmc.facebook.com/intern/diff/D32089763/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D32089763/)!

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67677,lezcano,pr,2021-11-02T11:48:00Z,Remove OpInfo non-contig inputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67677

This follows
https://github.com/pytorch/pytorch/issues/63341#issuecomment-899690614

Fixes https://github.com/pytorch/pytorch/issues/67012

Note. I wrote the OpInfo for `index_fill`, so removing those inputs in
there is right. @kshitij12345 mentioned that the same thing is true for
the inputs for tile / repeat.
https://github.com/pytorch/pytorch/issues/67012#issuecomment-948537446

There are more uses of `transpose` within the OpInfos, but most of them
are for testing `mm` and `baddmm`. I did not touch those, as those
operations are so important that it won't hurt to test those more
thoroughly.

cc @mruberry

Differential Revision: [D32311729](https://our.internmc.facebook.com/intern/diff/D32311729)",pytorch
67679,lezcano,pr,2021-11-02T12:15:57Z,Fix failing test due to a bug in NumPy when using OpenBLAS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67679

implementations

Fixes https://github.com/pytorch/pytorch/issues/67675

cc @mruberry

Differential Revision: [D32368698](https://our.internmc.facebook.com/intern/diff/D32368698)",pytorch
67695,b0noI,pr,2021-11-02T17:49:15Z,generated-pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit migrated to GHA,"Summary:
in scope of: #67301. Main changes:
* generated-pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit deleted from circle
* pytorch_android_gradle_custom_build_single removed since it is no longer used
* generated-pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit added to GHA",pytorch
67725,neerajprad,pr,2021-11-03T00:11:16Z,Remove duplicate check in distributions arg validation,"Partial fix for #66800.

#61056 added a more verbose error message for distributions failing argument validation. However, it did not replace the earlier error check as was originally intended and was flagged by @xuzhao9 as being the potential cause of a perf regression in `test_eval[soft_actor_critic-cuda-eager]`. 

@xuzhao9: Is there a way for me to check if this resolves the perf issue you mentioned?

cc @VitalyFedyunin @ngimel

Note that existing tests already check for the error message and should verify that the removed lines are redundant. 

RUN_TORCHBENCH: soft_actor_critic",pytorch
67735,crcrpar,pr,2021-11-03T01:10:30Z,Resubmit #67161,"Skip building extensions if windows following https://github.com/pytorch/pytorch/pull/67161#issuecomment-958062611

Related issue: https://github.com/pytorch/pytorch/issues/67073

cc @ngimel @xwang233 @ptrblck ",pytorch
67741,neerajprad,pr,2021-11-03T03:22:39Z,Remove duplicate check in distributions arg validation,"Partial fix for #66800. (Duplicate of #67725 against pytorch/pytorch so as to trigger TorchBench)

#61056 added a more verbose error message for distributions failing argument validation. However, it did not replace the earlier error check as was originally intended and was flagged by @xuzhao9 as being the potential cause of a perf regression in `test_eval[soft_actor_critic-cuda-eager]`. 

@xuzhao9: Is there a way for me to check if this resolves the perf issue you mentioned?

cc @VitalyFedyunin @ngimel

Note that existing tests already check for the error message and should verify that the removed lines are redundant. 

RUN_TORCHBENCH: soft_actor_critic
",pytorch
67785,r-barnes,pr,2021-11-03T21:02:11Z,Try to eliminate a copy in proto_utils.cc,"Summary: Motivated by [""Inline caffe2::ArgumentHelper::GetSingleArgument @ caffe2/caffe2/utils/proto_utils.cc:417""] taking 100,559,526,000 cycles per day

Differential Revision: D32145803

",pytorch
67789,lezcano,pr,2021-11-03T21:39:21Z,Add contiguous_strides as a correct replacement of defaultStride,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* #68184
* #68183
* #67014
* #66934
* __->__ #67789
* #66933

`at::defaultStride` was added in https://github.com/pytorch/pytorch/pull/18779.
As it was noted in that PR, it differs from the actual computation of
the default strides when one or more of the dimensions of the tensor are
zero. See https://github.com/pytorch/pytorch/pull/18779#discussion_r272296140

We add two functions, `contiguous_strides` and `contiguous_strides_vec`
which correct this issue and we replace the previous (wrong) uses of
`defaultStride`.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32684852](https://our.internmc.facebook.com/intern/diff/D32684852)",pytorch
67801,rohan-varma,pr,2021-11-03T23:19:05Z,Update distributed contributing guide to show how to run one test in test_distributed_spawn,"Running one test in test_distributed_spawn is a bit confusing but possible. Add documentation to the CONTRIBUTING.md for this.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67802,rohan-varma,pr,2021-11-03T23:28:16Z,[RPC] Add exception logging to constValue(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67802

In RPC C++ code, we might sometimes call constValue() when the future actually has an exception, and in unittests we want to assert on the exception. What happens is that we get a message basically saying ""!eptr_"" which indicates there is some exception but we don't know what it is.

This diff simply adds logging for the exception and mentions that `value` over `constValue` should be used when the future can have an exception. The contract of `constValue` to throw when `eptr_` is set is still held, it is just enhanced with additional logging.

Differential Revision: [D32156552](https://our.internmc.facebook.com/intern/diff/D32156552/)",pytorch
67813,rohan-varma,pr,2021-11-03T23:53:57Z,[FSDP] Address follow up comments for CPU offload,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67813

Address Shen's comments in
https://github.com/pytorch/pytorch/pull/67249/files

Differential Revision: [D32157545](https://our.internmc.facebook.com/intern/diff/D32157545/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
67822,ngimel,pr,2021-11-04T03:41:44Z,fix bfloat16 autocast skip,"Per title
",pytorch
67832,lezcano,pr,2021-11-04T08:59:42Z,Add linalg.lu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* __->__ #67832
* #67996
* #67014
* #66934
* #67789
* #66933



cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
67833,lezcano,pr,2021-11-04T08:59:49Z,Add linalg.lu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #73806
* #73804
* #73803
* #72935
* __->__ #67833

This PR modifies `lu_unpack` by:
- Using less memory when unpacking `L` and `U`
- Fuse the subtraction by `-1` with `unpack_pivots_stub`
- Define tensors of the correct types to avoid copies
- Port `lu_unpack` to be a strucutred kernel so that its `_out` version does not incur on extra copies

Then we implement `linalg.lu` as a structured kernel, as we want to
compute its derivative manually. We do so because composing the derivatives
of `torch.lu_factor` and `torch.lu_unpack` would be less efficient.

This new function and `lu_unpack` comes with all the things it can come:
forward and backward ad, decent docs, correctness tests, OpInfo, complex support,
support for metatensors and support for vmap and vmap over the gradients.

I really hope we don't continue adding more features.

This PR also avoids saving some of the tensors that were previously
saved unnecessarily for the backward in `lu_factor_ex_backward` and
`lu_backward` and does some other general improvements here and there
to the forward and backward AD formulae of other related functions.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
67867,ngimel,pr,2021-11-04T20:02:15Z,"remove use of THGenerateAllTypes, clean up","Per title
",pytorch
67901,rohan-varma,pr,2021-11-05T07:48:05Z,[Dist CI][BE] Run each test in its own process for test_distributed_spawn,"Context: https://github.com/pytorch/pytorch/issues/67061

Use `run_test.py`'s provided flag `""--subprocess""`, passed in like `extra_unittest_args=[""--subprocess""]` when running test_distributed_spawn. This will ensure that each test is run separately in its own process. The goal is to more closely simulate how a developer would run a single test when reproducing a CI failure and make reproducibility easier in general.

Also, when a test fails, print out the exact command that was issued so developer knows how to reproduce it.

For example test fails, it will print out something like the following to logs - 

```
Test exited with non-zero exitcode 1. Command to reproduce: BACKEND=gloo WORLD_SIZE=3 /fsx/users/rvarm1/conda/envs/pytorch/bin/python distributed/test_distributed_spawn.py -v TestDistBackendWithSpawn.test_Backend_enum_class
```

running test_distributed_spawn is still the same cmd as before:

`
python test/run_test.py --verbose -i distributed/test_distributed_spawn
`

as seen in [distributed contributing](https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md) guide.",pytorch
67922,eqy,pr,2021-11-05T18:46:57Z,Add host-side memory requirement for `test_softmax_64bit_indexing`,"#67910
The original `largeTensorTest` decorator didn't account for the additional host-side memory requirements.
Thanks @crcrpar for raising the issue, CC @ptrblck ",pytorch
67940,ngimel,pr,2021-11-06T00:11:44Z,remove Generate* macro files,,pytorch
67946,eqy,pr,2021-11-06T01:47:20Z,Add an option to disable reduced precision reductions for FP16 GEMM,"#67578 disabled reduced precision reductions for FP16 GEMMs. After benchmarking, we've found that this has substantial performance impacts for common GEMM shapes (e.g., those found in popular instantiations of multiheaded-attention) on architectures such as Volta. As these performance regressions may come as a surprise to current users, this PR adds a toggle to disable reduced precision reductions
`torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = `
rather than making it the default behavior.

CC @ngimel @ptrblck
@stas00 Note that the behavior after the previous PR can be replicated with
`torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False`",pytorch
67948,crcrpar,pr,2021-11-06T02:00:29Z,Disable TF32 in `pinv_jvp` and `pinv_backward`,"Fixes #67947 

cc @ptrblck @xwang233 @zasdfgbnm ",pytorch
67977,ngimel,pr,2021-11-08T03:03:57Z,Fast cuda layer norm,"This adds apex-inspired fast layer norm forward kernel to pytorch (it is a significant rewrite though). 
It's much faster than current implementation, for a typical transformer size (32*196, 1024) time goes down from ~180us to ~49 us on Volta. Compared to apex, it also produces bitwise accurate results between float inputs representable in fp16, and fp16 inputs. It produces slightly different results compared to current implementation though, because welford summation is implemented differently. 
It is slower than lightSeq (~37 us), but lightseq uses inaccurate variance approximation, and doesn't guarantee float - fp16 bitwise accuracy. 

",pytorch
67993,vfdev-5,pr,2021-11-08T11:30:01Z,Fixed deprection warnings with `.data<T>()` in SpectalOps.cpp,"Description:
- Fixed deprection warnings `.data<T>()` -> `.data_ptr<T>()` in SpectralOps.cpp shown while building pytorch from source

```c++
../aten/src/ATen/native/mkl/SpectralOps.cpp:213:10: warning: â€˜T* at::Tensor::data() const [with T = c10::complex<double>]â€™ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.
data_ptr<T>() instead. [-Wdeprecated-declarations]
  213 |   return reinterpret_cast<std::complex<T>*>(t.data<c10::complex<T>>());
```
",pytorch
67996,lezcano,pr,2021-11-08T12:43:25Z,Generalize noncontiguous tests to several outputs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* __->__ #67996
* #68523
* #68522
* #69909
* #69908

This is necessary for most matrix decompositions in `linalg`.

cc @mruberry

Differential Revision: [D33774418](https://our.internmc.facebook.com/intern/diff/D33774418)",pytorch
68077,ngimel,pr,2021-11-09T19:32:24Z,don't hardcode mask type in mha,"Fixes #{issue number}
",pytorch
68107,crcrpar,pr,2021-11-10T05:05:54Z,[Foreach Reduction] Use `OpMathType` tensor for intermediate results,"Follow-up of https://github.com/pytorch/pytorch/pull/62646

In APEX, multi_tensor_norm only supports float and half and the dtype of `output` and `output_per_tensor` is hardcoded as single-precision (see https://github.com/NVIDIA/apex/blob/ae757634efa26a4ed852324f1d32f2159774997b/csrc/multi_tensor_l2norm_kernel.cu#L318).

But in my previous PR, any tensor created in the kernel has the same dtype as the input tensors.
I'm not quite sure why I didn't see any failures in the previous PR but internal math should be performed in 32 bits for 16-bit tensors, in my opinion.

rel: https://github.com/pytorch/pytorch/issues/58833

cc @ptrblck @mcarilli @ngimel ",pytorch
68155,rohan-varma,pr,2021-11-11T05:22:27Z,[FSDP] AutoWrap Main API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #68776
* __->__ #68155

Implements auto wrapping API as part of FSDP constructor, to make it easier for users to recursively wrap FSDP module. 
API proposal: https://github.com/pytorch/pytorch/issues/64394. 

Now, FSDP can be used as follows, with user providing a policy or using `default_auto_wrap_policy`: 

```
class model:
    def _init(self):
        self.layer1 = nn.linear()
        self.layer2 = nn.linear()
        self.layer3 = nn.linear()
fsdp_model = FSDP(model(), 
                  auto_wrap_policy=functools.partial(default_auto_wrap_policy, min_num_params=1e7))
```

note that `auto_wrap_policy` currently only applies to child modules, and the outer-most (i.e. module passed into FSDP constructor will always be wrapped with FSDP, and be an instance of FSDP since we call FSDP constructor on it). 

If we need more flexibility and run into situations where the outer module should also not be wrapped, we can have a workaround where we run the outer module like a normal nn.Module by providing a special mode for this.

## Follow up items (will file issues)
- Need to remove the dependency on ConfigAutoWrap and make _recursive_wrap work in a standalone, functional fashion, so that eventually we can remove most `wrap.py` code and consolidate one way to do wrapping. 
- Update other callsites in test code to use this wrap API
- Update lightning/other callsites --> this will help us uncover any shortcomings in the API.
- Make it easier to use policies for wrapping etc. i.e. provide a `wrap_policies` class with functions for common policies (such as # of parameters).

Differential Revision: [D32327954](https://our.internmc.facebook.com/intern/diff/D32327954/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68183,lezcano,pr,2021-11-11T16:32:26Z,Remove random_fullrank_matrix_distinc_singular_value,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* #68184
* __->__ #68183
* #67014
* #66934
* #67789
* #66933

We do so in favour of
`make_fullrank_matrices_with_distinct_singular_values` as this latter
one not only has an even longer name, but also generates inputs
correctly for them to work with the PR that tests noncontig inputs
latter in this stack.

We also heavily simplified the generation of samples for the SVD, as it was
fairly convoluted and it was not generating the inputs correclty for
the noncontiguous test.

To do the transition, we also needed to fix the following issue, as it was popping 
up in the tests:

Fixes https://github.com/pytorch/pytorch/issues/66856

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32684853](https://our.internmc.facebook.com/intern/diff/D32684853)",pytorch
68184,lezcano,pr,2021-11-11T16:32:30Z,"Merge index_{add,fill,copy,select} sampling","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* #68185
* __->__ #68184
* #68183
* #67014
* #66934
* #67789
* #66933

This was in the TODO list as the three operations are very similar.
Did this as one of them was failing in the noncontig tests and I wanted
to make sure that all of them were tested properly, as they all appear
in the derivative formulas of each other.

After this PR, these operations do pass the noncontiguous tests.

cc @mruberry

Differential Revision: [D32684854](https://our.internmc.facebook.com/intern/diff/D32684854)",pytorch
68185,lezcano,pr,2021-11-11T16:32:36Z,Prefer maybe_multiply when multiplying by a constant,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* #69908
* __->__ #68185
* #68184
* #68183
* #67014
* #66934
* #67789
* #66933

As per title

We also fix the first input to `handle_r_to_c` fo `rsub`as it was
flipped for the two inputs.

Differential Revision: [D32684855](https://our.internmc.facebook.com/intern/diff/D32684855)",pytorch
68188,lezcano,pr,2021-11-11T18:37:21Z,Remove unnecessary non_contiguous and gradient tests from test_linalg,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* __->__ #68188
* #67996
* #68523
* #68522
* #69909
* #69908

As per title

cc @mruberry @jianyuh @nikitaved @pearu @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33774419](https://our.internmc.facebook.com/intern/diff/D33774419)",pytorch
68223,rohan-varma,pr,2021-11-11T23:37:30Z,[c10d] Fix object-based collectives for debug mode,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68223

DETAIL debug mode didn't work with object-based collectives for NCCL backend, because we'd only check if backend is NCCL and then move tensors to CUDA.

Instead, check if it is a wrapped PG, and then check the pg that is wrapped to see if its nccl.

Differential Revision: [D32366840](https://our.internmc.facebook.com/intern/diff/D32366840/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68238,ngimel,pr,2021-11-12T06:15:35Z,Improve native layer norm backward perf,"Benchmarks
At this PR
```
[------------------------------------------------------ ln ------------------------------------------------------]
                  |  fwd, torch.float32  |  fwdbwd, torch.float32  |  fwd, torch.float16  |  fwdbwd, torch.float16
1 threads: -------------------------------------------------------------------------------------------------------
      200, 256    |         17.5         |          106.6          |         18.1         |           94.7
      1000, 256   |         18.7         |          116.6          |         18.7         |          110.7
      6000, 256   |         28.1         |          111.8          |         19.4         |           92.3
      6272, 256   |         29.3         |          108.5          |         20.1         |           92.7
      200, 512    |         19.3         |           83.8          |         19.1         |          116.3
      1000, 512   |         17.9         |           88.0          |         17.9         |           93.0
      6000, 512   |         36.9         |          141.2          |         27.4         |          103.3
      6272, 512   |         38.2         |          146.5          |         28.1         |          107.9
      200, 1024   |         18.1         |           89.5          |         21.1         |          102.7
      1000, 1024  |         17.9         |           88.7          |         18.5         |           92.5
      6000, 1024  |         77.6         |          277.5          |         40.3         |          148.5
      6272, 1024  |         80.7         |          288.1          |         42.0         |          154.0
      200, 1536   |         17.9         |          117.3          |         18.1         |           88.1
      1000, 1536  |         22.9         |           92.0          |         19.4         |           89.0
      6000, 1536  |        123.4         |          436.3          |         61.7         |          228.5
      6272, 1536  |        129.1         |          457.3          |         64.3         |          238.5
      200, 2048   |         18.0         |           90.5          |         19.1         |          101.6
      1000, 2048  |         31.1         |          109.8          |         25.3         |          107.9
      6000, 2048  |        174.5         |          589.8          |         87.1         |          310.5
      6272, 2048  |        182.2         |          617.0          |         91.2         |          316.7
      200, 3072   |         19.8         |           96.4          |         19.4         |           89.3
      1000, 3072  |         48.1         |          168.7          |         23.5         |          100.9
      6000, 3072  |        267.1         |          930.0          |        134.8         |          519.2
      6272, 3072  |        278.2         |          971.2          |        140.7         |          540.2
```
Pre-#67977
```
[------------------------------------------------------- ln -------------------------------------------------------]
                    |  fwd, torch.float32  |  fwdbwd, torch.float32  |  fwd, torch.float16  |  fwdbwd, torch.float16
1 threads: ---------------------------------------------------------------------------------------------------------
        200,   256  |         20.9         |            92.6         |         21.3         |          110.1
       1000,   256  |         20.3         |            91.8         |         28.1         |          115.6
       6000,   256  |         93.0         |           310.7         |         86.3         |          299.8
       6272,   256  |         97.3         |           323.5         |         90.0         |          314.1
        200,   512  |         20.9         |           110.2         |         21.1         |           95.0
       1000,   512  |         24.0         |           102.8         |         22.2         |           95.9
       6000,   512  |        121.7         |           367.2         |        105.6         |          337.4
       6272,   512  |        127.0         |           382.3         |        111.3         |          352.0
        200,  1024  |         21.0         |           131.8         |         20.4         |           93.3
       1000,  1024  |         35.5         |           108.7         |         27.7         |           99.4
       6000,  1024  |        170.4         |           495.5         |        137.7         |          411.4
       6272,  1024  |        177.5         |           517.6         |        143.6         |          428.6
        200,  1536  |         21.9         |            97.6         |         20.8         |           92.7
       1000,  1536  |         44.3         |           129.7         |         33.9         |          100.1
       6000,  1536  |        215.8         |           619.2         |        167.2         |          480.9
       6272,  1536  |        225.0         |           646.9         |        174.8         |          505.9
        200,  2048  |         21.8         |           100.8         |         20.7         |           96.7
       1000,  2048  |         53.7         |           152.4         |         41.4         |          118.3
       6000,  2048  |        267.0         |           753.6         |        220.4         |          571.5
       6272,  2048  |        278.6         |           785.8         |        211.4         |          589.2
        200,  3072  |         20.9         |           103.7         |         21.9         |          104.6
       1000,  3072  |         71.4         |           201.1         |         53.1         |          148.3
       6000,  3072  |        365.7         |          1040.3         |        262.0         |          731.5
       6272,  3072  |        382.0         |          1084.4         |        273.3         |          766.3
```
Benchmarking script 
```
import torch
from torch.utils.benchmark import Timer, Compare

results = []
for dtype in (torch.float, torch.half):
    for fs in (256, 512, 1024, 1536, 2048, 3072):
        for bs in (200, 1000, 6000, 196*32):
            ln = torch.nn.LayerNorm((fs,), device=""cuda"", dtype=dtype)
            X = torch.randn(bs, fs, device=""cuda"", dtype=dtype, requires_grad=True)
            gO = torch.rand_like(X)
            stmtfwd = ""ln(X)""
            stmtfwdbwd = ""X.grad=None; ln.zero_grad(set_to_none=True); out = ln(X); out.backward(gO)""
            tfwd = Timer(stmt=stmtfwd, label=""ln"", sub_label=f""{bs:5}, {fs:5}"", description=f""fwd, {dtype}"", globals=globals())
            tfwdbwd = Timer(stmt=stmtfwdbwd, label=""ln"", sub_label=f""{bs:5}, {fs:5}"", description=f""fwdbwd, {dtype}"", globals=globals())
            for t in (tfwd, tfwdbwd):
                results.append(t.blocked_autorange())
        print(fs, end='\r')
c = Compare(results)
c.print()
```",pytorch
68275,rohan-varma,pr,2021-11-12T20:37:10Z,skip flaky rocm tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

https://github.com/pytorch/pytorch/issues/68173 and
https://github.com/pytorch/pytorch/issues/68222

Differential Revision: [D32399724](https://our.internmc.facebook.com/intern/diff/D32399724/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
68293,r-barnes,pr,2021-11-12T22:48:10Z,Fix a sign comparison issue in BatchLinearAlgebraLib.cpp,"Differential Revision: D32403788

",pytorch
68294,r-barnes,pr,2021-11-12T22:48:54Z,Fix sign comparison issue in Histogram.cpp,"Differential Revision: D32403821

",pytorch
68335,lezcano,pr,2021-11-15T11:55:00Z,Correct `householder_product` docs.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68335

When discussing https://github.com/pytorch/pytorch/pull/63880, we
realised that the docs of `householder_product` were not correct. This
PR fixes this.

The new docs are slightly more difficult, but hopefully correct. Note
that this is a LAPACK function in disguise, so it is expected the
specification to be more difficult than normal.

cc @brianjo @mruberry @jianyuh @nikitaved @pearu @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D32429755](https://our.internmc.facebook.com/intern/diff/D32429755)",pytorch
68338,OverLordGoldDragon,pr,2021-11-15T13:11:08Z,`print` fix in `lr_scheduler`,"`{:5d}` fails for `CosineAnnealingWarmRestarts` which has float `epoch`
",pytorch
68361,r-barnes,pr,2021-11-15T17:39:31Z,Fix some sign issues,"Summary:
Fixes
```
caffe2/aten/src/ATen/FunctionalizeFallbackKernel.cpp:36:31: error: comparison of integers of different signs: 'int64_t' (aka 'long') and 'const unsigned long' [-Werror,-Wsign-compare]
    for (int64_t idx = 0; idx < num_returns; ++idx) {
                          ~~~ ^ ~~~~~~~~~~~
caffe2/aten/src/ATen/native/cuda/Sorting.cpp:87:16: error: comparison of integers of different signs: 'int64_t' (aka 'long') and 'std::vector::size_type' (aka 'unsigned long') [-Werror,-Wsign-compare]
    assert(dim < out_shape.size());
           ~~~ ^ ~~~~~~~~~~~~~~~~
```

Differential Revision: D32433063

",pytorch
68399,eqy,pr,2021-11-16T01:50:47Z,Use reduced precision switch in `test_addmm_baddbmm_overflow`,"Fixes #68125
Checking to see if actually using the switch fixes the test...

CC @mruberry @ngimel @ptrblck ",pytorch
68403,rohan-varma,pr,2021-11-16T02:21:53Z,Fix flaky test_nccl_timeout,"Fixes https://github.com/pytorch/pytorch/issues/66882

- Remove time.sleep call
- Use gloo barrier to enforce rank synchronization
- Reduce timeouts for allrduce
- Pass in timeout and call wait() in _check_for_nccl_abort()


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68496,crcrpar,pr,2021-11-17T00:36:07Z,[Docs] Mention `torch.bfloat16` in `torch.finfo`,https://pytorch.org/docs/master/type_info.html#torch.torch.finfo seems to miss `torch.bfloat16`.,pytorch
68500,guoyejun,pr,2021-11-17T02:18:03Z,gen_backend_stubs.py: fix issue when there's no autograd in backend yaml,"Fixes #{issue number}
",pytorch
68503,rohan-varma,pr,2021-11-17T03:42:56Z,[Dist CI][BE] test_c10d_nccl run in subprocess,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #68504
* __->__ #68503

Per title

Differential Revision: [D32484990](https://our.internmc.facebook.com/intern/diff/D32484990/)",pytorch
68504,rohan-varma,pr,2021-11-17T03:43:01Z,[Dist CI][BE] c10d gloo tests run in subprocess,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68504
* #68503

Per title

Differential Revision: [D32485100](https://our.internmc.facebook.com/intern/diff/D32485100/)",pytorch
68522,lezcano,pr,2021-11-17T16:09:38Z,Deactivate the tracking of gradients in sampling functions within OpInfos,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* __->__ #68522
* #69909
* #69908

Some OpInfos were inadvertibly generating samples with `grad_fn`. For
example, when using functions like `transpose()` or `conj()` on the
inputs to generate transposed or conjugated inputs. This PR corrects
this and deactivates the tracking of gradients in all the sampling
functions.

Differential Revision: [D33774420](https://our.internmc.facebook.com/intern/diff/D33774420)",pytorch
68523,lezcano,pr,2021-11-17T16:09:43Z,Make repeat_interleave respect the conj and neg bits.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* __->__ #68523
* #68522
* #69909
* #69908

As per title.

cc @ezyang @gchanan @anjali411 @dylanbespalko @mruberry @Lezcano @nikitaved

Differential Revision: [D33774421](https://our.internmc.facebook.com/intern/diff/D33774421)",pytorch
68544,rohan-varma,pr,2021-11-17T21:39:07Z,[reland] Fix flaky test_nccl_timeout ,"Fixes #66882

In addition to changes in https://github.com/pytorch/pytorch/pull/68403, add one more error check that can be raised when a collective times out

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68562,guoyejun,pr,2021-11-18T01:46:01Z,gen_backend_stubs.py: fix typo for supported_autograd,"Fixes #{issue number}
",pytorch
68600,rohan-varma,pr,2021-11-18T18:15:20Z,Fix retry on connect failure decorator,Fixes https://github.com/pytorch/pytorch/issues/68541 by checking string contains instead of exact eror,pytorch
68620,wangkuiyi,pr,2021-11-19T00:32:02Z,Add return type annoatation of type ShardedTensor,"Summary: As colin2328 and divchenko pointed out, I'd have to split D32502393 into two diffs. Thanks to wanchaol for this suggestion.

Test Plan: CI

Differential Revision: D32548878



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68633,crcrpar,pr,2021-11-19T05:20:04Z,Rewrite `torch.optim._multi_tensor.SGD` in functional form,"
Related issue: #58833
Related PR: https://github.com/pytorch/pytorch/pull/60962

cc @ptrblck @ngimel @albanD",pytorch
68670,eqy,pr,2021-11-19T20:06:48Z,"Abate unused variable warnings in ProcessGroupGlooTest, FileStoreTest.cpp",cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang,pytorch
68758,rohan-varma,pr,2021-11-22T20:31:24Z,[not for land] test run tests in subprocess,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68758

Per title

Differential Revision: [D32601189](https://our.internmc.facebook.com/intern/diff/D32601189/)",pytorch
68776,rohan-varma,pr,2021-11-22T22:35:58Z,"[FSDP] Make recursive_wrap, wrap APIs independent of ConfigAutoWrap.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69358
* #69357
* #69356
* __->__ #68776

Makes these APIs independent of ConfigAutoWrap so that they can be
used by FSDP ctor without it knowing about ConfigAutoWrap. Although this has been done in a way such that the APIs are fully backward compatible.

Also gets us one step closer to killing ConfigAutoWrap.recursive_wrap and
auto_wrap(), as we will only support enable_wrap() and wrap() moving forward.

ConfigAutoWrap.recursive_wrap and wrap() are refactored to reduce code duplication. 

Will test via unittests and FSDP benchmarks to ensure the wrapping still works.

Closes https://github.com/pytorch/pytorch/issues/68241

Differential Revision: [D32604021](https://our.internmc.facebook.com/intern/diff/D32604021/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68779,bryant1410,pr,2021-11-22T23:04:20Z,Keep the sequence or mapping type in `default_collate`,"`default_collate`, `default_convert`, and `pin_memory` convert sequences into lists. I believe they should keep the original type when possible (e.g., I have a class that inherits from `list`, which comes from a 3rd party library that I can't change, and provides extra functionality).

Note it's easy to do when the type supports an iterable in its creation but it's not always the case (e.g., `range`).

Even though this can be accomplished if using a custom `default_collate`/`default_convert`, 1) this is behavior they should support out-of-the-box IMHO, and 2) `pin_memory` still does it.

cc @VitalyFedyunin @ejguan @NivekT",pytorch
68791,guoyejun,pr,2021-11-23T01:47:49Z,native_functions.yaml: remove SparseXPU which is added by accident,"gen_backend_stubs.py will report 'assert' when generate code with
SparseXPU dispatch key for external backends, if SparseXPU is in
native_functions.yaml.",pytorch
68792,rohan-varma,pr,2021-11-23T01:51:54Z,Refactor DDP checkpoint tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #68827
* __->__ #68792

Refactor tests to be more clear what features are supported and
unsupported under certain DDP configs.

Differential Revision: [D32609498](https://our.internmc.facebook.com/intern/diff/D32609498/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68819,vfdev-5,pr,2021-11-23T18:38:06Z,"Added antialias flag to interpolate (CPU only, bicubic)","Description:
- Added antialias flag to interpolate (CPU only)
  - forward and backward for bicubic mode
  - added tests

Previous PR for bilinear, https://github.com/pytorch/pytorch/pull/65142

### Benchmarks

<details>
<summary>
Forward pass, CPU. PTH interpolation vs PIL
</summary>

Cases:
- PTH RGB 3 Channels, float32 vs PIL RGB uint8 (apples vs pears)
- PTH 1 Channel, float32 vs PIL 1 Channel Float

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 

Num threads: 1
[------------------- Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (320, 196) -------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitb0bdf58
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                4.5                |          5.2        
      channels_last non-contiguous torch.float32  |                4.5                |          5.3        

Times are in milliseconds (ms).

[------------------- Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (460, 220) -------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitb0bdf58
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                5.7                |          6.4        
      channels_last non-contiguous torch.float32  |                5.7                |          6.4        

Times are in milliseconds (ms).

[------------------- Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 96) --------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitb0bdf58
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                3.0                |          4.0        
      channels_last non-contiguous torch.float32  |                2.9                |          4.1        

Times are in milliseconds (ms).

[------------------ Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (1200, 196) -------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitb0bdf58
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                14.7               |          17.1       
      channels_last non-contiguous torch.float32  |                14.8               |          17.2       

Times are in milliseconds (ms).

[------------------ Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 1200) -------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitb0bdf58
1 threads: -------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |                3.5                |          3.9        
      channels_last non-contiguous torch.float32  |                3.5                |          3.9        

Times are in milliseconds (ms).

[---------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (320, 196) ---------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitb0bdf58
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               2.4               |          1.8        

Times are in milliseconds (ms).

[---------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (460, 220) ---------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitb0bdf58
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               3.1               |          2.2        

Times are in milliseconds (ms).

[---------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 96) ----------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitb0bdf58
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               1.6               |          1.4        

Times are in milliseconds (ms).

[--------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (1200, 196) ---------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitb0bdf58
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               7.9               |          5.7        

Times are in milliseconds (ms).

[--------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 1200) ---------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitb0bdf58
1 threads: ------------------------------------------------------------------------------
       contiguous torch.float32  |               1.7               |          1.3        

Times are in milliseconds (ms).


```

</details>


Code is moved from torchvision: https://github.com/pytorch/vision/pull/3810 and https://github.com/pytorch/vision/pull/4208

",pytorch
68821,rohan-varma,pr,2021-11-23T19:02:49Z,[RPC][Dist CI][BE] RPC tests run in subprocess,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #68822
* __->__ #68821

Continuing effort to move most distributed tests to run in subprocess
for better reproducibility + reduce flakiness.

Differential Revision: [D32624199](https://our.internmc.facebook.com/intern/diff/D32624199/)",pytorch
68822,rohan-varma,pr,2021-11-23T19:02:54Z,[Dist CI][BE] Remainder of c10d/store tests run in subprocess,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68822
* #68821

Per title, we switched over c10d_gloo and nccl and results look good
so far, so switch the rest of them as well. After the only dist tests that
won't run in subprocess are pipe and fsdp tests, which historically haven't had
much flakiness.

Differential Revision: [D32624330](https://our.internmc.facebook.com/intern/diff/D32624330/)",pytorch
68827,rohan-varma,pr,2021-11-23T19:23:16Z,[DDP][BE][Docs] Clarify checkpoint support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68827
* #68792

Add a note about current checkpoint support with DDP. Note that this
does not include the features enabled with _set_static_graph yet, as it is an
undocumented private API. Once we support static graph as beta feature in OSS
we can add to the note here.

Differential Revision: [D32624957](https://our.internmc.facebook.com/intern/diff/D32624957/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68835,ngimel,pr,2021-11-23T23:06:49Z,Reduce binary size of TensorCompare.cu,"This PR does several things
1) eliminates `where` instantiations for deprecated `byte` condition dtype, and casts `condition` to `bool` in this case. This is a perf penalty for people using deprecated calls
2) Makes `clamp_{min/max}.Tensor` overload reuse `clamp_{min/max}.Scalar` kernels if limit argument is cpu scalar, instead of instantiating `gpu_kernel_with_scalars`
3) Unifies all clamp_scalar kernels to use a single kernel with lambda picking the correct operation. I've verified that it doesn't degrade kernel performance. 
4) Eliminates redundant TensorIterator construction that `clamp` structured kernel was doing when only `min` or `max` was specified

This reduces the cubin size for TensorCompare.cu on V100 from 15751920 bytes to 7691120 bytes, with corresponding reduction in compile time.  
",pytorch
68843,b0noI,pr,2021-11-23T23:34:12Z,Android build migrated to GHA.,"All for builds of the Android (arm32/64 and x86_32/64) are not migrated to the GHA, away from circleCI. Since this part of the workflow creates final binary with all architectures in it, it was not possible to do migration step by step.",pytorch
68869,lezcano,pr,2021-11-24T09:40:42Z,Add docs entry for `adjoint`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68869

As per title.

cc @brianjo @mruberry @anjali411

Differential Revision: [D32647456](https://our.internmc.facebook.com/intern/diff/D32647456)",pytorch
68933,zhuzilin,pr,2021-11-26T08:32:50Z,Fix bug on empty GLOO_SOCKET_IFNAME_ENV,"This PR is trying to fix the no device bug when user resets the `GLOO_SOCKET_IFNAME_ENV` with

```bash
export GLOO_SOCKET_IFNAME_ENV=
```

Thank you for your time on reviewing this PR :).

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
68983,guoyejun,pr,2021-11-29T12:16:34Z,RegisterDispatchKey.cpp: remove redundant code,remove the line since line 10 has already included this header file,pytorch
69010,ngimel,pr,2021-11-29T17:58:26Z,Unify gpu compare kernels,"This unifies 6 compare ops (NE, EQ, LT, LE, GE, GT) into 2 kernels, reducing context size. Performance is better for broadcasted cases, on-par for non-broadcasted
With this PR, benchmarks for contiguous, 1M-MM, 1M-M1, op with scalar (size in MB and bandwidth in GB/s):
```
(7, 0)
    5.0,   518.4
   10.0,   666.1
   15.0,   704.4
   20.0,   734.3
   25.0,   746.5
   30.0,   757.4
   35.0,   762.6
   40.0,   770.6
   45.0,   775.0
   50.0,   780.3
   55.0,   783.1
   60.0,   785.5
   65.0,   787.9
   70.0,   788.9
   75.0,   790.6
   80.0,   790.9
   85.0,   794.2
   90.0,   795.9
   95.0,   797.1
  100.0,   798.7
    3.0,   403.8     1.0,   136.3     3.0,   388.5
    6.0,   464.2     2.0,   158.9     6.0,   771.6
    9.0,   473.9     3.0,   169.4     9.0,   691.1
   12.0,   505.2     4.0,   174.3    12.0,   687.5
   15.0,   507.2     5.0,   178.0    15.0,   723.7
   18.0,   521.1     6.0,   180.1    18.0,   736.6
   21.0,   527.8     7.0,   181.8    21.0,   744.8
   24.0,   538.6     8.0,   183.3    24.0,   757.0
   27.0,   541.0     9.0,   184.2    27.0,   762.5
   30.0,   539.1    10.0,   185.0    30.0,   770.3
   33.0,   543.9    11.0,   185.2    33.0,   775.7
   36.0,   544.9    12.0,   185.4    36.0,   781.3
   39.0,   547.4    13.0,   186.1    39.0,   785.0
   42.0,   544.1    14.0,   185.9    42.0,   788.4
   45.0,   546.8    15.0,   186.3    45.0,   790.3
   48.0,   554.3    16.0,   187.2    48.0,   793.4
   51.0,   552.5    17.0,   186.9    51.0,   795.4
   54.0,   549.4    18.0,   186.7    54.0,   797.8
   57.0,   549.2    19.0,   186.8    57.0,   799.6
   60.0,   552.2    20.0,   187.7    60.0,   801.8

```
Master
```
    5.0,   743.4
   10.0,   665.7
   15.0,   702.3
   20.0,   727.5
   25.0,   740.7
   30.0,   757.5
   35.0,   760.3
   40.0,   768.5
   45.0,   775.7
   50.0,   776.8
   55.0,   781.1
   60.0,   786.5
   65.0,   786.8
   70.0,   790.1
   75.0,   789.7
   80.0,   789.1
   85.0,   793.2
   90.0,   793.8
   95.0,   795.9
  100.0,   796.0
    3.0,   383.1     1.0,   129.0     3.0,   337.0
    6.0,   445.0     2.0,   149.6     6.0,   670.6
    9.0,   445.3     3.0,   159.6     9.0,   678.6
   12.0,   474.9     4.0,   164.1    12.0,   705.5
   15.0,   480.8     5.0,   167.2    15.0,   718.3
   18.0,   490.3     6.0,   169.1    18.0,   733.3
   21.0,   493.9     7.0,   168.5    21.0,   742.5
   24.0,   503.8     8.0,   171.9    24.0,   756.4
   27.0,   506.7     9.0,   171.3    27.0,   759.8
   30.0,   508.7    10.0,   172.4    30.0,   767.1
   33.0,   515.7    11.0,   174.2    33.0,   773.7
   36.0,   516.7    12.0,   170.4    36.0,   781.7
   39.0,   519.1    13.0,   174.4    39.0,   782.1
   42.0,   515.7    14.0,   174.1    42.0,   787.0
   45.0,   519.2    15.0,   172.7    45.0,   788.1
   48.0,   522.2    16.0,   175.4    48.0,   791.7
   51.0,   519.6    17.0,   175.1    51.0,   795.7
   54.0,   518.5    18.0,   174.8    54.0,   795.8
   57.0,   519.1    19.0,   174.4    57.0,   796.6
   60.0,   521.5    20.0,   175.6    60.0,   800.1
```
<details>
<summary>Benchmarking script </summary>

```
import torch
from matplotlib import pyplot as plt
from torch.utils.benchmark import Timer, Compare
import math
import click
print(torch.cuda.get_device_capability()) # check that we are on Volta (compute capability 7,0)
#torch.cuda.set_device(1)
# don't benchmark on anything too small, you'll see only overhead
@click.command()
@click.option('--op_str', default=""torch.gt"")
@click.option('--dtype_str', default=""float"", type=click.Choice(['float', 'half']))
def bench(op_str, dtype_str):
    if dtype_str == ""float"":
        dtype = torch.float
    elif dtype_str == ""half"":
        dtype = torch.half

    MB = 1024 * 1024
    size = MB
    results = []
    sizes = []
    for _ in range(20):
        torch.cuda.memory.empty_cache()
        a=torch.randn(int(size), device=""cuda"", dtype=dtype)
        b=torch.randn(int(size), device=""cuda"", dtype=dtype)
        t = Timer(stmt=f""{op_str}(a,b)"", label = op_str, sub_label=f""{size/MB} MB"", description=""contiguous"", globals = {""a"":a, ""b"":b})
        res = t.blocked_autorange()
        results.append(res)
        sizes.append(size)
        size +=  MB
        del a #to save memory for next iterations
        del b
    c=Compare(results)
    #print(c)
    bw=[]
    bytes=[]
    element_size = torch.tensor([], dtype=dtype).element_size()
    output_element_size = 1
    for res, size in zip(results,sizes):
        bytes_io = 2*size*element_size + output_element_size * size
        bytes.append(bytes_io/MB)
        # we'll report bandwidth in GB/s
        bw.append(bytes_io/res.median * 1e-9)
        print(f""{bytes_io/MB:7.1f}, {bw[-1]:7.1f}"")

    sizes = []
    results = [[],[],[]]

    size = MB
    for _ in range(20):
        torch.cuda.memory.empty_cache()
        M = math.floor(math.sqrt(size))
        a=torch.randn(1, M, device=""cuda"", dtype=dtype)
        b=torch.randn(M, M, device=""cuda"", dtype=dtype)
        b1 = torch.randn(M, 1, device=""cuda"", dtype=dtype)
        tb = Timer(stmt=f""{op_str}(a,b)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""MMM1"", globals = {""a"":a, ""b"":b})
        t1 = Timer(stmt=f""{op_str}(a,b1)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""M11M"", globals = {""a"":a, ""b1"":b1})
        ts = Timer(stmt=f""{op_str}(b,1.)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""scalar"", globals = {""a"":a, ""b"":b})

        res = [t.blocked_autorange() for t in (tb, t1, ts)]
        for (rl, r) in zip(results, res):
            rl.append(r)
        sizes.append(M)
        size += MB
        del a #to save memory for next iterations
        del b
    comps = [Compare(r) for r in results]
    #[print(c) for c in comps]
    bw=[[],[],[]]

    for res, res1, ress, size in zip(results[0],results[1],results[2], sizes):
        bytes_io = (size+size*size)*element_size + output_element_size * size*size #(size+size+size*size)*4
        bytes_io1 = (size+size)*element_size + output_element_size * size*size #(size+size+size*size)*4
        bytes_ios = (size*size)*element_size + output_element_size * size * size
        bytes_iol = (bytes_io, bytes_io1, bytes_ios)
        for (bw_elem, bytes_elem, res_elem) in zip(bw, bytes_iol, (res, res1, ress)):
            bw_elem.append(bytes_elem/res_elem.median * 1e-9)
        print(f""{bytes_iol[0]/MB:7.1f}, {bw[0][-1]:7.1f}"", f""{bytes_iol[1]/MB:7.1f}, {bw[1][-1]:7.1f}"",
        f""{bytes_iol[2]/MB:7.1f}, {bw[2][-1]:7.1f}"")

if __name__ == '__main__':
    bench()
```
</details>",pytorch
69027,rohan-varma,pr,2021-11-29T21:09:59Z,[Autograd/Checkpoint] Checkpoint implementation without reentrant autograd,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69060
* __->__ #69027

Resubmission of https://github.com/pytorch/pytorch/pull/62964 with the
suggestions and tests discussed in
https://github.com/pytorch/pytorch/issues/65537.

Adds a `use_reentrant=False` flag to `checkpoint` function. When
`use_reentrant=True` is specified, a checkpointing implementation that uses
SavedVariableHooks instead of re-entrant autograd is used. This makes it more
composable with things such as `autograd.grad` as well as DDP (still need to
add thorough distributed testing).

As discussed in https://github.com/pytorch/pytorch/issues/65537, the tests that we need to add are:

- [x] Gradient hooks are called once
- [x] works when input does require grads but Tensor that require grads are captures (like first layer in a nn)
- [x] works for functions with arbitrary input/output objects
- [x] distributed tests (next PR)

Note that this is only for `torch.utils.checkpoint`, if this approach overall looks good, we will do something similar for `checkpoint_sequential`. 


Differential Revision: [D32704467](https://our.internmc.facebook.com/intern/diff/D32704467/)",pytorch
69060,rohan-varma,pr,2021-11-30T02:02:39Z,[DDP Checkpointing] non-reentrant checkpoint tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69060

Saved variable hooks checkpointing was added in https://github.com/pytorch/pytorch/pull/69508, this PR adds some tests for DDP.
Specifically, we can support almost all DDP use cases with this new API, such as dynamic module with find_unused_parameters=True. One case remains to be supported, which is static_graph + non-reentrant based checkpointing. The underlying reason this does not work is https://github.com/pytorch/pytorch/issues/58111.

Tests:

- [x] layer checkpointed once with and without reentrant
- [x] layer checkpointed with unused parameters, only for non-reentrant
- [x] Checkpointing the same module twice, only for non-reentrant
- [x] checkpointing twice with static graph, for both reentrant and non-reentrant
- [x] Dynamic module checkpoint, with weight-sharing with non-reentrant
- [ ] DDP static graph checkpoint with non-reentrant

Differential Revision: [D32712126](https://our.internmc.facebook.com/intern/diff/D32712126/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69074,guoyejun,pr,2021-11-30T12:33:22Z,codegen: do not generate code for dispatch_namespaced_definitions,,pytorch
69075,crcrpar,pr,2021-11-30T13:11:41Z,Add `CUDAExtension` with cusparse to cpp extension test,"Add a small CUDA extension using `TORCH_CUDASPARSE_CHECK` to assure that `TORCH_CUDASPARSE_CHECK` is usable in `CUDAExtension` as well as `TORCH_CUDABLAS_CHECK` and `TORCH_CUSOLVER_CHECK`.

Follow-up of https://github.com/pytorch/pytorch/issues/67073

cc @xwang233 @ptrblck ",pytorch
69111,ngimel,pr,2021-11-30T20:48:07Z,unify compare kernels,"This unifies 6 compare ops (NE, EQ, LT, LE, GE, GT) into 2 kernels, reducing context size. Performance is ~5% worse for low width broadcasted cases, on-par for non-broadcasted
With this PR, benchmarks for contiguous, 1M-MM, 1M-M1, op with scalar (size in MB and bandwidth in GB/s):
```
    5.0,   795.9
   10.0,   650.5
   15.0,   706.2
   20.0,   731.6
   25.0,   744.9
   30.0,   758.1
   35.0,   762.6
   40.0,   768.8
   45.0,   775.7
   50.0,   780.7
   55.0,   781.7
   60.0,   783.0
   65.0,   784.8
   70.0,   790.7
   75.0,   789.2
   80.0,   794.4
   85.0,   794.2
   90.0,   797.4
   95.0,   796.3
  100.0,   798.0
    3.0,   363.7     1.0,   122.2     3.0,   385.5
    6.0,   420.4     2.0,   142.9     6.0,   755.5
    9.0,   438.3     3.0,   151.6     9.0,   684.5
   12.0,   449.5     4.0,   156.4    12.0,   702.9
   15.0,   463.7     5.0,   159.6    15.0,   716.8
   18.0,   472.7     6.0,   161.4    18.0,   737.0
   21.0,   477.6     7.0,   162.4    21.0,   745.6
   24.0,   480.9     8.0,   164.1    24.0,   755.4
   27.0,   483.7     9.0,   163.7    27.0,   760.7
   30.0,   487.3    10.0,   165.9    30.0,   770.4
   33.0,   491.4    11.0,   166.3    33.0,   774.3
   36.0,   492.9    12.0,   166.2    36.0,   779.0
   39.0,   494.7    13.0,   166.7    39.0,   782.5
   42.0,   491.3    14.0,   166.7    42.0,   789.0
   45.0,   495.1    15.0,   167.5    45.0,   790.0
   48.0,   499.7    16.0,   167.7    48.0,   791.8
   51.0,   496.2    17.0,   166.9    51.0,   794.0
   54.0,   497.6    18.0,   167.7    54.0,   797.4
   57.0,   497.1    19.0,   167.5    57.0,   798.6
   60.0,   498.8    20.0,   168.8    60.0,   802.1

```
Master
```
    5.0,   743.4
   10.0,   665.7
   15.0,   702.3
   20.0,   727.5
   25.0,   740.7
   30.0,   757.5
   35.0,   760.3
   40.0,   768.5
   45.0,   775.7
   50.0,   776.8
   55.0,   781.1
   60.0,   786.5
   65.0,   786.8
   70.0,   790.1
   75.0,   789.7
   80.0,   789.1
   85.0,   793.2
   90.0,   793.8
   95.0,   795.9
  100.0,   796.0
    3.0,   383.1     1.0,   129.0     3.0,   337.0
    6.0,   445.0     2.0,   149.6     6.0,   670.6
    9.0,   445.3     3.0,   159.6     9.0,   678.6
   12.0,   474.9     4.0,   164.1    12.0,   705.5
   15.0,   480.8     5.0,   167.2    15.0,   718.3
   18.0,   490.3     6.0,   169.1    18.0,   733.3
   21.0,   493.9     7.0,   168.5    21.0,   742.5
   24.0,   503.8     8.0,   171.9    24.0,   756.4
   27.0,   506.7     9.0,   171.3    27.0,   759.8
   30.0,   508.7    10.0,   172.4    30.0,   767.1
   33.0,   515.7    11.0,   174.2    33.0,   773.7
   36.0,   516.7    12.0,   170.4    36.0,   781.7
   39.0,   519.1    13.0,   174.4    39.0,   782.1
   42.0,   515.7    14.0,   174.1    42.0,   787.0
   45.0,   519.2    15.0,   172.7    45.0,   788.1
   48.0,   522.2    16.0,   175.4    48.0,   791.7
   51.0,   519.6    17.0,   175.1    51.0,   795.7
   54.0,   518.5    18.0,   174.8    54.0,   795.8
   57.0,   519.1    19.0,   174.4    57.0,   796.6
   60.0,   521.5    20.0,   175.6    60.0,   800.1
```
<details>
<summary>Benchmarking script </summary>

```
import torch
from matplotlib import pyplot as plt
from torch.utils.benchmark import Timer, Compare
import math
import click
print(torch.cuda.get_device_capability()) # check that we are on Volta (compute capability 7,0)
#torch.cuda.set_device(1)
# don't benchmark on anything too small, you'll see only overhead
@click.command()
@click.option('--op_str', default=""torch.gt"")
@click.option('--dtype_str', default=""float"", type=click.Choice(['float', 'half']))
def bench(op_str, dtype_str):
    if dtype_str == ""float"":
        dtype = torch.float
    elif dtype_str == ""half"":
        dtype = torch.half

    MB = 1024 * 1024
    size = MB
    results = []
    sizes = []
    for _ in range(20):
        torch.cuda.memory.empty_cache()
        a=torch.randn(int(size), device=""cuda"", dtype=dtype)
        b=torch.randn(int(size), device=""cuda"", dtype=dtype)
        t = Timer(stmt=f""{op_str}(a,b)"", label = op_str, sub_label=f""{size/MB} MB"", description=""contiguous"", globals = {""a"":a, ""b"":b})
        res = t.blocked_autorange()
        results.append(res)
        sizes.append(size)
        size +=  MB
        del a #to save memory for next iterations
        del b
    c=Compare(results)
    #print(c)
    bw=[]
    bytes=[]
    element_size = torch.tensor([], dtype=dtype).element_size()
    output_element_size = 1
    for res, size in zip(results,sizes):
        bytes_io = 2*size*element_size + output_element_size * size
        bytes.append(bytes_io/MB)
        # we'll report bandwidth in GB/s
        bw.append(bytes_io/res.median * 1e-9)
        print(f""{bytes_io/MB:7.1f}, {bw[-1]:7.1f}"")

    sizes = []
    results = [[],[],[]]

    size = MB
    for _ in range(20):
        torch.cuda.memory.empty_cache()
        M = math.floor(math.sqrt(size))
        a=torch.randn(1, M, device=""cuda"", dtype=dtype)
        b=torch.randn(M, M, device=""cuda"", dtype=dtype)
        b1 = torch.randn(M, 1, device=""cuda"", dtype=dtype)
        tb = Timer(stmt=f""{op_str}(a,b)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""MMM1"", globals = {""a"":a, ""b"":b})
        t1 = Timer(stmt=f""{op_str}(a,b1)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""M11M"", globals = {""a"":a, ""b1"":b1})
        ts = Timer(stmt=f""{op_str}(b,1.)"", label = op_str, sub_label=f""{M*M/MB} MB"", description=""scalar"", globals = {""a"":a, ""b"":b})

        res = [t.blocked_autorange() for t in (tb, t1, ts)]
        for (rl, r) in zip(results, res):
            rl.append(r)
        sizes.append(M)
        size += MB
        del a #to save memory for next iterations
        del b
    comps = [Compare(r) for r in results]
    #[print(c) for c in comps]
    bw=[[],[],[]]

    for res, res1, ress, size in zip(results[0],results[1],results[2], sizes):
        bytes_io = (size+size*size)*element_size + output_element_size * size*size #(size+size+size*size)*4
        bytes_io1 = (size+size)*element_size + output_element_size * size*size #(size+size+size*size)*4
        bytes_ios = (size*size)*element_size + output_element_size * size * size
        bytes_iol = (bytes_io, bytes_io1, bytes_ios)
        for (bw_elem, bytes_elem, res_elem) in zip(bw, bytes_iol, (res, res1, ress)):
            bw_elem.append(bytes_elem/res_elem.median * 1e-9)
        print(f""{bytes_iol[0]/MB:7.1f}, {bw[0][-1]:7.1f}"", f""{bytes_iol[1]/MB:7.1f}, {bw[1][-1]:7.1f}"",
        f""{bytes_iol[2]/MB:7.1f}, {bw[2][-1]:7.1f}"")

if __name__ == '__main__':
    bench()
```
</details>
",pytorch
69167,rohan-varma,pr,2021-11-30T22:02:45Z,[FileStore] log timeout in err msg,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69175
* __->__ #69167

Per title

Differential Revision: [D32736119](https://our.internmc.facebook.com/intern/diff/D32736119/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69175,rohan-varma,pr,2021-11-30T23:56:37Z,[BE] Enable C++ stacktraces for MultiProcessTestCase,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69175

Shows C++ stacktraces for python distributed tests that inherit from
MultiProcessTestCase. Closes https://github.com/pytorch/pytorch/issues/69168

Example trace - 

```
File ""/data/sandcastle/boxes/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/distributed_nccl_spawn#binary,link-tree/torch/nn/parallel/distributed.py"", line 904, in forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: test error
Exception raised from rebuild_buckets at caffe2/torch/csrc/distributed/c10d/reducer.cpp:1652 (most recent call first):
# 0  c10::get_backtrace[abi:cxx11](unsigned long, unsigned long, bool)
# 1  std::_Function_handler<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (), c10::(anonymous namespace)::GetFetchStackTrace()::$_0>::_M_invoke(std::_Any_data const&)
# 2  c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)
# 3  c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*)
# 4  c10d::Reducer::rebuild_buckets()
# 5  pybind11::cpp_function::initialize<pybind11::cpp_function::initialize<bool, c10d::Reducer, , pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::call_guard<pybind11::gil_scoped_release> >(bool (c10d::Reducer::*)(), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(c10d::Reducer*)#1}, bool, c10d::Reducer*, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::call_guard<pybind11::gil_scoped_release> >(pybind11::cpp_function::initialize<bool, c10d::Reducer, , pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::call_guard<pybind11::gil_scoped_release> >(bool (c10d::Reducer::*)(), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(c10d::Reducer*)#1}&&, bool (*)(c10d::Reducer*), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(pybind11::detail::function_call&)#1}::operator()(pybind11::detail::function_call) const
```

Differential Revision: [D32736872](https://our.internmc.facebook.com/intern/diff/D32736872/)",pytorch
69186,bryant1410,pr,2021-12-01T01:41:37Z,Fix RAdam docstring on LR default value,,pytorch
69206,peterjc123,pr,2021-12-01T11:00:19Z,add `extra_repr` for `nn.ZeroPad2d`,"Fixes https://github.com/pytorch/pytorch/issues/69205
",pytorch
69280,neerajprad,pr,2021-12-02T07:10:05Z,Implement LKJCholesky.rsample to enable gradient wrt concentration,,pytorch
69318,vfdev-5,pr,2021-12-02T21:29:35Z,[C++ API] Added missing nearest-exact mode and anti-alias flag,"Description:

Following https://github.com/pytorch/pytorch/pull/65142#issuecomment-981995692 adding missing nearest-exact mode and anti-alias flag to C++ frontend.

- https://github.com/pytorch/pytorch/pull/65142
- https://github.com/pytorch/pytorch/pull/64501


- added tests in pytorch/test/cpp/api/functional.cpp


",pytorch
69356,rohan-varma,pr,2021-12-03T09:00:43Z,[FSDP] Remove auto_wrap(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69358
* #69357
* __->__ #69356
* #68776

remove `auto_wrap` and `recursive_wrap` from wrap code, as they are not used

Differential Revision: [D32816150](https://our.internmc.facebook.com/intern/diff/D32816150/)",pytorch
69357,rohan-varma,pr,2021-12-03T09:00:53Z,"[FSDP] Kill config_auto_wrap_policy, remove policy from enable_wrap,
ConfigAutoWrap","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69358
* __->__ #69357
* #69356
* #68776

Since we only want to support enable_wrap() and wrap() manual wrapping
APIs without them accepting auto_wrap_policy, remove all this unneeded code.

Differential Revision: [D32826318](https://our.internmc.facebook.com/intern/diff/D32826318/)",pytorch
69358,rohan-varma,pr,2021-12-03T09:19:56Z,[FSDP] Enforce wrapper_cls as a mandatory kwarg in enable_wrap.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69358
* #69357
* #69356
* #68776

Enforces and raises error earlier if wrapper_cls is not provided as an
arg into enable_wrap() function. Also improves the documentation.

Differential Revision: [D32826963](https://our.internmc.facebook.com/intern/diff/D32826963/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69394,r-barnes,pr,2021-12-03T21:43:10Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format
```
for(TYPE var=x0;var<x_max;x++)
```
to the format
```
for(const auto var: irange(xmax))
```

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D32837991

",pytorch
69508,rohan-varma,pr,2021-12-07T04:04:17Z,[Reland][Autograd/Checkpoint] Checkpoint implementation without reentrant autograd,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69508

Original Phabricator Diff: D32704467

Reland, fix is to not test original checkpoint when input does not require grad as that is unsupported as documented.

Original PR body:

Resubmission of https://github.com/pytorch/pytorch/pull/62964 with the
suggestions and tests discussed in
https://github.com/pytorch/pytorch/issues/65537.

Adds a `use_reentrant=False` flag to `checkpoint` function. When
`use_reentrant=True` is specified, a checkpointing implementation that uses
SavedVariableHooks instead of re-entrant autograd is used. This makes it more
composable with things such as `autograd.grad` as well as DDP (still need to
add thorough distributed testing).

As discussed in https://github.com/pytorch/pytorch/issues/65537, the tests that we need to add are:

- [x] Gradient hooks are called once
- [x] works when input does require grads but Tensor that require grads are captures (like first layer in a nn)
- [x] works for functions with arbitrary input/output objects
- [x] distributed tests (next PR)

Note that this is only for `torch.utils.checkpoint`, if this approach overall looks good, we will do something similar for `checkpoint_sequential`.

Differential Revision: [D32902634](https://our.internmc.facebook.com/intern/diff/D32902634/)",pytorch
69533,r-barnes,pr,2021-12-07T15:56:27Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format
```
for(TYPE var=x0;var<x_max;x++)
```
to the format
```
for(const auto var: irange(xmax))
```

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Reviewed By: malfet

Differential Revision: D32837942

",pytorch
69639,ngimel,pr,2021-12-08T21:42:29Z,don't use low precision in zeta,"Per title
",pytorch
69789,supriyar,pr,2021-12-10T21:01:22Z,[quant][docs] quantized model save/load instructions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69789

Summary:
Add details on how to save and load quantized models without hitting errors
Address issue #69426
Test Plan:
cd docs
make html

preview docs page, looks good.

![image](https://user-images.githubusercontent.com/2657489/145655866-f1bf7917-6861-412b-9703-676dff63263d.png)


Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D33030991](https://our.internmc.facebook.com/intern/diff/D33030991)",pytorch
69824,lezcano,pr,2021-12-13T09:45:07Z,Move function definitions from LinearAlgebraUtils.h to LinearAlgebraUtils.cpp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70253
* #69827
* #69826
* #69825
* __->__ #69824

Out of probably laziness, we had not split LinearAlgebraUtils.h into a
header and a `.cpp` file. Instead, all the functions were defined as
static inline.

This is not a good practice in general, but also, it disallows to use
generated headers that should not be included in headers, such as
`ATen/cuda/CUDAConfig.h`, which states:

> // NB: This header MUST NOT be included from other headers; it should
> // only be included from C++ files.

This PR moves the definition of the functions into `LinearAlgebraUtils.cpp`. ~~to be able to include in the `.cpp` file the `CUDAConfig.h` header safely in a later PR in this stack.~~

It so happens that I did not need to use the `CUDAConfig.h` file in the
next PR of the stack. As such, this PR is not strictily necessary, but
I think it's worth it regardless.

I needed to sprinkle a few TORCH_API decorators on the functions that
are used both on cpu and cuda builds. If you ever encounter a linker
error when using a function from `LinearAlgebraUtils.h`, stamp its
definition with a `TORCH_API` and that should solve it @nikitaved
@IvanYashchuk

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33202688](https://our.internmc.facebook.com/intern/diff/D33202688)",pytorch
69825,lezcano,pr,2021-12-13T09:45:12Z,Add hascuSOLVER flag to Context,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70253
* #69827
* #69826
* __->__ #69825

As per title.

cc @ngimel @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33751986](https://our.internmc.facebook.com/intern/diff/D33751986)",pytorch
69826,lezcano,pr,2021-12-13T09:45:16Z,Check the availability of MAGMA / cuSOLVER when setting the Linalg backend.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70253
* #69827
* __->__ #69826
* #69825

This simplifies the logic needed to handle the defaultBackend flag in linalg functions.

cc @ngimel @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33751984](https://our.internmc.facebook.com/intern/diff/D33751984)",pytorch
69827,lezcano,pr,2021-12-13T09:45:21Z,Rewrite svd and linalg.svd as structured kernels,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70253
* __->__ #69827

In general, the current pattern allows for implementing optimisations
for all the backends in a common place (see for example the optimisation
for empty matrices).

After this PR, `torch.svd` is implemented in terms of `linalg.svd` and
`linalg.svdvals`, as expected. This makes it differentiable in the case
when `compute_uv=False`, although this is not particularly important, as
`torch.svd` will eventually be deprecated.

This PR also instantiates smaller `U` / `V` when calling cusolver_gesvdj
in the cases when `full_matrices=False` or `compute_uv=False`.

The memory for auxiliary `U` and `V` in the cases above, needed for some
cuSOLVER routines is allocated raw allocators rather than through fully
fledged tensors, as it's just a blob of memory the algorithm requests.
As the code is better structured now, it was easier to see that `U` and
`Vh` needn't be allocated when calling `svd_cusolver_gesvd`.

Now `linalg.svdvals` work as expected wrt the `out=` parameter.
Note that in the test `test_svd_memory_allocation` we were
passing a tensor of the wrong size and dtype and the test seemed to
pass...

This PR also changes the backward formula to avoid saving the input
matrix, as it's not necessary. In a follow up PR, I will clean the
backward formula and make it more numerically stable and efficient.

This PR also does a number of memory optimisations here and there, and fixes
the call to cusolver_gesvd, which were incorrect for m <= n. To test
this path, I compiled the code with a flag to unconditionally execute
the `if (!gesvdj_convergence_check.empty())` branch, and all the tests
passed.

I also took this chance to simplify the tests for these functions in
`test_linalg.py`, as we had lots of tests that were testing some
functionality that is already currently tested in the corresponding
OpInfos. I used @xwang233's feature to test both MAGMA and CUDA
backends. This is particularly good for SVD, as cuSOLVER is always
chosen over MAGMA when available, so testing MAGMA otherwise would be
tricky.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33751983](https://our.internmc.facebook.com/intern/diff/D33751983)",pytorch
69861,ngimel,pr,2021-12-13T19:14:18Z,Enable cpu scalar arguments for jiterator,"Creates analog of `gpu_kernel_with_scalars` for jiterator kernels
",pytorch
69908,lezcano,pr,2021-12-14T16:59:16Z,Implement derivatives for torch.remainder and torch.fmod wrt the second argument and update the docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* #69909
* __->__ #69908

I also took this chance to clarify a bit the documentation of these
functions.

cc @brianjo @mruberry

Differential Revision: [D33774417](https://our.internmc.facebook.com/intern/diff/D33774417)",pytorch
69909,lezcano,pr,2021-12-14T16:59:21Z,Add OpInfo test to check that floating point inputs in OpInfos have requires_grad set to True,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #67833
* #68188
* #67996
* #68523
* #68522
* __->__ #69909
* #69908

This test detected a number of sampling methods that were not generating
the samples as expected, e.g. `index_put`, `cosine_embedding`, `stft`, but
perhaps most notably the generator for `BinOps`.

It also detected that `reminder` and `fmod` did not have implemented the
backward formula for the second input. I added this in the previous PR.

Differential Revision: [D33774422](https://our.internmc.facebook.com/intern/diff/D33774422)",pytorch
69926,r-barnes,pr,2021-12-14T21:11:10Z,Try dropping Torch from typeshed_internal,"Summary: See D26410012 for example

Differential Revision: D32292834

",pytorch
69929,ngimel,pr,2021-12-14T21:46:46Z,delete TH,"Move TH<C>GenerateByteType includes into torch/csrc (the only place they are used), and we can remove TH folder altogether!
The only thing left in THC are includes left for bc compatibility. 
",pytorch
69942,micmelesse,pr,2021-12-14T23:47:05Z,[ROCM] Navi21 Enablement - fix TI num_threads for ROCm,"This pr is the first in a series of prs that will introduce support for Navi 21 GPUs. 

There is one change here which is that for ROCM we define an alternative num_threads function that returns a constant number 256 instead of warpsize dependent function used by CUDA.


cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
69955,rohan-varma,pr,2021-12-15T02:00:54Z,Prototype checkpoint_wrapper,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70025
* #70024
* __->__ #69955

Implements a checkpoint_wrapper function, which wraps nn.Module with checkpointing so user won't have to call checkpoint() everytime they want to checkpoint the module.

Currently only support for reentrant-based checkpointing is added and only tested with FSDP to unblock a use case.

Future work is to add support for activation offloading, add support for new checkpointing API, add more tests, upstream to torch.utils.checkpoint.

Differential Revision: [D33107276](https://our.internmc.facebook.com/intern/diff/D33107276/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70013,vfdev-5,pr,2021-12-15T23:40:37Z,Added AutocastCPU string,"Description:
- Added ""AutocastCPU"" string repr into `toString` method

Before
```
std::cout << c10::DispatchKey::AutocastCPU;
> UNKNOWN_TENSOR_TYPE_ID
```
and now:
```
std::cout << c10::DispatchKey::AutocastCPU;
> AutocastCPU
```",pytorch
70016,vfdev-5,pr,2021-12-16T00:30:58Z,Apply contiguous on inputs of cdist backward,"Description:
- Apply contiguous on inputs of cdist backward
- Added a test

Fixes #69997

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7 ",pytorch
70017,ngimel,pr,2021-12-16T00:40:35Z,don't compile pow kernels for non-existent case,"Per title
",pytorch
70020,supriyar,pr,2021-12-16T02:00:01Z,[quant] don't print affine/symmetric qscheme for qtensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70020

Summary:
We don't accept the qscheme information when we call `quantize_per_tensor` so printing per_tensor_affine in the qscheme is a bit misleading to the users.

In the future we could consider storing the qscheme with the tensor and accept it for quantize_per_tensor

As discussed in https://github.com/pytorch/pytorch/issues/51152

Test Plan:
```
>>> import torch
>>> x = torch.rand(2, 2)
>>> qx = torch.quantize_per_tensor(x, 1.0, 0, torch.quint8)
>>> qx
tensor([[1., 1.],
        [1., 1.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=per_tensor, scale=1.0, zero_point=0)
```

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
70024,rohan-varma,pr,2021-12-16T04:47:22Z,[Checkpoint] Activation offloading,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70025
* __->__ #70024
* #69955

Implements activation offloading for reentrant-based backward.

This is implemented by remembering devices, grad requirements and moving input tensors to CPU before we save them. Then in backward, we restore these tensors using the saved information and can re-run the computation.

It saves memory by holding one less reference to input GPU tensors to a layer, so as a result GPU memory can be claimed back once the inputs to that layer are GC'd.

Follow ups:
- Support for non-reentrant-based checkpoint will be added in future diffs
- Integration with checkpoint_wrapper (next diff)
- Additional tests for DDP, FSDP (FSDP tests are in next diff)

Differential Revision: [D33119436](https://our.internmc.facebook.com/intern/diff/D33119436/)",pytorch
70025,rohan-varma,pr,2021-12-16T04:47:27Z,[Checkpoint] Integrate activation offload to checkpoint_wrapper,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70025
* #70024
* #69955

Integrate activation offload feature to `checkpoint_wrapper`, currently only for reentrant-based checkpoint.

Tests are added for FSDP only to unblock a use case. Follow up changes will generalize the feature and test it more thoroughly.

Differential Revision: [D33152444](https://our.internmc.facebook.com/intern/diff/D33152444/)",pytorch
70039,crcrpar,pr,2021-12-16T11:48:50Z,Fix build on latest main branch of thrust - SoftMax.cu,"Similar to #69985

I think there's no any other source file  which should `#include <thrust/iterator/constant_iterator.h>` as of https://github.com/pytorch/pytorch/commit/73a6c36f1bfbf9aff04ba41cfe6ab06aa99883d9

```
mkozuki@mkozuki-srv ~/ghq/github.com/crcrpar/torch-0 master
torch-0 â¯ git rev-parse HEAD; rg -inw make_constant_iterator
73a6c36f1bfbf9aff04ba41cfe6ab06aa99883d9
aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
54:    thrust::make_constant_iterator(1),

aten/src/ATen/native/sparse/cuda/SoftMax.cu
301:      thrust::make_constant_iterator(int64_t(1)),
```

## build error

```console
#22 2048. /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMAGMA_V2 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_MPI -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -I../cmake/../third_party/cudnn_frontend/include -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -Iinclude -I../torch/csrc/distributed -I../aten/src/TH -I../aten/src/THC -I../aten/src/ATen/cuda -Icaffe2/aten/src -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -Inccl/include -I../c10/cuda/../.. -I../c10/.. -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../torch/csrc/api -I../torch/csrc/api/include -isystem=third_party/gloo -isystem=../cmake/../third_party/gloo -isystem=../cmake/../third_party/googletest/googlemock/include -isystem=../cmake/../third_party/googletest/googletest/include -isystem=../third_party/protobuf/src -isystem=/opt/conda/include -isystem=../third_party/gemmlowp -isystem=../third_party/neon2sse -isystem=../third_party/XNNPACK/include -isystem=../third_party -isystem=../cmake/../third_party/eigen -isystem=/opt/conda/include/python3.8 -isystem=/opt/conda/lib/python3.8/site-packages/numpy/core/include -isystem=../cmake/../third_party/pybind11/include -isystem=/opt/hpcx/ompi/include/openmpi -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem=/opt/hpcx/ompi/include -isystem=/usr/local/cuda/include -isystem=../third_party/ideep/mkl-dnn/third_party/oneDNN/include -isystem=../third_party/ideep/include -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20236 -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -Xcompiler=-fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Xcompiler=-Wall,-Wextra,-Wno-unused-parameter,-Wno-unused-variable,-Wno-unused-function,-Wno-unused-result,-Wno-unused-local-typedefs,-Wno-missing-field-initializers,-Wno-write-strings,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-sign-compare,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-error=deprecated-declarations,-Wno-missing-braces,-Wno-maybe-uninitialized -DTORCH_CUDA_BUILD_MAIN_LIB -Xcompiler -pthread -std=c++14 -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/sparse/cuda/SoftMax.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/sparse/cuda/SoftMax.cu.o.d -x cu -c ../aten/src/ATen/native/sparse/cuda/SoftMax.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/sparse/cuda/SoftMax.cu.o
#22 2048. ../aten/src/ATen/native/sparse/cuda/SoftMax.cu(301): error: namespace ""thrust"" has no member ""make_constant_iterator""
...
#22 2048. 13 errors detected in the compilation of ""../aten/src/ATen/native/sparse/cuda/SoftMax.cu"".
```

cc @xwang233 @zasdfgbnm @ptrblck ",pytorch
70104,bryant1410,pr,2021-12-17T13:57:27Z,Add a missing precondition to `DistributedSampler` docstring,"Distributed sampler sets different indices for different processes. By doing this, it assumes that the data is the same across the board and in the same order. This may seem trivial, however, there are times that users don't guarantee the order items are gonna have, because they rely on something such as the order the filesystem lists a directory (which is not guaranteed and may vary on different computers), or the order a `set` is iterated.

I think it's better to make it clearer.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70157,ngimel,pr,2021-12-18T19:27:33Z,Low precision support for jiterator,"This adds support for bfloat16 and fp16 types for jiterator by adding at::Half and at::BFloat16 classes to the jiterator code template. The only methods defined in those classes are construction from float and implicit conversion to float. Mathematical operations on them never need to be defined, because jiterator is written in a way to implicitly upcast the inputs to the functor, so all math has to be performed on float only (e.g. compute part of the kernel would always be written as 
```
        out[j] = i0<float>(arg0[j]);
```
It also adds support for casting to complex outputs, by adding a similar templated class c10::complex<T>. Originally I planned to only support float -> complex complex for it, but to compile fetch_and_cast function we also need complex -> float conversion. We can avoid it by compiling fetch_and_cast for a different subset of types, but I'm not doing it in this PR. Thus, technically, we can compile a kernel that would accept complex inputs and produce wrong results, but we are guarding against it by static asserting that none of the functor datatype are complex, and runtime-checking that none of the inputs are complex.
Adding bfloat16, half and complex support allows us to remove special handling for type promotion tests for gcd.
i0 (that supports half and bfloat16 inputs) is moved to use jiterator. 
 
",pytorch
70164,rohan-varma,pr,2021-12-19T21:38:38Z,[Checkpoint] Make checkpoint_wrapper an nn.Module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70165
* __->__ #70164

Implement Alban's suggestion to make checkpoint_wrapper an nn.Module
instead of patching the forward pass, which is too hacky.

Differential Revision: [D33214696](https://our.internmc.facebook.com/intern/diff/D33214696/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70165,rohan-varma,pr,2021-12-19T21:38:43Z,[FSDP/Checkpoint] Activation offload support in checkpoint_wrapper,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70165

Implements activation offload support in checkpoint_wrapper API via
save_on_cpu hooks. We avoid modifying the torch.utils.checkpoint implementation
and instead compose offload + checkpoint using the save_on_cpu hook for the
former.

Differential Revision: [D33228820](https://our.internmc.facebook.com/intern/diff/D33228820/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70168,ngimel,pr,2021-12-20T02:26:59Z,fix cpu binary size increase for clamp,"Per title
",pytorch
70169,ngimel,pr,2021-12-20T02:56:13Z,reduce the number of instantiations for bernoulli tensor tensor kernel,"Reduces the binary size of DistributionBernoulli.cu 12282600 -> 3946792
Tensor-tensor bernoulli kernels are rarely used, we limit dispatches to double probability type for double `self` tensor, and `float` probability type for everything else. This would be a minor perf hit if probability tensor is of the different dtype, but given how rarely these kernels are used (and how rarely the probability tensor is not float) this is not a problem. ",pytorch
70178,vfdev-5,pr,2021-12-20T11:30:27Z,Fixed typo in torch check for cdist,"Description:
- Fixed typo in torch check for cdist

cc @zou3519 

",pytorch
70213,thuyen,pr,2021-12-20T23:51:49Z,[Bazel] Remove TH/THC in BUILD.bazel,"Codes in TH/THC were removed, this PR removes references to them in `BUILD.bazel`.
",pytorch
70218,rohan-varma,pr,2021-12-21T00:03:24Z,Minor optimization to save_on_cpu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70218

Offload to CPU by using `tensor.to(""cpu"", non_blocking=True)` instead of allocating an empty tensor and then non-blocking copy to it.

Also, cache `torch.cuda.is_available()` in the constructor.

Overall this provides a 3-4% win on some workloads with large model + large activations that we are testing.

Differential Revision: [D33246189](https://our.internmc.facebook.com/intern/diff/D33246189/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D33246189/)!",pytorch
70219,ngimel,pr,2021-12-21T00:35:34Z,Change TH_BLAS_MKL into AT_MKL_ENABLED() (#69419),"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69419

Test Plan: Imported from OSS

Differential Revision: D33246758

Pulled By: ngimel

",pytorch
70248,r-barnes,pr,2021-12-21T16:23:25Z,use irange for loops,"Summary:
Modified loops in files under fbsource/fbcode/caffe2/ from the format
```
for(TYPE var=x0;var<x_max;x++)
```
to the format
```
for(const auto var: irange(xmax))
```

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Differential Revision: D32813863

",pytorch
70253,lezcano,pr,2021-12-21T18:58:54Z,Implement forward AD for linalg.svd and improve svd_backward,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70253
* #69827

I included a derivation of the formula in the complex case, as it is
particularly tricky. As far as I know, this is the first time this formula
is derived in the literature.

I also implemented a more efficient and more accurate version of svd_backward.
More importantly, I also added a lax check in the complex case making sure the loss
function just depends on the subspaces spanned by the pairs of singular
vectors, and not their joint phase.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33751982](https://our.internmc.facebook.com/intern/diff/D33751982)",pytorch
70259,ngimel,pr,2021-12-21T21:11:55Z,AT_MKL_SEQUENTIAL and build changes (#69419),"Re-land of  https://github.com/pytorch/pytorch/pull/69419
",pytorch
70260,rohan-varma,pr,2021-12-21T21:34:33Z,[BE][DDP] Fix inplace modification issue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70260

When DDP forward pass is called twice (i.e. in checkpointing) and module has buffers (i.e. BatchNorm layer), DDP fails with the error:

```
 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
```

See https://github.com/pytorch/pytorch/issues/66504, https://discuss.pytorch.org/t/why-i-occur-runtimeerror-if-i-use-distributeddataparallel/139793 for examples where this has occured in the wild.

The problematic line is https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/comm.cpp#L34, where we do an inplace copy_ that autograd raises an error about.

The fix is to avoid the inplace `copy_` and instead directly assign the tensor to the output. This should be functionally the same and the assignment should bump the refcount of the tensor and it shouldn't be freed from underneath us.

Differential Revision: [D33262177](https://our.internmc.facebook.com/intern/diff/D33262177/)",pytorch
70276,ngimel,pr,2021-12-22T00:22:54Z,make inverse call linalg_inv,"`linalg.inv` and `inverse` are aliases according to documentation, yet their implementation is somewhat diverged. This makes `inverse` call into `linalg_inv`. 
",pytorch
70278,rohan-varma,pr,2021-12-22T01:28:31Z,[RPC] Time out the proceed signal according to rpc timeout,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70278

threading.event.wait() will block forever if a timeout is not passed.
We're seeing several tests hang here, so pass in the timeout and add better
error handling to improve the debug.

Differential Revision: [D33271775](https://our.internmc.facebook.com/intern/diff/D33271775/)",pytorch
70288,ngimel,pr,2021-12-22T05:21:31Z,don't unsqueeze every stack arg if possible,"Fixes T98738497
Use `cat` and `view` if possible, instead of unsqueezing every arg. Helps perf when there are a lot of small arguments to `stack`.
Benchmark:
```
import torch
from torch.utils.benchmark import Timer

inputs =  [torch.randn([1, 128]) for _ in range(500)]
out = torch.empty(1,500,128)
def stack_cat(inputs):
    cat_result = torch.concat(inputs, dim=1)
    return cat_result.view( [1, 500, 128])

timer_stack = Timer(stmt=""torch.stack(inputs, dim=1)"", globals=globals())
timer_cat = Timer(stmt=""stack_cat(inputs)"", globals=globals())
print(""stack "", timer_stack.blocked_autorange().median)
print(""cat "", timer_cat.blocked_autorange().median)
```
Before:
```
stack  0.00023390522226691247
cat  7.437262553721667e-05
```
After
```
stack  7.397504318505526e-05
cat  7.37407322973013e-05
```
",pytorch
70323,supriyar,pr,2021-12-22T20:26:26Z,[quant] Refactor Quantizer to remove the Affine keyword,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70324
* __->__ #70323

Summary:
the current Quantizer only uses the Affine keyword even though it may be used for quantized tensors created using affine/symmetric qscheme.
This PR refactors the code to remove the affine keyword from the class name and make it accept qscheme as input.
Future PRs will update the callsites for creating affine/symmetric quantized tensor.

Addresses https://github.com/pytorch/pytorch/issues/51152
Test Plan:
python test/test_quantization.py

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
70324,supriyar,pr,2021-12-22T20:26:31Z,[quant] make quantize_per_tensor/per_channel accept qscheme argument,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70324
* #70323

Summary:
qscheme is used to denote which mode was used to calculate the scale/zero_point for the tensor

Test Plan:
```
>>> x = torch.rand(2, 2)
>>> qx = torch.quantize_per_tensor(x, 1.0, 0, torch.quint8, torch.per_tensor_affine)
>>> qx
tensor([[0., 0.],
        [0., 1.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)
>>> qx = torch.quantize_per_tensor(x, 1.0, 0, torch.quint8, torch.per_tensor_symmetric)
>>> qx
tensor([[0., 0.],
        [0., 1.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_symmetric, scale=1.0, zero_point=0)

>>> qx = torch.quantize_per_channel(x, torch.tensor([1.0, 1.0]), torch.tensor([0, 0]), 0, torch.quint8, torch.per_channel_symmetric)
>>> qx
tensor([[0., 0.],
        [0., 1.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_channel_symmetric,
       scale=tensor([1., 1.], dtype=torch.float64), zero_point=tensor([0, 0]),
       axis=0)
>>> qx = torch.quantize_per_channel(x, torch.tensor([1.0, 1.0]), torch.tensor([0, 0]), 0, torch.quint8, torch.per_channel_affine)
>>> qx
tensor([[0., 0.],
        [0., 1.]], size=(2, 2), dtype=torch.quint8,
       quantization_scheme=torch.per_channel_affine,
       scale=tensor([1., 1.], dtype=torch.float64), zero_point=tensor([0, 0]),
       axis=0)
```

Reviewers:

Subscribers:

Tasks:

Tags:",pytorch
70336,suphoff,pr,2021-12-22T22:45:35Z,Prevent sum overflow in broadcast_object_list,"Summary: broadcast_object_list casted the sum of all object lengths to int from long causing overflows.

Test Plan:
sandcastle
buck test mode/dev-nosan //caffe2/test/distributed:distributed_gloo_spawn

Differential Revision: D33281300



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70340,rohan-varma,pr,2021-12-23T01:22:17Z,[FSDP] Remove module.wrapper_config support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70341
* __->__ #70340

Some wrap APIs support module.wrapper_config to specify the FSDP
arguments, though this feature is currently unused in all use cases and there
is no plan to support this API. enable_wrap() and wrap() along with FSDP
constructor wrapping should be enough for all use cases, so get rid of the
unnecessary code.

Differential Revision: [D33290066](https://our.internmc.facebook.com/intern/diff/D33290066/)",pytorch
70341,rohan-varma,pr,2021-12-23T01:22:22Z,[Easy] Lint wrap.py file,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70341
* #70340

Per title

Differential Revision: [D33290099](https://our.internmc.facebook.com/intern/diff/D33290099/)",pytorch
70406,ngimel,pr,2021-12-26T01:23:31Z,Sets device guard in _cudnn_impl functions,"Fixes #70404
",pytorch
70433,vfdev-5,pr,2021-12-27T17:38:50Z,WIP: Port `index_fill` to structured kernel,"Related to https://github.com/pytorch/pytorch/issues/55070

Description:
- ported `index_fill` to structured kernel
- updated OpInfo
- updated docs

To enable index_fill op in functorch: https://github.com/pytorch/functorch/issues/260
cc @zou3519 
",pytorch
70435,bryant1410,pr,2021-12-27T18:44:49Z,Make `get_file_pathnames_from_root` output order deterministic,"Fixes #70103

I used an argument so it can be disabled. I called it `deterministic_order` because `sort` can be confusing, as it's actually sorted but by dir levels.
",pytorch
70439,vfdev-5,pr,2021-12-27T23:07:56Z,[Docs] Fixed missing format common args,"Description:
- Fixing missing format common args: https://pytorch.org/docs/master/generated/torch.select.html#torch.select
",pytorch
70443,guoyejun,pr,2021-12-28T06:42:43Z,add SparseXPU to dispatch key set autogradother_backends,"According to dispatch table computation logic, if no kernel
register to a certain dispatch key, will use CompositeExplicitAutograd
backend kernel, so we need add sparseXPU key to the alias key pool.

Signed-off-by: Ma, Jing1 <jing1.ma@intel.com>
",pytorch
70477,r-barnes,pr,2021-12-29T00:30:36Z,Have type_parser return const reference,"Differential Revision: D33340030

",pytorch
70478,r-barnes,pr,2021-12-29T00:32:47Z,Reserve vector in gather_ranges_to_dense_op.h,"Differential Revision: D33339890

",pytorch
70491,lithuak,pr,2021-12-29T14:16:53Z,Fix docs rendering for nn.Module.named_modules(),"The documentation rendering for nn.Module.named_modules() is a bit broken, see the description of the last argument [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules).

This PR fixes that.",pytorch
70527,lezcano,pr,2021-12-30T16:07:22Z,Correct forward AD for linalg.eig and add checks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #70528
* __->__ #70527

This PR adds checks for the backward of `linalg.eig`, similar to those
deduced in https://github.com/pytorch/pytorch/pull/70253

It also modifies the function so that it does not save the input matrix,
as it's not necessary.

It also corrects the forward AD formula for it to be correct. Now all
the tests pass for `linalg.eig` and `linalg.eigvals`.

It also updates the docs to reflect better what's going on here.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33530148](https://our.internmc.facebook.com/intern/diff/D33530148)",pytorch
70528,lezcano,pr,2021-12-30T16:07:27Z,Simplify forward / backward AD for linalg.eigh and add checks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70528
* #70527

This PR adds checks for the backward of `linalg.eigh`, similar to those
deduced in https://github.com/pytorch/pytorch/pull/70253

It also makes its the implementation parallel that of the (fwd/bwd) derivative of
`torch.linalg.eig` and it makes most OpInfo tests pass.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano

Differential Revision: [D33530149](https://our.internmc.facebook.com/intern/diff/D33530149)",pytorch
70542,lezcano,pr,2021-12-31T16:01:47Z,[Array API] Add linalg.vecdot,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70542

This PR adds the function `linalg.vecdot` specified by the [Array
API](https://data-apis.org/array-api/latest/API_specification/linear_algebra_functions.html#function-vecdot)

For the complex case, it chooses to implement \sum x_i y_i. See the
discussion in https://github.com/data-apis/array-api/issues/356

Edit. When it comes to testing, this function is not quite a binopt, nor a reduction opt. As such, we're this close to be able to get the extra testing, but we don't quite make it. Now, it's such a simple op that I think we'll make it without this.

Resolves https://github.com/pytorch/pytorch/issues/18027.

cc @mruberry @rgommers @pmeier @asmeurer @leofang @AnirudhDagar @asi1024 @emcastillo @kmaehashi",pytorch
70556,jaketae,pr,2022-01-02T09:27:34Z,Allow RNN hidden_size to be 0,"Fixes https://github.com/pytorch/pytorch/issues/56767.
",pytorch
70557,jaketae,pr,2022-01-02T09:58:41Z,Fix `torch.dsplit` docs dim specification,"Fixes https://github.com/pytorch/pytorch/issues/70445.
",pytorch
70558,jaketae,pr,2022-01-02T12:38:14Z,SequentialLR update _last_lr on step,"Fixes https://github.com/pytorch/pytorch/issues/68956.
",pytorch
70574,lithuak,pr,2022-01-03T12:47:51Z,Fix docstring for nn.ELU,"Fixes nn.ELU's docstring problem reported at #70498.
",pytorch
70576,lithuak,pr,2022-01-03T14:11:33Z,Fix docstring for nn.Softplus,"Fixes nn.Softplus' docstring problem reported at #70498.

",pytorch
70577,lithuak,pr,2022-01-03T14:21:09Z,Fix docstring for nn.Tanh,Fixes nn.Tanh's docstring problem reported at #70498.,pytorch
70579,r-barnes,pr,2022-01-03T16:14:15Z,Drop omp simd from batch_permutation_op.cc,"Summary:
Fixes
```
     36 stderr: caffe2/caffe2/operators/batch_permutation_op.cc:25:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
      3 caffe2/caffe2/operators/batch_permutation_op.cc:25:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
```

Test Plan: Sandcastle

Reviewed By: meyering

Differential Revision: D33378925

",pytorch
70588,r-barnes,pr,2022-01-04T01:50:38Z,Cleaning code in fbcode/caffe2/c10/core/TensorImpl.h,"Test Plan: Sandcastle

Differential Revision: D33399751

",pytorch
70599,lezcano,pr,2022-01-04T12:38:03Z,[Array API] Add linalg.diagonal,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70599

This PR adds `linalg.diagonal` following the Array API:
https://data-apis.org/array-api/latest/extensions/linear_algebra_functions.html#linalg-diagonal-x-axis1-0-axis2-1-offset-0

Fixes https://github.com/pytorch/pytorch/issues/62813

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @rgommers @pmeier @asmeurer @leofang @AnirudhDagar @asi1024 @emcastillo @kmaehashi

Differential Revision: [D33760506](https://our.internmc.facebook.com/intern/diff/D33760506)",pytorch
70605,suphoff,pr,2022-01-04T16:35:12Z,Prevent sum overflow in broadcast_object_list (#70336),"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70336

broadcast_object_list casted the sum of all object lengths to int from long causing overflows.

Test Plan:
Increased size of Tensor used in object transfers to have  >2GB storage requirement (in distributed_test.py)

Without fix the length will overflow and the program will request a negative sized Tensor:
```
RuntimeError: Trying to create tensor with negative dimension -2147482417: [-2147482417]
```
With fix it will pass the test. (Nccl and gloo backend only)

Test used on server with GPUs:

buck test  mode/dev-nosan //caffe2/test/distributed:distributed_nccl_spawn --local -- broadcast_object
buck test  mode/dev-nosan //caffe2/test/distributed:distributed_gloo_spawn --local -- broadcast_object

Differential Revision: D33405741



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70628,rohan-varma,pr,2022-01-04T19:58:23Z,[Dist tests] Make event_listener work for all dist tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70628

event_listener thread is used to log process tracebacks when a timed
out process sends it a request to get its traceback. Although, this thread is
created in `_run` function which is overridden by some classes such as
`TestDistBackend` (https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/distributed/distributed_test.py#L521) and so those tests did not have this feature. Move the
event_listener setup logic to `run_test` which is called by all distributed
test classes, which enables it for all distributed tests. Also modify logger
setup to ensure that logging.info calls are printed in the subprocess.

Differential Revision: [D33410613](https://our.internmc.facebook.com/intern/diff/D33410613/)",pytorch
70639,rohan-varma,pr,2022-01-04T21:41:38Z,[FSDP] Correctly pin_memory for CPU offload.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #70639

x.pin_memory() results in a copied tensor in pinned memory, so it
needs to be reassigned. Internal benchmark shows ~6% win on ResNet model and ~2% win on roberta model.

Differential Revision: [D33300535](https://our.internmc.facebook.com/intern/diff/D33300535/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D33300535/)!",pytorch
70666,ngimel,pr,2022-01-05T07:04:17Z,reduce igamma instantiations,"Don't compile scalar versions of the kernel (there is no scalar overload), combine igamma and igammac kernels. 
Igamma cubin size 10 MB -> 2 MB on V100
",pytorch
70668,jaketae,pr,2022-01-05T08:33:04Z,Remove unused `optimizers` variable in test,"In `TestLRScheduler._test()`, an unused variable `optimizers` is created. This PR is a minor refactoring that removes the variable and the loop block that populates the set.",pytorch
70671,lithuak,pr,2022-01-05T10:09:44Z,Fix test failure when compiled without LAPACK support,"Fixes #70670
",pytorch
70673,jaketae,pr,2022-01-05T11:24:33Z,docs: clarify smooth l1 == l1 when beta == 0,"Fixes https://github.com/pytorch/pytorch/issues/68558.
",pytorch
70677,lithuak,pr,2022-01-05T13:16:37Z,Skip distributed tests if built with USE_DISTRIBUTED=0,"Fixes #70676
",pytorch
70768,r-barnes,pr,2022-01-05T18:18:57Z,Use C10_UNUSED instead of (void)X,"Summary:
Auto-generated with
```
buck run //scripts/rbarnes/regex_multiline_replacer:regex_multiline_replacer -- --find '^(\s*for\s*\()(const.*\n)\s*\(void\)[A-Za-z]+;\s*//\s*Suppress.*\s*\n(.*)'  --replace '\1C10_UNUSED \2\3' `find caffe2/ -regex "".*\.\(cpp\|h\)""`
```

Differential Revision: D33432600

",pytorch
70879,r-barnes,pr,2022-01-05T23:15:45Z,Drop some unused variables,"Summary: Sandcastle from layer_norm_kernel.cu

Test Plan: Sandcastle

Differential Revision: D33439040

",pytorch
70884,jaketae,pr,2022-01-06T01:58:07Z,Fix incorrect variable in autograd docs,"Fixes #68362.
",pytorch
70887,jaketae,pr,2022-01-06T03:48:25Z,ModuleList concatenation,"Fixes https://github.com/pytorch/pytorch/issues/70441.
",pytorch
70913,lithuak,pr,2022-01-06T12:27:29Z,Skip failing tests in test_nn if compiled without LAPACK,Fixes #70912,pytorch
70929,r-barnes,pr,2022-01-06T17:18:55Z,TensorImpl: be specified about ImplType,"Summary: Use `enum class` for safety and `int8_t` to be specific about the size

Differential Revision: D33456413

",pytorch
70930,vfdev-5,pr,2022-01-06T17:29:54Z,"Added antialias flag to interpolate (CUDA, bilinear and bicubic)","Description:
- Added antialias flag to interpolate (CUDA)
  - forward and backward for bicubic mode
  - added tests

Previous PR for CPU bilinear, https://github.com/pytorch/pytorch/pull/65142
Previous PR for CPU bicubic, https://github.com/pytorch/pytorch/pull/68819

### Benchmarks

<details>
<summary>
Bilinear forward pass, PIL, PTH CPU and PTH CUDA
</summary>

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```

Torch version: 1.11.0a0+gitd032369
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 

Num threads: 8
[----------------------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (320, 196) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               2851.2              |            874.1          |            57.1          
      channels_last non-contiguous torch.float32  |               2856.1              |           1155.8          |           130.6          

Times are in microseconds (us).

[----------------------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (460, 220) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               3705.9              |           1005.8          |            66.3          
      channels_last non-contiguous torch.float32  |               3742.9              |           1332.8          |           143.5          

Times are in microseconds (us).

[------------------------------------ Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 96) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               1768.0              |           725.2           |            77.9          
      channels_last non-contiguous torch.float32  |               1753.7              |           942.5           |           144.0          

Times are in microseconds (us).

[----------------------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (1200, 196) ----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               9522.6              |           2593.8          |           157.8          
      channels_last non-contiguous torch.float32  |               9513.5              |           3622.7          |           241.5          

Times are in microseconds (us).

[----------------------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 1200) ----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               2240.1              |           565.5           |            93.3          
      channels_last non-contiguous torch.float32  |               2244.2              |           972.7           |           170.8          

Times are in microseconds (us).

[------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (320, 196) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              1441.3             |           386.1           |            22.3          

Times are in microseconds (us).

[------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (460, 220) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              1815.2             |           376.8           |            27.8          

Times are in microseconds (us).

[-------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 96) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              962.3              |           400.0           |            29.4          

Times are in microseconds (us).

[------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (1200, 196) -------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              4749.7             |           910.1           |            63.7          

Times are in microseconds (us).

[------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 1200) -------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              1098.1             |           272.0           |            36.4          

Times are in microseconds (us).

```

</details>


<details>
<summary>
Bicubic forward pass, PIL, PTH CPU and PTH CUDA
</summary>

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```

Torch version: 1.11.0a0+gitd032369
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 

Num threads: 8
[------------------------------------ Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (320, 196) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               4522.4              |           1406.7          |           170.3          
      channels_last non-contiguous torch.float32  |               4530.0              |           1435.4          |           242.2          

Times are in microseconds (us).

[------------------------------------ Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (460, 220) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               5726.4              |           1628.6          |           164.0          
      channels_last non-contiguous torch.float32  |               5722.6              |           1665.6          |           234.7          

Times are in microseconds (us).

[------------------------------------ Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 96) ------------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               2909.1              |           1461.5          |           276.9          
      channels_last non-contiguous torch.float32  |               2892.9              |           1458.7          |           345.1          

Times are in microseconds (us).

[----------------------------------- Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (1200, 196) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |              14699.2              |           4283.9          |           407.1          
      channels_last non-contiguous torch.float32  |              14711.3              |           4321.1          |           477.0          

Times are in microseconds (us).

[----------------------------------- Downsampling (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 1200) -----------------------------------]
                                                  |  Reference, PIL 8.4.0, mode: RGB  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |               3467.0              |           980.0           |           339.2          
      channels_last non-contiguous torch.float32  |               3465.2              |           982.3           |           407.8          

Times are in microseconds (us).

[-------------------------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (320, 196) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              2396.7             |           877.8           |            68.1          

Times are in microseconds (us).

[-------------------------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (460, 220) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              3068.2             |           777.3           |            64.7          

Times are in microseconds (us).

[-------------------------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 96) ---------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              1540.2             |           829.3           |           100.4          

Times are in microseconds (us).

[------------------------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (1200, 196) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              7919.5             |           1467.8          |           151.6          

Times are in microseconds (us).

[------------------------- Downsampling (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 1200) --------------------------]
                                 |  Reference, PIL 8.4.0, mode: F  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ---------------------------------------------------------------------------------------------------------------
       contiguous torch.float32  |              1695.7             |           631.2           |           117.7          

Times are in microseconds (us).

```

</details>

<details>
<summary>
Bilinear backward pass, PTH CPU and PTH CUDA
</summary>

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```
- Measure only backward op

Torch version: 1.11.0a0+gitd032369
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 

Num threads: 8
[------------- Downsampling backward (bilinear): torch.Size([1, 3, 906, 438]) -> (320, 196) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           4686.8          |           215.7          
      channels_last non-contiguous torch.float32  |           5101.1          |           220.5          

Times are in microseconds (us).

[------------- Downsampling backward (bilinear): torch.Size([1, 3, 906, 438]) -> (460, 220) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           6011.2          |           204.4          
      channels_last non-contiguous torch.float32  |           6396.0          |           210.0          

Times are in microseconds (us).

[------------- Downsampling backward (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 96) -------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           2035.6          |           250.2          
      channels_last non-contiguous torch.float32  |           1589.6          |           252.5          

Times are in microseconds (us).

[------------ Downsampling backward (bilinear): torch.Size([1, 3, 906, 438]) -> (1200, 196) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |          11392.5          |           256.5          
      channels_last non-contiguous torch.float32  |          11640.2          |           263.9          

Times are in microseconds (us).

[------------ Downsampling backward (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 1200) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |          11769.6          |           465.9          
      channels_last non-contiguous torch.float32  |          12407.0          |           474.4          

Times are in microseconds (us).

[---- Downsampling backward (bilinear): torch.Size([1, 1, 906, 438]) -> (320, 196) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           3931.0          |           133.3          

Times are in microseconds (us).

[---- Downsampling backward (bilinear): torch.Size([1, 1, 906, 438]) -> (460, 220) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           5594.8          |           133.9          

Times are in microseconds (us).

[---- Downsampling backward (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 96) -----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           1272.6          |           133.0          

Times are in microseconds (us).

[--- Downsampling backward (bilinear): torch.Size([1, 1, 906, 438]) -> (1200, 196) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |          10618.1          |           134.0          

Times are in microseconds (us).

[--- Downsampling backward (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 1200) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |          11082.2          |           154.6          

Times are in microseconds (us).

```

</details>


<details>
<summary>
Bicubic backward pass, PTH CPU and PTH CUDA
</summary>

Code: https://gist.github.com/vfdev-5/b173761a567f2283b3c649c3c0574112

```
- Measure only backward op

Torch version: 1.11.0a0+gitd032369
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.0.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 

Num threads: 8
[------------- Downsampling backward (bicubic): torch.Size([1, 3, 906, 438]) -> (320, 196) -------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           6791.2          |           618.9          
      channels_last non-contiguous torch.float32  |           7125.2          |           622.9          

Times are in microseconds (us).

[------------- Downsampling backward (bicubic): torch.Size([1, 3, 906, 438]) -> (460, 220) -------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           8806.2          |           600.3          
      channels_last non-contiguous torch.float32  |           9167.6          |           607.5          

Times are in microseconds (us).

[-------------- Downsampling backward (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 96) -------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |           3683.6          |           693.8          
      channels_last non-contiguous torch.float32  |           3617.4          |           695.0          

Times are in microseconds (us).

[------------- Downsampling backward (bicubic): torch.Size([1, 3, 906, 438]) -> (1200, 196) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |          17548.2          |           779.4          
      channels_last non-contiguous torch.float32  |          17966.2          |           786.5          

Times are in microseconds (us).

[------------- Downsampling backward (bicubic): torch.Size([1, 3, 906, 438]) -> (120, 1200) ------------]
                                                  |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: ----------------------------------------------------------------------------------------------
      channels_first contiguous torch.float32     |            28.4           |            1.6           
      channels_last non-contiguous torch.float32  |            28.4           |            1.6           

Times are in milliseconds (ms).

[---- Downsampling backward (bicubic): torch.Size([1, 1, 906, 438]) -> (320, 196) -----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           6266.1          |           208.5          

Times are in microseconds (us).

[---- Downsampling backward (bicubic): torch.Size([1, 1, 906, 438]) -> (460, 220) -----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           8218.3          |           200.8          

Times are in microseconds (us).

[----- Downsampling backward (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 96) -----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |           3458.9          |           231.9          

Times are in microseconds (us).

[---- Downsampling backward (bicubic): torch.Size([1, 1, 906, 438]) -> (1200, 196) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |          15729.3          |           261.6          

Times are in microseconds (us).

[---- Downsampling backward (bicubic): torch.Size([1, 1, 906, 438]) -> (120, 1200) ----]
                                 |  1.11.0a0+gitd032369 cpu  |  1.11.0a0+gitd032369 cuda
8 threads: -----------------------------------------------------------------------------
       contiguous torch.float32  |          26279.8          |           547.0          

Times are in microseconds (us).

```

</details>

Code is moved from torchvision: https://github.com/pytorch/vision/pull/4211 and optimized


",pytorch
70946,lithuak,pr,2022-01-06T19:57:10Z,Remove obsolete 'dataclasses' dependency,"Since [Python 3.6 was deprecated](https://github.com/pytorch/pytorch/pull/70493) and Python 3.7 has `dataclasses` as a [part of std library](https://docs.python.org/3/library/dataclasses.html), we don't need them as a dependency anymore.",pytorch
70950,vfdev-5,pr,2022-01-06T21:27:09Z,Update gradcheck.py,"Following https://github.com/pytorch/pytorch/pull/64837#discussion_r779870974

Changed torch.equal to torch.allclose as exact comparision could be flaky





cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7",pytorch
70973,r-barnes,pr,2022-01-07T05:51:33Z,caffe2/c10/core/TensorImpl.h: adapt to clang 12,"Summary:
clang12 builds fail like this:

  caffe2/c10/core/TensorImpl.h:2615:1: error: static_assert failed due to requirement 'sizeof(void *) != sizeof(long) || sizeof(c10::TensorImpl) == sizeof(long) * 24' ""You changed the size of TensorImpl on 64-bit arch.See Note [TensorImpl size constraints] on how to proceed.""

Yet eliciting the size of that struct with this one-line addition:

  char (*__show_sizeof)[sizeof( TensorImpl )] = 1;

reports that its size is indeed 192 (aka 8 * 24):

  caffe2/c10/core/TensorImpl.h:2615:8: error: cannot initialize a variable of type 'char (*)[192]' with an rvalue of type 'int'

Clearly, someone needs to figure out why this is failing, but
as a temporary measure, I propose to relax the test from requiring exactly 192 to permitting <= 200.

Test Plan: Building no longer triggers that static_assert failure.

Differential Revision: D32749655

",pytorch
70987,lithuak,pr,2022-01-07T14:16:24Z,Fix docstring for nn.Hardsigmoid,Fixes nn.Hardsigmoid's docstring problem reported at #70498.,pytorch
70993,lithuak,pr,2022-01-07T15:20:36Z,Fix docstring for nn.Hardswish,Fixes nn.Hardswish's docstring problem reported at #70498.,pytorch
70998,r-barnes,pr,2022-01-07T16:22:45Z,Fix some unused variable warnings and make some stuff const in ReplicationPadding.cu,"Differential Revision: D33460035

",pytorch
70999,r-barnes,pr,2022-01-07T16:23:52Z,Suppress some unused variable warnings in Sorting.cu and TensorTopK.cu,"Differential Revision: D33470240

",pytorch
71000,r-barnes,pr,2022-01-07T16:36:05Z,Use (void)error_unused to avoid unused warning,"Test Plan: Sandcastle

Differential Revision: D33470600

",pytorch
71007,r-barnes,pr,2022-01-07T18:17:07Z,Use a reference in GetSingleArgument,"Summary:
A string copy at Line 417 is currently consuming 125,749,287,000 cycles/day. I suspect the issue is with a copy-on-return, but we can experiment with introducing a reference in the middle to see if that produces a good savings without changing the interface.

Reference
```
[""Inline caffe2::ArgumentHelper::GetSingleArgument @ caffe2/caffe2/utils/proto_utils.cc:417""]
```

Differential Revision: D33478883

",pytorch
71012,lithuak,pr,2022-01-07T18:40:33Z,Fix docstring for nn.Hardshrink,"Fixes nn.Hardshrkink's docstring problem reported at #70498.

",pytorch
71047,r-barnes,pr,2022-01-08T03:17:37Z,Have getFilesToLevels return a reference,"Summary:
The copy induced by getFilesToLevels is currently consuming 3,457,470,000 cycles per day. A reference might fix that.

Reference:
```
[""Inline torch::jit::JitLoggingConfig::getFilesToLevels[abi:cxx11] @ caffe2/torch/csrc/jit/jit_log.cpp:54""]
```

Differential Revision: D33479180

",pytorch
71097,lithuak,pr,2022-01-10T15:45:18Z,Fix docstring for nn.LeakyReLU,"Fixes nn.ReLU's docstring problem reported at #70498.



cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345",pytorch
71099,lithuak,pr,2022-01-10T15:52:54Z,Fix docstring for nn.LogSigmoid,"Fixes nn.Logsigmoid's docstring problem reported at #70498.



cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345",pytorch
71100,lithuak,pr,2022-01-10T15:59:30Z,Fix docstring for nn.MultiHeadAttention,"Fixes nn.MultiHeadAttention's docstring problem reported at #70498.



cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345",pytorch
71101,lithuak,pr,2022-01-10T16:04:22Z,Fix docstring for nn.PReLU,"Fixes nn.PReLU's docstring problem reported at #70498.



cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345",pytorch
71106,r-barnes,pr,2022-01-10T16:27:19Z,Drop unused variables and add some const,"Differential Revision: D33490855

",pytorch
71107,r-barnes,pr,2022-01-10T16:38:00Z,Drop unused variables; make things const; use some auto,"Differential Revision: D33490773

",pytorch
71123,r-barnes,pr,2022-01-10T20:39:21Z,Drop more unused variables,"Differential Revision: D33511656

",pytorch
71159,vfdev-5,pr,2022-01-11T12:58:19Z,Fixed docs for forward_ad.make_dual,"Minor docs change.
",pytorch
71231,jaketae,pr,2022-01-12T18:32:25Z,docs: add equation for linear lr,"Fixes https://github.com/pytorch/pytorch/issues/68058.
",pytorch
71326,jaketae,pr,2022-01-14T20:36:54Z,Add append method for nn.Sequential,"Partially addresses https://github.com/pytorch/pytorch/issues/71249, and potentially supersedes https://github.com/pytorch/pytorch/pull/20274. ",pytorch
71338,rohan-varma,pr,2022-01-14T23:53:16Z,"Back out ""[pytorch][PR] Revert ""Revert D33480077: .github: Re-enable xla test config""""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71338

Original commit changeset: ee0985768d1d

Original Phabricator Diff: D33569109

Differential Revision: [D33592520](https://our.internmc.facebook.com/intern/diff/D33592520/)",pytorch
71363,rohan-varma,pr,2022-01-16T04:27:01Z,[Docs][BE] DDP doc fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71363

Looks like DDP example is currently broken as per
https://discuss.pytorch.org/t/official-ddp-example-is-broken/141493. Fix the
issue by setting the correct env variable.

Differential Revision: [D33607250](https://our.internmc.facebook.com/intern/diff/D33607250/)",pytorch
71460,rohan-varma,pr,2022-01-19T00:37:18Z,[RPC] Fix a few flaky RPC tsan tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71460

When running with TSAN, we use a larger RPC timeout: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L68. As a result, the assertions here are invalid.

Tried to fix this by just setting `self.rpc_backend_options.rpc_timeout` to the new timeout, but `rpc_backend_options` is reconstructed every time it is accessed, so this doesn't work:: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py#L15


Just removing the asserts should be fine as they don't really add value to what's being tested.

Differential Revision: [D33648421](https://our.internmc.facebook.com/intern/diff/D33648421/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
71462,r-barnes,pr,2022-01-19T01:03:22Z,Remove simd qualifier for pragma omp loop in upsample_nearest_op.h,"Summary:
Fixes
```
      6 aienv/aienv_ig_reels_base:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
      6 deep_entity_classification/si_dec_gnn:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
      6 feed_recommendation_infra/multifeed_execution_graph_service_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
     12 mobile_cv/mobile-vision_experimental:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
     30 mobile_cv/mobile-vision_xraymobilev2_detection_caffe2:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
     42 aienv/aienv:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
    128 feed_recommendation_infra/multifeed_recagg_dev:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
    136 fluent2/fblearner_flow_projects_fluent2_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
   1338 f6/f6_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]
```

Test Plan: Sandcastle

Reviewed By: luciang

Differential Revision: D33641869

",pytorch
71521,eqy,pr,2022-01-20T02:09:02Z,Change `test_conv_large` parameter initialization,"This PR twiddles the parameters of the conv layer in `test_conv_large` to better avoid NaN values. Previously, this test would cause a NaN to be computed for `scale` (propagated from `.mean()` on the `.grad` tensor). This NaN would then be propagated to the scaled gradients via division, resulting in a bogus `assertEqual` check as `NaN == NaN` is by default true. (This behavior was observed on V100 and A100).
To improve visibility of failures in the event of NaNs in `grad1`, scale is now computed from `grad2`.

Interestingly enough, we discovered this issue when trying out some less common setups that broke this test; it turns out those breakages were cases where there were no NaN values (leading to an actual `assertEqual` check that would fail for `float16`).

CC @ptrblck @ngimel @puririshi98 ",pytorch
71525,rohan-varma,pr,2022-01-20T03:00:17Z,[BE] Fix FSDP flaky test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71525

Closes https://github.com/pytorch/pytorch/issues/71496. Use file init
for test as opposed to TCP init which runs into some port racing conditions as
seen in the failures for that issue.

Differential Revision: [D33676165](https://our.internmc.facebook.com/intern/diff/D33676165/)",pytorch
71555,r-barnes,pr,2022-01-20T16:49:18Z,Fix unused variable warning in Sorting.cu,"Test Plan: Sandcastle

Differential Revision: D33675174

",pytorch
71556,r-barnes,pr,2022-01-20T16:49:33Z,Fix unused variable warning in adagrad_fused_op_gpu.cu,"Differential Revision: D33675154

",pytorch
71557,r-barnes,pr,2022-01-20T16:52:29Z,Fix unused variable warning in lp_pool_op.cu,"Test Plan: Sandcastle

Differential Revision: D33675165

",pytorch
71584,r-barnes,pr,2022-01-20T22:08:20Z,Fix unused variable warning in DepthwiseConv2d.cu,"Differential Revision: D33692624

",pytorch
71585,r-barnes,pr,2022-01-20T22:08:42Z,Fix unused variable warning in AveragePool2d,"Differential Revision: D33692619

",pytorch
71586,r-barnes,pr,2022-01-20T22:08:43Z,Fix unused variable warning in DistanceKernel.cu,"Differential Revision: D33692618

",pytorch
71587,r-barnes,pr,2022-01-20T22:08:54Z,Fix unused variable warning in layer_norm_kernel.cu,"Differential Revision: D33692615

",pytorch
71588,r-barnes,pr,2022-01-20T22:09:36Z,Fix unused variable warning in LossCTC.cu,"Differential Revision: D33692622

",pytorch
71589,r-barnes,pr,2022-01-20T22:09:41Z,Fix unused variable warning in EmbeddingBag.cu,"Differential Revision: D33692625

",pytorch
71590,r-barnes,pr,2022-01-20T22:09:42Z,Fix unused variable warning in DilatedMaxPool3d.cu,"Differential Revision: D33692617

",pytorch
71591,r-barnes,pr,2022-01-20T22:10:46Z,Fix unused variable warning in FractionalMaxPool3d,"Differential Revision: D33692623

",pytorch
71593,r-barnes,pr,2022-01-20T22:32:53Z,Fix unused variable warning in ConvolutionMM2d.cu,"Differential Revision: D33692621

",pytorch
71594,r-barnes,pr,2022-01-20T22:33:01Z,Fix unused variable warning in MultiLabelMarginCriterion.cu,"Differential Revision: D33692620

",pytorch
71600,rohan-varma,pr,2022-01-20T23:57:51Z,[Opt Overlap] Remove redundant tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* #71605
* #71604
* #71603
* #71602
* #71601
* __->__ #71600

These tests in test_c10d_nccl test a subset of functionality that's
already covered by distributed_test.py: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/distributed/distributed_test.py#L3915, no need for these additional tests.

Differential Revision: [D33662679](https://our.internmc.facebook.com/intern/diff/D33662679/)",pytorch
71601,rohan-varma,pr,2022-01-20T23:57:57Z,[Optimizer Overlap] Move hooks to own file,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* #71605
* #71604
* #71603
* #71602
* __->__ #71601
* #71600

Moves current prototype optimizer overlap to its own file for a better
namespace. No code changes besides a few comment fixes. Note that this code is
still prototype and not expected to be used by an end user.

Differential Revision: [D33662678](https://our.internmc.facebook.com/intern/diff/D33662678/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D33662678/)!",pytorch
71602,rohan-varma,pr,2022-01-20T23:58:02Z,[Opt Overlap] Create Optimizer Hook State directly from functional optim,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* #71605
* #71604
* #71603
* __->__ #71602

The design in https://github.com/pytorch/pytorch/issues/67570 requires
`_OptimizerHookState` to be created directly from a functional optimizer. Add
support and tests for this. Now, `OptimizerHookState` can be created with an already instantiated `_FunctionalOptimizer` with `OptimizerHookState.from_functional_optim`. Also refactor a few tests.

Differential Revision: [D33687477](https://our.internmc.facebook.com/intern/diff/D33687477/)",pytorch
71603,rohan-varma,pr,2022-01-20T23:58:08Z,[Easy] Add comment explaining DistributedOptimizer gating,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* #71605
* #71604
* __->__ #71603
* #71602

Small comment to clarify this.

Differential Revision: [D33688994](https://our.internmc.facebook.com/intern/diff/D33688994/)",pytorch
71604,rohan-varma,pr,2022-01-20T23:58:14Z,[Opt Overlap] Implement as_functional_optim and create_functional_optim,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* #71605
* __->__ #71604
* #71603
* #71602

Implement 2 helper functions:
- as_functional_optim which takes in a torch.optim class type and arguments and
  creates the corresponding functional optimizer.
- create_functional_optim which takes in the functional optimizer class type
  and constructs it. Note that as_functional_optim calls into
  create_functional_optim.

  The first will be used in future PRs as described in
  https://github.com/pytorch/pytorch/issues/67570 to create a functional
  optimizer from a traditional optimizer. The latter is used in
  _OptimizerHookState to create a functional optimizer.

  Both new helper functions are covered by unittests.

Differential Revision: [D33688995](https://our.internmc.facebook.com/intern/diff/D33688995/)",pytorch
71605,rohan-varma,pr,2022-01-20T23:58:19Z,[Opt Overlap] Implement _OverlappedOptimizer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* #71606
* __->__ #71605
* #71604
* #71603
* #71602

Implements the main `_OverlappedOptimizer` as designed in https://github.com/pytorch/pytorch/issues/67570:

- `register_overlapped` decorator that allows an `_OverlappedOptimizer` to register the types of optimizers it supports overlap for
-  `_OverlappedStandardOptimizer` to overlap a standard `torch.optim.Optimizer`.
- `register_ddp` function to register the communication hook with DDP
- `_as_overlapped_optim` to return an `OverlappedOptimizer` that supports overlapping optimizer for the given optimizer type.

Example of usage:

```
overlapped_optimizer = _as_overlapped_optim(optim_cls, *functional_optim_args, **functional_optim_kwargs)
overlapped_optimizer.register_ddp(ddp_model_with_optimizer_hook)
```

However, note that the user is not expected to do this, and this will be called by libraries such as DDP and FSDP to register overlapped optimizers.

Differential Revision: [D33692686](https://our.internmc.facebook.com/intern/diff/D33692686/)",pytorch
71606,rohan-varma,pr,2022-01-20T23:58:25Z,[Opt Overlap] Implement register_fused_optim in DDP,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* #71607
* __->__ #71606
* #71605
* #71604
* #71603
* #71602

Implements a `_register_fused_optim` function in DDP that allows users to register an optimizer to run in step with the allreduce. The function takes in a optimizer class, args and kwargs and calls into the `_OverlappedOptimizer` built below to construct the appropriate functional optimizer and eventually register it as a communication hook.

Current usage and constraints are documented in the function, and https://github.com/pytorch/pytorch/issues/71595 is filed to track use cases that are currently unsupported.

Example of usage:

```
ddp_model_with_optimizer_hook._register_fused_optim(
                        optim_cls,
                        *functional_optim_args,
                        **functional_optim_kwargs,
                    )
```
Differential Revision: [D33694037](https://our.internmc.facebook.com/intern/diff/D33694037/)",pytorch
71607,rohan-varma,pr,2022-01-20T23:58:31Z,[Easy] DDP typo fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* #71608
* __->__ #71607
* #71606
* #71605
* #71604
* #71603
* #71602

Per title

Differential Revision: [D33694038](https://our.internmc.facebook.com/intern/diff/D33694038/)",pytorch
71608,rohan-varma,pr,2022-01-20T23:58:36Z,[Opt Overlap] Support optimizing partial set of parameters,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71620
* __->__ #71608
* #71607
* #71606
* #71605
* #71604
* #71603
* #71602

Typical optimizer usage might be something like `optim.SGD(model.parameters(), lr=1e-2)`, but it is common to freeze a portion of the model so that only a part of it is optimized. One way of implementing this is only passing a subset of parameters you'd like optimized:

```
params_to_opt = [list(model.parameters())[0]]
opt = optim.SGD(params_to_opt, lr=1e-2)
```

However `_register_fused_optim` had no way of supporting this for overlapped optimizers. This diff introduces a `optim_params` argument that can be passed into `_register_fused_optimizer`, allowing only those parameters to be modified during the optimization process. Unittests added to ensure this works as expected. 

Differential Revision: [D33696382](https://our.internmc.facebook.com/intern/diff/D33696382/)

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
71620,rohan-varma,pr,2022-01-21T03:14:49Z,[Opt Overlap] Clean up code in _OptimizerHookState,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71620
* #71608
* #71607
* #71606
* #71605
* #71604
* #71603
* #71602

Remove from_functional_optim and make it the default constructor since
that is the only way _OptimizerHookState is now being built. Also, no longer
need to expose create_functional_optim helper function

Differential Revision: [D33700593](https://our.internmc.facebook.com/intern/diff/D33700593/)",pytorch
71621,rohan-varma,pr,2022-01-21T03:43:46Z,"[LocalSGD] Move feature to Beta, clean up some docs","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71621

Moves this feature to beta as discussed, and cleans up some docs.
Synced offline with @wayi1 who mentioned that the current names are preferred
as he works to prototype hierarchical allreduce as discussed in this RFC: https://github.com/pytorch/pytorch/issues/71325.

Differential Revision: [D33700444](https://our.internmc.facebook.com/intern/diff/D33700444/)",pytorch
71685,r-barnes,pr,2022-01-23T18:19:55Z,Drop unused variables (#5258),"Summary: Pull Request resolved: https://github.com/pytorch/vision/pull/5258

Test Plan: Sandcastle

Differential Revision: D33728264

",pytorch
71710,ngimel,pr,2022-01-24T17:17:49Z,use legacy unrolled kernel for non-trivial offset calc cases,"This leads to across the board improvements on Pascals, big perf improvements for some broadcasting patterns and datatypes on V100 (along with some 3-5% regressions for some other patterns). The most common improving pattern on V100 is half-precision x+bias, that improves by ~5%. Full V100, 3090 and A100 results in https://docs.google.com/spreadsheets/d/1K67x-6_TPT9Yt6533NfECEhUyfbqBxLH9M5Z3gymzXE/edit#gid=1218963246, benchmarking script in https://gist.github.com/ngimel/986ee84a1dd234a0485e99544e0fc8b6 
Most importantly, it reduces context size by 40 MB. ",pytorch
71727,silvasean,pr,2022-01-24T22:16:34Z,Fix symbolic shape function for `flatten`,"Python `range` is (start,end) not (start, length). Sot it should be `for i in range(start_dim, end_dim + 1):` instead of `for i in range(start_dim, end_dim - start_dim + 1):`.

",pytorch
71730,ngimel,pr,2022-01-24T23:24:48Z,don't include Loops.cuh from Reduce.cuh,,pytorch
71802,rohan-varma,pr,2022-01-26T01:48:37Z,[Easy] Format DDP error,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71905
* #71804
* #71803
* __->__ #71802

Per title

Differential Revision: [D33774513](https://our.internmc.facebook.com/intern/diff/D33774513/)",pytorch
71803,rohan-varma,pr,2022-01-26T01:48:42Z,[FSDP] Add/refactor unit test for wrap,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71905
* #71804
* __->__ #71803
* #71802

1. Extra check for wrapping with override args,

2. Enhance UT to make sure
`wrap` doesn't wrap outside of ctx.

Differential Revision: [D33774512](https://our.internmc.facebook.com/intern/diff/D33774512/)",pytorch
71804,rohan-varma,pr,2022-01-26T01:48:48Z,[FSDP] Backward prefetch in recursive call,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #71905
* __->__ #71804
* #71803
* #71802

Add backward prefetch arg when using auto_wrap_policy. Unittests are
updated appropriately.

Differential Revision: [D33782346](https://our.internmc.facebook.com/intern/diff/D33782346/)",pytorch
71870,tillahoffmann,pr,2022-01-26T18:36:55Z,Add scalar support for `logsumexp`.,"This PR aims to bring `logsumexp` on par with the `scipy` implementation by adding an optional scalar argument, e.g. to subtract elements after taking the exponential. It fixes #32097. 

Gradients of the result are implemented with respect to both the original argument and the optional scalars. The optional sign information being returned is not differentiable.

Because the output depends on the `return_sign` argument, I have implemented an auxiliary function `_logsumexp2` akin to `_unique2` and then exposed the function using `boolean_dispatch`. This should maintain backwards compatibility both for python as well as the C++ api.

I was unsure on how to best add tests (input appreciated), but here's the basic idea.

```python
import torch as th
from scipy import special, optimize
import numpy as np
import itertools as it
th.set_default_dtype(th.float64)


def finite_difference_grad(func, x, *args, epsilon=1e-6, return_sign, **kwargs):
    """"""
    Compute the gradient of the first element returned by `func`, discarding signs
    if necessary.
    """"""
    x1 = x.detach().numpy().copy()
    x2 = x1.copy()
    x2.ravel()[0] += epsilon
    x1.ravel()[0] -= epsilon
    y2 = func(x2, *args, **kwargs, return_sign=return_sign)
    y1 = func(x1, *args, **kwargs, return_sign=return_sign)
    if return_sign:
        y2, _ = y2
        y1, _ = y1
    grad = (y2 - y1) / (2 * epsilon)
    return grad


def maybe_detach(x):
    return None if x is None else x.detach()


for return_sign, scaled, use_out in it.product(*([(False, True)] * 3)):
    print(f'return_sign={return_sign}, scaled={scaled}, use_out={use_out}')
    
    # Test for random values.
    x = (th.rand((4, 5)) - .5)
    if scaled:
        b = (th.rand(5) - .5)
    else:
        b = None
        
    if use_out:
        result_out = th.empty(4)
        sign_out = th.empty(4)
        out = (result_out, sign_out)
    else:
        out = None
        x.requires_grad_()
        if b is not None:
            b.requires_grad_()
    
    # Verify the output.
    expected = special.logsumexp(x.detach(), axis=-1, b=maybe_detach(b), return_sign=return_sign)
    actual = x.logsumexp(dim=-1, b=b, return_sign=return_sign, out=out)

    if return_sign:
        expected, expected_sign = expected
        actual, actual_sign = actual
        np.testing.assert_array_equal(actual_sign, expected_sign)
    np.testing.assert_allclose(actual.detach(), expected)

    # Skip the gradients when using out.
    if use_out:
        continue

    # Verify the gradients...
    actual[0].backward()

    # ... for the main input and ...
    actual_grad = x.grad
    expected_grad = finite_difference_grad(special.logsumexp, x, b=maybe_detach(b), 
                                           return_sign=return_sign, axis=-1)
    np.testing.assert_allclose(actual_grad.ravel()[0], expected_grad[0], rtol=1e-6)

    # ... for the scalars.
    if scaled:
        actual_grad = b.grad
        expected_grad = finite_difference_grad(
            lambda b, *args, **kwargs: special.logsumexp(x.detach(), axis=-1, b=b, return_sign=return_sign), 
            maybe_detach(b), return_sign=return_sign,
        )
        np.testing.assert_allclose(actual_grad.ravel()[0], expected_grad[0], rtol=1e-6)
```

I've also added a slightly more verbose error message to the parsing of `native_functions.yaml` to help with debugging.",pytorch
71905,rohan-varma,pr,2022-01-27T08:48:25Z,[Checkpoint] Doc improvements and deprecate not passing arg,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71905
* #71804
* #71803
* #71802

- Describe non-reentrant implementation in docs
- Mention that nested return with autograd is supported when use_reentrant=False
- Document that use_reentrant=False is preferred and use_reentrant=True will be deprecated
- Add warnings.warn() when use_reentrant=True is specified.

Differential Revision: [D33813032](https://our.internmc.facebook.com/intern/diff/D33813032/)",pytorch
71937,vfdev-5,pr,2022-01-27T21:02:41Z,Removed JIT FC tweaks for interpolation options,"Description:
- Removed JIT FC tweaks for interpolation options : nearest-exact and antialiasing

They were added in 
- https://github.com/pytorch/pytorch/pull/64501 (Sept 04 2021)
- https://github.com/pytorch/pytorch/pull/65142 (Sept 16 2021)

cc @jbschlosser

",pytorch
71970,rohan-varma,pr,2022-01-28T05:07:46Z,Fix hooks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #71970

- Provide default arg for power SGD convenience wrapper that matches the main API default

Differential Revision: [D33837457](https://our.internmc.facebook.com/intern/diff/D33837457/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D33837457/)!",pytorch
72066,vfdev-5,pr,2022-01-31T16:33:39Z,Improved error message for interpolation,"Description:
- Improved error message for CUDA interpolation with antialiasing 

@jbschlosser could you please check this PR and the wording if the error message is more clear now ? Thank.
I'm skipping all the tests now and once we are agreed on the wording if any updates are required, I update and restart the tests to ensure nothing is broken.
",pytorch
72118,vfdev-5,pr,2022-02-01T17:16:28Z,Fixed issue with linalg_svdvals for forwardAD mode and inplace ops,"Description:
- Fixed issue with linalg_svdvals for forwardAD mode and inplace ops
- This should fix a number of failing tests in functorch

cc @Lezcano 
",pytorch
72155,r-barnes,pr,2022-02-02T00:38:40Z,Fix unused variable warning in LostCTC.cu,"Summary:
Fixes:
```

caffe2/aten/src/ATen/native/cuda/LossCTC.cu(420): warning: parameter ""max_target_length"" was declared but never referenced
          detected during instantiation of ""at::Tensor at::native::<unnamed>::ctc_loss_backward_gpu_template<scalar_t,target_scalar_type>(const at::Tensor &, const at::Tensor &, const at::Tensor &, c10::IntArrayRef, c10::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, __nv_bool) [with scalar_t=double, target_scalar_type=c10::ScalarType::Long]""
(758): here

caffe2/aten/src/ATen/native/cuda/LossCTC.cu(427): warning: parameter ""num_labels"" was declared but never referenced
          detected during instantiation of ""at::Tensor at::native::<unnamed>::ctc_loss_backward_gpu_template<scalar_t,target_scalar_type>(const at::Tensor &, const at::Tensor &, const at::Tensor &, c10::IntArrayRef, c10::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, __nv_bool) [with scalar_t=double, target_scalar_type=c10::ScalarType::Long]""
(758): here

caffe2/aten/src/ATen/native/cuda/LossCTC.cu(427): warning: parameter ""BLANK"" was declared but never referenced
          detected during instantiation of ""at::Tensor at::native::<unnamed>::ctc_loss_backward_gpu_template<scalar_t,target_scalar_type>(const at::Tensor &, const at::Tensor &, const at::Tensor &, c10::IntArrayRef, c10::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, __nv_bool) [with scalar_t=double, target_scalar_type=c10::ScalarType::Long]""
(758): here

caffe2/aten/src/ATen/native/cuda/LossCTC.cu(420): warning: parameter ""max_target_length"" was declared but never referenced
          detected during instantiation of ""at::Tensor at::native::<unnamed>::ctc_loss_backward_gpu_template<scalar_t,target_scalar_type>(const at::Tensor &, const at::Tensor &, const at::Tensor &, c10::IntArrayRef, c10::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, __nv_bool) [with scalar_t=double, target_scalar_type=c10::ScalarType::Long]""
(758): here
```

Differential Revision: D33930498

",pytorch
72157,eqy,pr,2022-02-02T00:59:05Z,Properly initialize `grad_weight` in `raw_cudnn_convolution_backward_weight_out`,"#71521 attempted to fix an issue where the `test_conv_large` test was producing `NaN` values after the backward pass, yielding a bogus comparison between the result and the expected result. While tweaking the initialization of the conv layer seemed to fix this behavior, it was actually just masking the real issue, which was that `grad_weight` is not guaranteed to be initialized in `raw_cudnn_convolution_backward_weight_out` when the backward operation is split.

Specifically, the `grad_weight` tensor is expected to be directly written to by a `cudnn` kernel (which does occur in most cases) so it does not need to be initialized, but splitting introduces an intermediate `grad_weight_` tensor that holds the intermediate gradients and then accumulates into `grad_weight` without initializing it first. This PR tweaks this behavior so that now accumulation is done with a zero'd tensor, and also adds the change of doing the accumulation in an accumulation dtype. The hacky workaround masking the issue is also reverted, with the safeguard against comparing `NaN` values (using the reference tensor for scale computation) kept in place.

CC @ngimel @ptrblck",pytorch
72168,rohan-varma,pr,2022-02-02T07:44:31Z,[FSDP] full state dict,"Summary:
Implements `state_dict` and `load_state_dict` APIs for FSDP, with the following limitations:

1. Does not support `state_dict_device` (i.e. specifying which device params should be on) which fairscale does currently support
2. Does not yet support offload of state_dict onto CPU
3. Loads state_dict on all ranks currently. In the future we could add support for loading this on only rank 0, to avoid redundancy across ranks as usually only one rank is responsible for saving/loading the model. Along with (2) this would enable larger models to have state_dict called.

As discussed in FSDP checkpoint API proposal, `state_dict` will basically be a `full_state_dict` where full parameters are returned on all ranks. As a result this implies that the model must actually be able to fit on a single GPU. We are in the process of building sharding solution to support similar checkpoint APIs without this constraint.

Test Plan: CI

Differential Revision: D33887577

",pytorch
72181,lezcano,pr,2022-02-02T16:53:16Z,Make svd / svdvals fully functorch compatible,"This should (hopefully) make all the CI from `functorch` go green (including jvp's!) after changing `VARIADIC_BDIMS_BOXED(_svd_helper);` with `VARIADIC_BDIMS_BOXED(_linalg_svd);` and removing all the skip and xfails associated to `linalg.svdvals`.

Locally, there's just one test that started failing because of this, and that is `test_vmapjvpall_norm_nuc_cpu_float32`. I have no idea what's going on here, but it's a jvp product, so not a regression, and it might very well be caused by the jvp of other operation within `norm_nuc` as this is a composite operation. 
",pytorch
72232,ngimel,pr,2022-02-03T02:09:02Z,use random number in shared file name,"A tentative fix for #67864
",pytorch
72296,r-barnes,pr,2022-02-03T22:04:14Z,Fix inline variable errors with const,"Summary:
Deals with inline variable errors for platform010 by using `const` instead of `constexpr`.

D33692616 looks at alternative strategies.

Test Plan:
```
reset && buck build --keep-going -c cxx.extra_cxxflags=-ferror-limit=300 -c fbcode.platform=platform010 glow/glow/...
```

Reviewed By: luciang, meyering

Differential Revision: D33988238

",pytorch
72410,r-barnes,pr,2022-02-07T06:15:24Z,Fix unused variable warnings,"Summary:
Fixes
```
caffe2/caffe2/operators/cross_entropy_op.cu(330): warning: parameter ""outer_size"" was declared but never referenced

caffe2/caffe2/operators/cross_entropy_op.cu(191): warning: parameter ""outer_size"" was declared but never referenced
caffe2/caffe2/operators/generate_proposals_op_util_nms.h(347): warning: variable ""order"" was declared but never referenced
caffe2/caffe2/operators/segment_reduction_op_gpu.cu(319): warning: parameter ""N"" was declared but never referenced
          detected during:
            instantiation of ""__nv_bool caffe2::CUDASparseLengthsWeightedSumOp<T, Context, SparseFused>::DoRunWithType<IndexType>() [with T=float, Context=caffe2::CUDAContext, SparseFused=true, IndexType=int32_t]""
caffe2/caffe2/core/operator.h(1304): here
            instantiation of ""__nv_bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types...>, ExtraArgs...>::call(Op *, caffe2::TypeMeta) [with FirstType=int32_t, Types=<int64_t>, ExtraArgs=<>, Op=caffe2::CUDASparseLengthsWeightedSumOp<float, caffe2::CUDAContext, true>]""
caffe2/caffe2/core/operator.h(1304): here
            instantiation of ""__nv_bool caffe2::DispatchHelper<caffe2::TensorTypes<FirstType, Types...>, ExtraArgs...>::call(Op *, const caffe2::Tensor &) [with FirstType=int32_t, Types=<int64_t>, ExtraArgs=<>, Op=caffe2::CUDASparseLengthsWeightedSumOp<float, caffe2::CUDAContext, true>]""
(786): here
caffe2/caffe2/operators/segment_reduction_op_gpu.cu(96): warning: parameter ""len_length"" was declared but never referenced
          detected during:
            instantiation of ""__nv_bool caffe2::CUDASparseLengthsSumGradientWithIndicesOp<T, Context>::RunOnDevice() [with T=float, Context=caffe2::CUDAContext]""
(1296): here
caffe2/caffe2/sgd/adagrad_fused_op_gpu.cu(1226): warning: variable ""N"" was declared but never referenced
          detected during:
            instantiation of ""__nv_bool caffe2::DispatchHelper<caffe2::TensorTypes2<FirstType, Types...>, ExtraArgs...>::call(Op *, caffe2::TypeMeta) [with FirstType=float, Types=<c10::Half>, ExtraArgs=<int32_t>, Op=caffe2::CUDARowWiseSparseAdagradFusedWithSparseLengthsSumGradientExactOp<float, int, false, caffe2::CUDAContext>]""
caffe2/caffe2/sgd/adagrad_fused_op_gpu.cu(259): warning: parameter ""indices"" was declared but never referenced
          detected during:
            instantiation of ""__nv_bool caffe2::CUDARowWiseSparseAdagradFusedWithSparseLengthsSumGradientExactOp<T, TLengths, is_mean, Context>::DoRunWithType2<IndexType,TParam>() [with T=float, TLengths=int, is_mean=false, Context=caffe2::CUDAContext, IndexType=int32_t, TParam=float]""
caffe2/caffe2/core/operator.h(1308): here
caffe2/caffe2/operators/piecewise_linear_transform_op.cu(15): warning: parameter ""num_grp"" was declared but never referenced

caffe2/caffe2/operators/piecewise_linear_transform_op.cu(50): warning: parameter ""M"" was declared but never referenced

caffe2/caffe2/operators/piecewise_linear_transform_op.cu(51): warning: parameter ""num_grp"" was declared but never referenced

caffe2/caffe2/operators/piecewise_linear_transform_op.cu(78): warning: parameter ""num_grp"" was declared but never referenced
```

Test Plan: Sandcastle

Differential Revision: D34034404

",pytorch
72411,r-barnes,pr,2022-02-07T06:15:25Z,Avoid type qualifier specified more than once,"Summary:
Fixes
```

caffe2/caffe2/operators/max_pool_with_index.cu(16): warning: type qualifier specified more than once

caffe2/caffe2/operators/max_pool_with_index.cu(28): warning: type qualifier specified more than once

caffe2/caffe2/operators/max_pool_with_index.cu(61): warning: type qualifier specified more than once

caffe2/caffe2/operators/max_pool_with_index.cu(62): warning: type qualifier specified more than once

caffe2/caffe2/operators/max_pool_with_index.cu(74): warning: type qualifier specified more than once
```

Test Plan: Sandcastle

Differential Revision: D34034382

",pytorch
72412,r-barnes,pr,2022-02-07T06:16:34Z,Fix unused variable warning,"Summary:
Fixes:
```
caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh(328): warning: parameter ""n"" was declared but never referenced
          detected during:
            instantiation of ""void at::cuda::<unnamed>::ApplyOp2<Op, scalar1, scalar2, IndexType, ADims, BDims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar1, IndexType> &, at::cuda::detail::TensorInfo<scalar2, IndexType> &, const Op &, int64_t, IndexType, Offsets..., Offsets...) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, remaining_steps=1, Offsets=<>]""
(370): here
            instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply2<Op,scalar1,scalar2,IndexType,ADims,BDims,step,max_threads_per_block,min_blocks_per_sm>(at::cuda::detail::TensorInfo<scalar1, IndexType>, at::cuda::detail::TensorInfo<scalar2, IndexType>, IndexType, Op) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, step=1, max_threads_per_block=512, min_blocks_per_sm=2]""
(487): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,step,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, step=1, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
(533): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(60): here
            instantiation of ""void <unnamed>::poisson_cuda_kernel<scalar_t>(at::Tensor &, const at::Tensor &, at::PhiloxCudaState) [with scalar_t=double]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(169): here

caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh(328): warning: parameter ""linearIndex"" was declared but never referenced
          detected during:
            instantiation of ""void at::cuda::<unnamed>::ApplyOp2<Op, scalar1, scalar2, IndexType, ADims, BDims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar1, IndexType> &, at::cuda::detail::TensorInfo<scalar2, IndexType> &, const Op &, int64_t, IndexType, Offsets..., Offsets...) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, remaining_steps=1, Offsets=<>]""
(370): here
            instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply2<Op,scalar1,scalar2,IndexType,ADims,BDims,step,max_threads_per_block,min_blocks_per_sm>(at::cuda::detail::TensorInfo<scalar1, IndexType>, at::cuda::detail::TensorInfo<scalar2, IndexType>, IndexType, Op) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, step=1, max_threads_per_block=512, min_blocks_per_sm=2]""
(487): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,step,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, step=1, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
(533): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(60): here
            instantiation of ""void <unnamed>::poisson_cuda_kernel<scalar_t>(at::Tensor &, const at::Tensor &, at::PhiloxCudaState) [with scalar_t=double]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(169): here
caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh(328): warning: parameter ""linearIndex"" was declared but never referenced
          detected during:
            instantiation of ""void at::cuda::<unnamed>::ApplyOp2<Op, scalar1, scalar2, IndexType, ADims, BDims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar1, IndexType> &, at::cuda::detail::TensorInfo<scalar2, IndexType> &, const Op &, int64_t, IndexType, Offsets..., Offsets...) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, remaining_steps=1, Offsets=<>]""
(370): here
            instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply2<Op,scalar1,scalar2,IndexType,ADims,BDims,step,max_threads_per_block,min_blocks_per_sm>(at::cuda::detail::TensorInfo<scalar1, IndexType>, at::cuda::detail::TensorInfo<scalar2, IndexType>, IndexType, Op) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, step=1, max_threads_per_block=512, min_blocks_per_sm=2]""
(487): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,step,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, step=1, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
(533): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(60): here
            instantiation of ""void <unnamed>::poisson_cuda_kernel<scalar_t>(at::Tensor &, const at::Tensor &, at::PhiloxCudaState) [with scalar_t=double]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(169): here

caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh(328): warning: parameter ""n"" was declared but never referenced
          detected during:
            instantiation of ""void at::cuda::<unnamed>::ApplyOp2<Op, scalar1, scalar2, IndexType, ADims, BDims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar1, IndexType> &, at::cuda::detail::TensorInfo<scalar2, IndexType> &, const Op &, int64_t, IndexType, Offsets..., Offsets...) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, remaining_steps=1, Offsets=<>]""
(370): here
            instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply2<Op,scalar1,scalar2,IndexType,ADims,BDims,step,max_threads_per_block,min_blocks_per_sm>(at::cuda::detail::TensorInfo<scalar1, IndexType>, at::cuda::detail::TensorInfo<scalar2, IndexType>, IndexType, Op) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, step=1, max_threads_per_block=512, min_blocks_per_sm=2]""
(487): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,step,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, step=1, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
(533): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(60): here
            instantiation of ""void <unnamed>::poisson_cuda_kernel<scalar_t>(at::Tensor &, const at::Tensor &, at::PhiloxCudaState) [with scalar_t=double]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(169): here

caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh(328): warning: parameter ""linearIndex"" was declared but never referenced
          detected during:
            instantiation of ""void at::cuda::<unnamed>::ApplyOp2<Op, scalar1, scalar2, IndexType, ADims, BDims, remaining_steps, Offsets...>::apply(at::cuda::detail::TensorInfo<scalar1, IndexType> &, at::cuda::detail::TensorInfo<scalar2, IndexType> &, const Op &, int64_t, IndexType, Offsets..., Offsets...) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, remaining_steps=1, Offsets=<>]""
(370): here
            instantiation of ""void at::cuda::<unnamed>::kernelPointwiseApply2<Op,scalar1,scalar2,IndexType,ADims,BDims,step,max_threads_per_block,min_blocks_per_sm>(at::cuda::detail::TensorInfo<scalar1, IndexType>, at::cuda::detail::TensorInfo<scalar2, IndexType>, IndexType, Op) [with Op=lambda [](double &, const double &)->void, scalar1=double, scalar2=double, IndexType=unsigned int, ADims=1, BDims=1, step=1, max_threads_per_block=512, min_blocks_per_sm=2]""
(487): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,step,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, step=1, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
(533): here
            instantiation of ""__nv_bool at::cuda::CUDA_tensor_apply2<scalar1,scalar2,Op,max_threads_per_block,min_blocks_per_sm>(at::Tensor, at::Tensor, Op, at::cuda::TensorArgType, at::cuda::TensorArgType) [with scalar1=double, scalar2=double, Op=lambda [](double &, const double &)->void, max_threads_per_block=512, min_blocks_per_sm=2]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(60): here
            instantiation of ""void <unnamed>::poisson_cuda_kernel<scalar_t>(at::Tensor &, const at::Tensor &, at::PhiloxCudaState) [with scalar_t=double]""
caffe2/aten/src/ATen/native/cuda/Distributions.cu(169): here
```

Test Plan: Sandcastle

Differential Revision: D34034374

",pytorch
72420,vfdev-5,pr,2022-02-07T11:57:59Z,Added missing antialias argument to functional.pyi.in,"
Description:
- Added missing antialias argument to functional.pyi.in
- mypy is happy if checking `interpolate` method with antialias argument

Related torchvision issue: https://github.com/pytorch/vision/pull/5329",pytorch
72438,r-barnes,pr,2022-02-07T17:18:43Z,Strip GCC5 stuff from PyTorch,"Summary: [This file](https://github.com/pytorch/pytorch/pull/63208/files) indicates that we don't support anything less than GCC 7.5. Given that, let's remove this GCC 5 stuff.

Test Plan: Sandcastle

Differential Revision: D34037127

",pytorch
72451,rohan-varma,pr,2022-02-07T20:46:42Z,[Easy] Small DDP fixes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

- Improve helper function
- Improve/fix some logging

Differential Revision: [D34044865](https://our.internmc.facebook.com/intern/diff/D34044865/)",pytorch
72454,rohan-varma,pr,2022-02-07T21:08:11Z,[Easy] Small DDP fixes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

- Improve helper function
- Improve/fix some logging

Differential Revision: [D34044865](https://our.internmc.facebook.com/intern/diff/D34044865/)",pytorch
72455,rohan-varma,pr,2022-02-07T21:23:23Z,[Easy] Small DDP fixes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #72456
* __->__ #72455

Pull request resolved: https://github.com/pytorch/pytorch/pull/72455

- Improve helper function to check debug level
- Small addition to logging

Differential Revision: [D34044865](https://our.internmc.facebook.com/intern/diff/D34044865/)",pytorch
72456,rohan-varma,pr,2022-02-07T21:23:29Z,Log static graph in constructor if it is set,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72456
* #72455

It is easier to log if static graph is set at construction time now that it is natively supported in DDP constructor, as opposed to waiting for the first iteration to finish. In some failure cases we're seeing the first iteration does not finish and thus we don't have this data which is vaulable to debug.

Differential Revision: [D34045204](https://our.internmc.facebook.com/intern/diff/D34045204/)",pytorch
72495,tillahoffmann,pr,2022-02-08T03:39:00Z,Add transformation using cdf of distribution.,"This PR adds a transform that uses the cumulative distribution function of a given probability distribution. 

For example, the following code constructs a simple Gaussian copula.

```python
# Construct a Gaussian copula from a multivariate normal.
base_dist = MultivariateNormal(
    loc=torch.zeros(2),
    scale_tril=LKJCholesky(2).sample(),
)
transform = CumulativeDistributionTransform(Normal(0, 1))
copula = TransformedDistribution(base_dist, [transform])
```

The following snippet creates a ""wrapped"" Gaussian copula for correlated positive variables with Weibull marginals.

```python
transforms = [
    CumulativeDistributionTransform(Normal(0, 1)),
    CumulativeDistributionTransform(Weibull(4, 2)).inv,
]
wrapped_copula = TransformedDistribution(base_dist, transforms)
```

cc @fritzo ",pytorch
72518,tillahoffmann,pr,2022-02-08T14:18:16Z,Implement gamma cdf.,Implements the cumulative distribution function for the gamma distribution. The tests needed a small adjustment to pass because gradients cannot be evaluated with respect to the first argument of the incomplete gamma function (and they're not needed for the test).,pytorch
72572,rohan-varma,pr,2022-02-09T06:02:44Z,[FSDP] Fix summon_full_params when not sharded,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #72573
* __->__ #72572

Use `continue` instead of `pass` which would result in AttributeError because `_full_param_padded` is not created for unsharded parameter when world_size == 1. Add a test to cover this case.

Differential Revision: [D34101124](https://our.internmc.facebook.com/intern/diff/D34101124/)",pytorch
72573,rohan-varma,pr,2022-02-09T06:02:49Z,[FSDP] Improved shape unflattening test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72573
* #72572

Verify shapes are restored appropriately in this test.

Differential Revision: [D34101125](https://our.internmc.facebook.com/intern/diff/D34101125/)",pytorch
72600,rohan-varma,pr,2022-02-09T19:29:28Z,[FSDP] Implement apply(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72600

Implements `apply()` which applies a `callable` of signature `f(m: Module) -> None` recursively to every submodule. The main difference from `nn.module.apply` is that this version summons the full parameters before apply() so it works appropriately with FSDP.

Closes https://github.com/pytorch/pytorch/issues/72554

Differential Revision: [D34111109](https://our.internmc.facebook.com/intern/diff/D34111109/)",pytorch
72626,ngimel,pr,2022-02-09T22:01:40Z,small optimizations of a view function,"Gets rid of double dispatch in _unsafe_view, and a few unnecessary checks in `alias_with_sizes_and_strides`
_unsafe_view (for 20 count) goes from ~41K -> 36K, view 58K -> 56K
",pytorch
72657,rohan-varma,pr,2022-02-10T16:31:06Z,[PG Wrapper] Small fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72657

_ProcessGroupWrapper check needs to be gated on Gloo availability,
this fails when gloo is not avail_ProcessGroupWrapper check needs to be gated
on Gloo availability, this fails when gloo is not avail.

Differential Revision: [D34144848](https://our.internmc.facebook.com/intern/diff/D34144848/)",pytorch
72682,micmelesse,pr,2022-02-10T22:58:37Z,[ROCM] Navi21 Enablement 2: Depthwise kernels,"This PR is a follow up to https://github.com/pytorch/pytorch/pull/69942.

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
72748,rohan-varma,pr,2022-02-11T21:50:25Z,[Checkpoint] Rename file,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #72754
* __->__ #72748

Removes underscore from file/class as directory is already private

Differential Revision: [D34179308](https://our.internmc.facebook.com/intern/diff/D34179308/)",pytorch
72754,rohan-varma,pr,2022-02-11T23:22:33Z,[monitored barrier] Slight logging enhancement,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72754
* #72748

Log to clarify that this msg is coming from rank 0

Differential Revision: [D34188480](https://our.internmc.facebook.com/intern/diff/D34188480/)",pytorch
72809,micmelesse,pr,2022-02-14T20:03:59Z,[ROCM] Navi21 Enablement 3: Embedding kernels,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
72925,rohan-varma,pr,2022-02-16T18:35:01Z,[Reland][FSDP] Implement apply(),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #72925

Reland with fix to add the owner string in test file

Differential Revision: [D34273858](https://our.internmc.facebook.com/intern/diff/D34273858/)",pytorch
72935,lezcano,pr,2022-02-16T20:08:53Z,Add linalg.lu_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #73806
* #73804
* #73803
* __->__ #72935

This PR adds `linalg.lu_solve`. While doing so, I found a bug in MAGMA
when calling the batched MAGMA backend with trans=True. We work around
that by solving the system solving two triangular systems.

We also update the heuristics for this function, as they were fairly
outdated. We found that cuSolver is king, so luckily we do not need to
rely on the buggy backend from magma for this function.

We added tests testing this function left and right. We also added tests
for the different backends. We also activated the tests for AMD, as
those should work as well.

### Benchmarking

<details>
<summary>
Benchmark Results (adjoint=False)
</summary>

```
--------------------------------------------------------------------------------------------- linalg.lu_solve CUDA ---------------------------------------------------------------------------------------------]   
                                          |  lu_solve looped_magma  |  lu_solve looped cusolver  |  lu_solve batched cublas  |  lu_solve batched magma  |  lu_solve unpack+solve_triangular  |  lu_solve heuristic   
1 threads: -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------   
      shape torch.Size([1, 1, 1])         |             750         |              34            |              28           |            252           |                  78                |           27          
      shape torch.Size([2, 1, 1])         |            1500         |              50            |              28           |            239           |                  85                |           27          
      shape torch.Size([4, 1, 1])         |            2995         |              83            |              28           |            239           |                  85                |           27          
      shape torch.Size([8, 1, 1])         |            6000         |             146            |              28           |            239           |                  85                |           27          
      shape torch.Size([16, 1, 1])        |           11900         |             272            |              28           |            241           |                  85                |           27          
      shape torch.Size([32, 1, 1])        |           23880         |             524            |              28           |            244           |                  85                |           27          
      shape torch.Size([64, 1, 1])        |           48000         |            1000            |              28           |            245           |                  85                |           27          
      shape torch.Size([128, 1, 1])       |           96000         |            2054            |              28           |            242           |                  86                |           27          
      shape torch.Size([512, 1, 1])       |          381900         |            8100            |              28           |            250           |                  85                |           27          
      shape torch.Size([1024, 1, 1])      |          763800         |           16200            |              28           |            257           |                  86                |           27          
      shape torch.Size([1, 2, 2])         |             750         |              33            |              28           |            240           |                  82                |           27          
      shape torch.Size([2, 2, 2])         |            1500         |              51            |              28           |            240           |                  88                |           27          
      shape torch.Size([4, 2, 2])         |            2991         |              82            |              28           |            241           |                  88                |           28          
      shape torch.Size([8, 2, 2])         |            6000         |             150            |              28           |            241           |                  88                |           27          
      shape torch.Size([16, 2, 2])        |           12000         |             275            |              28           |            242           |                  88                |           27          
      shape torch.Size([32, 2, 2])        |           23980         |             530            |              28           |            246           |                  90                |           28          
      shape torch.Size([64, 2, 2])        |           48000         |            1000            |              28           |            244           |                  89                |           27          
      shape torch.Size([128, 2, 2])       |           96000         |            2063            |              28           |            245           |                  89                |           28          
      shape torch.Size([512, 2, 2])       |          382000         |            8300            |              28           |            257           |                  89                |           28          
      shape torch.Size([1024, 2, 2])      |          764000         |           20000            |              28           |            271           |                  88                |           28          
      shape torch.Size([1, 8, 8])         |             749         |              34            |              28           |            243           |                  82                |           28          
      shape torch.Size([2, 8, 8])         |            1500         |              50            |              28           |            244           |                  89                |           28          
      shape torch.Size([4, 8, 8])         |            2988         |              83            |              28           |            244           |                  89                |           28          
      shape torch.Size([8, 8, 8])         |            5980         |             150            |              28           |            245           |                  88                |           28          
      shape torch.Size([16, 8, 8])        |           12000         |             278            |              28           |            246           |                  89                |           28          
      shape torch.Size([32, 8, 8])        |           23910         |             536            |              28           |            249           |                  89                |           28          
      shape torch.Size([64, 8, 8])        |           47800         |            1100            |              28           |            247           |                  89                |           28          
      shape torch.Size([128, 8, 8])       |           96000         |            2075            |              28           |            248           |                  89                |           28          
      shape torch.Size([512, 8, 8])       |          382100         |            8300            |              28           |            270           |                  89                |           28          
      shape torch.Size([1024, 8, 8])      |          764100         |           16400            |              28           |            291           |                  89                |           28          
      shape torch.Size([1, 16, 16])       |             750         |              33            |              28           |            248           |                  82                |           28       
      shape torch.Size([2, 16, 16])       |            1500         |              50            |              28           |            250           |                  89                |           28       
      shape torch.Size([4, 16, 16])       |            2996         |              83            |              28           |            250           |                  89                |           28       
      shape torch.Size([8, 16, 16])       |            5980         |             147            |              28           |            251           |                  90                |           28       
      shape torch.Size([16, 16, 16])      |           11900         |             274            |              28           |            251           |                  89                |           28       
      shape torch.Size([32, 16, 16])      |           24040         |             527            |              28           |            252           |                  89                |           28       
      shape torch.Size([64, 16, 16])      |           47800         |            1037            |              28           |            251           |                  89                |           28       
      shape torch.Size([128, 16, 16])     |           95600         |            2044            |              28           |            252           |                  89                |           28       
      shape torch.Size([512, 16, 16])     |          388200         |            8100            |              28           |            280           |                  88                |           28       
      shape torch.Size([1024, 16, 16])    |          769700         |           16000            |              28           |            322           |                  89                |           28       
      shape torch.Size([1, 32, 32])       |             760         |              33            |              28           |            255           |                  82                |           28       
      shape torch.Size([2, 32, 32])       |            1510         |              50            |              28           |            256           |                  89                |           28       
      shape torch.Size([4, 32, 32])       |            3022         |              82            |              31           |            256           |                  89                |           30       
      shape torch.Size([8, 32, 32])       |            6000         |             140            |              31           |            257           |                  89                |           31       
      shape torch.Size([16, 32, 32])      |           12000         |             281            |              31           |            258           |                  89                |           31       
      shape torch.Size([32, 32, 32])      |           24150         |             563            |              35           |            258           |                  89                |           35       
      shape torch.Size([64, 32, 32])      |           48300         |            1119            |              36           |            258           |                  90                |           36       
      shape torch.Size([128, 32, 32])     |           96500         |            2235            |              43           |            261           |                  88                |           43       
      shape torch.Size([512, 32, 32])     |          383100         |            8930            |              82           |            317           |                 137                |           82       
      shape torch.Size([1024, 32, 32])    |          766300         |           19200            |             122           |            400           |                 187                |          122       
      shape torch.Size([1, 64, 64])       |             760         |              33            |              55           |            272           |                  71                |           34       
      shape torch.Size([2, 64, 64])       |            1500         |              52            |              58           |            273           |                  90                |           52       
      shape torch.Size([4, 64, 64])       |            3127         |             102            |              65           |            273           |                 110                |           65       
      shape torch.Size([8, 64, 64])       |            6070         |             201            |              65           |            275           |                 160                |           65       
      shape torch.Size([16, 64, 64])      |           12000         |             399            |              66           |            274           |                  88                |           67       
      shape torch.Size([32, 64, 64])      |           23900         |             796            |              73           |            275           |                  91                |           73       
      shape torch.Size([64, 64, 64])      |           48000         |            1594            |              75           |            283           |                  94                |           76       
      shape torch.Size([128, 64, 64])     |           95000         |            3177            |              96           |            292           |                 122                |           96       
      shape torch.Size([512, 64, 64])     |          379300         |           13520            |             208           |            426           |                 313                |          208       
      shape torch.Size([1024, 64, 64])    |          758700         |           27100            |             306           |            570           |                 437                |          306       
      shape torch.Size([1, 128, 128])     |             750         |              42            |             115           |            306           |                  71                |           42       
      shape torch.Size([2, 128, 128])     |            1500         |              82            |             122           |            307           |                  90                |           83       
      shape torch.Size([4, 128, 128])     |            2966         |             162            |             136           |            307           |                 117                |          136       
      shape torch.Size([8, 128, 128])     |            5930         |             317            |             137           |            308           |                 202                |          138       
      shape torch.Size([16, 128, 128])    |           12000         |             635            |             143           |            316           |                 162                |          143       
      shape torch.Size([32, 128, 128])    |           23700         |            1266            |             152           |            322           |                 168                |          152       
      shape torch.Size([64, 128, 128])    |           48000         |            2668            |             177           |            337           |                 196                |          177       
      shape torch.Size([128, 128, 128])   |           96000         |            5366            |             228           |            365           |                 260                |          228       
      shape torch.Size([512, 128, 128])   |          379400         |           21490            |             502           |            620           |                 669                |          502       
      shape torch.Size([1024, 128, 128])  |          755700         |           43040            |             764           |            903           |                 978                |          770       
      shape torch.Size([1, 256, 256])     |             750         |              70            |             235           |            383           |                  78                |           72       
      shape torch.Size([2, 256, 256])     |            2000         |             138            |             250           |            384           |                 120                |          139       
      shape torch.Size([4, 256, 256])     |            2988         |             277            |             279           |            404           |                 191                |          278       
      shape torch.Size([8, 256, 256])     |            6100         |             546            |             283           |            420           |                 332                |          286       
      shape torch.Size([16, 256, 256])    |           12100         |            1149            |             330           |            441           |                 364                |          330       
      shape torch.Size([32, 256, 256])    |           24040         |            2303            |             359           |            453           |                 401                |          360       
      shape torch.Size([64, 256, 256])    |           48000         |            4626            |             408           |            472           |                 466                |          408       
      shape torch.Size([128, 256, 256])   |           94700         |            9247            |             543           |            543           |                 658                |          543       
      shape torch.Size([512, 256, 256])   |          372000         |           37030            |            1310           |           1185           |                1896                |         1310       
      shape torch.Size([1024, 256, 256])  |          747200         |           74100            |            2116           |           1910           |                3051                |         2122 
```
</details>

<details>
<summary>
Benchmark Results (adjoint=True)
</summary>

```
[----------------------------------------------------------------------------------------- linalg.lu_solve CUDA Adjoint -----------------------------------------------------------------------------------------]   
                                          |  lu_solve looped_magma  |  lu_solve looped cusolver  |  lu_solve batched cublas  |  lu_solve batched magma  |  lu_solve unpack+solve_triangular  |  lu_solve heuristic   
1 threads: -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------   
      shape torch.Size([1, 1, 1])         |             749         |              34            |              28           |             33           |                  60                |           27          
      shape torch.Size([2, 1, 1])         |            1500         |              50            |              28           |             50           |                  67                |           27          
      shape torch.Size([4, 1, 1])         |            3005         |              82            |              28           |             81           |                  67                |           27          
      shape torch.Size([8, 1, 1])         |            5999         |             145            |              28           |            140           |                  71                |           27          
      shape torch.Size([16, 1, 1])        |           12000         |             273            |              28           |             77           |                  67                |           27          
      shape torch.Size([32, 1, 1])        |           24000         |             522            |              28           |             78           |                  67                |           27          
      shape torch.Size([64, 1, 1])        |           48000         |            1000            |              28           |             77           |                  67                |           27          
      shape torch.Size([128, 1, 1])       |           96000         |            2029            |              28           |             78           |                  67                |           27          
      shape torch.Size([512, 1, 1])       |          383300         |            8100            |              28           |             78           |                  67                |           28          
      shape torch.Size([1024, 1, 1])      |          767500         |           16100            |              28           |             77           |                  68                |           27          
      shape torch.Size([1, 2, 2])         |             753         |              33            |              28           |             33           |                  60                |           28          
      shape torch.Size([2, 2, 2])         |            1500         |              50            |              28           |             50           |                  67                |           28          
      shape torch.Size([4, 2, 2])         |            3002         |              82            |              28           |             80           |                  67                |           27          
      shape torch.Size([8, 2, 2])         |            6000         |             145            |              28           |            144           |                  67                |           27          
      shape torch.Size([16, 2, 2])        |           12000         |             271            |              28           |             78           |                  66                |           27          
      shape torch.Size([32, 2, 2])        |           24120         |             524            |              28           |             78           |                  69                |           28          
      shape torch.Size([64, 2, 2])        |           48300         |            1030            |              28           |             78           |                  66                |           27          
      shape torch.Size([128, 2, 2])       |           96100         |            2041            |              28           |             78           |                  67                |           28          
      shape torch.Size([512, 2, 2])       |          383000         |            8100            |              28           |             79           |                  67                |           28          
      shape torch.Size([1024, 2, 2])      |          766100         |           16000            |              28           |             78           |                  67                |           28          
      shape torch.Size([1, 8, 8])         |             750         |              34            |              28           |             34           |                  60                |           28          
      shape torch.Size([2, 8, 8])         |            1500         |              50            |              28           |             50           |                  67                |           28          
      shape torch.Size([4, 8, 8])         |            2998         |              82            |              28           |             82           |                  67                |           28          
      shape torch.Size([8, 8, 8])         |            5990         |             146            |              28           |            150           |                  66                |           28          
      shape torch.Size([16, 8, 8])        |           11980         |             272            |              28           |             79           |                  67                |           28          
      shape torch.Size([32, 8, 8])        |           23970         |             530            |              28           |             79           |                  67                |           28          
      shape torch.Size([64, 8, 8])        |           47900         |            1040            |              28           |             79           |                  67                |           28          
      shape torch.Size([128, 8, 8])       |           96000         |            2048            |              28           |             78           |                  67                |           28          
      shape torch.Size([512, 8, 8])       |          383700         |            8100            |              28           |             80           |                  67                |           28          
      shape torch.Size([1024, 8, 8])      |          766200         |           16300            |              28           |             80           |                  68                |           28          
      shape torch.Size([1, 16, 16])       |             760         |              33            |              28           |             34           |                  60                |           28          
      shape torch.Size([2, 16, 16])       |            1500         |              50            |              28           |             50           |                  67                |           28          
      shape torch.Size([4, 16, 16])       |            3001         |              81            |              28           |             82           |                  67                |           28          
      shape torch.Size([8, 16, 16])       |            6000         |             145            |              28           |            140           |                  67                |           28          
      shape torch.Size([16, 16, 16])      |           12000         |             276            |              28           |             79           |                  67                |           28          
      shape torch.Size([32, 16, 16])      |           23870         |             549            |              28           |             79           |                  67                |           28          
      shape torch.Size([64, 16, 16])      |           47900         |            1098            |              29           |             80           |                  68                |           28          
      shape torch.Size([128, 16, 16])     |           95800         |            2184            |              28           |             79           |                  68                |           28          
      shape torch.Size([512, 16, 16])     |          386900         |            8769            |              28           |             80           |                  67                |           28          
      shape torch.Size([1024, 16, 16])    |          769800         |           17460            |              37           |             80           |                  67                |           37          
      shape torch.Size([1, 32, 32])       |             760         |              33            |              28           |             34           |                  60                |           28          
      shape torch.Size([2, 32, 32])       |            1500         |              50            |              28           |             50           |                  67                |           29          
      shape torch.Size([4, 32, 32])       |            3021         |              86            |              31           |             84           |                  67                |           32          
      shape torch.Size([8, 32, 32])       |            6040         |             167            |              32           |            167           |                  67                |           32          
      shape torch.Size([16, 32, 32])      |           12100         |             330            |              33           |             78           |                  67                |           33          
      shape torch.Size([32, 32, 32])      |           24150         |             662            |              35           |             78           |                  66                |           35          
      shape torch.Size([64, 32, 32])      |           48200         |            1323            |              36           |             79           |                  67                |           36          
      shape torch.Size([128, 32, 32])     |           97000         |            2637            |              44           |             79           |                  67                |           43          
      shape torch.Size([512, 32, 32])     |          382500         |           10580            |              83           |            180           |                 121                |           83          
      shape torch.Size([1024, 32, 32])    |          766600         |           22670            |             123           |            260           |                 165                |          120        
      shape torch.Size([1, 64, 64])       |             760         |              33            |              58           |             33           |                  49                |           34       
      shape torch.Size([2, 64, 64])       |            1520         |              60            |              60           |             60           |                  67                |           59       
      shape torch.Size([4, 64, 64])       |            3016         |             115            |              67           |            119           |                  90                |           66       
      shape torch.Size([8, 64, 64])       |            6120         |             230            |              67           |            233           |                 153                |           68       
      shape torch.Size([16, 64, 64])      |           12100         |             457            |              69           |             86           |                  77                |           69       
      shape torch.Size([32, 64, 64])      |           24000         |             912            |              74           |             95           |                  80                |           74       
      shape torch.Size([64, 64, 64])      |           48000         |            1833            |              76           |            106           |                  82                |           76       
      shape torch.Size([128, 64, 64])     |           95000         |            3636            |              97           |            163           |                 108                |           97       
      shape torch.Size([512, 64, 64])     |          380600         |           15600            |             210           |            464           |                 278                |          210       
      shape torch.Size([1024, 64, 64])    |          761200         |           31140            |             308           |            741           |                 377                |          308       
      shape torch.Size([1, 128, 128])     |             756         |              46            |             120           |             47           |                  49                |           46       
      shape torch.Size([2, 128, 128])     |            1500         |              91            |             123           |             89           |                  67                |           92       
      shape torch.Size([4, 128, 128])     |            2994         |             178            |             139           |            180           |                 117                |          139       
      shape torch.Size([8, 128, 128])     |            5960         |             350            |             140           |            354           |                 214                |          142       
      shape torch.Size([16, 128, 128])    |           12000         |             701            |             144           |            177           |                 148                |          143       
      shape torch.Size([32, 128, 128])    |           23870         |            1401            |             155           |            225           |                 159                |          155       
      shape torch.Size([64, 128, 128])    |           47600         |            2948            |             179           |            288           |                 184                |          180       
      shape torch.Size([128, 128, 128])   |           96000         |            5910            |             231           |            442           |                 242                |          231       
      shape torch.Size([512, 128, 128])   |          381200         |           23640            |             519           |           1400           |                 643                |          519       
      shape torch.Size([1024, 128, 128])  |          755800         |           47340            |             794           |           2436           |                 930                |          794       
      shape torch.Size([1, 256, 256])     |             760         |              74            |             246           |             77           |                  62                |           78       
      shape torch.Size([2, 256, 256])     |            1510         |             150            |             256           |            150           |                 106                |          150       
      shape torch.Size([4, 256, 256])     |            3030         |             296            |             284           |            296           |                 183                |          284       
      shape torch.Size([8, 256, 256])     |            6100         |             588            |             286           |            592           |                 337                |          288       
      shape torch.Size([16, 256, 256])    |           12200         |            1238            |             330           |            445           |                 330                |          330       
      shape torch.Size([32, 256, 256])    |           24430         |            2476            |             368           |            568           |                 365                |          367       
      shape torch.Size([64, 256, 256])    |           49000         |            4950            |             415           |            800           |                 408                |          414       
      shape torch.Size([128, 256, 256])   |           96000         |            9900            |             552           |           1330           |                 561                |          553       
      shape torch.Size([512, 256, 256])   |          369400         |           39580            |            1410           |           4614           |                1649                |         1410       
      shape torch.Size([1024, 256, 256])  |          716200         |           79200            |            2270           |           8472           |                2528                |         2277      

Times are in microseconds (us).
```

</details>

To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`.
<details>
<summary>
Benchmarking script
</summary>

```python
import torch
import pickle
import itertools
from functools import partial
from torch.utils.benchmark import Timer, Compare

name = ""heuristic""
label = ""lu_solve {}"".format(name)
shapes = [1, 2, 8, 16, 32, 64, 128, 256]
batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]
results = []
make_arg = partial(torch.randn, dtype=torch.float32, device=""cuda"")

for n, batch in itertools.product(shapes, batches):
    LU, pivots = torch.linalg.lu_factor(make_arg(batch + (n, n)))
    B = make_arg(batch + (n, 1))
    print(LU.shape)
    stmt = ""torch.linalg.lu_solve(LU, pivots, B, adjoint=adjoint)""
    for adjoint in (True, False):
        timer = Timer(stmt,
                      globals=globals(),
                      label=""linalg.lu_solve CUDA{}"".format("" Adjoint"" if adjoint else """"),
                      description=label,
                      sub_label=f""shape {LU.shape}"",
                      num_threads=1)
        results.append(timer.blocked_autorange())


compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(""{}_lu_solve.pickle"".format(name), 'wb') as f:
    pickle.dump(results, f)
```
</details>

Finally, I joined all the results with the following script:
<details>
<summary>
Script to join the results
</summary>

```python
import pickle
from torch.utils.benchmark import Timer, Compare

files = [
         ""looped_magma"",
         ""looped cusolver"",
         ""batched cublas"",
         ""batched magma"",
         ""unpack+solve_triangular"",
         ""heuristic"",
        ]

timers = []
for name in files:
    with open(""{}_lu_solve.pickle"".format(name), 'rb') as f:
        timers += pickle.load(f)

compare = Compare(timers)
compare.trim_significant_figures()
compare.print()
```
</details>

### Fix for Magma's batched lu_solve when `adjoint=True`
I also developed the following fix around MAGMA's bug, but I ended up not using it, and preferring the triangular solves over it, as they were faster. I'm leaving it here in case it's useful in the future.

<details>
<summary>
Fix for MAGMA's issue with `adjoint=True`
</summary>

```cpp
auto lu_solve_batched_magma_fn = [m](const Tensor& LU, const Tensor& pivots, const Tensor& B, TransposeType trans) {
    if (trans == TransposeType::NoTranspose) {
      lu_solve_batched_magma(LU, pivots, B, trans);
      return;
    }
    // There's a bug in magma for the other cases, so we need to properly perform mT or mH on LU
    // The LU of the transpose is not the transpose of the LU
    // We need to do LU = LDU' = L'U' where L' = LD, U' = D^{-1}U and D = diag(U)
    auto diag = LU.diagonal(0, -2, -1);
    auto LU_f = LU.tril(-1).mul_(diag.unsqueeze(-2)) +
                LU.triu(1).div_(diag.unsqueeze(-1));
    LU_f.diagonal(0, -2, -1).copy_(diag);

    if (trans == TransposeType::ConjTranspose) {
      LU_f = LU_f.conj_physical();
    }
    LU_f.transpose(-2, -1);
    // At this point LU_f is F-contiguous, because triu / tril / conj_phisical return contiguous tensors
    TORCH_INTERNAL_ASSERT_DEBUG_ONLY(LU_f.mT().is_contiguous());

    // Trivial permutation
    auto pivots_aux = at::arange(1, m + 1, pivots.options()).expand_as(pivots).contiguous();

    lu_solve_batched_magma(LU_f, pivots_aux, B, TransposeType::NoTranspose);

    // We then need to multiply B by P on the right as (PLU)^T = B iff U^TL^T = BP
    // Fill `perm` with the identity permutation (perhaps batched)
    // This is faster than torch.lu_unpack + matmul, as this logic is borrowed from lu_unpack
    const auto perm = at::arange(m, pivots.options().dtype(kLong)).expand(pivots.sizes()).contiguous();
    auto iter = TensorIteratorConfig()
      .set_check_mem_overlap(false)
      .check_all_same_dtype(false)
      .resize_outputs(false)
      .declare_static_shape(pivots.sizes(), /*squash_dim=*/pivots.dim() - 1)
      .add_output(perm)
      .add_input(pivots)
      .build();
    unpack_pivots_stub(pivots.device().type(), iter, m);
    B.scatter_(-2, perm.unsqueeze(-1).expand_as(B), B.clone());
};
```

</details>



Fixes https://github.com/pytorch/pytorch/issues/61657",pytorch
73018,vfdev-5,pr,2022-02-17T17:48:55Z,Fixed docstring typo for nn.Module.get_submodule,"Description:
- Fixed docstring typo for nn.Module.get_submodule

otherwise output is invisible: https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.get_submodule
",pytorch
73057,guoyejun,pr,2022-02-18T01:56:54Z,codegen/templates: disable clang-format check,"It is possible that external backends check the generated code
by clang-format with different configs, and so just disable the
check.",pytorch
73146,r-barnes,pr,2022-02-19T23:15:08Z,Fix binary search in bisect_percentile_op,"Summary: Binary search can overflow; this fixes it.

Test Plan: Sandcastle

Differential Revision: D34365186

",pytorch
73147,r-barnes,pr,2022-02-19T23:16:20Z,Fix nasty bug in bisect_percentile_op,"Summary:
Code used `reserve` instead of `resize` leading to platform010 test failures:
```
Trying example: test_bisect_percentil_op_large(
    self=<caffe2.caffe2.python.operator_test.bisect_percentile_op_test.TestBisectPercentileOp testMethod=test_bisect_percentil_op_large>,
    N=20,
    lengths=[2, 2],
    max_value=100,
    discrete=False,
    p=0.0,
    gc=,
    dc=[],
)

stderr:
E0219 13:14:52.601948 995877 JustKnobsConfigeratorLoader.cpp:114] Failed to load config justknobs/movefast/knobs after 55000ms timeout
E0219 13:14:52.602150 995877 JustKnobsConfigeratorLoader.cpp:114] Failed to load config justknobs/pytorch/compiler after 55000ms timeout
test_bisect_percentil_op_large (caffe2.caffe2.python.operator_test.bisect_percentile_op_test.TestBisectPercentileOp) ... third-party-buck/platform010/build/libgcc/include/c++/trunk/bits/stl_vector.h:1045: std::vector::reference std::vector<int>::operator[](std::vector::size_type) [_Tp = int, _Alloc = std::allocator<int>]: Assertion '__n < this->size()' failed.
*** Aborted at 1645305292 (Unix time, try 'date -d 1645305292') ***
*** Signal 6 (SIGABRT) (0x8556000f3225) received by PID 995877 (pthread TID 0x7f13a79c51c0) (linux TID 995877) (maybe from PID 995877, UID 34134) (code: -6), stack trace: ***
W0219 13:14:52.682251 995932 RetryingSender.cpp:433] Failed to make rpc. Sender name: pr-scubasing. Reason: apache::thrift::transport::TTransportException: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused.
    @ 000000000000431b folly::symbolizer::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)
                       ./folly/experimental/symbolizer/SignalHandler.cpp:449
    @ 0000000000000000 (unknown)
    @ 000000000009c9f3 __GI___pthread_kill
```

Test Plan: Sandcastle

Differential Revision: D34365188

",pytorch
73148,r-barnes,pr,2022-02-19T23:16:21Z,Clean up bisect_percentile_op,"Summary: Makes a bunch of things const, eliminates extraneous variables

Test Plan: Sandcastle

Differential Revision: D34365183

",pytorch
73151,r-barnes,pr,2022-02-20T04:04:01Z,Clean up some unused variable warnings,"Differential Revision: D34365492

",pytorch
73201,guoyejun,pr,2022-02-22T06:06:44Z,"Allow ingesters of gen_backend_stubs codegen to customize ""class name""","When a backend (and its corresponding dispatch key) like `XPU` or `Lazy` runs the external codegen pipeline, all of its kernels get generated as methods inside of a `{DispatchKey}NativeFunctions` class. This PR allows the backend to customize the generated name if they wish to.",pytorch
73215,ziky90,pr,2022-02-22T17:34:42Z,[PyTorch/d2go] fix optim _multi_tensor,"Summary: Fixing an issue in optimizers from _multi_tensor, for `sgd_mt` introduced in https://github.com/pytorch/pytorch/commit/2cb03e926f013493cb3986bb1c9446594b602385

Differential Revision: D34389034

",pytorch
73239,r-barnes,pr,2022-02-22T20:54:55Z,Fix more binary search overflow issues,"Test Plan: Sandcastle

Differential Revision: D34372187

",pytorch
73242,rohan-varma,pr,2022-02-22T21:12:08Z,[FSDP][BE] get_full_params uses summon_full_params,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73242
* #73314

Can use summon_full_params instead.

Differential Revision: [D34399789](https://our.internmc.facebook.com/intern/diff/D34399789/)",pytorch
73251,neerajprad,pr,2022-02-22T22:35:05Z,Fix discrete sampler test to correctly run Chi2 test,"Summary: Scipy's chisquare test requires that the observed frequencies should sum up to the same number as the expected frequences. This modifies `_check_sampler_discrete` to ensure that two match. See: https://github.com/scipy/scipy/issues/12282 for details.

Test Plan: Unit tests pass on platform010

Differential Revision: D34402314

",pytorch
73261,rohan-varma,pr,2022-02-22T23:53:04Z,Skip optimizer overlap tests that have issues with NCCL async error handling,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73295
* __->__ #73261

Skip these tests which sometimes have issues on unrelated PRs such as
https://github.com/pytorch/pytorch/runs/5291461671?check_suite_focus=true. See
https://github.com/pytorch/pytorch/issues/73259 for additional detail 

Differential Revision: [D34404857](https://our.internmc.facebook.com/intern/diff/D34404857/)",pytorch
73295,rohan-varma,pr,2022-02-23T15:57:38Z,[FSDP][Reland] Implement local_state_dict and load_local_state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73295

1. Implement the framework to allow user to choose among `state_dict`, `local_state_dict`, and `sharded_state_dict`.
2. Implement ShardedTensor compatible local_state_dict() and load_local_state_dict().

Differential Revision: [D34383925](https://our.internmc.facebook.com/intern/diff/D34383925/)",pytorch
73314,rohan-varma,pr,2022-02-23T21:38:54Z,[FSDP] summon_full_params fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73242
* __->__ #73314

Needs to synchronize all_gather stream. Added test fails without this
fix

Differential Revision: [D34430602](https://our.internmc.facebook.com/intern/diff/D34430602/)",pytorch
73323,rohan-varma,pr,2022-02-23T23:04:57Z,[FSDP] Generic arguments for state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73324
* __->__ #73323

These arguments are just passed to nn.Module APIs, make them generic
args/kwargs so the main APIs can update their args and we won't have to.

Differential Revision: [D34431023](https://our.internmc.facebook.com/intern/diff/D34431023/)",pytorch
73324,rohan-varma,pr,2022-02-23T23:05:03Z,[FSDP] full_state_dict impl,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73324
* #73323

Implements `state_dict` and `load_state_dict` APIs for FSDP, with the following limitations:

1. Does not support `state_dict_device` (i.e. specifying which device params should be on) which fairscale does currently support
2. Does not yet support offload of state_dict onto CPU
3. Loads state_dict on all ranks currently. In the future we could add support for loading this on only rank 0, to avoid redundancy across ranks as usually only one rank is responsible for saving/loading the model. Along with (2) this would enable larger models to have state_dict called.

As discussed in FSDP checkpoint API proposal, `state_dict` will basically be a `full_state_dict` where full parameters are returned on all ranks. As a result this implies that the model must actually be able to fit on a single GPU.

Differential Revision: [D34433514](https://our.internmc.facebook.com/intern/diff/D34433514/)",pytorch
73325,rohan-varma,pr,2022-02-23T23:05:10Z,[FSDP] generic argument forward for load_local_state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73325

Make sure we don't have to modify this function if the API signature
of load_state_dict changes, because we don't really handle these args, just
pass them on to nn.Module impl.

Differential Revision: [D34433606](https://our.internmc.facebook.com/intern/diff/D34433606/)",pytorch
73366,rohan-varma,pr,2022-02-24T18:48:20Z,[FSDP] Add state_dict() save/reload in parity test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73532
* #73453
* __->__ #73366

Adds state_dict() save/reload in parity with DDP test to ensure
checkpointing doesn't cause issue with accuracy/model params.

Differential Revision: [D34434358](https://our.internmc.facebook.com/intern/diff/D34434358/)",pytorch
73374,rohan-varma,pr,2022-02-24T19:48:39Z,Manual skip sparse tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73374

manual skip because not properly disabled by automation

Differential Revision: [D34456851](https://our.internmc.facebook.com/intern/diff/D34456851/)",pytorch
73389,rohan-varma,pr,2022-02-24T21:28:47Z,"Add world size, # of avail cuda devices when running distributed tests","Will help clarify / debug the world size and total # of cuda devices avail on the runner.
",pytorch
73414,r-barnes,pr,2022-02-25T03:23:55Z,Don't use vector accessor methods to do pointer math; unblock platform010,"Summary:
The code here previously used this creative pointer math
```
const auto end = reinterpret_cast<uintptr_t>(     &managed_tensor_storage_impls_.at(managed_tensor_storage_impls_.size()));
```
This has the form
```
const auto end = &A[N];
```
this works just fine if `A` is C-style array since `&A[N]` can get transformed to `(A+N)` where `A` is a simple pointer without ever dereferencing.

But this is C++ and `A` is a vector, so `A[N]` calls the accessor method, reaches into an illegal place in memory, and then we get the address of that. (Or so I deduce.)

We sidestep the issue by using `data()` to get the desired memory address directly.

Test Plan: Sandcastle

Differential Revision: D34468166

",pytorch
73453,rohan-varma,pr,2022-02-25T19:40:16Z,[FSDP] Improve the documentation of state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73532
* __->__ #73453
* #73366

Add more documentation to state_dict and local_state_dict including
basic examplesAdd more documentation to state_dict and local_state_dict
including basic examples.

Differential Revision: [D34483812](https://our.internmc.facebook.com/intern/diff/D34483812/)",pytorch
73468,ngimel,pr,2022-02-26T06:17:25Z,remove _s_where,"Per title
Fixes #73135
",pytorch
73532,rohan-varma,pr,2022-02-28T20:34:24Z,[FSDP] s/local_state_dict/_local_state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73532
* #73453
* #73366

Also a few other small changes to documentation.

Differential Revision: [D34524478](https://our.internmc.facebook.com/intern/diff/D34524478/)",pytorch
73543,micmelesse,pr,2022-02-28T23:18:58Z,[ROCM] Navi21 Enablement 4: Normalization kernels,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73545,micmelesse,pr,2022-02-28T23:29:12Z,[ROCM] Navi21 Enablement 5: Softmax kernels,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809
https://github.com/pytorch/pytorch/pull/73543

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73546,micmelesse,pr,2022-02-28T23:34:52Z,[ROCM] Navi21 Enablement 6: Tensor kernels ,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809
https://github.com/pytorch/pytorch/pull/73543
https://github.com/pytorch/pytorch/pull/73545

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73548,micmelesse,pr,2022-02-28T23:40:50Z,[ROCM] Navi21 Enablement 7: Sparse kernels ,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809
https://github.com/pytorch/pytorch/pull/73543
https://github.com/pytorch/pytorch/pull/73545
https://github.com/pytorch/pytorch/pull/73546

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73549,micmelesse,pr,2022-02-28T23:46:09Z,"[ROCM] Navi21 Enablement 8:  Index, Repeat and Sort kernels","This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809
https://github.com/pytorch/pytorch/pull/73543
https://github.com/pytorch/pytorch/pull/73545
https://github.com/pytorch/pytorch/pull/73546
https://github.com/pytorch/pytorch/pull/73548

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73550,micmelesse,pr,2022-02-28T23:49:18Z,[ROCM] Navi21 Enablement 9: Range and Multinomial Kernels,"This PR is a follow up to the following prs.
https://github.com/pytorch/pytorch/pull/69942
https://github.com/pytorch/pytorch/pull/72682
https://github.com/pytorch/pytorch/pull/72809
https://github.com/pytorch/pytorch/pull/73543
https://github.com/pytorch/pytorch/pull/73545
https://github.com/pytorch/pytorch/pull/73546
https://github.com/pytorch/pytorch/pull/73548
https://github.com/pytorch/pytorch/pull/73549

We are adding support to Navi21 GPUs which have a warpsize of 32. We cannot rely on a constant so we have to dynamically look up the warpsize when launching the kernel on the host side. Inside device functions this is not needed and the compiler can correctly detect the correct warpsize to replace the C10_WARP_SIZE constant.",pytorch
73551,rohan-varma,pr,2022-03-01T00:20:22Z,[Easy][Tests] Rename module in test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73551

Rename to better indicate what it is.

Differential Revision: [D34537964](https://our.internmc.facebook.com/intern/diff/D34537964/)",pytorch
73579,ngimel,pr,2022-03-01T17:49:25Z,"Back out ""Revert D34524207: [pytorch][PR] remove _s_where""","Summary:
Original commit changeset: 87b1220d851c

Original Phabricator Diff: D34524207 (https://github.com/pytorch/pytorch/commit/4eb248256801103b08726bf5d85496641cebcdbb)

Test Plan: OSS tests

Differential Revision: D34554432

",pytorch
73625,r-barnes,pr,2022-03-02T03:39:11Z,Eliminate unused parameters in PyTorch,"Summary: Unused parameters cause compiler warnings which distract from real issues. Let's remove unused parameters!

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D34567733

",pytorch
73687,rohan-varma,pr,2022-03-02T21:55:05Z,[FSDP] Add always_wrap policy,"Add a smaller helper policy that always returns True to automatically always wrap all FSDP submodules. This is the first and simplest step of providing a set of policies that allow users to seamlessly experiment with different FSDP config. 

More Context: https://github.com/pytorch/pytorch/issues/68789",pytorch
73691,rohan-varma,pr,2022-03-02T22:10:47Z,[not for land] FSDP meta,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73691

Differential Revision: [D33797106](https://our.internmc.facebook.com/intern/diff/D33797106/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D33797106/)!",pytorch
73700,rohan-varma,pr,2022-03-02T22:53:57Z,[Not for land] Fsdp meta,"Fixes #ISSUE_NUMBER
",pytorch
73747,fritzo,pr,2022-03-03T18:26:59Z,Fix docstring hiding due to #45689,"Fixes hiding of the `CatTransform` docstring due to a misplaced type annotation in #45689.
Also fixes that annotation and adds to to `StackTransform` to keep `CatTransform` similar.
",pytorch
73749,r-barnes,pr,2022-03-03T18:55:05Z,Eliminate unused parameters in PyTorch,"Summary: Unused parameters cause compiler warnings which distract from real issues. Let's remove unused parameters!

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D34567731

",pytorch
73751,r-barnes,pr,2022-03-03T18:58:11Z,Eliminate unused parameters in PyTorch,"Summary: Unused parameters cause compiler warnings which distract from real issues. Let's remove unused parameters!

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D34567734

",pytorch
73759,rohan-varma,pr,2022-03-03T22:57:43Z,[FSDP][BE] s/ConfigAutoWrap/_ConfigAutoWrap,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #73759

Closes https://github.com/pytorch/pytorch/issues/73757

Differential Revision: [D34627118](https://our.internmc.facebook.com/intern/diff/D34627118/)",pytorch
73803,lezcano,pr,2022-03-04T19:17:25Z,Update torch.lu_unpack docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #73806
* #73804
* __->__ #73803
* #72935

As per title",pytorch
73804,lezcano,pr,2022-03-04T19:17:31Z,Deprecate torch.lu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #73806
* __->__ #73804
* #73803
* #72935

**BC-breaking note**:

This PR deprecates `torch.lu` in favor of `torch.linalg.lu_factor`.
A upgrade guide is added to the documentation for `torch.lu`.

Note this PR DOES NOT remove `torch.lu`.",pytorch
73806,lezcano,pr,2022-03-04T19:31:48Z,Deprecate torch.lu_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* __->__ #73806
* #73804
* #73803
* #72935

**BC-breaking note**:

This PR deprecates `torch.lu_solve` in favor of `torch.linalg.lu_solve_factor`.
A upgrade guide is added to the documentation for `torch.lu_solve`.

Note this PR DOES NOT remove `torch.lu_solve`.",pytorch
73821,r-barnes,pr,2022-03-05T03:22:20Z,Suppress unused parameters in PyTorch,"Differential Revision: D34631600

",pytorch
73877,lezcano,pr,2022-03-07T21:35:50Z,Expose lu_factor_batched_cublas,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* __->__ #73877
* #74046

We had bindings for this function, but they were not exposed as a
function inside ATen. This PR exposes them.

This function is tested in the next PR of the stack, where it is used in
`linalg.lu_factor`.",pytorch
73878,lezcano,pr,2022-03-07T21:35:55Z,Update and improve the heuristics for linalg.lu_factor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #73878
* #73877
* #74046

This PR adds getrf_cublas to the functions considered in the heuristics
for `lu_factor`. It also updates the heuristics of the function.

## Benchmark

I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.

<details>
<summary>
Benchmark Results
</summary>

```
[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          
                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          
1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          
      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    
      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    
      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    
      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    
      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    
      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    
      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    
      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    
      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    
      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    
      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    
      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    
      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    
      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    
      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    
      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    
      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    
      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    
      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    
      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    
      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    
      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    
      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    
      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    
      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    
      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    
      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    
      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    
      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    
      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    
      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    
      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    
      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          
      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          
      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          
      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          
      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          
      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          
      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          
      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          
      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          
      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          
      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          
      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          
      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          
      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          
      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          
      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          
      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          
      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          
      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          
      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          
      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          
      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          
      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          
      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          
      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          
      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          
      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          
      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          
      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          
      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          
      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          
      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          
      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          
      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          
      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          
      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          
      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          
      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          
      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          
      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          
      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          
      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          
      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          
      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          
      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          
      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          
      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          
      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          
```

</details>

<details>
<summary>
Benchmark Results all algorithms up to `n=2048`
</summary>

```
[----------------------------------------------------------------- linalg.lu_factor CUDA ------------------------------------------------------------------]
                                          |  lu_factor_magma_batched  |  lu_factor_cusolver_batched  |  lu_factor_cusolver_looped  |  lu_factor_magma_looped
1 threads: -------------------------------------------------------------------------------------------------------------------------------------------------
      shape torch.Size([1, 1, 1])         |               51          |                30            |                27           |            1390
      shape torch.Size([2, 1, 1])         |               42          |                20            |                26           |            2798
      shape torch.Size([4, 1, 1])         |               42          |                20            |                42           |            5589
      shape torch.Size([8, 1, 1])         |               42          |                20            |                72           |           11000
      shape torch.Size([16, 1, 1])        |               42          |                20            |               132           |           22400
      shape torch.Size([32, 1, 1])        |               42          |                20            |               253           |           44620
      shape torch.Size([64, 1, 1])        |               42          |                20            |               496           |           89200
      shape torch.Size([128, 1, 1])       |               42          |                20            |               980           |          180000
      shape torch.Size([512, 1, 1])       |               43          |                20            |              3868           |          714100
      shape torch.Size([1024, 1, 1])      |               44          |                20            |              7800           |         1430000
      shape torch.Size([1, 2, 2])         |               43          |                21            |                19           |            1400
      shape torch.Size([2, 2, 2])         |               42          |                21            |                27           |            2898
      shape torch.Size([4, 2, 2])         |               43          |                21            |                42           |            5800
      shape torch.Size([8, 2, 2])         |               43          |                21            |                73           |           11600
      shape torch.Size([16, 2, 2])        |               43          |                21            |               133           |           23170
      shape torch.Size([32, 2, 2])        |               43          |                21            |               254           |           46290
      shape torch.Size([64, 2, 2])        |               43          |                21            |               500           |           94000
      shape torch.Size([128, 2, 2])       |               43          |                21            |               980           |          190000
      shape torch.Size([512, 2, 2])       |               44          |                21            |              3860           |          741900
      shape torch.Size([1024, 2, 2])      |               44          |                21            |              7640           |         1484000
      shape torch.Size([1, 8, 8])         |               45          |                21            |                19           |            1450
      shape torch.Size([2, 8, 8])         |               45          |                21            |                27           |            2917
      shape torch.Size([4, 8, 8])         |               45          |                21            |                53           |            5800
      shape torch.Size([8, 8, 8])         |               45          |                21            |               105           |           11580
      shape torch.Size([16, 8, 8])        |               45          |                21            |               207           |           23160
      shape torch.Size([32, 8, 8])        |               46          |                21            |               413           |           46400
      shape torch.Size([64, 8, 8])        |               46          |                21            |               824           |           93000
      shape torch.Size([128, 8, 8])       |               46          |                21            |              1645           |          185000
      shape torch.Size([512, 8, 8])       |               47          |                21            |              6574           |          742000
      shape torch.Size([1024, 8, 8])      |               49          |                21            |             13150           |         1481000
      shape torch.Size([1, 16, 16])       |               49          |                21            |                24           |            1460
      shape torch.Size([2, 16, 16])       |               49          |                21            |                46           |            2902
      shape torch.Size([4, 16, 16])       |               49          |                21            |                90           |            5800
      shape torch.Size([8, 16, 16])       |               49          |                21            |               177           |           11600
      shape torch.Size([16, 16, 16])      |               49          |                21            |               352           |           23150
      shape torch.Size([32, 16, 16])      |               49          |                21            |               703           |           46300
      shape torch.Size([64, 16, 16])      |               49          |                21            |              1404           |           92700
      shape torch.Size([128, 16, 16])     |               50          |                21            |              2807           |          185000
      shape torch.Size([512, 16, 16])     |               55          |                29            |             11220           |          741700
      shape torch.Size([1024, 16, 16])    |               64          |                42            |             22440           |         1480000
      shape torch.Size([1, 32, 32])       |               55          |                56            |                58           |            1460
      shape torch.Size([2, 32, 32])       |               55          |                57            |               114           |            2920
      shape torch.Size([4, 32, 32])       |               55          |                57            |               225           |            5830
      shape torch.Size([8, 32, 32])       |               55          |                61            |               449           |           11700
      shape torch.Size([16, 32, 32])      |               56          |                61            |               896           |           23300
      shape torch.Size([32, 32, 32])      |               56          |                62            |              1791           |           46600
      shape torch.Size([64, 32, 32])      |               56          |                63            |              3581           |           93100
      shape torch.Size([128, 32, 32])     |               58          |                66            |              7156           |          186000
      shape torch.Size([512, 32, 32])     |              100          |               194            |             28700           |          742400
      shape torch.Size([1024, 32, 32])    |              169          |               335            |             57620           |         1485000
      shape torch.Size([1, 64, 64])       |              224          |               101            |               132           |            1500
      shape torch.Size([2, 64, 64])       |              227          |               100            |               262           |            2951
      shape torch.Size([4, 64, 64])       |              229          |               101            |               523           |            5890
      shape torch.Size([8, 64, 64])       |              231          |               102            |              1040           |           12000
      shape torch.Size([16, 64, 64])      |              237          |               109            |              2088           |           23530
      shape torch.Size([32, 64, 64])      |              242          |               127            |              4171           |           46900
      shape torch.Size([64, 64, 64])      |              247          |               156            |              8330           |           95000
      shape torch.Size([128, 64, 64])     |              293          |               244            |             16710           |          189000
      shape torch.Size([512, 64, 64])     |              685          |              1180            |             67000           |          750900
      shape torch.Size([1024, 64, 64])    |             1300          |              2076            |            134000           |         1505000
      shape torch.Size([1, 128, 128])     |              490          |               309            |               298           |            1560
      shape torch.Size([2, 128, 128])     |              503          |               309            |               594           |            3120
      shape torch.Size([4, 128, 128])     |              515          |               312            |              1185           |            6230
      shape torch.Size([8, 128, 128])     |              523          |               317            |              2370           |           12500
      shape torch.Size([16, 128, 128])    |              547          |               336            |              4734           |           24890
      shape torch.Size([32, 128, 128])    |              596          |               472            |              9491           |           49800
      shape torch.Size([64, 128, 128])    |              700          |               741            |             19000           |          100000
      shape torch.Size([128, 128, 128])   |              930          |              1770            |             37990           |          199000
      shape torch.Size([512, 128, 128])   |             2810          |             11000            |            152000           |          797100
      shape torch.Size([1024, 128, 128])  |             5430          |             22430            |            303900           |         1595000
      shape torch.Size([1, 256, 256])     |             1120          |              1580            |               666           |            1890
      shape torch.Size([2, 256, 256])     |             1160          |              1574            |              1330           |            3784
      shape torch.Size([4, 256, 256])     |             1190          |              1580            |              2658           |            7570
      shape torch.Size([8, 256, 256])     |             1250          |              1613            |              5325           |           15100
      shape torch.Size([16, 256, 256])    |             1394          |              1880            |             10700           |           30260
      shape torch.Size([32, 256, 256])    |             1633          |              3360            |             21300           |           61000
      shape torch.Size([64, 256, 256])    |             2258          |              6730            |             42600           |          120000
      shape torch.Size([128, 256, 256])   |             3639          |             19200            |             85170           |          242200
      shape torch.Size([512, 256, 256])   |            12600          |             87200            |            340600           |          969000
      shape torch.Size([1024, 256, 256])  |            24530          |            176000            |            681300           |         1943000
      shape torch.Size([1, 512, 512])     |             2557          |              9117            |              1724           |            2577
      shape torch.Size([2, 512, 512])     |             2691          |              9209            |              3464           |            5200
      shape torch.Size([4, 512, 512])     |             2853          |              9860            |              6940           |           10000
      shape torch.Size([8, 512, 512])     |             3153          |             11000            |             13900           |           20570
      shape torch.Size([16, 512, 512])    |             3765          |             13000            |             27720           |           41360
      shape torch.Size([32, 512, 512])    |             5500          |             21400            |             55420           |           82000
      shape torch.Size([64, 512, 512])    |             8790          |             44000            |            111000           |          165000
      shape torch.Size([128, 512, 512])   |            15300          |             98000            |            221700           |          329800
      shape torch.Size([512, 512, 512])   |            55400          |            424100            |            886600           |         1325000
      shape torch.Size([1024, 512, 512])  |           110000          |            856200            |           1773000           |         2691000
      shape torch.Size([1, 1024, 1024])   |            10350          |             69290            |              5020           |            5327
      shape torch.Size([2, 1024, 1024])   |            11200          |             74860            |             10040           |           11000
      shape torch.Size([4, 1024, 1024])   |            12200          |             78030            |             20080           |           21290
      shape torch.Size([8, 1024, 1024])   |            14000          |             81200            |             40160           |           42850
      shape torch.Size([16, 1024, 1024])  |            17700          |             96000            |             80300           |           85500
      shape torch.Size([32, 1024, 1024])  |            27740          |            150000            |            160700           |          171000
      shape torch.Size([64, 1024, 1024])  |            45940          |            233400            |            321200           |          344100
      shape torch.Size([1, 2048, 2048])   |            29860          |            579800            |             12920           |           13500
      shape torch.Size([2, 2048, 2048])   |            34000          |            585000            |             25840           |           26840
      shape torch.Size([4, 2048, 2048])   |            39770          |            593900            |             51670           |           54000
      shape torch.Size([8, 2048, 2048])   |            51720          |            632100            |            103000           |          109000
      shape torch.Size([16, 2048, 2048])  |            76900          |            845500            |            206600           |          218400
      shape torch.Size([32, 2048, 2048])  |           130000          |           1058000            |            413900           |          437300

Times are in microseconds (us).


```

</details>
To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. 
<details>
<summary>
Benchmarking script
</summary>

```python
import torch
import pickle
import itertools
from functools import partial
from torch.utils.benchmark import Timer, Compare

benchmark_name = ""linalg.lu_factor CUDA""
name = ""magma_looped""
label = ""lu_factor_{}"".format(name)
shapes = [1, 2, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]
batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]
results = []
make_arg = partial(torch.randn, dtype=torch.float32, device=""cuda"")


for n, batch in itertools.product(shapes, batches):
    if n == 1024 and batch[0] >= 128:
        continue
    if n == 2048 and batch[0] >= 64:
        continue
    A = make_arg(batch + (n, n))
    print(A.shape)
    stmt = ""torch.linalg.lu_factor_ex(A)""
    timer = Timer(stmt,
                  globals=globals(),
                  label=benchmark_name,
                  description=label,
                  sub_label=f""shape {A.shape}"",
                  num_threads=1)
    results.append(timer.blocked_autorange())

compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(f""{label}.pickle"", 'wb') as f:
    pickle.dump(results, f)
```

</details>

See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.",pytorch
73903,rohan-varma,pr,2022-03-08T05:19:03Z,[FSDP] Option to summon on rank 0 only,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #73904
* __->__ #73903

Add an option to summon full parameter on rank0_only. writeback=True is not supported. 

Differential Revision: [D34706449](https://our.internmc.facebook.com/intern/diff/D34706449/)",pytorch
73904,rohan-varma,pr,2022-03-08T05:19:08Z,[FSDP] summon offload to CPU,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #73904
* #73903

Implement ability to offload full params to CPU in summon_full_params.

Differential Revision: [D34707801](https://our.internmc.facebook.com/intern/diff/D34707801/)",pytorch
73934,eqy,pr,2022-03-08T19:28:44Z,[cuDNN] [cuDNN v8 API] Support cuDNN Errata Filter,"Not originally mentioned in the tracking issue #58414, but is a nice-to-have feature. In summary, the errata filter allows known problematic kernels to be skipped instead of irrecoverably crashing a CUDA context (e.g., via an illegal memory access) via a JSON file supplied at run time. cuDNN frontend description: https://github.com/NVIDIA/cudnn-frontend#errata-filter

Sample errata filter JSON:
```
{
  ""version"" : 1,
  ""rules"" : [
    {
      ""rule_id"" : ""avoid_bad_bwd_data"",
      ""operation"" : ""ConvBwdData"",
      ""engine"" : 12,
      ""cudnn_version_start"" : 8000,
      ""cudnn_version_end"" : 9000
    }
  ]
}
```
CC @ngimel @zasdfgbnm @ptrblck 
",pytorch
73977,ngimel,pr,2022-03-09T19:53:21Z,port torch cov tests to error inputs,"Per title
",pytorch
73986,r-barnes,pr,2022-03-09T22:26:05Z,Don't do math with null pointers in SortingKernel.cpp,"Test Plan:
```
buck test -c fbcode.platform=platform010 //dper_lib/silvertorch/modules/filters/tests:external_filter_test -- test_empty_features
```

Differential Revision: D34757198

",pytorch
74021,ngimel,pr,2022-03-10T06:58:50Z,disable LT interface,"Summary: Disables cublasLt as it is buggy with cuda 11.0. We'll reenable it based on cuda version (some known bugs are fixed in 11.5)

Test Plan: Existing tests

Differential Revision: D34775050

",pytorch
74045,lezcano,pr,2022-03-10T19:02:51Z,Add support for vectors on the rhs of linalg.lu_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* __->__ #74045
* #73806
* #73804
* #73803
* #72935
* #67833

I really don't like this feature, but I guess that's what we are
supporting. I need it to make `linalg.solve` a composition of two
functions (albeit with custom derivative formulas).

Rant: This PR and the next one in this stack show how annoying and
inconvenient is to support NumPy's behaviour
of allowing the rhs to be a vector. We have to hardcode in every formula
(forward AD / backward AD / the function itself / every test) a few lines to handle
that case. This should be done by the user by calling `.unsqueeze(-1)` before calling
this function if they want to operate on column vectors.",pytorch
74046,lezcano,pr,2022-03-10T19:02:57Z,Simplify and optimize linalg.solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* __->__ #74046

This PR heavily simplifies the code of `linalg.solve`. At the same time,
this implementation saves quite a some copies of the input data in some
cases (e.g. A is contiguous)

We also implement it in such a way that the derivative goes from
computing two LU decompositions and two LU solves to no LU
decompositions and one LU solves. It also avoids a number of unnecessary
copies the derivative was unnecessarily performing (at least the copy of
two matrices).

On top of this, we add a `left` kw-only arg that allows the user to
solve `XA = B` rather concisely.

This PR also makes `torch.solve` an alias of `torch.linalg.solve`.

**Note:** This used to be the last PR of the stack. Now it's here because some tests were not passing in a PR that was before this one in the stack, and reshuffling the stack solved those problems. The benchmarks below are performed wrt the last PR fo this stack.

We compare the performance of `linalg.solve` against master (before merging https://github.com/pytorch/pytorch/pull/67833, but already with a few PRs of the LU stack merged). We see that we got between  **x2.5 and x10 speed-ups in `linalg.solve`**.


<details>
<summary>
Benchmark Results
</summary>

```
[--------------------- linalg.solve + backward --------------------]
                                    |  master |  This PR
1 threads: ----------------------------------------------
      torch.Size([1, 1, 1])         |     1280  |    267
      torch.Size([2, 1, 1])         |     1300  |    200
      torch.Size([4, 1, 1])         |      500  |    231
      torch.Size([8, 1, 1])         |      600  |    232
      torch.Size([16, 1, 1])        |     1200  |    234
      torch.Size([32, 1, 1])        |     1300  |    239
      torch.Size([64, 1, 1])        |     1300  |    300
      torch.Size([128, 1, 1])       |     1340  |    331
      torch.Size([512, 1, 1])       |     1664  |    380
      torch.Size([1024, 1, 1])      |     2000  |    430
      torch.Size([1, 2, 2])         |     1200  |    300
      torch.Size([2, 2, 2])         |     1250  |    237
      torch.Size([4, 2, 2])         |      479  |    240
      torch.Size([8, 2, 2])         |      600  |    239
      torch.Size([16, 2, 2])        |     1300  |    242
      torch.Size([32, 2, 2])        |     1300  |    245
      torch.Size([64, 2, 2])        |     1300  |    260
      torch.Size([128, 2, 2])       |     1400  |    340
      torch.Size([512, 2, 2])       |     1680  |    380
      torch.Size([1024, 2, 2])      |     2100  |    430
      torch.Size([1, 8, 8])         |     1200  |    250
      torch.Size([2, 8, 8])         |     1240  |    238
      torch.Size([4, 8, 8])         |      480  |    240
      torch.Size([8, 8, 8])         |      600  |    240
      torch.Size([16, 8, 8])        |     1330  |    243
      torch.Size([32, 8, 8])        |     1340  |    250
      torch.Size([64, 8, 8])        |     1370  |    257
      torch.Size([128, 8, 8])       |     1400  |    280
      torch.Size([512, 8, 8])       |     1720  |    346
      torch.Size([1024, 8, 8])      |     2300  |    390
      torch.Size([1, 16, 16])       |     1380  |    245
      torch.Size([2, 16, 16])       |     1000  |    300
      torch.Size([4, 16, 16])       |      610  |    260
      torch.Size([8, 16, 16])       |      862  |    260
      torch.Size([16, 16, 16])      |     1350  |    260
      torch.Size([32, 16, 16])      |     1370  |    260
      torch.Size([64, 16, 16])      |     1440  |    273
      torch.Size([128, 16, 16])     |     1520  |    289
      torch.Size([512, 16, 16])     |     1880  |    350
      torch.Size([1024, 16, 16])    |     2540  |    530
      torch.Size([1, 32, 32])       |     1500  |    290
      torch.Size([2, 32, 32])       |     2100  |    287
      torch.Size([4, 32, 32])       |     1370  |    288
      torch.Size([8, 32, 32])       |     1389  |    290
      torch.Size([16, 32, 32])      |     1400  |    290
      torch.Size([32, 32, 32])      |     1500  |    476
      torch.Size([64, 32, 32])      |     1600  |    468
      torch.Size([128, 32, 32])     |     1700  |    479
      torch.Size([512, 32, 32])     |     2300  |    696
      torch.Size([1024, 32, 32])    |     3200  |   1200
      torch.Size([1, 64, 64])       |     1700  |    340
      torch.Size([2, 64, 64])       |     2800  |    353
      torch.Size([4, 64, 64])       |     1990  |    328
      torch.Size([8, 64, 64])       |     2040  |    330
      torch.Size([16, 64, 64])      |     2100  |    350
      torch.Size([32, 64, 64])      |     2300  |    680
      torch.Size([64, 64, 64])      |     2430  |    725
      torch.Size([128, 64, 64])     |     2600  |    845
      torch.Size([512, 64, 64])     |     4700  |   1900
      torch.Size([1024, 64, 64])    |     9200  |   4280
      torch.Size([1, 128, 128])     |     2300  |    497
      torch.Size([2, 128, 128])     |     4000  |    562
      torch.Size([4, 128, 128])     |     3140  |    669
      torch.Size([8, 128, 128])     |     3200  |    698
      torch.Size([16, 128, 128])    |     3400  |    810
      torch.Size([32, 128, 128])    |     3866  |   1410
      torch.Size([64, 128, 128])    |     4200  |   1670
      torch.Size([128, 128, 128])   |     5050  |   2170
      torch.Size([512, 128, 128])   |    14000  |   6417
      torch.Size([1024, 128, 128])  |    28900  |  14700
      torch.Size([1, 256, 256])     |     4100  |   1559
      torch.Size([2, 256, 256])     |     6800  |   1792
      torch.Size([4, 256, 256])     |     7000  |   2000
      torch.Size([8, 256, 256])     |     7300  |   2200
      torch.Size([16, 256, 256])    |     7730  |   2540
      torch.Size([32, 256, 256])    |     8500  |   3390
      torch.Size([64, 256, 256])    |    11000  |   4470
      torch.Size([128, 256, 256])   |    15900  |   6757
      torch.Size([512, 256, 256])   |    50000  |  30000
      torch.Size([1024, 256, 256])  |   102600  |  56400
      torch.Size([1, 512, 512])     |     8793  |   3230
      torch.Size([2, 512, 512])     |    13000  |   3920
      torch.Size([4, 512, 512])     |    14000  |   4531
      torch.Size([8, 512, 512])     |    15000  |   5114
      torch.Size([16, 512, 512])    |    16700  |   6280
      torch.Size([32, 512, 512])    |    22400  |   9530
      torch.Size([64, 512, 512])    |    33700  |  14260
      torch.Size([128, 512, 512])   |    56500  |  20000

Times are in microseconds (us).
```

</details>

<details>
<summary>
Benchmarking Script
</summary>

```python
import torch
import pickle
import itertools
from functools import partial
from torch.utils.benchmark import Timer, Compare

benchmark_name = ""linalg.solve""
label = ""master""
shapes = [1, 2, 8, 16, 32, 64, 128, 256, 512]
batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]
results = []
make_arg = partial(torch.randn, dtype=torch.float32, device=""cuda"", requires_grad=True)

for n, batch in itertools.product(shapes, batches):
    if n == 512 and batch[0] >= 512:
        continue
    A = make_arg(batch + (n, n))
    B = make_arg(batch + (n, 16))
    ones = torch.ones(B.shape, device=B.device)
    print(A.shape)
    for adjoint in (True, False):
        timer = Timer(""torch.linalg.solve(A, B).backward(gradient=ones, inputs=[A, B])"",
                      globals=globals(),
                      label=benchmark_name,
                      description=label,
                      sub_label=f""{A.shape}"",
                      num_threads=1)
        results.append(timer.blocked_autorange())


compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(""{}.pickle"".format(label), 'wb') as f:
    pickle.dump(results, f)
```
</details>

See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.
",pytorch
74054,lezcano,pr,2022-03-10T20:17:19Z,linalg_solve_triangular should not be a method,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #74054

Fix a function that was incorrectly exposed.",pytorch
74113,rohan-varma,pr,2022-03-11T18:02:08Z,[ddp] parameter verification,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #74127
* __->__ #74113

Check mismatch in # of parameters by broadcasting and verifying from rank 0. As a result, non-zero ranks raise an error when # of parameters are mismatched across ranks.

Closes https://github.com/pytorch/pytorch/issues/73547

Differential Revision: [D34772067](https://our.internmc.facebook.com/intern/diff/D34772067/)",pytorch
74127,rohan-varma,pr,2022-03-11T22:36:17Z,[Reducer] small fix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74127
* #74113

getRank() is now public.

Differential Revision: [D34833222](https://our.internmc.facebook.com/intern/diff/D34833222/)",pytorch
74130,rohan-varma,pr,2022-03-11T23:44:24Z,[DDP] Generalize activation checkpoint tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #74252
* __->__ #74130

- Enables activation checkpointing tests to run for GLOO backend in addition to NCCL.
- Follow up changes will add support for non reentrant-based checkpointing and CheckpointWrapper.
- Changes AbstractDDPTest to CommonDDPTest to better indicate that it now has tests that are common to both backends
- 
Differential Revision: [D34281684](https://our.internmc.facebook.com/intern/diff/D34281684/)",pytorch
74150,r-barnes,pr,2022-03-13T18:08:00Z,use gmock 1.10 instead of 1.8,"Summary: See D34622171 for details

Test Plan: Sandcastle

Differential Revision: D34688117

",pytorch
74181,jaketae,pr,2022-03-14T18:42:00Z,"Fix math formatting, misc edit",Fixes #74147.,pytorch
74195,rohan-varma,pr,2022-03-14T21:43:14Z,[Easy] Remove erroneous comment,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74195

Per title

Differential Revision: [D34876138](https://our.internmc.facebook.com/intern/diff/D34876138/)",pytorch
74252,rohan-varma,pr,2022-03-15T20:14:53Z,[DDP][Tests] Fix weight sharing test,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74252
* #74130

Test wasn't actually doing weight sharing as pointed out by Andrew in https://github.com/pytorch/pytorch/pull/74130#discussion_r826954228

Differential Revision: [D34904535](https://our.internmc.facebook.com/intern/diff/D34904535/)",pytorch
74446,ngimel,pr,2022-03-21T02:54:39Z,WIP Jiterator reduction,"This PR enables jit-compiled reductions and moves `prod` to be jit-compiled. 
Currently, only reductions that can use `func_wrapper` for automatic implementation of `reduce/project/translate_idx` opes are supported, there are a few TODOs for support of more complex reductions such as norms and max, that typically require full-fledged ReduceOps functor. Similarly, only reductions with a single input are supported.
Number of inputs is hardcoded to 1, which is true for our current reductions, but can be relaxed in the future.
",pytorch
74452,rohan-varma,pr,2022-03-21T05:25:00Z,[FSDP] Mixed precision enablement,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74452

Enables mixed_precision training for PT FSDP.

### High level overview

- We add a `MixedPrecision` argument to FSDP API that allows user to control precision of inputs/parameters, gradient reduction, and buffers. We support any `torch.dtype` and the `torch.dtype` does not have to be the same between the 3 mixed precision flags we support. The goal of mixed precision training is to provide peak memory reduction and faster training due to the full param and gradient being in a reduced precision during forward/backward pass. Further, we decouple reduction precision from param/grad/input precision to allow for additional experimentation for faster communication algorithms.

### Mixed precision for inputs
- The root module simply casts inputs to the reduced precision at the beginning of the forward pass.

### Mixed precision for parameters
- In _rebuild_full_params, if we need to cast parameters to reduced precision, we call `_cast_param_shards_to_dtype`. This allocates a `p._mp_shard` of the reduced precision type and copies into this shard in a separate stream, with synchronization taken care of. As a result, all_gather will thus happen in the reduced precision and we'll have a reduced precision full parameter to run the user's forward pass with.
- After forwards/backwards passes, we have the full precision parameter shard in memory, and the mixed precision shard has been freed.
- Full precision for parameters is restored when taking checkpoints and for summon_full_params.

### Mixed precision for gradients
- Backward computation will occur in the reduced precision since activations/params/inputs were in reduced precision. As a result, in `_post_backward_hook`, we are left with a full/unsharded gradient in the reduced precision. Note that at the end of _post_backward_hook, we ensure the gradient is cast back to full precision so that the optimizer step can occur in full precision, and we handle all necessary stream synchronization.
- After the backwards pass, we have the full precision gradient shard in memory and no reduced precision gradient shards.

### Communication mixed precision
- If the mixed_precision config indicates a different reduction type under which to run _reduce_scatter_base, we cast gradients to this type before communicating them.

### Buffers mixed precision
- Buffers are unsharded and are cast only once by the root module in forward pass, and remain in their reduced precision throughout the training / in between forward and backward passes. Their full precision _is_ restored for checkpoint with full_state_dict, and _not_ restored in summon_full_params.
- See notes below for more details around differences on how PT FSDP vs FairScale implements support for buffer mixed precision.

### Changes to _rebuild_full_param
- Changes are made to _rebuild_full_param to cast parameters to their reduced precision. The main complication is supporting `summon_full_params` which must actually ignore mixed precision and summon in full precision mode. As a result, at the beginning of the function we set `force_full_precision` based on whether we are in summon_full_params.
- To further support summon_full_params which also needs to free full parameters, we refactor _rebuild_full_params similar to FairScale to return a tuple(tensor, bool) which indicates if the tensor can be freed or not. The tensor possibly cannot be freed in the case of world_size == 1 when the parameter is not sharded as the resulting full param points to the original model parameter. Another case is when we're returning the full parameter and reshard_after_forward=False (because we need to ensure p._full_param_padded stays intact)
- One subtlety is in the case of calling `update_p_data`, we need to update above tuple _before_ and not after, because after `update_p_data` the full param has padding trimmed, and this will cause issues with `writeback`.
- Finally, we don't necessarily call `all_gather` on `full_param_padded` anymore, i.e. particularly in the case of summon_full_param. This is because `full_param_padded` would be the reduced precision type but we need to all_gather in full precision. This is also why _rebuild_full_param returns a list of full params to summon_full_params as we can no longer rely on assuming `p._full_param_padded` is the full parameter.

### Changes to summon_full_params
- ``summon_full_params`` mostly consumes the above return value from `_rebuild_full_param` and the way `writeback` is done and full parameters are freed is refactored.
- For `writeback`, we can no longer assume that `p._full_param_padded` is the full shard that may have been modified (i.e. this is not the case for mixed_precision). As a result, we use the returned full parameters instead of hardcoding `p._full_param_padded` to writeback. 
- For freeing full params, similar to above we cannot assume that `p._full_param_padded` is the full parameter as `_collect_local_params` did. Instead, we consume the return value from `_rebuild_full_params` which explicitly tells us whether we can free the parameter or not.

### How checkpoint works
- For full_state_dict checkpoint, parameters are checkpointed in full precision which happens automatically due to summoning them in full precision as explained above.
- For buffers, in full_state_dict we explicitly cast buffers to their full precision before taking checkpoint. One subtlety is that we need to do this after we've entered summon_full_params context as summon_full_params calls `_lazy_init` which casts buffers to their reduced dtype.
- After checkpointing, buffers are restored back to their reduced precision 
- Note that buffer checkpointing for local_state_dict is not tested at the moment and this is left as follow up work.
 
### Useful clarifications while reviewing the diff:

##### How fairscale implements MP for buffers:
- Accepts buffer_dtype argument in ctor that is the dtype for computation for buffers. By default this is the compute_dtype.
- During _lazy_init, for root module, _cast_buffers is called which casts buffers to buffer_dtype.
- During state_dict, buffers are cast to torch.float32, then checkpoint is taken. They are restored back to buffer_dtype after that.

#####  How PT FSDP implements MP for buffers in this diff:
- Rather than buffer_dtype in ctor, we accept MixedPrecision.buffer_dtype which is the compute type for buffers.
- During lazy_init, similar to fairscale we cast the buffers to the type given by the MP config. In the case of no mixed precision the default behavior is maintained.
- Note that one subtlety is the recursive call. We need to make sure that each submodule uses its own `self.mixed_precision` config instead of passing in this arg to the recursive call, because different submodules may disable mixed precision. One example is BatchNorm usually disables mixed precision.
- Similar to FairScale, integer buffers are not cast.
- In _cast_buffers, we remember the original buffer dtype into a member variable. We then may cast them to a new dtype if given by the user.
- During state_dict, we use the above remembered type (stored as self._orig_buffer_dtypes) and restore this type to the buffers prior to taking checkpoint. After state_dict, we restore it back to the casted type as buffers remain in this mixed precision type even after forward/backwards passes (so this is done for consistency). FairScale seems to assume all buffers have original type as fp32, but we maintain a mapping that remembers the actual type.
- The improvement here is that we remember and restore the correct dtype of buffer the model originally had. 


### Test coverage:
- [x] nested model with same param / reduce dtypes
- [x] nested model with distinct param / buffer / reduce dtypes
- [x] model where buffer is a different type than parameter
- [x] nested model checkpoint, with verification that buffers and params are checkpointed in full precision (checks that force_full_precision in summon_full_params is respected)
- [x] After taking checkpoint, verified that buffers are back in the reduced precision
- [x] test that summon_full_params summons params in full precision
- [x] tests that gradient was appropriate type in backwards pass. This is done by patching `_reduce_scatter_base` to run the mixed precision checks.
- [x] Above test, but checks that we run reduce_scatter in the higher precision if specified by the mixed precision config.
- [x] Tests that after backward, gradient and param shards are in the full precision (and on correct device for optimizer step to happen). 
- [x] Tests that after forward, the reduced precision param shard is freed
- [x] Tests that after backward, the reduced precision param shard is freed
- [x] Test that buffers remain in the reduced precision type after forward / backward, and are not affected by summon_full_param. Within summon_full_param the buffer is _not_ restored to the full type.
- [x]  all of the above tests, but with reshard_after_forward=False i.e. zero-2 
- [x]  test that summon_full_params respects reshard_after_forward in the case of mixed precision as well
- [x]  parametrize a few relevant tests in summon_full_params as summon_full_param uses rebuild_full_param a bit differently. In particular we make sure things work as expected if the rebuilt parameter is not p._full_param_padded which is the case in mixed precision.
- [x] tests for world_size == 1 i.e. when the parameter is not sharded. Not adding this for initial enablement as all use cases in question have world_size > 1


Follow up work (https://github.com/pytorch/pytorch/issues/74515):
[- [ ] Test local_state_dict checkpoint works with mixed precision. In particular we have to be careful about buffer casting.
- [ ] Enhance test_fsdp_state_dict to checkpoint buffers and ensure dtypes are as expected. Although note that this is also already tested in this PR.
- [ ] Test summon_full_params with reshard_after_forward (with and without mixed precision)](https://github.com/pytorch/pytorch/issues/74515)


Differential Revision: [D35000703](https://our.internmc.facebook.com/intern/diff/D35000703/)",pytorch
74456,rohan-varma,pr,2022-03-21T07:17:24Z,[FSDP] Fix summon_full_params test,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #74452
* #74517
* __->__ #74456

This test did not actually do any CPU offloading. Adding it revealed
an issue in no_shard (currently only when world_size == 1) case which is
tracked for a fix in https://github.com/pytorch/pytorch/issues/74166

Differential Revision: [D35003793](https://our.internmc.facebook.com/intern/diff/D35003793/)",pytorch
74517,rohan-varma,pr,2022-03-22T02:49:43Z,[FSDP] named_buffers fix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74517

Partially addresses https://github.com/pytorch/pytorch/issues/73890 to fix named_buffers by stripping FSDP info in summon_full_params context, similar to named_params.

Differential Revision: [D35023191](https://our.internmc.facebook.com/intern/diff/D35023191/)",pytorch
74690,b0noI,pr,2022-03-24T16:36:16Z,Jinja2 version pinned to 3.0.*,"Fixes #74684
",pytorch
74713,micmelesse,pr,2022-03-24T21:49:50Z,[ROCm] unskip FFT tests,"This pr enables a lot of fft tests on rocm, see the full list here https://github.com/ROCmSoftwarePlatform/pytorch/issues/924. 

After enabling the tests we found that 3 tests , test_reference_1d, test_reference_nd and test_fn_grad have issue.  We skip those tests on ROCM in this pr as well. We will address those skipped tests in a subsequent pr.

",pytorch
74718,b0noI,pr,2022-03-24T22:53:09Z,Jinja2 for docs/cpp build set to version 3.0,"Fixes https://github.com/pytorch/pytorch/issues/74684
",pytorch
74833,rohan-varma,pr,2022-03-28T15:54:11Z,[FSDP] exclude from typing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #74452
* __->__ #74833

FSDP has 74 type ignores and more being added, it may be better to
exclude from typechecking until we can type it properly.

Differential Revision: [D35186441](https://our.internmc.facebook.com/intern/diff/D35186441/)",pytorch
74865,r-barnes,pr,2022-03-28T21:18:22Z,Add additional CUDA error handling macros,"Test Plan: Sandcastle

Differential Revision: D35194530

",pytorch
74866,crcrpar,pr,2022-03-28T21:28:54Z,Fix conda install command of `ccache`,"ref: https://anaconda.org/conda-forge/ccache
",pytorch
74880,joker-eph,pr,2022-03-29T01:34:24Z,Fix docstring for torch.roll,"The doc was indicating ""If a dimension is not specified, the tensor will
be flattened"", whereas the actual behavior is that the input tensor is
flattened only if the `dims` argument is not provided at all.
",pytorch
74881,eqy,pr,2022-03-29T01:38:50Z,[cuDNN V8 API] Allow the number of kernels profiled under `torch.backends.cudnn.benchmark = True` to be limited,"The cuDNN V8 API (main support merged in #60755) potentially exposes many more kernels with `benchmark=True`. While these additional kernels can improve performance, it is often unnecessary to run _every_ kernel returned by the heuristic and doing so may degrade the user experience by causing the first model iteration to be very slow. To alleviate this issue, this PR introduces `torch.backends.cudnn.benchmark_limit`. `benchmark_limit` specifies the maximum number of working cuDNN kernels to try for a given workload, with the default being `10` (similar to what TensorFlow does). `benchmark_limit = 0` yields the current behavior of trying every kernel returned by the heuristic.

CC @ptrblck @ngimel @xwang233 ",pytorch
74917,r-barnes,pr,2022-03-29T20:11:10Z,Check all CUDA API calls for errors in aten/src/ATen,"Test Plan: Sandcastle

Differential Revision: D35194692

",pytorch
74918,r-barnes,pr,2022-03-29T20:11:41Z,Check all CUDA API calls for errors in caffe2/c10/,"Test Plan: Sandcastle

Differential Revision: D35194795

",pytorch
74919,r-barnes,pr,2022-03-29T20:11:41Z,Check all CUDA API calls in aten/src/ATen/test for errors,"Test Plan: Sandcastle

Differential Revision: D35194596

",pytorch
74920,r-barnes,pr,2022-03-29T20:12:22Z,Check all CUDA API calls for errors in benchmarks/cpp/nvfuser,"Test Plan: Sandcastle

Differential Revision: D35194656

",pytorch
74921,r-barnes,pr,2022-03-29T20:13:32Z,Check all CUDA API calls for errors in test/,"Test Plan: Sandcastle

Differential Revision: D35194966

",pytorch
74922,r-barnes,pr,2022-03-29T20:13:42Z,Check all CUDA API calls for errors in caffe2/,"Test Plan: Sandcastle

Differential Revision: D35194868

",pytorch
74923,r-barnes,pr,2022-03-29T20:13:52Z,Check all CUDA API calls for errors in torch/,"Test Plan: Sandcastle

Differential Revision: D35194935

",pytorch
74945,jaketae,pr,2022-03-30T05:27:14Z,Docs: Fix `log_target` example in kl divergence,"Fixes #74453.
",pytorch
74946,rohan-varma,pr,2022-03-30T06:55:08Z,[FSDP] Warning when fail to clone,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #74946

Warn instead of hard failure when fail to clone state_dict, as this
param might not be managed by FSDP and thus we do not expect to clone it.

Differential Revision: [D35242306](https://our.internmc.facebook.com/intern/diff/D35242306/)",pytorch
74980,silvasean,pr,2022-03-30T20:52:27Z,Shape functions: Use friendlier clamping pattern,"When start_val == 0, using the comparison `start_val > self[dim]` can be folded easily (0 is never strictly greater than the result of `self[dim]`), but `start_val >= self[dim]` can't. Since we assign `start_val = sef[dim]` in the body anyway, both these are equivalent

",pytorch
75000,eqy,pr,2022-03-31T03:30:16Z,Abate spurious resize warnings in `MultiMarginLoss` on CUDA,"The current `multi_margin_loss_cuda_out` implementation has a few cases where an output is intended to have shape `{0}` before resizing but is improperly set before this step, triggering `UserWarning`s. This PR changes the initial `at::empty` call to avoid this warning and alters the current resizing logic to match that of the CPU version to avoid the warning when calling `at::sum_out`.

The original issue was reported from the user forum by @hadaev8 [here](https://discuss.pytorch.org/t/warning-spam-then-using-multimarginloss/147492).

CC @ptrblck

Original repro provided by @hadaev8:
```
import torch
from torch import nn
from torch.nn import functional as F

bs = 56
model = nn.Linear(128, 22).cuda()
loss = nn.MultiMarginLoss()
x = torch.rand((bs, 128)).cuda()
targets = torch.randint(22, (bs,)).cuda()
out = model(x)
print(targets.shape)
print(out.shape)
loss(out, targets)
```",pytorch
75024,rohan-varma,pr,2022-03-31T16:55:40Z,"[Reland][FSDP] Mixed precision enablement""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75236
* #75227
* __->__ #75024

Reland https://github.com/pytorch/pytorch/pull/74452

Issue was older nccl version does not support bf16. Will take an approach similar to https://github.com/pytorch/pytorch/pull/67843 to ensure test only runs with later nccl versions.

Original commit changeset: 99295ea4ff02

Original Phabricator Diff: D35000703

Differential Revision: [D35287501](https://our.internmc.facebook.com/intern/diff/D35287501/)",pytorch
75032,r-barnes,pr,2022-03-31T19:09:15Z,Map new CUDA error handling to HIP,"Test Plan: Sandcastle

Differential Revision: D35253785

",pytorch
75040,lezcano,pr,2022-03-31T21:25:48Z,Disallow functions that are in submodules to be also methods,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #75040

Follows the discussion in https://github.com/pytorch/pytorch/pull/74054

We show that it works by example, given that it caught the
`special_gamma` error.",pytorch
75113,micmelesse,pr,2022-04-01T17:39:20Z,[ROCM] unskip test_fn_grad,The skip was not needed.,pytorch
75193,lezcano,pr,2022-04-04T16:21:31Z,Improved matmul tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #75197
* #75196
* #75195
* #75279
* #75194
* __->__ #75193

Let's make sure we don't break anything in the next PRs of the stack.
Also some comprehensive testing of matmul on CPU and CUDA was long due.

Running this tests we see that the `out=` variant of matmul is broken
when used on 4D tensors and a number of other cases. This hints what would be the amount of people
that use out= variants...",pytorch
75194,lezcano,pr,2022-04-04T16:21:36Z,Refactor the API of the matmul implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* #75197
* #75196
* #75195
* #75279
* __->__ #75194

Previously we used an odd overload using `c10::optional` to implement
the matmul logic of `matmul` and `matmul_out` simultaneously. This made
some functions (those in `linalg.matrix_exp`) call into this native::matmul
implementation, rather than going through the dispatcher.

In this PR we remove the use of `c10::optional` and rename the
implementation of matmul, to make sure that no one mistakenly calls it.",pytorch
75195,lezcano,pr,2022-04-04T16:21:41Z,Dispatch to mv rather than mm in the case that tensor1.ndim == 1 and tensor2.ndim == 2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* #75197
* #75196
* __->__ #75195
* #75279
* #75194

This should hopefully be faster, it makes the calling code simpler, and
it solves a bug when using matmul with the out= parameter (before it
would throw an incorrect error).",pytorch
75196,lezcano,pr,2022-04-04T16:21:47Z,Go through the dispatcher in matmul_out for the 1D-1D case,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* #75197
* __->__ #75196
* #75195
* #75279
* #75194

Putting this one separate as it may break things like mobile.",pytorch
75197,lezcano,pr,2022-04-04T16:21:52Z,Micro-optimisations for matmul 2.0: Electric boogaloo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* __->__ #75197
* #77735

This PR implements the bulk of https://github.com/pytorch/pytorch/pull/64387

Part of the optimisations were already merged in https://github.com/pytorch/pytorch/pull/72230

A number of these optimisations include:
- Make the code `const` correct.
- Create `DimVector`'s more efficiently (e.g. prefer `append` over
`insert`).
- Access sizes of the tensors via `sizes().front()` / `sizes().back()`
  / `sizes().end()[-2]`
- Do not create intermediary tensors / vectors when it can be avoided.
- Call `reshape` rather than `expect_contiguous`  + `view`

On top of these, it fixes a correctness issue of `matmul_out`, where the
out parameter was not resized correctly when passed to the backends.
This involves removing the use of `set_` from the calling code, as
requested by @ezyang, and it incurs on most of the complexity of the
code that this PR adds.",pytorch
75200,lezcano,pr,2022-04-04T16:52:28Z,Fix addmm_cpu for int64,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #75197
* #75196
* #75195
* #75279
* #75220
* #75194
* #75193
* __->__ #75200

There was a bug for the case [a, 0] x [0, b] in addmm_cpu. We implement
a short-cut for this case.
Fixes #71774",pytorch
75211,crcrpar,pr,2022-04-04T19:09:51Z,[foreach][mta] Implement `torch._foreach_global_norm`,"# Summary
- Implement `torch._foreach_global_norm` which calculates the global norm of `List[Tensor]`.
    - Fast path only supports L1 & L2

This function can replace https://github.com/rwightman/pytorch-image-models/blob/01a0e25a67305b94ea767083f4113ff002e4435c/timm/optim/lamb.py#L110-L120
with 

```python
       grads = []
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad
                if grad.is_sparse:
                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')
                grads.append(grad)
        global_grad_norm = torch._foreach_global_norm(grads)
```

<!--
# TODO
- [x] Fix the number of `cudaLaunchKernels` in slowpath
-->

# Benchmark

used commit: 50fe376dc31d3f22d2b62eb4388037e5fcd6842b

## Env
```
CUDA used to build PyTorch: 11.6
GPU : RTX 3070 Ti
```

## Reference implementation

```python
def reference_global_norm(tensors: List[Tensor]) -> Tensor:
    return torch.sqrt(sum(t.pow(2).sum() for t in tensors))
```

## Global Norm - fastpath
| dtype          |   num_tensors | tensor_shape   |   reference |     foreach |   speedup |
|:---------------|--------------:|:---------------|------------:|------------:|----------:|
| torch.float32  |            50 | 50 x 50        |   0.0428228 | 0.000941515 |   45.4829 |
| torch.float32  |           100 | 100 x 100      |   0.0846331 | 0.00120974  |   69.96   |
| torch.float16  |            50 | 50 x 50        |   0.043551  | 0.000976086 |   44.618  |
| torch.float16  |           100 | 100 x 100      |   0.0872855 | 0.00122571  |   71.2122 |
| torch.bfloat16 |            50 | 50 x 50        |   0.04457   | 0.00101542  |   43.8929 |
| torch.bfloat16 |           100 | 100 x 100      |   0.0857499 | 0.0011723   |   73.1464 |

## Global Norm - slowpath
| dtype          |   num_tensors | tensor_shape   |   reference |   foreach |   speedup |
|:---------------|--------------:|:---------------|------------:|----------:|----------:|
| torch.float32  |            50 | 50 x 50        |   0.0443747 | 0.0318217 |   1.39448 |
| torch.float32  |           100 | 100 x 100      |   0.0890992 | 0.062454  |   1.42664 |
| torch.float16  |            50 | 50 x 50        |   0.0443466 | 0.0321701 |   1.3785  |
| torch.float16  |           100 | 100 x 100      |   0.0917869 | 0.066494  |   1.38038 |
| torch.bfloat16 |            50 | 50 x 50        |   0.0460787 | 0.0340297 |   1.35407 |
| torch.bfloat16 |           100 | 100 x 100      |   0.0913305 | 0.0648458 |   1.40843 |

<!--

## Norm Per Tensor - fastpath
| dtype          |   num_tensors | tensor_shape   |   reference |    foreach |   speedup |
|:---------------|--------------:|:---------------|------------:|-----------:|----------:|
| torch.float32  |            50 | 50 x 50        |   0.0176275 | 0.00201941 |   8.72904 |
| torch.float32  |           100 | 100 x 100      |   0.033504  | 0.00326371 |  10.2656  |
| torch.float16  |            50 | 50 x 50        |   0.0168457 | 0.00200891 |   8.38547 |
| torch.float16  |           100 | 100 x 100      |   0.0330038 | 0.00327325 |  10.0829  |
| torch.bfloat16 |            50 | 50 x 50        |   0.0164759 | 0.00202084 |   8.15302 |
| torch.bfloat16 |           100 | 100 x 100      |   0.0335269 | 0.0033462  |  10.0194  |

## Norm Per Tensor - slwowpath
| dtype          |   num_tensors | tensor_shape   |   reference |   foreach |   speedup |
|:---------------|--------------:|:---------------|------------:|----------:|----------:|
| torch.float32  |            50 | 50 x 50        |   0.0171256 | 0.0130904 |   1.30826 |
| torch.float32  |           100 | 100 x 100      |   0.0339911 | 0.0258839 |   1.31321 |
| torch.float16  |            50 | 50 x 50        |   0.0170689 | 0.0126636 |   1.34787 |
| torch.float16  |           100 | 100 x 100      |   0.0338097 | 0.0255606 |   1.32272 |
| torch.bfloat16 |            50 | 50 x 50        |   0.0172422 | 0.0127335 |   1.35409 |
| torch.bfloat16 |           100 | 100 x 100      |   0.0341241 | 0.0256011 |   1.33291 |

-->

Rel: #58833

cc @ptrblck @mcarilli @ngimel ",pytorch
75220,lezcano,pr,2022-04-04T20:29:09Z,Make mv and addmv support torch.float16,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #75197
* #75196
* #75195
* #75279
* __->__ #75220
* #75194
* #75193
* #75200

In the next PR of this stack we're changing the use of an at::mm by an
at::mv. For us to be able to do this, we need at::mv to support the same
input dtypes as at::mm.",pytorch
75227,rohan-varma,pr,2022-04-04T21:42:33Z,[FSDP] Code simplification,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75236
* __->__ #75227
* #75024

Small change to reduce code duplication.

Differential Revision: [D35374608](https://our.internmc.facebook.com/intern/diff/D35374608/)",pytorch
75231,ngimel,pr,2022-04-04T22:21:56Z,"Enable simple reductions with jiterator, move prod to be jiterated","Reland of #74446
",pytorch
75236,rohan-varma,pr,2022-04-04T22:57:49Z,[FSDP] Enhance test for checkpoint,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75236
* #75227

Make sure that the param names are such as we can load from checkpoint
into a local model.

Differential Revision: [D35377774](https://our.internmc.facebook.com/intern/diff/D35377774/)",pytorch
75250,guoyejun,pr,2022-04-05T09:05:00Z,add XPU support for autocast,,pytorch
75279,lezcano,pr,2022-04-05T21:15:50Z,Fix mv/addmv on CUDA when dealing with vectors of size=1 and stride=0,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* #75197
* #75196
* #75195
* __->__ #75279
* #75194

This fix was suggested by @peterbell10",pytorch
75361,rohan-varma,pr,2022-04-06T21:38:39Z,Add test to make sure submodule hooks fire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

As part of FSDP work, we will be relying on `_register_load_state_dict_pre_hook` to manage some specific logic related to loading state dicts.

This PR adds a test to ensure that _register_load_state_dict_pre_hook can be
used to register hooks on modules that will be used in a nested way, and then
calling load_state_dict on the overall module still calls those hooks
appropriately.

Differential Revision: [D35434726](https://our.internmc.facebook.com/intern/diff/D35434726/)",pytorch
75362,rohan-varma,pr,2022-04-06T21:38:44Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Adds load_state_dict_post_hook. Proposal: https://github.com/pytorch/pytorch/issues/75287

Went with no args except for module optionally for now. Additional args can be
added on use case basis.

A few notes:
- Similar to state_dict post hook, we run it after this module and all submodule children have been loaded.
- Added try/finally to ensure the hook runs even if load_state_dict throws.

Differential Revision: [D35439200](https://our.internmc.facebook.com/intern/diff/D35439200/)",pytorch
75364,rohan-varma,pr,2022-04-06T21:40:50Z,Add test to make sure submodule hooks fire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

As part of FSDP work, we will be relying on `_register_load_state_dict_pre_hook` to manage some specific logic related to loading state dicts.

This PR adds a test to ensure that _register_load_state_dict_pre_hook can be
used to register hooks on modules that will be used in a nested way, and then
calling load_state_dict on the overall module still calls those hooks
appropriately.

Differential Revision: [D35434726](https://our.internmc.facebook.com/intern/diff/D35434726/)",pytorch
75365,rohan-varma,pr,2022-04-06T21:40:56Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Adds load_state_dict_post_hook. Proposal: https://github.com/pytorch/pytorch/issues/75287

Went with no args except for module optionally for now. Additional args can be
added on use case basis.

A few notes:
- Similar to state_dict post hook, we run it after this module and all submodule children have been loaded.
- Added try/finally to ensure the hook runs even if load_state_dict throws.

Differential Revision: [D35439200](https://our.internmc.facebook.com/intern/diff/D35439200/)",pytorch
75366,rohan-varma,pr,2022-04-06T21:44:51Z,Add test to make sure submodule hooks fire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

As part of FSDP work, we will be relying on `_register_load_state_dict_pre_hook` to manage some specific logic related to loading state dicts.

This PR adds a test to ensure that _register_load_state_dict_pre_hook can be
used to register hooks on modules that will be used in a nested way, and then
calling load_state_dict on the overall module still calls those hooks
appropriately.

Differential Revision: [D35434726](https://our.internmc.facebook.com/intern/diff/D35434726/)",pytorch
75367,rohan-varma,pr,2022-04-06T21:44:57Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Adds load_state_dict_post_hook. Proposal: https://github.com/pytorch/pytorch/issues/75287

Went with no args except for module optionally for now. Additional args can be
added on use case basis.

A few notes:
- Similar to state_dict post hook, we run it after this module and all submodule children have been loaded.
- Added try/finally to ensure the hook runs even if load_state_dict throws.

Differential Revision: [D35439200](https://our.internmc.facebook.com/intern/diff/D35439200/)",pytorch
75393,ngimel,pr,2022-04-07T00:19:16Z,use Timer for cuda benchmarks,"`torch.cuda.synchronize()` is a heavy hammer and distorts benchmarking results a lot. Timer provides results that are closer to kernel times observed in profiler. 
If you want, instead of `blocked_autorange` you can use `timeit` that repeats the stmt fixed number of times. ",pytorch
75421,rohan-varma,pr,2022-04-07T14:22:49Z,Add test to make sure submodule hooks fire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75421

As part of FSDP work, we will be relying on `_register_load_state_dict_pre_hook` to manage some specific logic related to loading state dicts.

This PR adds a test to ensure that _register_load_state_dict_pre_hook can be
used to register hooks on modules that will be used in a nested way, and then
calling load_state_dict on the overall module still calls those hooks
appropriately.

Differential Revision: [D35434726](https://our.internmc.facebook.com/intern/diff/D35434726/)",pytorch
75422,rohan-varma,pr,2022-04-07T14:22:59Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75424
* __->__ #75422

Adds load_state_dict_post_hook. Proposal: https://github.com/pytorch/pytorch/issues/75287

Went with no args except for module optionally for now. Additional args can be
added on use case basis.

A few notes:
- Similar to state_dict post hook, we run it after this module and all submodule children have been loaded.
- Added try/finally to ensure the hook runs even if load_state_dict throws.

Differential Revision: [D35439200](https://our.internmc.facebook.com/intern/diff/D35439200/)",pytorch
75423,rohan-varma,pr,2022-04-07T14:23:11Z,[FSDP] summon full params staticmethod,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75423

Make summon_full_params a static method. We still retain the old summon_full_params as a private API `_summon_full_params` and there are a couple of callsites to this within only FSDP file, but we can remove these as well.

Differential Revision: [D35444539](https://our.internmc.facebook.com/intern/diff/D35444539/)",pytorch
75424,rohan-varma,pr,2022-04-07T14:23:17Z,[FSDP] Separate shared code into hooks,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75424
* #75422

All load_state_dict implementations call synchronize(), so put this into hook for preparation of doing all FSDP-specific state_dict loading logic in pre/post hooks.

Differential Revision: [D35439351](https://our.internmc.facebook.com/intern/diff/D35439351/)",pytorch
75425,rohan-varma,pr,2022-04-07T14:23:23Z,[FSDP] Remove unnecesary case,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75426
* __->__ #75425
* #75424
* #75423
* #75422
* #75421

This case is already covered by sharded_pre_load_state_dict_hook
logic. i.e., before sharded_load_state_dict hook runs, the pre-hook for this will run, which already throws.

Differential Revision: [D35444540](https://our.internmc.facebook.com/intern/diff/D35444540/)",pytorch
75426,rohan-varma,pr,2022-04-07T14:23:31Z,[FSDP] Use post load state dict hooks,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75426
* #75425
* #75424
* #75423
* #75422
* #75421

Use the post load state dict hooks added in below diff to make all FSDP state_dict loading logic done in pre and post hooks.

As as result, we support full_state_dict to load from modules that contain FSDP units but the high-level module is not necessarily FSDP and added the relevant tests.

Differential Revision: [D35444541](https://our.internmc.facebook.com/intern/diff/D35444541/)",pytorch
75466,eqy,pr,2022-04-07T22:06:57Z,[cuDNN V8 API] Enable building cuDNN v8 API by default,Testing via CI,pytorch
75523,ngimel,pr,2022-04-08T17:01:11Z,Dispatch to mv rather than mm when appropriate,"

This should hopefully be faster, it makes the calling code simpler, and
it solves a bug when using matmul with the out= parameter (before it
would throw an incorrect error).

cc @lezcano, I separated this PR from your stack because I want to merge it faster
",pytorch
75528,rohan-varma,pr,2022-04-08T18:16:23Z,Fix tsan issue,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75528

- Running into a race in `TensorImpl` described here: https://github.com/pytorch/pytorch/issues/75529 that is triggered by passing in a temporary list of tensor into allgather, which results in a race when python deallocation runs concurrently with gloo's io thread reading/writing from the tensor. This was identified by @kumpera 
- Workaround is to allocate the python tensor as a local variable so deallocation does not run until after the allgather async call is completed. This fixes the distributed test but the proper fix should be in TensorImpl.

Differential Revision: [D35504430](https://our.internmc.facebook.com/intern/diff/D35504430/)",pytorch
75552,jaketae,pr,2022-04-09T02:45:30Z,Docs: Detail 3D tensor shape for transformer masks,"Fixes #74612.
",pytorch
75553,jaketae,pr,2022-04-09T02:51:07Z,Fix: Make `nn.init.orthogonal_` no-op for empty input,"Fixes #73503.
",pytorch
75555,jaketae,pr,2022-04-09T03:10:29Z,Docs: Add missing zero-ing step in Rprop algorithm,"Fixes ##70418.
",pytorch
75561,Yulv-git,pr,2022-04-09T14:40:21Z,Fix some typos.,"Fixes #ISSUE_NUMBER
",pytorch
75647,ngimel,pr,2022-04-12T03:33:18Z,remove fp16 support from cpu linalg functions,"fp16 on cpu produces slow and inaccurate results, see #69969
",pytorch
75689,vfdev-5,pr,2022-04-12T19:32:21Z,Added functorch to functional_autograd_benchmark,"Description:

- Following https://github.com/pytorch/functorch/issues/497 adding an option to run benchmarks with functorch and compare to original functional autograd results.

Running the benchmark we get below table:

<details>
<summary>
Table
</summary>

```
| model | task | mean | var |
| -- | -- | -- | -- |
| resnet18 | vjp | 0.03826599195599556 | 4.3332115637895186e-06 |
| resnet18 | functorch vjp | 0.037201929837465286 | 6.139693198292662e-09 |
| resnet18 | vhp | 0.2202976644039154 | 2.8687209052691287e-08 |
| resnet18 | functorch vhp | 0.22117868065834045 | 4.108771278765744e-08 |
| resnet18 | jvp | 0.18679651618003845 | 1.832455254202614e-08 |
| resnet18 | functorch jvp | 0.05305683612823486 | 1.6690266946284282e-08 |
| fcn_resnet | vjp | 0.6071907877922058 | 7.436695454998699e-07 |
| fcn_resnet | functorch vjp | 0.6115708947181702 | 1.121692207561864e-06 |
| fcn_resnet | vhp | 3.419469118118286 | 0.020633839070796967 |
| fcn_resnet | jvp | 2.5421929359436035 | 3.1765587209520163e-06 |
| fcn_resnet | functorch jvp | 0.7628333568572998 | 1.4555752159139956e-07 |
| detr | vjp | 0.19494840502738953 | 1.9122715457342565e-05 |
| detr | vhp | 1.1664292812347412 | 0.000948643428273499 |
| detr | jvp | 0.9990308880805969 | 1.0214127541985363e-05 |
| ppl_simple_reg | vjp | 0.0007535457843914628 | 6.024204690646684e-09 |
| ppl_simple_reg | functorch vjp | 0.0016954183811321855 | 1.160151974488599e-08 |
| ppl_simple_reg | vhp | 0.0011888503795489669 | 5.93119386937957e-10 |
| ppl_simple_reg | functorch vhp | 0.0026826143730431795 | 1.6787025103326414e-08 |
| ppl_simple_reg | jvp | 0.001067900680936873 | 7.409912128331086e-10 |
| ppl_simple_reg | functorch jvp | 0.002065300941467285 | 9.710328185974504e-08 |
| ppl_simple_reg | hvp | 0.001212477684020996 | 1.974137298077494e-09 |
| ppl_simple_reg | functorch hvp | 0.00482442369684577 | 2.327668653379078e-07 |
| ppl_simple_reg | jacobian | 0.0009108781814575195 | 3.489469158068914e-09 |
| ppl_simple_reg | functorch jacobian | 0.0019866942893713713 | 1.938326299466553e-08 |
| ppl_simple_reg | hessian | 0.005053090862929821 | 3.370298600202659e-07 |
| ppl_simple_reg | functorch hessian | 0.006374978926032782 | 7.556796077778927e-08 |
| ppl_simple_reg | hessian_fwdrev | 0.0036706924438476562 | 1.996075527088692e-09 |
| ppl_simple_reg | functorch hessian_fwdrev | 0.0058908225037157536 | 7.548283775804521e-08 |
| ppl_simple_reg | hessian_revrev | 0.0015769004821777344 | 1.5754418214442012e-08 |
| ppl_simple_reg | functorch hessian_revrev | 0.0041002752259373665 | 6.713568723171193e-08 |
| ppl_simple_reg | jacfwd | 0.0018048763740807772 | 2.7375660849315864e-08 |
| ppl_simple_reg | functorch jacfwd | 0.002047991845756769 | 2.432247070416338e-09 |
| ppl_simple_reg | jacrev | 0.0009733677143231034 | 1.0078769818733235e-08 |
| ppl_simple_reg | functorch jacrev | 0.0021971464157104492 | 1.2729884701911942e-08 |
| ppl_robust_reg | vjp | 0.005820560269057751 | 8.582588151284654e-08 |
| ppl_robust_reg | functorch vjp | 0.00796132069081068 | 9.663100541956737e-09 |
| ppl_robust_reg | vhp | 0.009825301356613636 | 2.0081762386325863e-07 |
| ppl_robust_reg | functorch vhp | 0.014890861697494984 | 4.558066279969353e-07 |
| ppl_robust_reg | jvp | 0.008297419175505638 | 2.9454400873873965e-07 |
| ppl_robust_reg | functorch jvp | 0.008052706718444824 | 7.120377176761394e-08 |
| ppl_robust_reg | hvp | 0.015414690598845482 | 7.42123745567369e-07 |
| ppl_robust_reg | functorch hvp | 0.02699306048452854 | 1.4650488537881756e-06 |
| ppl_robust_reg | jacobian | 0.006207776255905628 | 1.7068457225377642e-07 |
| ppl_robust_reg | functorch jacobian | 0.009173822589218616 | 1.2214455580306094e-07 |
| ppl_robust_reg | hessian | 0.04670915752649307 | 1.4299343092716299e-05 |
| ppl_robust_reg | functorch hessian | 0.02337808534502983 | 3.0397418413485866e-06 |
| ppl_robust_reg | hessian_fwdrev | 0.024229884147644043 | 2.0425247839739313e-06 |
| ppl_robust_reg | functorch hessian_fwdrev | 0.022021746262907982 | 3.512146236062108e-07 |
| ppl_robust_reg | hessian_revrev | 0.012355780228972435 | 7.090877147675201e-07 |
| ppl_robust_reg | functorch hessian_revrev | 0.013960313983261585 | 6.326549737423193e-07 |
| ppl_robust_reg | jacfwd | 0.008112502284348011 | 2.88503088086145e-08 |
| ppl_robust_reg | functorch jacfwd | 0.008947920985519886 | 4.2070990247111695e-08 |
| ppl_robust_reg | jacrev | 0.00635871896520257 | 1.3403841592207755e-07 |
| ppl_robust_reg | functorch jacrev | 0.009123563766479492 | 2.677554675756255e-07 |
| wav2letter | vjp | 0.02078995667397976 | 2.1110793113621185e-06 |
| wav2letter | functorch vjp | 0.019202351570129395 | 9.210506135559626e-09 |
| wav2letter | vhp | 0.05997290462255478 | 8.558587616391833e-09 |
| wav2letter | functorch vhp | 0.06035261228680611 | 1.6448565842708263e-09 |
| wav2letter | jvp | 0.04507789760828018 | 1.5771547401399744e-09 |
| wav2letter | functorch jvp | 0.013057494536042213 | 3.804750292601966e-09 |
| deepspeech | vjp | 0.3648746609687805 | 1.5359055396402255e-05 |
| transformer | vjp | 0.05496881157159805 | 1.242562319703211e-08 |
| transformer | functorch vjp | 0.057835936546325684 | 2.6113376350167528e-08 |
| transformer | vhp | 0.18313491344451904 | 7.226336151688884e-08 |
| transformer | jvp | 0.13924935460090637 | 1.6989159234981344e-07 |
| multiheadattn | vjp | 0.0014708995586261153 | 3.710916729460223e-08 |
| multiheadattn | functorch vjp | 0.002404856728389859 | 2.1910574687922235e-08 |
| multiheadattn | vhp | 0.003382015274837613 | 5.3098595742540056e-08 |
| multiheadattn | functorch vhp | 0.005340623669326305 | 5.897558708056749e-08 |
| multiheadattn | jvp | 0.0027526854537427425 | 3.508620949332908e-08 |
| multiheadattn | functorch jvp | 0.0022981404326856136 | 1.327894807445773e-07 |

```

</details>

<details>
<summary>
Stdout
</summary>

```
Found functorch: 0.2.0a0+386a541
Results for model resnet18 on task vjp: 0.03826599195599556s (var: 4.3332115637895186e-06)
Results for model resnet18 on task vjp using Functorch: 0.037201929837465286s (var: 6.139693198292662e-09)
Results for model resnet18 on task vhp: 0.2202976644039154s (var: 2.8687209052691287e-08)
Results for model resnet18 on task vhp using Functorch: 0.22117868065834045s (var: 4.108771278765744e-08)
Results for model resnet18 on task jvp: 0.18679651618003845s (var: 1.832455254202614e-08)
Results for model resnet18 on task jvp using Functorch: 0.05305683612823486s (var: 1.6690266946284282e-08)
Results for model fcn_resnet on task vjp: 0.6071907877922058s (var: 7.436695454998699e-07)
Results for model fcn_resnet on task vjp using Functorch: 0.6115708947181702s (var: 1.121692207561864e-06)
Results for model fcn_resnet on task vhp: 3.419469118118286s (var: 0.020633839070796967)
Failed model using Functorch: fcn_resnet, task: vhp, Error message: 
	 CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 47.46 GiB total capacity; 45.62 GiB already allocated; 5.31 MiB free; 46.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Results for model fcn_resnet on task jvp: 2.5421929359436035s (var: 3.1765587209520163e-06)
Results for model fcn_resnet on task jvp using Functorch: 0.7628333568572998s (var: 1.4555752159139956e-07)
Results for model detr on task vjp: 0.19494840502738953s (var: 1.9122715457342565e-05)
Failed model using Functorch: detr, task: vjp, Error message: 
	 Cannot access data pointer of Tensor that doesn't have storage
Results for model detr on task vhp: 1.1664292812347412s (var: 0.000948643428273499)
Failed model using Functorch: detr, task: vhp, Error message: 
	 Cannot access data pointer of Tensor that doesn't have storage
Results for model detr on task jvp: 0.9990308880805969s (var: 1.0214127541985363e-05)
Failed model using Functorch: detr, task: jvp, Error message: 
	 Trying to use forward AD with _cdist_forward that does not support it because it has not been implemented yet.
Please file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.
Results for model ppl_simple_reg on task vjp: 0.0007535457843914628s (var: 6.024204690646684e-09)
Results for model ppl_simple_reg on task vjp using Functorch: 0.0016954183811321855s (var: 1.160151974488599e-08)
Results for model ppl_simple_reg on task vhp: 0.0011888503795489669s (var: 5.93119386937957e-10)
Results for model ppl_simple_reg on task vhp using Functorch: 0.0026826143730431795s (var: 1.6787025103326414e-08)
Results for model ppl_simple_reg on task jvp: 0.001067900680936873s (var: 7.409912128331086e-10)
Results for model ppl_simple_reg on task jvp using Functorch: 0.002065300941467285s (var: 9.710328185974504e-08)
Results for model ppl_simple_reg on task hvp: 0.001212477684020996s (var: 1.974137298077494e-09)
Results for model ppl_simple_reg on task hvp using Functorch: 0.00482442369684577s (var: 2.327668653379078e-07)
Results for model ppl_simple_reg on task jacobian: 0.0009108781814575195s (var: 3.489469158068914e-09)
Results for model ppl_simple_reg on task jacobian using Functorch: 0.0019866942893713713s (var: 1.938326299466553e-08)
Results for model ppl_simple_reg on task hessian: 0.005053090862929821s (var: 3.370298600202659e-07)
Results for model ppl_simple_reg on task hessian using Functorch: 0.006374978926032782s (var: 7.556796077778927e-08)
Results for model ppl_simple_reg on task hessian_fwdrev: 0.0036706924438476562s (var: 1.996075527088692e-09)
Results for model ppl_simple_reg on task hessian_fwdrev using Functorch: 0.0058908225037157536s (var: 7.548283775804521e-08)
Results for model ppl_simple_reg on task hessian_revrev: 0.0015769004821777344s (var: 1.5754418214442012e-08)
Results for model ppl_simple_reg on task hessian_revrev using Functorch: 0.0041002752259373665s (var: 6.713568723171193e-08)
Results for model ppl_simple_reg on task jacfwd: 0.0018048763740807772s (var: 2.7375660849315864e-08)
Results for model ppl_simple_reg on task jacfwd using Functorch: 0.002047991845756769s (var: 2.432247070416338e-09)
Results for model ppl_simple_reg on task jacrev: 0.0009733677143231034s (var: 1.0078769818733235e-08)
Results for model ppl_simple_reg on task jacrev using Functorch: 0.0021971464157104492s (var: 1.2729884701911942e-08)
Results for model ppl_robust_reg on task vjp: 0.005820560269057751s (var: 8.582588151284654e-08)
Results for model ppl_robust_reg on task vjp using Functorch: 0.00796132069081068s (var: 9.663100541956737e-09)
Results for model ppl_robust_reg on task vhp: 0.009825301356613636s (var: 2.0081762386325863e-07)
Results for model ppl_robust_reg on task vhp using Functorch: 0.014890861697494984s (var: 4.558066279969353e-07)
Results for model ppl_robust_reg on task jvp: 0.008297419175505638s (var: 2.9454400873873965e-07)
Results for model ppl_robust_reg on task jvp using Functorch: 0.008052706718444824s (var: 7.120377176761394e-08)
Results for model ppl_robust_reg on task hvp: 0.015414690598845482s (var: 7.42123745567369e-07)
Results for model ppl_robust_reg on task hvp using Functorch: 0.02699306048452854s (var: 1.4650488537881756e-06)
Results for model ppl_robust_reg on task jacobian: 0.006207776255905628s (var: 1.7068457225377642e-07)
Results for model ppl_robust_reg on task jacobian using Functorch: 0.009173822589218616s (var: 1.2214455580306094e-07)
Results for model ppl_robust_reg on task hessian: 0.04670915752649307s (var: 1.4299343092716299e-05)
Results for model ppl_robust_reg on task hessian using Functorch: 0.02337808534502983s (var: 3.0397418413485866e-06)
Results for model ppl_robust_reg on task hessian_fwdrev: 0.024229884147644043s (var: 2.0425247839739313e-06)
Results for model ppl_robust_reg on task hessian_fwdrev using Functorch: 0.022021746262907982s (var: 3.512146236062108e-07)
Results for model ppl_robust_reg on task hessian_revrev: 0.012355780228972435s (var: 7.090877147675201e-07)
Results for model ppl_robust_reg on task hessian_revrev using Functorch: 0.013960313983261585s (var: 6.326549737423193e-07)
Results for model ppl_robust_reg on task jacfwd: 0.008112502284348011s (var: 2.88503088086145e-08)
Results for model ppl_robust_reg on task jacfwd using Functorch: 0.008947920985519886s (var: 4.2070990247111695e-08)
Results for model ppl_robust_reg on task jacrev: 0.00635871896520257s (var: 1.3403841592207755e-07)
Results for model ppl_robust_reg on task jacrev using Functorch: 0.009123563766479492s (var: 2.677554675756255e-07)
Results for model wav2letter on task vjp: 0.02078995667397976s (var: 2.1110793113621185e-06)
Results for model wav2letter on task vjp using Functorch: 0.019202351570129395s (var: 9.210506135559626e-09)
Results for model wav2letter on task vhp: 0.05997290462255478s (var: 8.558587616391833e-09)
Results for model wav2letter on task vhp using Functorch: 0.06035261228680611s (var: 1.6448565842708263e-09)
Results for model wav2letter on task jvp: 0.04507789760828018s (var: 1.5771547401399744e-09)
Results for model wav2letter on task jvp using Functorch: 0.013057494536042213s (var: 3.804750292601966e-09)
Results for model deepspeech on task vjp: 0.3648746609687805s (var: 1.5359055396402255e-05)
Failed model using Functorch: deepspeech, task: vjp, Error message: 
	 Cannot access storage of TensorWrapper
Results for model transformer on task vjp: 0.05496881157159805s (var: 1.242562319703211e-08)
Results for model transformer on task vjp using Functorch: 0.057835936546325684s (var: 2.6113376350167528e-08)
Results for model transformer on task vhp: 0.18313491344451904s (var: 7.226336151688884e-08)
Failed model using Functorch: transformer, task: vhp, Error message: 
	 bad optional access
Results for model transformer on task jvp: 0.13924935460090637s (var: 1.6989159234981344e-07)
Failed model using Functorch: transformer, task: jvp, Error message: 
	 Trying to use forward AD with embedding that does not support it because it has not been implemented yet.
Please file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=feature-request.yml so that we can prioritize its implementation.
Results for model multiheadattn on task vjp: 0.0014708995586261153s (var: 3.710916729460223e-08)
Results for model multiheadattn on task vjp using Functorch: 0.002404856728389859s (var: 2.1910574687922235e-08)
Results for model multiheadattn on task vhp: 0.003382015274837613s (var: 5.3098595742540056e-08)
Results for model multiheadattn on task vhp using Functorch: 0.005340623669326305s (var: 5.897558708056749e-08)
Results for model multiheadattn on task jvp: 0.0027526854537427425s (var: 3.508620949332908e-08)
Results for model multiheadattn on task jvp using Functorch: 0.0022981404326856136s (var: 1.327894807445773e-07)
```

</details>

All functorch errors are reported in its repository.

cc @zou3519  ",pytorch
75846,rohan-varma,pr,2022-04-14T22:05:44Z,[FSDP] Meta device support,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75846

Adds support for meta device initialization in FSDP.

Differential Revision: [D35650975](https://our.internmc.facebook.com/intern/diff/D35650975/)",pytorch
75880,rohan-varma,pr,2022-04-15T15:56:24Z,[FSDP] Support initialization of modules on meta device,"Adds support for FSDP to initialize modules on a ""meta"" device, either through the in-tree device=""meta"" + reset_parameters approach or [torchdistx](https://github.com/cbalioglu/torchdistx)'s deferred_init + materialize_module APIs.

Current constraints:

1. Argument added is `param_init_fn` which is a single lambda that specifies how a module should be initialized. If needed to initialize different modules in different ways, user can do this in the function they specify, or we can enhance FSDP to take a Dict[str, Callable] to support this more natively.
2. Does not cleanly support `ignored_modules` at the moment. Concretely, if a module is not sharded at all, it may remain unitialized, and user will have to initialize it on their own. Will dig into this as follow up work.

Currently, unittests only contain device=""meta"" + reset_parameters approach because torchdistx is not available in CI. Will work with CI team in order to allow unittesting with torchdistx.
",pytorch
75908,rohan-varma,pr,2022-04-15T19:35:56Z,[FSDP] Full state_dict rank0 only and CPU offload,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75970
* #75945
* __->__ #75908

Adds a `FullStateDictConfig` that users can use to allow full state dict checkpoint with rank 0 only and CPU offload. We simply dispatch these args into summon_full_params.

Example:


```
with fsdp.state_dict_type(full_state_dict, StateDictConfig(offload=True,rank0=True):
    state_dict = model.state_dict()
```

Differential Revision: [D35663270](https://our.internmc.facebook.com/intern/diff/D35663270/)",pytorch
75945,rohan-varma,pr,2022-04-17T05:50:18Z,[FSDP] Add tests for checkpointing buffers,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75977
* #75976
* #75970
* __->__ #75945

Ensures buffers can be checkpointed, currently only tested for full_state_dict. local_state_dict can be tested in follow up changes.

Differential Revision: [D35698458](https://our.internmc.facebook.com/intern/diff/D35698458/)",pytorch
75958,ngimel,pr,2022-04-18T05:02:17Z,Remove extra conv OpInfos,"OpInfos for conversion ops don't need separate entries for channels-last, it's enough to extend sample inputs. ",pytorch
75970,rohan-varma,pr,2022-04-18T14:26:38Z,Generalize parameter verification and broadcast,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75977
* #75976
* __->__ #75970
* #75945

Generalizes the two verifcation / module sync APIs out of DDP to be
consumed by FSDP.

Differential Revision: [D35712025](https://our.internmc.facebook.com/intern/diff/D35712025/)",pytorch
75976,rohan-varma,pr,2022-04-18T16:03:20Z,[FSDP] Parameter shape verification and synchronization,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #75977
* __->__ #75976
* #75970
* #75945

Use generalized APIs to perform param shape verification and module
synchronization.

Differential Revision: [D35712026](https://our.internmc.facebook.com/intern/diff/D35712026/)",pytorch
75977,rohan-varma,pr,2022-04-18T16:03:25Z,[FSDP] Add device_id argument,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #75977
* #75976
* #75970
* #75945

device_id argument will move module to CUDA device specified by ID, to
allow sharding, broadcast, flattening etc to take place on CUDA device even for
passed in CPU modules to help the initialization time.

Similar to DDP, inputs are also cast if this argument is specified. `to_kwargs`
from DDP is generalized.

Differential Revision: [D35712024](https://our.internmc.facebook.com/intern/diff/D35712024/)",pytorch
76001,0x00b1,pr,2022-04-18T22:49:27Z,Fast Bessel functions,"Adds missing fast Bessel functions:

- [x] `bessel_j0`, Bessel function of the first kind of order 0.
- [x] `bessel_j1`, Bessel function of the first kind of order 1.
- [x] `bessel_y0`, Bessel function of the second kind of order 0.
- [x] `bessel_y1`, Bessel function of the second kind of order 1.
- [x] `bessel_k0`, modified Bessel function of the second kind of order 0.
- [x] `bessel_k0e`, exponentially scaled modified Bessel function of order 0.
- [x] `bessel_k1`, modified Bessel function of the second kind of order 1.
- [x] `bessel_k1e`, exponentially scaled modified Bessel function of order 0.

They complement the Bessel functions currently provided by PyTorch:

- `i0`, modified function of order 0.
- `i0e`, exponentially scaled modified Bessel function of order 0.
- `i1`, modified Bessel function of order 1.
- `i1e`, exponentially scaled modified Bessel function of order 1.

Like many of the other special functions provided by PyTorch, the CPU implementation is adapted from two sources, both from Stephen Moshier, â€œMethods and Programs for Mathematical Functionsâ€ and the Cephes Mathematical Library. The CUDA implementation is practically identical to the CPU implementation. TensorIterator is used to perform element-wise computation. Each of the provided operations should be considered numerically equivalent to their corresponding SciPy operations and tests are provided to verify this equivalency. 

While each of these operations are useful â€œas is,â€ they also provide a foundation for adding other special functions like _n_-order Bessel functions, derivatives of Bessel functions, zeros of Bessel functions, etc.

A few notes:

- This is my first time adding new operations so nits are encouraged! I want to get a good understanding of the trade-offs as I plan on contributing a number of new special functions. 
- The [special documentation](https://pytorch.org/docs/stable/special.html) could use some organization (e.g., adding sections for things like Bessel or error functions). I am more than happy to do this in a subsequent pull request.
- Functions are, for the moment, `non_differentiable`. I will add derivatives once `jv`, `yv`, and `kv` are merged. ",pytorch
76032,lezcano,pr,2022-04-19T11:24:12Z,Update Cholesky's forward and backward derivative,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #76032

This PR:
- Derives formally a new rule for Cholesky (write-up to come)
- Implements it without using in-place operations in the forward or backward.
- Does not instantiate inverses explicitly, but rather it solves two triangular systems of equations (2 triang vs 1 triang and 2 matmuls should be comparable, but the first one should be more stable).
- Fixes almost all the batching transform / composite compliance issues these gradients had.",pytorch
76062,ngimel,pr,2022-04-19T18:55:52Z,do intermediate math in opmath_t in lerp,"Per title
",pytorch
76092,micmelesse,pr,2022-04-20T05:03:24Z,[ROCM] remove rocfft workaround,We enable the last remaining fft tests on rocm. The issue was caused by a workaround which is no longer needed.,pytorch
76115,lezcano,pr,2022-04-20T16:11:53Z,Improve the derivative of the QR decomposition,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #76115

We derive and implement a more concise rule for the forward and backward
derivatives of the QR decomposition. While doing this we:
- Fix the composite compliance of `linalg.qr` and we make it support functorch's batching transform
- Add preconditions to the forward AD.
- Correct / improve the docs, as they contained some inaccuracies.
- Improve the performance and simplify the implementation of both forward and backward
- Avoid saving the input matrix for the backward computation.
- Activate the tests when there's no magma but there's cusolver.",pytorch
76130,rohan-varma,pr,2022-04-20T19:19:58Z,Add mixed precision doc,Adds docstring for mixed precision,pytorch
76159,crcrpar,pr,2022-04-21T01:46:03Z,[torch.onnx] support `torch.nn.functional.grid_sample`,"summary

- Adds `F.grid_sample` support
- Adds a test case

Fixes #27212",pytorch
76220,ngimel,pr,2022-04-22T06:18:28Z,Fix deterministic indexing with non-contiguous tensor,"Fixes #76176 (first case)
",pytorch
76234,rohan-varma,pr,2022-04-22T14:29:31Z,Disable RPC profiling for kineto profilers,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76234

RPC profiling is only enabled when the profiler is of legacy type.

Differential Revision: [D35484579](https://our.internmc.facebook.com/intern/diff/D35484579/)",pytorch
76303,lezcano,pr,2022-04-25T11:55:54Z,Add linalg.vander,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #76303

This PR adds `linalg.vander`, the linalg version of `torch.vander`.

We add autograd support and support for batched inputs.

We also take this chance to improve the docs (TODO: Check that they
render correctly!) and add an OpInfo.

**Discussion**: The current default for the `increasing` kwargs is extremely
odd as it is the opposite of the classical definition (see [wiki](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is
reflected in the docs, where I explicit both the odd defaults that we
use and the classical definition. See also [this stackoverflow post](https://stackoverflow.com/a/71758047/5280578), which 
shows how people are confused by this defaults.

My take on this would be to correct the default to be `increasing=True`
and document the divergence with NumPy (as we do for other `linalg`
functions) as:

- It is what people expect
- It gives the correct determinant called ""the [Vandermonde determinant](https://en.wikipedia.org/wiki/Vandermonde_polynomial)"" rather than (-1)^{n-1} times that quantity (ugh).
- [Minor] It is more efficient (no `flip` needed)
- Since it's under `linalg.vander`, it's strictly not a drop-in replacement for `np.vander`.

We will deprecate `torch.vander` in a PR after this one in this stack
(once we settle on what's the correct default).

Thoughts? @mruberry

cc @kgryte @rgommers 

Fixes https://github.com/pytorch/pytorch/issues/60197",pytorch
76313,0x00b1,pr,2022-04-25T15:34:24Z,Polynomial operations,"WIP WIP WIP

The aim of this pull request is to motivate a discussion about integrating polynomial operations into PyTorch. The functionality provided is the exact functionality provided by [numpy.polynomial](https://numpy.org/doc/stable/reference/routines.polynomials.html). However, this pull request, for the sake of simplicity, only implements power seriesâ€™. Other polynomial families will be added once, and if, an API has been agreed on.

Needless to say, polynomials are foundational to numerical analysis. This point was especially evident to me as I started implementing a variety of special functions that relied on the evaluation of pre-computed polynomial coefficients. Yet, polynomials play a limited or no role in a lot of the features that PyTorch is best known (e.g., neural networks). So I would expect a debate about whether or not they should be included and/or exposed publicly! My hope is that this PR encourages that debate.",pytorch
76356,ngimel,pr,2022-04-26T00:31:14Z,remove unneeded overload for nansum,"Per title, fixes a few OpInfo skips
",pytorch
76357,neerajprad,pr,2022-04-26T00:38:14Z,Fix seed in distributions test for deterministic results,"Summary:
Fix for https://github.com/pytorch/pytorch/issues/76160

This fixes the random seed for the `test_wishart_log_prob` (like all random tests in `test_distributions`) to prevent non-determinism.

Test Plan: Tested locally.

Differential Revision: D35914795

",pytorch
76374,rohan-varma,pr,2022-04-26T15:50:35Z,Generalize param verification and broadcast,New PR for https://github.com/pytorch/pytorch/pull/75970 to be compatible with GHF.,pytorch
76423,rohan-varma,pr,2022-04-27T01:47:46Z,Change mixed precision API,"Change the API such that if a precision type is not explicitly speciifed in `MixedPrecision` config, we don't cast it at all.

Previously, we would default to fp16 reduced precision, but this doesn't account for the case where user might want to, for example, use only reduced gradient comm precision. Trying to do this via:
```
MixedPrecision(reduce_dtype=torch.float16, param_dtype=torch.float32, buffer_dtype=torch.float32)
```

does not work for all use cases because the code will still attempt to cast params to fp32, but user's model may be assuming double type/fp64 somewhere. 

Now, specifying
```
MixedPrecision(reduce_dtype=torch.float16)
```

only affects gradient comm precision, and does not touch casting of params / buffers.

Note that if user specifies reduced precision for only parameters, gradients will be of this reduced type in _post_backward_hook and are therefore communicated in this precision. Therefore, the priority of precision in which grads are communicated is:

reduce_dtype, if specified -> param_dtype, if specified -> full precision param type.

We take additional care to make sure grads are cast back to the full precision param type for optimizer step: either if parameter was in reduced precision or if parameter was not in reduced precision but reduced gradient precision for comm was configured.",pytorch
76442,rohan-varma,pr,2022-04-27T15:23:27Z,[FSDP] Parameter shape verification and synchronization,"GHF version of https://github.com/pytorch/pytorch/pull/75976

for CPU models, we move the model to GPU to perform synchronization, shape check, flattening + sharding and then move the module back after init and log a warning saying that we did this.

Note that this adds a dependency that FSDP model __init__ depends on having GPU and no longer does CPU-only initialization, which should be a fine tradeoff given that forward pass requires GPU. We can design CPU support in the future, use case pending. ",pytorch
76509,eqy,pr,2022-04-28T00:00:38Z,Add high level control of fp32 matmul precision; disable TF32 for matmuls by default,"#76440

CC @mruberry @ptrblck
",pytorch
76545,lezcano,pr,2022-04-28T18:10:39Z,Simplify the OpInfos for norm / linalg_norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76551
* #76547
* __->__ #76545

This will be helpful later on when we add tests for `dtype` (after correcting its gradient formula).",pytorch
76546,lezcano,pr,2022-04-28T18:10:45Z,Make linalg_vector_norm use the same TI implementation than norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76551
* #76547
* __->__ #76546
* #76545

As per title",pytorch
76547,lezcano,pr,2022-04-28T18:10:52Z,Update linalg.*norm functions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76551
* __->__ #76547

This PR does a number of things:
- Move `linalg.vector_norm` to structured kernels and simplify the logic (
- Fixes a number of prexisting issues with the dtype kwarg of these ops
- Heavily simplifies and corrects the logic of `linalg.matrix_norm` and `linalg.norm` to be consistent with the docs
  - Before the `_out` versions of these functions were incorrect. This can be seen in the removal of many composite compliance `expectedFailure`s
  - Their implementation is now as efficient as expected, as it avoids reimplementing these operations whenever possible.
- Deprecates `torch.frobenius_norm` and `torch.nuclear_norm`, as they were exposed in the API and they are apparently being used in mobile (??!!) even though they were not documented and their implementation      was slow.
  - I'd love to get rid of these functions already, but I guess we have to go through their deprecation.

TODO: 
- Update the `dtype=` docs in `linalg.norm` and `linalg.matrix_norm`.
- Cannot deprecate `torch.frobenius_norm` and `torch.nuclear_norm` yet as they are used within `torch.norm`, which is not deprecated",pytorch
76551,lezcano,pr,2022-04-28T18:44:16Z,"Fix derivatives for linalg.vector_norm(..., dtype=)","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #76551
* #76547

As per title",pytorch
76579,rohan-varma,pr,2022-04-29T04:44:38Z,Implement register_load_state_dict_post_hook,"Implements `register_load_state_dict_post_hook` API as discussed in https://github.com/pytorch/pytorch/issues/75287.

Unittests cover:
- Ensuring hooks are called with the correct module
- Hook is called with `IncompatibleKeys` field
- If hook modifies this, load_state_dict returns the modified result
",pytorch
76592,silvasean,pr,2022-04-29T13:53:23Z,Upstream `argmax` shape function.,"Keeping this first commit simple to test out the flow. Will bulk-add the
rest once this one goes through.

Shape function taken from:
https://github.com/llvm/torch-mlir/blob/5192a4e9f3e4cbc77838b70153cd4632aa43dd7f/python/torch_mlir/dialects/torch/importer/jit_ir/build_tools/shape_lib_gen.py#L488

",pytorch
76624,rohan-varma,pr,2022-05-01T02:43:29Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76624

Implements `register_load_state_dict_post_hook` API as discussed in https://github.com/pytorch/pytorch/issues/75287.

Unittests cover:
- Ensuring hooks are called with the correct module
- Hook is called with `IncompatibleKeys` field
- If hook modifies this, load_state_dict returns the modified result",pytorch
76629,AdityaKane2001,pr,2022-05-01T06:51:29Z,Nit in `TripletMarginLoss`,"`:math` -> `:math:`
",pytorch
76641,0x00b1,pr,2022-05-02T00:56:15Z,Airy functions,"Adds two new special functions: the Airy function Ai(input) and the Airy function Bi(input):

```Python
airy_ai(input: Tensor) â†’ Tensor
```

```Python
airy_bi(input: Tensor) â†’ Tensor
```

The Airy functions are linearly independent solutions to the differential equation:

![image](https://user-images.githubusercontent.com/315821/166516120-05e2e7b0-cc64-4e70-b204-d355734b050b.png)

They are useful directly (e.g., I use Airy functions in my optical work at FAIR) and indirectly (e.g., their relationship to the Bessel functions and modified Bessel functions). 

The CPU implementation is adapted from three sources: Stephen Moshier â€œMethods and Programs for Mathematical Functions,â€ [Cephes](https://www.netlib.org/cephes/), and [AMOS](http://www.netlib.org/amos/). The CUDA implementation is practically identical to the CPU implementation. `TensorIterator` is used to perform element-wise computation. 

Each of the provided operations should be considered numerically equivalent to the corresponding output of the SciPy `scipy.special.airy` operation for `torch.float32` and `float64` values in the interval [-10, 10] and tests are provided to verify this equivalency.",pytorch
76642,rohan-varma,pr,2022-05-02T01:21:32Z,Mixed precision batchnorm fix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76642

Currently, if BN such as SyncBN is used in model with FSDP mixed precision enabled, auto wrapping will produce the following error:

```
RuntimeError: Expected counts to have type Half but got Float
```

the root cause being in this case that SyncBN kernels seems to assume fp32 somewhere and crashes for fp16 inputs.

Long term solution might be to make all BN kernels support fp16 / reduced precision types, but FSDP also supports other precision such as bloat16.

As a result, this PR fixes the issue by disabling mixed precision for BN units. To do this, we:

1. Add a policy that always wraps batchnorm layers in their own FSDP unit
2. Add a notion of ""or policy"" which returns True if either policy A or policy B fires
3. Use this in FSDP, where if mixed precision is enabled and model has BN, we `or` the given auto_wrap_policy with the policy that always wraps BN in its own unit. This works because children modules are always wrapped first, so recursive wrapping will go to leaf BN units and always wrap those.
4. Add an attribute to module `_wrap_override` that turns mixed precision off in FSDP config.

Differential Revision: [D36032761](https://our.internmc.facebook.com/intern/diff/D36032761/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36032761/)!",pytorch
76690,tillahoffmann,pr,2022-05-02T20:22:39Z,Add mode property to distributions.,"This PR fixes #69466 and introduces some other minor changes. Tests are somewhat more involved because a reference implementation in `scipy` is not available; tests proceed differently for discrete and continuous distributions.

For continuous distributions, we evaluate the gradient of the `log_prob` at the mode. Tests pass if the gradient is zero OR (the mode is at the boundary of the support of the distribution AND the `log_prob` decreases as we move away from the boundary to the interior of the support).

For discrete distributions, the notion of a gradient is not well defined. We thus ""look"" ahead and behind one step (e.g. if the mode of a Poisson distribution is 9, we consider 8 and 10). If the step ahead/behind is still within the support of the distribution, we assert that the `log_prob` is smaller than at the mode.

For one-hot encoded distributions (currently just `OneHotCategorical`), we evaluate the underlying mode (i.e. encoded as an integral tensor), ""advance"" by one label to get another sample that should have lower probability using `other = (mode + 1) % event_size` and re-encode as one-hot. The resultant `other` sample should have lower probability than the mode.

Furthermore, Gamma, half Cauchy, and half normal distributions have their support changed from positive to nonnegative. This change is necessary because the mode of the ""half"" distributions is zero, and the mode of the gamma distribution is zero for `concentration <= 1`.

cc @fritzo 
",pytorch
76691,ngimel,pr,2022-05-02T21:27:57Z,fix where type promotion,"Fixes #73298
I don't know whether `where` kernel actually supports type promotion, nor am I in the mood to find out, so it's manual type promotion. 
Edit: nah, i can't tell TI to ""promote to common dtype"" because of bool condition, so manual type promotion is our only option. 
I'll see what tests start failing and fix. 
Uses some parts from #62084 ",pytorch
76714,ngimel,pr,2022-05-03T02:01:45Z,[WIP] sum reference,"Per title
",pytorch
76748,rohan-varma,pr,2022-05-03T16:53:36Z,Load state dict post hook,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #76749
* __->__ #76748

",pytorch
76749,rohan-varma,pr,2022-05-03T16:53:42Z,Profiling range for FSDP.forward,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76749
* #76748

",pytorch
76777,tillahoffmann,pr,2022-05-03T23:59:22Z,Add a transform for positive-definite matrices.,"The `PositiveDefiniteTransform` is required to transform from an unconstrained space to positive definite matrices, e.g. to support testing the Wishart mode in #76690. It is a simple extension of the `LowerCholeskyTransform`.

I've also added a small test that ensures the generated data belong to the domain of the associated transform. Previously, the data generated for the inverse transform of the `LowerCholeskyTransform` wasn't part of the domain, and the test only passed because the comparison uses `equal_nan=True`.

cc @fritzo, @neerajprad ",pytorch
76817,rohan-varma,pr,2022-05-04T15:56:37Z,Upstream `argmax` shape function.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Keeping this first commit simple to test out the flow. Will bulk-add the
rest once this one goes through.

Shape function taken from:
https://github.com/llvm/torch-mlir/blob/5192a4e9f3e4cbc77838b70153cd4632aa43dd7f/python/torch_mlir/dialects/torch/importer/jit_ir/build_tools/shape_lib_gen.py#L488
Approved by: https://github.com/eellison",pytorch
76818,rohan-varma,pr,2022-05-04T15:57:24Z,Upstream `argmax` shape function.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Keeping this first commit simple to test out the flow. Will bulk-add the
rest once this one goes through.

Shape function taken from:
https://github.com/llvm/torch-mlir/blob/5192a4e9f3e4cbc77838b70153cd4632aa43dd7f/python/torch_mlir/dialects/torch/importer/jit_ir/build_tools/shape_lib_gen.py#L488
Approved by: https://github.com/eellison",pytorch
76819,rohan-varma,pr,2022-05-04T15:57:31Z,Add `TORCH_CPP_LOG_LEVEL` to the docs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Fixes #70667

`TORCH_CPP_LOG_LEVEL=INFO` is needed for `TORCH_DISTRIBUTED_DEBUG` to be effective.

For reference, https://github.com/pytorch/pytorch/pull/71746 introduced the environment variable `TORCH_CPP_LOG_LEVEL` and https://github.com/pytorch/pytorch/pull/73361 documented it.
Approved by: https://github.com/rohan-varma",pytorch
76823,rohan-varma,pr,2022-05-04T16:24:12Z,Load state dict post hook,"Implements `register_load_state_dict_post_hook` API as discussed in https://github.com/pytorch/pytorch/issues/75287.

Unittests cover:
- Ensuring hooks are called with the correct module
- Hook is called with `IncompatibleKeys` field
- If hook modifies this, load_state_dict returns the modified result
",pytorch
76828,lezcano,pr,2022-05-04T18:08:13Z,Avoid copies in matmul,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76828

With this PR, matmul just folds a bmm into a mm o mv if and only if it
can achieve so without copying. We add tests for this to make sure that
our algorithm to detect this is accurate.

For the cases where it was copying before see https://github.com/pytorch/pytorch/pull/75197#discussion_r843413208 https://github.com/pytorch/pytorch/pull/75197#discussion_r863489479 https://github.com/pytorch/pytorch/pull/75197#discussion_r863489805

Fixes https://github.com/pytorch/pytorch/issues/76702

cc @ngimel @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
76837,b0noI,pr,2022-05-04T18:33:10Z,aten::_validate_sparse_compressed_tensor_args ops added to the ALLOW_LISST,"aten::_validate_sparse_compressed_tensor_args ops added to the ALLOW_LISST of the ops can be backwards incompatible (till May 15 2022).

Fixes build problems, introduced by https://github.com/pytorch/pytorch/pull/76635
",pytorch
76855,ngimel,pr,2022-05-04T22:45:57Z,Adds amax and amin references,"Also extends reference testing to error inputs. 
",pytorch
76874,ngimel,pr,2022-05-05T03:52:02Z,Port clamp_min and clamp_max to structured,"per title
",pytorch
76878,rohan-varma,pr,2022-05-05T05:10:19Z,Fix FSDP CI,"Sometimes we randomly see unrelated FSDP CI failures such as https://github.com/pytorch/pytorch/runs/6298275361?check_suite_focus=true which are unrelated to the diff at hand. Suspicion is that because some other tests set `BACKEND` which is a generic env var for distributed tests, if those tests are run in same CI container before, they won't get unset and we'll use gloo for FSDP backend. 

But gloo is not currently supported, and this was mostly added for easy testing during early FSDP development, so remove this entirely.",pytorch
76889,silvasean,pr,2022-05-05T10:03:08Z,Upstream remaining shape functions from Torch-MLIR.,"Follow-on to https://github.com/pytorch/pytorch/pull/76592 adding the
rest.
",pytorch
76899,rohan-varma,pr,2022-05-05T16:39:56Z,Profiling range for FSDP.forward,Same as https://github.com/pytorch/pytorch/pull/76749 which had issues in updating via ghstack.,pytorch
76912,rohan-varma,pr,2022-05-05T19:57:09Z,[FSDP] Use post load_state_dict hooks,Rehash of https://github.com/pytorch/pytorch/pull/75426 now that a revised version of load_state_dict_post_hook has landed.,pytorch
76926,ngimel,pr,2022-05-05T23:00:17Z,port clamp_min/max tensor overloads to structured,"Per title
",pytorch
76981,0x00b1,pr,2022-05-06T20:18:13Z,Jacobi elliptic functions,"Adds the Jacobi elliptic functions!

- I modeled the implementation on the implementation of `torch.special.zeta`. The ability to use a scalar in either position and the ability to differentiate by either position is awesome and, despite the added boilerplate, I think it should be the model for all binary operations.
- Implementations trade speed for accuracy. We can reevaluate that in the future.
- I believe the logical next steps for elliptic function coverage are inverse Jacobi, Weierstrass, inverse Weierstrass, and theta functions.
- They do not match 1:1 functions in SciPy.  SciPy, however, does provide `scipy.special.ellipj` but it uses an `m` rather than `k` parameterization (hence the `_k` suffix to give us space to add the other parameterization) and it awkwardly returns `sn`, `cn`, and `dn` together. ",pytorch
77002,eqy,pr,2022-05-06T22:58:48Z,[cuDNN V8 API] (reopen) Allow the number of kernels profiled under torch.backends.cudnn.benchmark = True to be limitedCudnnv8 benchmark limit,"(reopening due to botched merge)
The cuDNN V8 API (main support merged in https://github.com/pytorch/pytorch/pull/60755) potentially exposes many more kernels with benchmark=True. While these additional kernels can improve performance, it is often unnecessary to run every kernel returned by the heuristic and doing so may degrade the user experience by causing the first model iteration to be very slow. To alleviate this issue, this PR introduces torch.backends.cudnn.benchmark_limit. benchmark_limit specifies the maximum number of working cuDNN kernels to try for a given workload, with the default being 10 (similar to what TensorFlow does). benchmark_limit = 0 yields the current behavior of trying every kernel returned by the heuristic.

CC @ptrblck @ngimel @xwang233",pytorch
77034,0x00b1,pr,2022-05-07T21:15:10Z,Bessel functions,"# Documentation

## Bessel and Related Functions

### Bessel Functions

```Python
bessel_j0(input: Tensor, *, out=None) â†’ Tensor

bessel_j1(input: Tensor, *, out=None) â†’ Tensor

bessel_y0(input: Tensor, *, out=None) â†’ Tensor

bessel_y1(input: Tensor, *, out=None) â†’ Tensor
```

### Modified Bessel Functions

```Python
modified_bessel_i0(input: Tensor, *, out=None) â†’ Tensor

modified_bessel_i1(input: Tensor, *, out=None) â†’ Tensor

modified_bessel_k0(input: Tensor, *, out=None) â†’ Tensor

modified_bessel_k1(input: Tensor, *, out=None) â†’ Tensor
```

### Scaled Modified Bessel Functions

```Python
scaled_modified_bessel_i0(input: Tensor, *, out=None) â†’ Tensor

scaled_modified_bessel_i1(input: Tensor, *, out=None) â†’ Tensor

scaled_modified_bessel_k0(input: Tensor, *, out=None) â†’ Tensor

scaled_modified_bessel_k1(input: Tensor, *, out=None) â†’ Tensor
```

### Airy Functions

```Python
airy_ai(input: Tensor, *, out=None) â†’ Tensor

airy_bi(input: Tensor, *, out=None) â†’ Tensor
```

### Airy Derivatives

```Python
airy_derivative_ai(input: Tensor, *, out=None) â†’ Tensor

airy_derivative_bi(input: Tensor, *, out=None) â†’ Tensor
```

## Gamma Functions

```Python
ln_gamma(input: Tensor, *, out=None) â†’ Tensor
```

# Names

This patch also proposes the following syntax conventions that I hope we adopt for other operators:

* Function family prefixes (e.g., Airy or Bessel) for all but the most common function families (e.g., exponential, logarithmic, trigonometric, etc.)
* A prefix to distinguish functions from derivatives (e.g., `airy_ai` and `airy_derivative_ai`). I also propose we adopt this for integrals too (e.g., elliptical or exponential integrals).",pytorch
77035,ngimel,pr,2022-05-07T21:35:20Z,type promote clamp,"Fixes #76630
When clamp(Tensor, Tensor) is structured, big parts of this PR won't be needed, but for now let's fix type promotion to make behavior more regular. 
Some special-casing is done in hardtanh to preserve legacy behavior of not promoting to boundaries. 
Also, relu is explicitly disabled on bool inputs to avoid accidental promotion to integer type. 
BC-breaking note:
Previously, `min` and `max` arguments in clamp did not participate in type promotion, which made it inconsistent with `minimum` and `maximum` operations. This PR (and PRs porting clamp_min/clamp_max to structured) make `min` and `max` arguments participate in type promotion. ",pytorch
77056,0x00b1,pr,2022-05-09T00:23:39Z,Fresnel integrals,"Adds the Fresnel integrals `S(x)` and `C(x)` to `torch.special`:

```Python
fresnel_integral_s(input, *, out=None) -> Tensor
fresnel_integral_c(input, *, out=None) -> Tensor
```

![image](https://user-images.githubusercontent.com/315821/168862620-b1a56057-9427-42bf-8eb0-66488035a570.png)

![image](https://user-images.githubusercontent.com/315821/168862573-ce1d8184-24e9-4b16-a338-8392618d4424.png)

- Conceptually equivalent to [`scipy.special.fresnel`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.fresnel.html#scipy.special.fresnel) but the implementation is very different. It will become strictly equivalent once I get around to implementing the Faddeeva function.",pytorch
77062,ngimel,pr,2022-05-09T05:55:19Z,use opmath and expm1 for elu,"Fixes #77054
",pytorch
77089,rohan-varma,pr,2022-05-09T17:39:22Z,Mixed Precision batchnorm fix,Rehash of https://github.com/pytorch/pytorch/pull/76642 which could not be updated due to GHF out of sync issue.,pytorch
77100,ngimel,pr,2022-05-09T20:59:55Z,speed up 1d sort,"This speeds up sort for 1d case (by approx 2x for large sizes) where segment sorting is not required and slightly reduces memory usage. I'm not sure if memory usage can be meaningfully improved.
Slightly helps #77049
I'll update PR with doing the same for multiple segments, provided that each segment size is large. 

cc @peterbell10, I had to comment out TORCH_ASSERT_NO_OPERATORS because otherwise I was getting compilation errors, do you know what's up?",pytorch
77114,eqy,pr,2022-05-09T23:37:59Z,[CUBLAS][TF32] Add environment variable to allow override of `allow_tf32_cublas`,"update: the env var is `TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1`, the originally committed git message below might be inaccurate 
---

#76509 changes the default behavior of matmuls to avoid using TF32. However, TF32 use cases still exist including CI/deployment environments that are often managed via environment variables. This PR just adds an environment variable check, currently called `TORCH_ALLOW_TF32_OVERRIDE` to support enabling TF32 via an environment variable rather than the C++/Python API.

CC @xwang233 @ptrblck @syed-ahmed @csarofeen @ngimel @mruberry ",pytorch
77149,ngimel,pr,2022-05-10T06:47:24Z,port clamp.Tensor to structured,Per title,pytorch
77156,rohan-varma,pr,2022-05-10T09:11:11Z,[WIP][FSDP] Add ability to ignore individual parameters,"Currently for discussion / validation, not yet for land.
",pytorch
77172,tillahoffmann,pr,2022-05-10T16:38:34Z,Add reshaping for distributions.,"In light of #76709, I've been experimenting with reshaping for distributions. There seems to be a generic-ish way of handling reshaping. It comprises two parts:

1. A property `_reshape_args` (questionable name) that yields the names of arguments that need to be reshaped. It defaults to `arg_constraints` such that distributions like `Beta`, `Normal`, `Gamma` (and others with unambiguous arguments) can be handled in a generic way. For distributions with ambiguous arguments (such as `Bernoulli` which takes `probs` or `logits`), we yield the name of whichever argument was used to create the class.
2. An implementation in the `Distribution` base class that fetches all arguments that need to be reshaped, reshapes them, and creates a new instance with `validate_args=False` to skip the validation step.

This PR is very much a draft and requires further discussion.",pytorch
77187,rohan-varma,pr,2022-05-10T19:19:59Z,Separate input moving to utils file,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77187

In preparation for FSDP `device_id` support which will move GPU inputs to the device specified, move these codes to a utils file.

TODO: type the code: https://github.com/pytorch/pytorch/issues/77190",pytorch
77224,rohan-varma,pr,2022-05-11T02:35:52Z,CheckpointWrapper state_dict fix,"- Uses state dict / load state dict hooks to ensure that modules wrapped with `CheckpointWrapper` can be loaded into non-checkpointed wrapped module.

This is because a training run can use activation checkpointing, then we can recover `state_dict`, and a future run may not want to wrap modules with activation checkpointing or decide to change activation checkpoint wrapping structure. To support this, we add hooks to remove / add the relevant prefix as needed.

Tests are added to ensure we can load into CheckpointWrapper module as well as local module from CheckpointWrapper-wrapped module. state_dict with FSDP is also verified.",pytorch
77227,ngimel,pr,2022-05-11T03:25:43Z,Make minimum and maximum aliases for clamp_max and clamp_min,"Also, fix nan propagation for clamp_max and clamp_min
",pytorch
77234,rohan-varma,pr,2022-05-11T07:31:25Z,[Reland] Mixed precision batchnorm fix,"Reland of https://github.com/pytorch/pytorch/pull/77089, which was reverted due to land race.",pytorch
77235,rohan-varma,pr,2022-05-11T07:44:30Z,Test fix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77235
* #77187

",pytorch
77281,rohan-varma,pr,2022-05-11T19:13:03Z,Fix load_state_dict_post_hooks BC,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77281

Certain modules (such as `nn.Softmax`) may not dispatch into __setstate__ of nn.Module, so `_load_state_dict_post_hooks` might be undefined, causing errors.

As a stopgap, fix this by checking if the attr exists on the module before accessing it. Also, create the attribute if it does not exist when registering these hooks, instead of assuming it will exist.

The proper fix is actually to fix https://github.com/pytorch/pytorch/issues/77280 and ensure nn.Module instances dispatch into superclass of __setstate__. We should probably also add documentation that if users override their own nn.Module setstate method, it needs to dispatch into the superclass, otherwise there will be BC issues.

More context in https://github.com/pytorch/pytorch/issues/77280.

Differential Revision: [D36321915](https://our.internmc.facebook.com/intern/diff/D36321915/)",pytorch
77298,eqy,pr,2022-05-11T22:05:02Z,[CUBLAS][TF32] Skip `test_cublas_allow_tf32_get_set` if `TORCH_ALLOW_TF32_CUBLAS_OVERRIDE` is set,"Follow-up to #77114 to prevent test breakages when the environment variable is set.

CC @xwang233 @ngimel @ptrblck ",pytorch
77306,ngimel,pr,2022-05-11T22:46:42Z,"make clamp_min/max use minimum/maximum kernels, make clamp* correctly propagate nans","Since clamp_min and maximum is the same op, reuse the same kernel (it also correctly propagate nans from both input and boundary, clamp* propagated from input only). 
Also fixed codegen to make Tensor? overloads come before Scalar? overloads, cc @alband. 
Fixes #67428 and #76795 (scalar overloads for clamp* are still not fixed, will do in the next PR). 
",pytorch
77310,rohan-varma,pr,2022-05-11T23:11:48Z,"[Prototype, not for land] Object-based collectives work with shared memory tensors",- Use pickling magic to try to make object-based collectives work with SHM tensors.,pytorch
77321,rohan-varma,pr,2022-05-12T01:53:42Z,Add device_id support to FSDP,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77321

Adds `device_id` argument to FSDP. If this is specified, and module is on CPU, we move module to this device.

In addition, even if device_Id is not specified, we move module to this device to do flattening, sharding, etc but then move it back to CPU before returning to user.

This PR also moves module inputs to the `compute_device` during forward of root FSDP instance. Note that we guarantee `compute_device == device_id` if `device_id` is specified.

This PR also detects if user passed in multi-device module, which FSDP does not support, and throws an error.


Benchmarking on single host 8 GPUs:

- GPT 700m parameter model FSDP init on CPU: ~28s, moving to GPU speeds it up to 2s, a 14x win
- GPT 6B parameter model FSDP init on CPU: ~47s, moving it to GPU speeds it up to 17s, 2.7x win",pytorch
77330,ngimel,pr,2022-05-12T05:17:44Z,reenable filtered op tests,"Re-enable previously filtered op tests. Expecting lotsa failures. Should dtype also be wrapped in list?
cc @mruberry, @suo
",pytorch
77371,ngimel,pr,2022-05-12T20:08:45Z,make scalar clamp overloads propagate nan,,pytorch
77373,rohan-varma,pr,2022-05-12T20:46:58Z,"Back out ""Load state dict post hook""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77373

Original commit changeset: a862bae9cc97

Original Phabricator Diff: D36188964

Differential Revision: [D36356811](https://our.internmc.facebook.com/intern/diff/D36356811/)",pytorch
77392,rohan-varma,pr,2022-05-13T00:01:57Z,[Reland] load_state_dict post hook,"Reland of https://github.com/pytorch/pytorch/pull/76823 with fixes to call `__setstate__` for softmax/softmin/logsoftmax as per discussion with @albanD and @jbschlosser. Original description:

Implements `register_load_state_dict_post_hook` API as discussed in https://github.com/pytorch/pytorch/issues/75287.

Unittests cover:
- Ensuring hooks are called with the correct module
- Hook is called with `IncompatibleKeys` field
- If hook modifies this, load_state_dict returns the modified result
",pytorch
77421,lezcano,pr,2022-05-13T15:21:10Z,A few forward AD formulas,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #77421

It includes all-time favourites like:
- `put`
- `nn.functional.embedding`
- `prelu`
- `nn.functional.bilinear`
- `nn.functional.rrelu`
- `nn.functional.logsigmoid`",pytorch
77433,0x00b1,pr,2022-05-13T18:07:52Z,Elliptic integrals,"Adds:

```Python
complete_elliptic_integral_k_k(input, *, out=None) -> Tensor
```

Complete elliptic integral of the first kind $K(\text{input})$.

The parameterization in terms of $\text{input}$, $k$, follows that of Abramowitz, et al. 

```Python
complete_elliptic_integral_k_e(input, *, out=None) -> Tensor
```

Complete elliptic integral of the second kind $E(\text{input})$.

The parameterization in terms of $\text{input}$, $k$, follows that of Abramowitz, et al. ",pytorch
77491,rohan-varma,pr,2022-05-14T23:25:40Z,Pass device_id into recursive_wrap,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77492
* __->__ #77491


`device_id` arg should be passed down in auto wrapping.

",pytorch
77492,rohan-varma,pr,2022-05-14T23:25:45Z,FSDP parameter sync,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77492
* #77491

Add a `sync_module_states` flag to FSDP which enables each rank to start off with the same module parameters if the user did not seed the same across ranks.

In addiiton, this PR helps with non-redundant loading of checkpoints from CPU. User can load checkpoint into CPU module before wrapping with FSDP on only rank 0, and then use FSDP to automatically synchronize the module across ranks.

Follow-up: Incur only one communication when auto-wrapping, because we can detect the root and kick off all communication only for the root.",pytorch
77500,ngimel,pr,2022-05-15T04:10:39Z,check that flip doesn't accept repeating dimensions,"Per title.
Before this PR `flip` throws errors on invalid inputs from ATen implementation itself, and not from error checks happening in prims/refs. 
We should make sure that prims/refs do all the necessary error checking (@mruberry is going to test that by moving reference error inputs testing to call meta implementations instead of real ones).  
In general, most error checking should live in refs, prims meta functions should propagate the necessary properties, but they should assume that they are getting valid inputs. The checks on the inputs should happen in refs, where they can be traced to the necessary guards, or lead to RuntimeErrors during tracing. ",pytorch
77584,0x00b1,pr,2022-05-16T19:07:33Z,Dawson,"Adds the Dawson integral to `torch.special`:

```Python
dawson(input, *, out=None) -> Tensor
```

![image](https://user-images.githubusercontent.com/315821/168666945-f2236a51-5701-4e5e-a48a-e81c9b8f2bb8.png)

- Conceptually equivalent to [`scipy.special.dawsn`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.dawsn.html#scipy.special.dawsn) but the implementation is very different. It will become strictly equivalent once I get around to implementing the Faddeeva function.",pytorch
77585,ngimel,pr,2022-05-16T19:30:55Z,fix complex abs/angle output format,Fixes https://github.com/pytorch/pytorch/issues/77526,pytorch
77610,ngimel,pr,2022-05-16T23:23:10Z,make `to` properly support permutation,"Fixes #77600 (tests to come from #77585)
",pytorch
77634,lezcano,pr,2022-05-17T08:49:37Z,Add linalg.lu_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #77637
* #77636
* #77635
* __->__ #77634

Relanding https://github.com/pytorch/pytorch/pull/72935

Differential Revision: [D36793144](https://our.internmc.facebook.com/intern/diff/D36793144)",pytorch
77635,lezcano,pr,2022-05-17T08:49:42Z,Improve torch.lu_unpack docs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #77637
* #77636
* __->__ #77635
* #77634

Relanding https://github.com/pytorch/pytorch/pull/73803",pytorch
77636,lezcano,pr,2022-05-17T08:49:47Z,Deprecate torch.lu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* #77637
* __->__ #77636
* #77635
* #77634

This PR deprecates `torch.lu` in favor of `torch.linalg.lu_factor`.
A upgrade guide is added to the documentation for `torch.lu`.

Note this PR DOES NOT remove `torch.lu`.

Reland https://github.com/pytorch/pytorch/pull/73804

## BC-breaking note

### Deprecate `torch.lu` in favor of `linalg.lu_factor`

The new operation has a cleaner API and better docs. The update rule is as follows:

Version 1.12.0
```python
LU1, pivots1, info = torch.lu(A, compute_pivots)
LU2, pivots2, info = torch.lu(A, compute_pivots, get_infos=True)
```

Version 1.13.0
```python
LU1, pivots1 = torch.linalg.lu_factor(A, compute_pivots)
LU2, pivots2, info = torch.linalg.lu_factor_ex(A, compute_pivots)
```",pytorch
77637,lezcano,pr,2022-05-17T08:49:53Z,Deprecate torch.lu_solve,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #73878
* #73877
* #74046
* __->__ #77637
* #77636
* #77635
* #77634

Relanding https://github.com/pytorch/pytorch/pull/73806

## BC-breaking note

### Deprecate `torch.lu_solve` in favor of `linalg.lu_solve`

The new operation has a notation consistent with `linalg.solve`, and has an extra parameter `adjoint=False`. The update rule is as follows:

Version 1.12.0
```python
X = torch.lu_solve(B, LU, pivots)
```

Version 1.13.0
```python
X = linalg.lu_solve(LU, pivots, B)
```",pytorch
77657,rohan-varma,pr,2022-05-17T16:13:59Z,[Not for land] Checkpoint fix,"Fixes #ISSUE_NUMBER
",pytorch
77720,rohan-varma,pr,2022-05-18T03:36:49Z,[FSDP] Dont move if on CPU,"After offline discussion, decided that by default moving CPU module to GPU is a bit too risky due to possible OOM during init issue.

Theoretically, we should not OOM because it is required for module that is being wrapped by FSDP to fit into GPU, i.e. during forward. But possibly can be temporary GPU tensors etc allocated during __init___ that break this assumption, it is better for now to allow users a way to init on CPU if needed.

We still warn to use `device_id` for faster init if model is on CPU.",pytorch
77726,rohan-varma,pr,2022-05-18T04:25:17Z,"[FSDP] Warning for shared params, small doc fixes","- Add warning about limited shared param suppport
- Some small doc fixes after combing through the docs ; we should do a more thorough doc lookthrough.",pytorch
77727,rohan-varma,pr,2022-05-18T04:32:27Z,Guard distributed imports,"Move distributed import after dist.is_avail check to fix builds with USE_DISTRIBUTED=0. Although, note that this issue is not caught by any CI at the moment.

Closes https://github.com/pytorch/pytorch/issues/77704",pytorch
77735,lezcano,pr,2022-05-18T11:12:28Z,Use any_type in test_out,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #76828
* #75197
* __->__ #77735

Previously, test_out used `OpDTypes.none` and then it pretty much
implemented `OpDtypes.any_type` inside. This PR changes it to use
`OpDTypes`. This has the advantage that the test now has a dtype, so it
can be used together with decorators that require a `dtype`, such as
`toleranceOverride`.",pytorch
77770,wangkuiyi,pr,2022-05-18T18:47:36Z,Update _symbolic_trace.py,"It seems that the up-to-date behavior has changed.  I got an assertion error instead.

```
>>> f = fx.symbolic_trace(f, concrete_args={""b"":False}); assert f(3, False)==6
>>> f(3, True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/wkyi/Library/Python/3.7/lib/python/site-packages/torch/fx/graph_module.py"", line 616, in wrapped_call
    raise e.with_traceback(None)
AssertionError: b has been specialized to have value False
```

Fixes #ISSUE_NUMBER
",pytorch
77810,rohan-varma,pr,2022-05-19T00:50:19Z,Remove commented out code in test file,,pytorch
77933,byronyi,pr,2022-05-20T02:55:32Z,Fix incorrect decomposition for native_dropout,Quick sanity check: it should be identity function if p=0.,pytorch
77947,rohan-varma,pr,2022-05-20T06:43:05Z,[BE][FSDP] Enable multigpu unittests,"Enables FSDP testing on > 2 GPUs.
",pytorch
77948,ngimel,pr,2022-05-20T06:48:58Z,adds std and var refs and var prim,"Per title
",pytorch
77975,lezcano,pr,2022-05-20T16:29:41Z,More forward AD formulas,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #79381
* __->__ #77975

Highlights of this PR:
- Corrects the forward AD formula of `torch.sgn`.
  - The reason why we can't use `auto_element_wise` for this operations is rather subtle. I left a comment.
  - This, in turn, fixes a problem we had in forward-over-backward for `linalg.svd` and other spectral decompositions (and `norm`, `linalg.norm`, `linalg.matrix_norm`) that were using `torch.abs` (whose derivative is given by `torch.sgn`.
- Implement the formula for a number of missing operations `nansum`, `amax`, `amin`...
- Simplified a few formulas, most notably the forward AD for `div` and the derivative of `norm`, `linalg.norm` and `vector_norm` for `ord=+-inf`.

Differential Revision: [D37282005](https://our.internmc.facebook.com/intern/diff/D37282005)",pytorch
78031,0x00b1,pr,2022-05-21T01:24:24Z,Beta function,"Euler beta function:

```Python
torch.special.beta(input, other, *, out=None) â†’ Tensor
```

`reentrant_gamma` and `reentrant_ln_gamma` implementations (using Stirlingâ€™s approximation) are provided. I started working on this before I realized we were missing a gamma implementation (despite providing incomplete gamma implementations). Uses the coefficients computed by Steve Moshier to replicate SciPyâ€™s implementation. Likewise, it mimics SciPyâ€™s behavior (instead of the behavior in Cephes).",pytorch
78066,ngimel,pr,2022-05-22T23:32:48Z,reduce overhead of get_current_stream,This reduces overhead of `torch.cuda.current_stream()` from ridiculous 8.7 us to still ridiculous 4.4 us. ,pytorch
78083,0x00b1,pr,2022-05-23T14:43:50Z,Euler gamma function,"Fixes #ISSUE_NUMBER
",pytorch
78105,lezcano,pr,2022-05-23T17:46:41Z,Fix derivatives of norm(p=inf),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77975
* #78106
* __->__ #78105

Following up on https://github.com/pytorch/pytorch/pull/51099#discussion_r583323915, we fix these derivatives, as they were incorrect until now.

As described in the note, the better solution would be to use vectorised operations on the preprocessing operation when reducing on CPU. It's not clear how difficult that may be.

Fixes https://github.com/pytorch/pytorch/issues/67517",pytorch
78106,lezcano,pr,2022-05-23T17:46:47Z,Random number generators are not and should not be differentiable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #78106

And that's it.

Now, it's not very clear to me what are the FC BC implications of these changes.",pytorch
78147,eqy,pr,2022-05-24T01:10:20Z,[cuDNN] Don't enforce bitwise exact results in `test_conv_transposed_large_cuda`,"`test_conv_transposed_large` expects bitwise perfect results in fp16 on CUDA, but this behavior isn't guaranteed by cuDNN (e.g., in the case of FFT algos).

This PR just changes the tolerance on the test to account for these cases.

CC @ptrblck @ngimel ",pytorch
78165,ngimel,pr,2022-05-24T06:04:02Z,expose fast get_current_stream,"Expose fast no-frills version of getting raw `cudaStream_t` in python (200 ns instead of 4 us)
",pytorch
78196,0x00b1,pr,2022-05-24T19:35:24Z,Chebyshev polynomial of the first kind,"Adds:

```Python
chebyshev_polynomial_t(input, n, *, out=None) -> Tensor
```

Chebyshev polynomial of the first kind $T_{n}(\text{input})$.

If $n = 0$, $1$ is returned. If $n = 1$, $\text{input}$ is returned. If $n < 6$ or $|\text{input}| > 1$ the recursion:

$$T_{n + 1}(\text{input}) = 2 \times \text{input} \times T_{n}(\text{input}) - T_{n - 1}(\text{input})$$

is evaluated. Otherwise, the explicit trigonometric formula:

$$T_{n}(\text{input}) = \text{cos}(n \times \text{arccos}(x))$$

is evaluated.

## Derivatives

Recommended $k$-derivative formula with respect to $\text{input}$: 

$$2^{-1 + k} \times n \times \Gamma(k) \times C_{-k + n}^{k}(\text{input})$$ 

where $C$ is the Gegenbauer polynomial.

Recommended $k$-derivative formula with respect to $\text{n}$: 

$$\text{arccos}(\text{input})^{k} \times \text{cos}(\frac{k \times \pi}{2} + n \times \text{arccos}(\text{input})).$$

## Example

```Python
x = torch.linspace(-1, 1, 256)

matplotlib.pyplot.plot(x, torch.special.chebyshev_polynomial_t(x, 10))
```

![image](https://user-images.githubusercontent.com/315821/170125525-60415735-4d49-4cbd-9278-26286413f635.png)",pytorch
78257,lezcano,pr,2022-05-25T14:38:01Z,Make l1_loss composite,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #79381
* #77975
* __->__ #78257

Fixing the forward AD for `sgn` in the next PR of this stack uncovered a
number of issues with the derivatives of `l1_loss`. Upon inspection,
`l1_loss` was just implemented as a composite function, but it was not
differentiable. This PR makes it a fully differentiable function.

As a side note, `l1_loss_out` was incorrect in a number of ways. Even
more, it is not exposed to the public as `F.l1_loss` does not accept an
`out=` parameter. As such it is not even tested. I wonder how useful is
to have `out=` variants for loss functions if we don't expose them at
all. Even more, I wonder how useful is to have `_out` variants  for loss
functions, given that their most normal use case is to return just a
real number cc @jbschlosser",pytorch
78278,rohan-varma,pr,2022-05-25T17:14:32Z,Clean prefixes when searching for params / buffers to ignore,"Co-authored with: @awgu 

When `state_dict` has a prefix attached to it, the current logic for ignoring parameters and buffers does not work since it doesn't account for this prefix. To fix this, we make the following changes:

- clean the key if it starts with prefix. Note that all keys may not start with prefix, i.e. if the current module's state_dict_post_hook is running and previous module `state_dict` has already been computed and previous module is on the same level of hierarchy as the current module.
- This prefixing makes it so that it is not current to override child module's ignored params and buffers with the root FSDP instance's (this wouldn't work if child FSDP instances had ignored modules, and root didn't, for example). We fix this by having each parent know about the ignored modules of their children, and computing fully qualified names for ignored params and buffers.
- This means that each for a particular FSDP instance, that instance knows about the names of itself and its children (in fully qualified form) that it needs to ignore. It wouldn't know about parent ignored params and buffers, but it doesn't need to store this data.",pytorch
78293,0x00b1,pr,2022-05-25T19:34:33Z,Chebyshev polynomial of the second kind,"Adds:

```Python
chebyshev_polynomial_u(input, n, *, out=None) -> Tensor
```

Chebyshev polynomial of the second kind $U_{n}(\text{input})$.

If $n = 0$, $1$ is returned. If $n = 1$, $2 \times \text{input}$ is returned. If $n < 6$ or $|\text{input}| > 1$ the recursion:

$$T_{n + 1}(\text{input}) = 2 \times \text{input} \times T_{n}(\text{input}) - T_{n - 1}(\text{input})$$

is evaluated. Otherwise, the explicit trigonometric formula:

$$\frac{\text{sin}((n + 1) \times \text{arccos}(\text{input}))}{\text{sin}(\text{arccos}(\text{input}))}$$

is evaluated.

## Derivatives

Recommended first derivative formula with respect to $\text{input}$: 

$$\frac{(-1 - n)\times U_{-1 + n}(\text{input}) + n \times \text{input} \times U_{n}(x)}{-1 + \text{input}^{2}}.$$

Recommended $k$-derivative formula with respect to $\text{n}$: 

$$\frac{\text{arccos}(\text{input})^{k} \times \text{sin}(\frac{k \times \pi}{2} + (1 + n) \times \text{arccos}(\text{input}))}{\sqrt{1 - \text{input}^{2}}}.$$

## Example

```Python
x = torch.linspace(-1.0, 1.0, 256)

matplotlib.pyplot.plot(x, torch.special.chebyshev_polynomial_u(x, 10))
```

![image](https://user-images.githubusercontent.com/315821/170352780-12af63d3-ce31-4948-8b68-8ecc37c71ac5.png)
",pytorch
78299,eqy,pr,2022-05-25T20:27:52Z,[cuDNN V8 API] (reopen 2) Allow the number of kernels profiled under torch.backends.cudnn.benchmark = True to be limitedCudnnv8 benchmark limit,"Reopen of #77002 to address comments by @malfet 

CC @ngimel @ptrblck ",pytorch
78304,0x00b1,pr,2022-05-25T21:23:52Z,Orthogonal Polynomials,"```Python
chebyshev_polynomial_v(input, n, *, out=None) -> Tensor
```

Chebyshev polynomial of the third kind $V_{n}(\text{input})$.

```Python
chebyshev_polynomial_w(input, n, *, out=None) -> Tensor
```

Chebyshev polynomial of the fourth kind $W_{n}(\text{input})$.

```Python
legendre_polynomial_p(input, n, *, out=None) -> Tensor
```

Legendre polynomial $P_{n}(\text{input})$.

```Python
shifted_chebyshev_polynomial_t(input, n, *, out=None) -> Tensor
```

Shifted Chebyshev polynomial of the first kind $T_{n}^{\ast}(\text{input})$.

```Python
shifted_chebyshev_polynomial_u(input, n, *, out=None) -> Tensor
```

Shifted Chebyshev polynomial of the second kind $U_{n}^{\ast}(\text{input})$.

```Python
shifted_chebyshev_polynomial_v(input, n, *, out=None) -> Tensor
```

Shifted Chebyshev polynomial of the third kind $V_{n}^{\ast}(\text{input})$.

```Python
shifted_chebyshev_polynomial_w(input, n, *, out=None) -> Tensor
```

Shifted Chebyshev polynomial of the fourth kind $W_{n}^{\ast}(\text{input})$.",pytorch
78350,lezcano,pr,2022-05-26T13:49:34Z,Reference for linalg.vector_norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81113
* #78616
* #78615
* __->__ #78350
* #80219



cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @ezyang @ngimel @peterbell10",pytorch
78352,0x00b1,pr,2022-05-26T14:02:22Z,Physicistâ€™s Hermite polynomial,"Adds:

```Python
hermite_polynomial_h(input, n, *, out=None) -> Tensor
```
Physicistâ€™s Hermite polynomial $H_{n}(\text{input})$.

If $n = 0$, $1$ is returned. If $n = 1$, $\text{input}$ is returned. Otherwise, the recursion:

$$H_{n + 1}(\text{input}) = 2 \times \text{input} \times H_{n}(\text{input}) - H_{n - 1}(\text{input})$$

is evaluated.

## Derivatives

Recommended $k$-derivative formula with respect to $\text{input}$: 

$$\frac{d^{k}}{d \times \text{input}^{k}} H_{n}^{(k)} = 2^{k} \times \frac{n!}{(n - k)!}H_{n - k}(\text{input})$$",pytorch
78357,0x00b1,pr,2022-05-26T15:14:12Z,Probabilistâ€™s Hermite polynomial,"Adds:

```Python
hermite_polynomial_he(input, n, *, out=None) -> Tensor
```
Physicistâ€™s Hermite polynomial $He_{n}(\text{input})$.

If $n = 0$, $1$ is returned. If $n = 1$, $\text{input}$ is returned. Otherwise, the recursion:

$$He_{n + 1}(\text{input}) = 2 \times \text{input} \times He_{n}(\text{input}) - He_{n - 1}(\text{input})$$

is evaluated.

## Derivatives

Recommended $k$-derivative formula with respect to $\text{input}$: 

$$\frac{d^{k}}{d \times \text{input}^{k}} He_{n}^{(k)} = \frac{n!}{(n - k)!}He_{n - k}(\text{input}).$$",pytorch
78366,0x00b1,pr,2022-05-26T16:08:46Z,Laguerre polynomial,"Adds:

```Python
laguerre_polynomial_l(input, n, *, out=None) -> Tensor
```

Laguerre polynomial $L_{n}(\text{input})$.

## Derivatives

Recommended $k$-derivative formula with respect to $\text{input}$: 

$$\frac{d^{k}}{d \times \text{input}^{k}} L_{n}(\text{input}) = -1^{k} \times L_{-k + n}^{k}(\text{input})$$

where $L_{n}^{\alpha}$ is the associated Laguerre polynomial.",pytorch
78372,0x00b1,pr,2022-05-26T17:16:50Z,Chebyshev polynomial of the third kind,,pytorch
78373,0x00b1,pr,2022-05-26T17:22:40Z,Chebyshev polynomial of the fourth kind,,pytorch
78379,0x00b1,pr,2022-05-26T19:30:38Z,Shifted Chebyshev polynomial of the first kind,,pytorch
78393,0x00b1,pr,2022-05-26T21:32:08Z,Shifted Chebyshev polynomial of the second kind,,pytorch
78400,rohan-varma,pr,2022-05-26T22:33:33Z,[FSDP][BE][Docs] Improve auto wrap policy doc,"Closes https://github.com/pytorch/pytorch/issues/78399

- Add expected type of callable
- Clarify what the policy function should return, and how its used (i.e. what's done depends on `recurse` flag). 
",pytorch
78423,lezcano,pr,2022-05-27T13:47:51Z,Add complex_to_float option in ReductionOpInfo,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78350
* #78424
* __->__ #78423

As per title",pytorch
78424,lezcano,pr,2022-05-27T13:47:55Z,Make linalg.vector_norm a proper reduction,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78350
* __->__ #78424
* #78423

",pytorch
78426,0x00b1,pr,2022-05-27T18:24:57Z,special.rst cleanup,"A complete rewrite of the Special Functions documentation that:

* organizes functions into relevant sections
* provides background information
* provides application information
* includes references to special functions outside of `torch.special` (e.g., piecewise, trigonometric, hyperbolic, etc.)",pytorch
78432,ssnl,pr,2022-05-27T20:32:44Z,add type annotation to distributions.kl_divergence,"
Fixes #78431
",pytorch
78433,0x00b1,pr,2022-05-27T20:47:14Z,Bessel J0,,pytorch
78437,eqy,pr,2022-05-27T22:15:02Z,[cuDNN][TF32] Threshold adjustments for TF32 on `>=sm80`,"CC @ptrblck @mcarilli 

Change to transformer multilayer test can potentially be swapped in favor of an rtol change? (see also: #75612).",pytorch
78438,eqy,pr,2022-05-27T22:17:23Z,[TF32] Fix typo in tf32 wrapper function,,pytorch
78451,0x00b1,pr,2022-05-28T15:02:04Z,Bessel functions,"Adds:

```Python
bessel_j0(input, *, out=None) -> Tensor
```

Bessel function of the first kind of order $0$, $J_{0}(\text{input})$.

```Python
bessel_j1(input, *, out=None) -> Tensor
```

Bessel function of the first kind of order $1$, $J_{1}(\text{input})$.

```Python
bessel_j0(input, *, out=None) -> Tensor
```

Bessel function of the second kind of order $0$, $Y_{0}(\text{input})$.

```Python
bessel_j1(input, *, out=None) -> Tensor
```

Bessel function of the second kind of order $1$, $Y_{1}(\text{input})$.

```Python
modified_bessel_i0(input, *, out=None) -> Tensor
```

Modified Bessel function of the first kind of order $0$, $I_{0}(\text{input})$.

```Python
modified_bessel_i1(input, *, out=None) -> Tensor
```

Modified Bessel function of the first kind of order $1$, $I_{1}(\text{input})$.

```Python
modified_bessel_k0(input, *, out=None) -> Tensor
```

Modified Bessel function of the second kind of order $0$, $K_{0}(\text{input})$.

```Python
modified_bessel_k1(input, *, out=None) -> Tensor
```

Modified Bessel function of the second kind of order $1$, $K_{1}(\text{input})$.",pytorch
78456,0x00b1,pr,2022-05-28T17:03:09Z,Bessel and Related Functions,"Adds:

```Python
airy_ai(input, *, out=None) -> Tensor
```

Airy function $\text{Ai}(\text{input})$.

```Python
airy_bi(input, *, out=None) -> Tensor
```

Airy function $\text{Bi}(\text{input})$.

```Python
airy_derivative_ai(input, *, out=None) -> Tensor
```

Derivative of Airy function $\text{Ai}(\text{input})$.

```Python
airy_derivative_bi(input, *, out=None) -> Tensor
```

Derivative of Airy function $\text{Bi}(\text{input})$.

```Python
scaled_modified_bessel_i0(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the first kind of order $0$, $\text{exp}(\text{input})\text{I}_{0}(\text{input})$.

```Python
scaled_modified_bessel_i1(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the first kind of order $1$, $\text{exp}(\text{input})\text{I}_{1}(\text{input})$.

```Python
scaled_modified_bessel_k0(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the second kind of order $0$, $\text{exp}(\text{input})\text{K}_{0}(\text{input})$.

```Python
scaled_modified_bessel_k1(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the second kind of order $1$, $\text{exp}(\text{input})\text{K}_{1}(\text{input})$.

```Python
spherical_bessel_j0(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $0$, $\text{j}_{0}(\text{input})$.

```Python
spherical_bessel_j1(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $1$, $\text{j}_{1}(\text{input})$.

```Python
spherical_bessel_y0(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $0$, $\text{y}_{0}(\text{input})$.

```Python
spherical_bessel_y1(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $1$, $\text{y}_{1}(\text{input})$.",pytorch
78491,vfdev-5,pr,2022-05-30T15:38:59Z,Added decomposition for nll_loss_forward,"Related to https://github.com/pytorch/functorch/issues/812

cc @Chillee 
",pytorch
78502,0x00b1,pr,2022-05-30T20:51:36Z,Gamma and Related Functions,"Adds:

```Python
gamma(input, *, out=None) -> Tensor
```

Gamma function $\Gamma(\text{input})$.",pytorch
78615,lezcano,pr,2022-06-01T12:04:52Z,Fix handling of device,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81113
* #78616
* __->__ #78615
* #78350
* #80219

Removes an unnecessary auxiliary function (we had already implemented
it), uses DeviceLikeType to denote str or dtype, and adds `is_cpu` and
`is_cuda` helper functions

cc @ezyang @mruberry @ngimel @Lezcano @peterbell10",pytorch
78616,lezcano,pr,2022-06-01T12:04:58Z,"Add prim.svd, refs.linalg.svd, and refs.linalg.svdvals","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81113
* __->__ #78616
* #78615
* #78350
* #80219

This is the first prim / ref added that has multiple returns.
There is an issue with `out_wrapper_multi` as currently implemented
(left a note). It assumes that the API is `svd(X, U=U, S=S, Vh=Vh)`,
when it's actually `svd(X, out=(U, S, Vh))`.

Even more, if we want to model PyTorch exactly, it should return a
`torch.return_types.svd`, rather than a `Tuple`.

There is an issue with
As per title",pytorch
78617,lezcano,pr,2022-06-01T13:17:26Z,Add randomness case to the autograd notes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78106
* __->__ #78617

I also took this chance to clean a bit the sphinx formatting and
reworded a few minor things.",pytorch
78702,rohan-varma,pr,2022-06-02T05:17:21Z,[Checkpoint] Test file for checkpoint_wrapper,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78752
* #78704
* __->__ #78702

As we make additional changes and don't couple checkpoint_wrapper with
FSDP, would be good to have standalone test file for it.

Currently, we port the only non FSDP specific unittest that exists, and will add more as development continues.",pytorch
78704,rohan-varma,pr,2022-06-02T06:05:32Z,Add apply_activation_checkpointing helper function,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78854
* #78752
* __->__ #78704


Adds `apply_activation_checkpointing` helper function, which will allow users to easily checkpoint multiple submodules within a module depending on some condition. 

The function takes in `model`, `checkpoint_wrap_fn` which indicates the checkpoint_wrapper config, and a lambda indicating whether a module should be wrapped or not. It does this wrapper and updates the model in place, i.e.

`Sequential(Linear, Linear)` would become `Sequential(wrapped(Linear), wrapped(Linear))` in place. 

This is useful for several uses cases we've been seeing that have been trying to apply activation checkpointing, where we've had to hack around the module hierarchy via setattr/getattr which can easily lead to issues if not done properly.",pytorch
78707,vfdev-5,pr,2022-06-02T07:00:04Z,Update extending.rst,"Follow-up fix for https://github.com/pytorch/pytorch/pull/78073 : https://github.com/pytorch/pytorch/pull/78073#discussion_r887621219
",pytorch
78747,syed-ahmed,pr,2022-06-02T18:29:53Z,"[primTorch] Implements refs for gcd, lcm and remainder","This PR implements the references for gcd, lcm and remainder. Additionally, `gcd` is added as a prim, since we currently don't have a while loop construct. ",pytorch
78752,rohan-varma,pr,2022-06-02T19:36:29Z,Fix non-reentrant hooks based checkpointing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78854
* __->__ #78752

Fixes the non-reentrant hooks based checkpointing to actually save memory. The issue was that `storage` was a list of autograd saved tensors and we weren't clearing this list out as tensors were accessed, so all activations would remain in memory. Now at the end of the layer's backwards pass, activations will be discarded as expected. 

Adding unittests to ensure:
- Memory savings for a basic model compared to no checkpointing
- Same or better memory savings when compared with the reentrant autograd based hooks checkpointing

Also, this means we can enable non-reentrant based checkpointing in `CheckpointWrapper`, will also add unittests for that.",pytorch
78797,rohan-varma,pr,2022-06-03T05:35:15Z,[BE] Fix distributed_test.py flakiness,"There are several recent distributed_test / DDP flaky tests: https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22oncall%3A+distributed%22+label%3A%22module%3A+flaky-tests%22+

From local experimentation, we see segfaults such as the error in https://github.com/pytorch/pytorch/issues/78684 quite a bit locally when running with NCCL. I switched the test to run with gloo, and these issues appeared gone. 

I then switched back to nccl but turned off async_errror_handling (some of the stacktrace had ncclCommWatchdog + workCleanupLoop in the trace, so I thought it might be an issue / race between the two or the like). Turning off async_error_handling also seems to alleviate the tests. If this indeed works, we should probably land this PR as we are losing a lot of CI signal, and prioritize to understand why async error handling / comm watchdog interaction might be causing these segfaults.

Closes https://github.com/pytorch/pytorch/issues/78768 https://github.com/pytorch/pytorch/issues/78767 https://github.com/pytorch/pytorch/issues/78748 https://github.com/pytorch/pytorch/issues/78685 https://github.com/pytorch/pytorch/issues/78684 https://github.com/pytorch/pytorch/issues/78641",pytorch
78801,lezcano,pr,2022-06-03T11:53:42Z,Make allocator check C10_UNLIKELY,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78801

This popped up while having a look at posible causes for https://github.com/pytorch/pytorch/issues/78800",pytorch
78854,rohan-varma,pr,2022-06-03T22:20:58Z,[Checkpoint wrapper] Forward attributes to wrapped module,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78854
* #78752

The nn.Module wrapped with checkpoint may have certain attributes that are set / created by user and aren't part of checkpoint wrapper. Thus if we wrap a module, we should forward calls for these attributes to the wrapped module, similar to what FSDP does.",pytorch
78856,rohan-varma,pr,2022-06-03T22:30:46Z,[BE] Fix FSDP flaky checkpoint test,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78856

Fixes `test_fsdp_checkpointing` flakiness by not relying on profiler to validate CPU offload, which was the source of flakiness. Instead, we patch `save_on_cpu` to validate it was called, which is a lot more robust.

Closes https://github.com/pytorch/pytorch/issues/70368
Closes https://github.com/pytorch/pytorch/issues/71009
Closes https://github.com/pytorch/pytorch/issues/70368",pytorch
78865,rohan-varma,pr,2022-06-04T01:01:04Z,Stress run distributed test,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78865

",pytorch
78881,ngimel,pr,2022-06-04T19:19:39Z,Don't check overflow for scalar arg in comparison ops,"That check was potentially synchronizing (people were running into synchronization in real workloads) and mostly unneeded. 
Type promotion takes care of comparing to values that cannot be safely converted to the type of other argument. 
This now works for `comp(int_tensor, float('inf'))` as expected. For `comp(uint8_tensor, large_int)` it silently wraps integer to uint8 whereas it would error out before, but this also happens with other arithmetic ops.
Also adds reference np implementation to comparison op OpInfos, and additional reference inputs to test newly enabled behavior. 

Fixes #76805",pytorch
78898,0x00b1,pr,2022-06-05T21:30:51Z,torch.special.scaled_modified_bessel_i0,"```Python
scaled_modified_bessel_i0(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the first kind of order $0$.",pytorch
78899,0x00b1,pr,2022-06-05T21:39:42Z,torch.special.scaled_modified_bessel_i1,"```Python
scaled_modified_bessel_i1(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the first kind of order $1$.",pytorch
78900,0x00b1,pr,2022-06-05T22:33:02Z,torch.special.scaled_modified_bessel_k0,"```Python
scaled_modified_bessel_k0(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the second kind of order $0$.",pytorch
78901,0x00b1,pr,2022-06-05T22:58:54Z,torch.special.scaled_modified_bessel_k1,"```Python
scaled_modified_bessel_k1(input, *, out=None) -> Tensor
```

Scaled modified Bessel function of the second kind of order $1$.",pytorch
78902,0x00b1,pr,2022-06-05T23:28:17Z,torch.special.airy_ai,"```Python
airy_ai(input, *, out=None) -> Tensor
```

Airy function $\text{Ai}\left(\text{input}\right)$.",pytorch
78904,0x00b1,pr,2022-06-06T00:23:44Z,torch.special.gamma,"```Python
gamma(input, *, out=None) -> Tensor
```

Gamma function $\Gamma\left(\text{input}\right)$.",pytorch
78910,0x00b1,pr,2022-06-06T01:40:56Z,c10 mathematical constants,Adds useful c10 mathematical constants,pytorch
78911,0x00b1,pr,2022-06-06T02:06:48Z,Update special.rst,"Fixes #ISSUE_NUMBER
",pytorch
78912,0x00b1,pr,2022-06-06T02:46:50Z,torch.special.spherical_bessel_j0,"```Python
spherical_bessel_j0(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $0$.",pytorch
78913,0x00b1,pr,2022-06-06T02:54:52Z,torch.special.spherical_bessel_j1,"```Python
spherical_bessel_j1(input, *, out=None) -> Tensor
```

Spherical Bessel function of the first kind of order $1$.",pytorch
78949,eqy,pr,2022-06-06T17:37:56Z,[CUBLAS][TF32] Fix broken docstring for `set_float32_matmul_precision`,CC @ngimel @ptrblck ,pytorch
79054,lezcano,pr,2022-06-07T20:23:50Z,Port linalg_qr to structured,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #79054

This PR simplifies the logic of `linalg.qr` using structured kernels. I
also took this chance and merged a few `copy_` operations with other
ops.

This PR removes a the previous magma implementation as is never faster
than that of cusolver and it's rather buggy. This has the side-effect
that now `qr` is not supported in Rocm. Ivan confirmed that this is
fine, given how incredibly slow was QR on Rocm anyway (we were marking
some tests as slow because of this...).

This PR also corrects the dispatch in geqrf. Before, if we called it
with a matrix for which `input.size(-2) <= 256 && batchCount(input) >= std::max<int64_t>(2, input.size(-2) / 16)` is false, and we have cublas but not cusolver, we would end up calling magma rather than cublas. This is not what the heuristic suggested.
Probaly we should benchmark these heuristics again, but that's beyond the scope of this PR.

Note. It looks like `torch.geqrf` maybe broken in MAGMA as per the
previous comment in `linalg_qr_helper_magma`. @IvanYashchuk wdyt?",pytorch
79060,eqy,pr,2022-06-07T21:10:02Z,[DDP] Fix broadcast for channels-last tensors,"#79043

CC @pritamdamania87 @ptrblck ",pytorch
79069,eqy,pr,2022-06-07T22:04:51Z,[DDP] Fix broadcast for channels-last tensors (cherry-picked from #79060),"#79043

CC @pritamdamania87 
",pytorch
79071,eqy,pr,2022-06-07T22:11:22Z,[DDP] Cherrypick support other memory formats #79060,"#79043

CC @pritamdamania87 ",pytorch
79072,lezcano,pr,2022-06-07T22:19:12Z,Port linalg_eigh and linalg_eigvalsh to structured,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #79072

This follows the structure of linalg.svd.",pytorch
79080,0x00b1,pr,2022-06-07T22:56:26Z,Exponential integrals,"```Python
exponential_integral_e1(input, *, out=None) -> Tensor
```

Exponential integral $\text{Ei}\left(\text{input}\right)$.

```Python
exponential_integral_ei(input, *, out=None) -> Tensor
```

Exponential integral $\text{E1}\left(\text{input}\right)$.",pytorch
79109,zhuzilin,pr,2022-06-08T05:21:17Z,Checkpoint distributed ShardedTensor when coordinator has no shard,"Relates to #79016
",pytorch
79121,zhuzilin,pr,2022-06-08T13:10:00Z,Support Tensor reads from ShardedTensor checkpoint,"Relates to #79016
",pytorch
79127,0x00b1,pr,2022-06-08T15:13:26Z,Alphabetize special.rst,I will eventually organize special.rst but this patch will make it easier to merge in-progress patches.,pytorch
79157,0x00b1,pr,2022-06-08T20:31:47Z,New Distributions (WIP),"A substantial update to the `torch.distributions` module that adds the following probability distributions:

```Python
Arcsine(a=0.0, b=1.0, validate_args=None)
```

```Python
Bates(n, a=0.0, b=1.0, validate_args=None)
```

```Python
Benford(b, validate_args=None)
```

```Python
Benini(a, b, s, validate_args=None)
```

```Python
BenktanderTypeI(a, b, validate_args=None)
```

```Python
BenktanderTypeII(a, b, validate_args=None)
```

```Python
BetaBinomial(n, a, b, validate_args=None)
```

```Python
BetaNegativeBinomial(r, a, b, validate_args=None)
```

```Python
BetaPrime(a, b, validate_args=None)
```

```Python
Borel(m, validate_args=None)
```

```Python
BorelTanner(m, k, validate_args=None)
```

```Python
Burr(c, k, validate_args=None)
```

```Python
Chi(validate_args=None)
```

```Python
Copula(validate_args=None)
```

```Python
Coxian(validate_args=None)
```

```Python
Dagum(validate_args=None)
```

```Python
Davis(validate_args=None)
```

```Python
Erlang(validate_args=None)
```

```Python
ExponentialPower(validate_args=None)
```

```Python
F(validate_args=None)
```

```Python
FisherNoncentralHypergeometric(validate_args=None)
```

```Python
FisherZ(validate_args=None)
```

```Python
Frechet(validate_args=None)
```

```Python
GeneralizedExtremeValue(validate_args=None)
```

```Python
GeneralizedHyperbolic(validate_args=None)
```

```Python
GeometricPoisson(validate_args=None)
```

```Python
Gompertz(validate_args=None)
```

```Python
HotellingTSquared(validate_args=None)
```

```Python
HyperbolicSecant(validate_args=None)
```

```Python
Hyperexponential(validate_args=None)
```

```Python
Hypergeometric(validate_args=None)
```

```Python
InverseChiSquared(validate_args=None)
```

```Python
InverseGamma(validate_args=None)
```

```Python
IrwinHall(validate_args=None)
```

```Python
Landau(validate_args=None)
```

```Python
Levy(validate_args=None)
```

```Python
Logarithmic(validate_args=None)
```

```Python
Logistic(loc, scale, validate_args=None)
```

```Python
LogisticNormal(validate_args=None)
```

```Python
LogLogistic(validate_args=None)
```

```Python
MarchenkoPastur(validate_args=None)
```

```Python
MaxwellBoltzmann(validate_args=None)
```

```Python
MultivariateHypergeometric(validate_args=None)
```

```Python
MultivariateLaplace(validate_args=None)
```

```Python
MultivariateT(validate_args=None)
```

```Python
Nakagami(validate_args=None)
```

```Python
NegativeMultinomial(validate_args=None)
```

```Python
NoncentralBeta(validate_args=None)
```

```Python
NoncentralChiSquared(validate_args=None)
```

```Python
NoncentralF(validate_args=None)
```

```Python
NoncentralT(validate_args=None)
```

```Python
NormalExponentialGamma(validate_args=None)
```

```Python
NormalInverseGaussian(validate_args=None)
```

```Python
PERT(validate_args=None)
```

```Python
QExponential(validate_args=None)
```

```Python
QGaussian(validate_args=None)
```

```Python
QWeibull(validate_args=None)
```

```Python
Rayleigh(validate_args=None)
```

```Python
Skellam(validate_args=None)
```

```Python
SkewNormal(validate_args=None)
```

```Python
Stable(validate_args=None)
```

```Python
TracyWidom(validate_args=None)
```

```Python
Triangular(validate_args=None)
```

```Python
TukeyLambda(validate_args=None)
```

```Python
VarianceGamma(validate_args=None)
```

```Python
Voigt(validate_args=None)
```

```Python
Wakeby(validate_args=None)
```

```Python
WalleniusNoncentralHypergeometric(validate_args=None)
```

```Python
WignerSemicircle(validate_args=None)
```

```Python
YuleSimon(validate_args=None)
```

```Python
Zipf(validate_args=None)
```",pytorch
79300,lezcano,pr,2022-06-10T19:21:38Z,Port cholesky to structured kernels,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #79300

Yeah.",pytorch
79381,lezcano,pr,2022-06-12T15:11:47Z,Fix backward of binary_cross_entropy_with_logits,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #79381
* #77975

The previous PR in this stack uncovered an error in the forward over
backward for this function.

In this PR, we fix this error and we also fix the gradgrad
implementation (and make it more stable and faster using `logsigmoid`).
We also move the double backward for this function to `FunctoinsManual`
as there's no reason for it to be in `native_functions`

Fixes https://github.com/pytorch/pytorch/issues/79609

Differential Revision: [D37282004](https://our.internmc.facebook.com/intern/diff/D37282004)",pytorch
79400,zhuzilin,pr,2022-06-13T05:35:08Z,Clone tensor to write in ShardedTensor checkpoint,"The `torch.save` api will save the origin tensor of a view, which will results in saving a much larger checkpoint when parameters are fused, e.g. in torchrec.

Relates to #79016
",pytorch
79455,ngimel,pr,2022-06-13T21:17:44Z,cherry-pick,Cherry pick of #79401,pytorch
79486,lezcano,pr,2022-06-14T00:19:54Z,Make linalg_det function structured,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80074
* #80073
* #79838
* #79742
* #79487
* __->__ #79486

As per title",pytorch
79487,lezcano,pr,2022-06-14T00:20:00Z,Add forward AD for linalg.det and simplify its backward,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80217
* #80074
* #80073
* #79838
* #79742
* __->__ #79487

This PR is in preparation for implementing `logdet` and `slogdet` as
structured kernels + implementing them with more efficient derivatives

We implement forward AD for det. We also simplify the implementation of
the backward, and leave a note on how to implement it properly for
singular matrices. We leave thad for future work.

Note (by looking at the OpInfo) that the current implementation passes
the same tests as the one before. We skip the forward-over-backward in
the singular case, as that one was not working in the gradgrad case
either.",pytorch
79490,rohan-varma,pr,2022-06-14T01:20:26Z,[v1.12.0] Fix non-reentrant hooks based checkpointing,"Link to landed master PR: https://github.com/pytorch/pytorch/pull/78752

Original commit description:

Fixes the non-reentrant hooks based checkpointing to actually save memory. The issue was that `storage` was a list of autograd saved tensors and we weren't clearing this list out as tensors were accessed, so all activations would remain in memory. Now at the end of the layer's backwards pass, activations will be discarded as expected. 

Adding unittests to ensure:
- Memory savings for a basic model compared to no checkpointing
- Same or better memory savings when compared with the reentrant autograd based hooks checkpointing

Also, this means we can enable non-reentrant based checkpointing in `CheckpointWrapper`, will also add unittests for that.",pytorch
79537,eqy,pr,2022-06-14T17:13:30Z,[CUBLAS][TF32][CUDNN] Update numerical_accuracy.rst,CC @mruberry @ptrblck ,pytorch
79538,eqy,pr,2022-06-14T17:21:03Z,Cherry pick tf32docs (#79537),,pytorch
79539,eqy,pr,2022-06-14T17:22:18Z,Cherry pick tf32docs (#79537),,pytorch
79551,powderluv,pr,2022-06-14T19:12:26Z,reorder cpuinfo and clog deps in TorchConfig.cmake,"cpuinfo has some symbols that need to be resolved with clog.
```

Static builds fail without this fix with this error:
api.c:(.text+0xc2): undefined reference to `clog_vlog_fatal'
init.c:(.text+0x19d1): undefined reference to `clog_vlog_error'
processors.c:(.text+0x551): undefined reference to `clog_vlog_error'
smallfile.c:(.text+0x172): undefined reference to `clog_vlog_error'


```",pytorch
79579,eqy,pr,2022-06-14T23:49:19Z,"[AMP] Use generic autocast in example, specify dtype",CC @mruberry @ptrblck ,pytorch
79642,rohan-varma,pr,2022-06-15T21:09:41Z,[BE][FSDP] Enhance summon full params test,"Summary:
To resolve https://github.com/pytorch/pytorch/issues/74166, ensure
that when main FSDP API is CPU offloading, summon_full_params exit always
results in FSDP params residing on CPUs, not GPUs, as expected. Ensuring that
we call `_free_full_params_and_use_local_shard` in summon_full_params does
this.

Test Plan: CI

Differential Revision: D37184499

",pytorch
79674,rohan-varma,pr,2022-06-16T03:17:01Z,[BE] Add FSDP helper classes to public documentation,"Adds misc classes such as mixed precision, CPU offload, ShardingStrategy to docs",pytorch
79710,crcrpar,pr,2022-06-16T15:23:35Z,`__launch_bounds__` for `torch.mode` with CUDA 11.7,"This is a temporary fix for `TestReductionsCUDA.test_mode_large_cuda` which fails with CUDA 11.7 due to the following:

```
Traceback (most recent call last):
  File ""/opt/pytorch/pytorch/torch/testing/_internal/common_utils.py"", line 1805, in wrapper
    method(*args, **kwargs)
  File ""/opt/pytorch/pytorch/torch/testing/_internal/common_utils.py"", line 1805, in wrapper
    method(*args, **kwargs)
  File ""/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py"", line 390, in instantiated_test
    raise rte
  File ""/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py"", line 377, in instantiated_test
    result = test(self, **param_kwargs)
  File ""/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py"", line 943, in only_fn
    return fn(slf, *args, **kwargs)
  File ""test_reductions.py"", line 891, in test_mode_large
    testset_for_shape((10, 2048), 10)
  File ""test_reductions.py"", line 883, in testset_for_shape
    self._test_mode_intervals(shape, [(i, d - i)], device)
  File ""test_reductions.py"", line 870, in _test_mode_intervals
    values, indices = torch.mode(x, -1, False)
RuntimeError: CUDA error: too many resources requested for launch
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
```

cc @ptrblck ",pytorch
79722,crcrpar,pr,2022-06-16T17:20:54Z,[docs] Include `batch_isend_irecv` and `P2POp` of `torch.distributed`,"It'd be helpful for the docs to render `torch.distributed.batch_isend_irecv` and `torch.distributed.P2POp`.

cc @ptrblck ",pytorch
79729,r-barnes,pr,2022-06-16T18:39:17Z,Readability in fbcode/caffe2/aten/src/ATen/native/cuda/Loss.cu,"Summary:
Range-check asserts should have the form
```
lower <= variable && variable <= upper
```

Test Plan: Sandcastle

Reviewed By: suphoff

Differential Revision: D37215937

",pytorch
79742,lezcano,pr,2022-06-16T20:50:38Z,"Make slogdet, linalg.sloget and logdet support metatensors","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80217
* #80074
* #80073
* #79838
* __->__ #79742

This PR also adds complex support for logdet, and makes all these
functions support out= and be composite depending on one function. We
also extend the support of `logdet` to complex numbers and improve the
docs of all these functions.

We also use `linalg_lu_factor_ex` in these functions, so we remove the
synchronisation present before.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @ezyang @eellison @bdhirsh @soumith @ngimel @peterbell10",pytorch
79743,lezcano,pr,2022-06-16T20:50:43Z,Implement forward AD for slogdet-related operations and fix their complex derivative,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80074
* #80073
* #79838
* __->__ #79743
* #79742
* #79487
* #79486

The derivative for complex inputs of slogdet was incorrect as it said
that the output `sign` was not differentiable. Same happened with
`logdet` which didn't even support complex inputs. Now these derivatives
are all correct and all the gradcheck tests pass.

We also add forward AD formulas for these functions.

We take this chance to announce in the docs that the derivatives of
these functions are much more better behaved than those from
`linalg.det`, so one should probably always use these functions over
`linalg.det` unless they have very good reasons not to do so.",pytorch
79758,orionr,pr,2022-06-16T23:10:28Z,Adjust wording for consistency,"Requested by some of our internal review. @svekars thoughts? Thanks.

",pytorch
79786,lezcano,pr,2022-06-17T14:40:22Z,Remove python implementation for eigh meta,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #79786

Following https://github.com/pytorch/pytorch/pull/79072#discussion_r898210048",pytorch
79803,lezcano,pr,2022-06-17T19:44:04Z,Make l1_loss composite,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Fixing the forward AD for `sgn` in the next PR of this stack uncovered a
number of issues with the derivatives of `l1_loss`. Upon inspection,
`l1_loss` was just implemented as a composite function, but it was not
differentiable. This PR makes it a fully differentiable function.

As a side note, `l1_loss_out` was incorrect in a number of ways. Even
more, it is not exposed to the public as `F.l1_loss` does not accept an
`out=` parameter. As such it is not even tested. I wonder how useful is
to have `out=` variants for loss functions if we don't expose them at
all. Even more, I wonder how useful is to have `_out` variants  for loss
functions, given that their most normal use case is to return just a
real number cc jbschlosser",pytorch
79804,lezcano,pr,2022-06-17T19:44:43Z,Make l1_loss composite,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #79381
* #77975
* __->__ #79804

Reland of https://github.com/pytorch/pytorch/pull/78257

Differential Revision: [D37281956](https://our.internmc.facebook.com/intern/diff/D37281956)",pytorch
79823,eqy,pr,2022-06-17T22:59:10Z,[PrimTorch] Adds bfloat16 reference,"Naive attempt at `bfloat16` which breaks, first error is about `out` working when it isn't expected:

`python test/test_ops.py -f -k test_out__refs_bfloat16_cpu_float32`

CC @mruberry 
",pytorch
79830,rohan-varma,pr,2022-06-18T01:27:47Z,[CheckpointWrapper] Replace generic mod prefix,,pytorch
79836,janosh,pr,2022-06-18T08:55:49Z,Fix `SWALR` doc string,In `torch/optim/swa_utils.py`.,pytorch
79838,lezcano,pr,2022-06-18T11:11:00Z,Improve heuristics for linalg_lu_solve when B is a matrix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80217
* #80074
* #80073
* __->__ #79838
* #79742

When linalg_lu_solve was added in
https://github.com/pytorch/pytorch/pull/72935 I made the big mistake of
assuming that the choice of backend would not depend on number of
columns of B. This turned out to be false by a large margin.

This PR amends this and provides a heuristic that takes the number of
columns of B into account. The heuristic is not simple and it was
crafted by hand, but as the results show, it is effective.

@xwang233 the cusolver team should look into this one, as I was able to
outperform both cublas and cusolvers algorithms by using triangular
solves...

The benchmarks for the heuristics are here: https://github.com/pytorch/pytorch/pull/79838#issuecomment-1163802792


cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano",pytorch
79858,ngimel,pr,2022-06-20T05:22:24Z,make refs executor handle kwargs,"Mostly fixes #78923
I had to disable function patching in fx for functions with kwonly args, see https://github.com/pytorch/pytorch/compare/ngimel/make_fx_fix?expand=1#diff-090b22122be0779cd14afd2ebaf20d1e7c0bfe837e9eefa1d84e7521bb1defc6R446, cc @jamesr66a 
But it looks like it was doing weird things anyway - it was patching signature of wrapped function with arbitrary local vars from wrapper, that can't be right, but I don't know what the intent there is.
A lot of functions now fail with nvfuser executor, and some still fail with aten, although with the different errors than before. 
Edit: undid the change to _symbolic_script.py, turns out inspect.unwrapping function is not needed, and fx never sees kwargs. 
cc @IvanYashchuk, @Chillee 
",pytorch
79941,lezcano,pr,2022-06-21T14:12:41Z,Add support for multiple inputs to out_wrapper and strict dtype checking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78616
* #78615
* #78350
* #80219
* #80443
* __->__ #79941
* #80516

When a function returns multiple parameters in PyTorch, the `out`
parameter takes a tuple of tensors (see `linalg.svd` for example).
The current implementation in `out_wrapper_multi` modelled this wrong,
as it assumed that it would take a number of different named
parameters.

This PR implements the correct behaviour in `out_wrapper`. As a small
side-effect, we now need to call `@out_wrapper()` when the output is
just one tensor.

This PR also implements an additional optional parameter that checks
whether the dtype of the given `out` is exactly the dtype that the meta
function requires. This is the behaviour that we currently have in
PyTorch, and this check is necessary in eager when we call with these
tensors into external libraries.

We also make the functions with several outputs return a namedtuple,
similar to what we do in PyTorch.",pytorch
79973,ngimel,pr,2022-06-21T20:43:01Z,small cleanup of executor,per title,pytorch
79988,eqy,pr,2022-06-21T22:35:06Z,Band-aid fix for deterministic `index_put`,"See #79987 

Soliciting better ideas on how to fix this issue

CC @ptrblck @ngimel 
",pytorch
80036,0x00b1,pr,2022-06-22T15:34:38Z,Elliptic integrals,,pytorch
80073,lezcano,pr,2022-06-22T19:57:23Z,Expose linalg.solve_ex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80217
* #80074
* __->__ #80073
* #79838
* #79742

This prepares for making `linalg.inv_ex` just a call into this function",pytorch
80074,lezcano,pr,2022-06-22T19:57:28Z,Make linalg.inv composite of linalg.solve,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80074

The `getri` kernel calls inside `getrs` so we can do so explicitly
ourselves and save ourselves from having to maintain an extra kernel.
This way we just need to optimise `lu_factor` and `lu_solve` and `inv`
will be as efficient as it can be, as it'll be choosing the best backend
to perform the factorisation and the best backend (not necessarily the
same) to perform the solve.

Fixes https://github.com/pytorch/pytorch/issues/77498

The benchmarks: https://github.com/pytorch/pytorch/pull/80074#issuecomment-1164309071",pytorch
80082,lezcano,pr,2022-06-22T20:29:20Z,More forward AD formulas,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #80083
* __->__ #80082

Reland of https://github.com/pytorch/pytorch/pull/77975

We remove the test that was failing, as now that functionality is being tested via the OpInfo.",pytorch
80083,lezcano,pr,2022-06-22T20:29:25Z,Fix backward of binary_cross_entropy_with_logits,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #80083
* #80082

Reland of https://github.com/pytorch/pytorch/pull/79381

Fixes https://github.com/pytorch/pytorch/issues/79609",pytorch
80093,crcrpar,pr,2022-06-22T21:42:03Z,Ship `ATen/native/utils/*.h` as `package_data` for nvFuser in custom CUDAExtension,"Related
- https://github.com/NVIDIA/apex/pull/1309
- https://github.com/pytorch/pytorch/pull/78281-- this was closed in favor of https://github.com/pytorch/pytorch/pull/79406, more specifically, https://github.com/pytorch/pytorch/commit/c9c402eae9e0d64bb346d9e47ed1bf019b4368b5#diff-60f61ab7a8d1910d86d9fda2261620314edcae5894d5aaa236b821c7256badd7.

cc @ptrblck @eqy ",pytorch
80217,lezcano,pr,2022-06-24T10:24:20Z,Support a stable double backward on linalg.det for real inputs,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80217
* #80074

The complex case still fails. I do not know why.

Fixes https://github.com/pytorch/pytorch/issues/62327
Fixes https://github.com/pytorch/pytorch/issues/53364",pytorch
80219,lezcano,pr,2022-06-24T12:48:06Z,Add shortcuts for refs.pow,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81113
* #78616
* #78615
* #78350
* __->__ #80219

As per title

cc @ezyang @mruberry @ngimel @Lezcano @peterbell10",pytorch
80239,rohan-varma,pr,2022-06-24T20:15:10Z,[not for land] tmm fix,"Fixes #ISSUE_NUMBER
",pytorch
80245,rohan-varma,pr,2022-06-24T20:47:13Z,Fix FSDP when not all outputs get gradient in backward,"In some use cases, FSDP runs into an issue where a training state assert in `_wait_for_post_backward` erroneously fires. Digging into the root cause, this is because `_post_backward_hook` which sets the module's training state to backward_post is never actually called, since no param in that module had gradient computed for it. Similar to DDP, this can happen when not all module outputs are used in loss computation, or module did not participate in forward at all. 

Fix this by tracking a variable `_post_backward_called` to track whether the hook is actually called or not.",pytorch
80334,lezcano,pr,2022-06-27T10:37:22Z,Make `kl_div` a composite function.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #80334

Benchmarks: https://github.com/pytorch/pytorch/pull/80334#issuecomment-1167229285

Fixes https://github.com/pytorch/pytorch/issues/80158
Fixes https://github.com/pytorch/pytorch/issues/78867
Fixes https://github.com/pytorch/pytorch/issues/69230

Supersedes https://github.com/pytorch/pytorch/pull/79007
Supersedes https://github.com/pytorch/pytorch/pull/69212
Supersedes https://github.com/pytorch/pytorch/pull/19659",pytorch
80425,rohan-varma,pr,2022-06-28T06:01:18Z,Fix comment in FSDP,Addresses @awgu comment on wording this comment better.,pytorch
80434,lezcano,pr,2022-06-28T09:18:53Z,Fix rrelu on CUDA,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #80434

Rrelu had a massive bug on CUDA, that would zero out the gradient in
the part where the function is the identity.

Fixes https://github.com/pytorch/pytorch/issues/80205",pytorch
80442,lezcano,pr,2022-06-28T11:19:52Z,Correct cbrt implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Following
https://github.com/pytorch/pytorch/pull/80219#discussion_r907680368",pytorch
80443,lezcano,pr,2022-06-28T11:20:46Z,Correct cbrt implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78616
* #78615
* #78350
* #80219
* __->__ #80443

Following
https://github.com/pytorch/pytorch/pull/80219#discussion_r907680368",pytorch
80453,rohan-varma,pr,2022-06-28T15:36:58Z,Remove unneeded TODO,"This TODO is no longer needed, as we use `_register_fused_optim` to register the overlapped optimizer in DDP.  Also, remove comment about API being experimental, as this API is no longer going to be used by end user. ",pytorch
80480,rohan-varma,pr,2022-06-28T22:17:27Z,[FSDP Optim State] Remove checkpoint prefix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80480

Remove `_checkpoint_wrapped_module` prefixes when creating keys for optimizer state_dict.

Having these does not actually create an issue for optim_state_dict save / load, but we'd like to strip these keys out for downstream code that consumes these APIs typically expecting checkpointing prefixes to not exist (as checkpointing should be a transparent operation which should not change module / parameter names).",pytorch
80511,cbalint13,pr,2022-06-29T13:46:54Z,Support python 3.11 changes.,"This PR adds support for required changes having python >= 3.11

---
* [Builds](https://copr.fedorainfracloud.org/coprs/rezso/ML/build/4584964/) on {py3.10, py3.11} x {aarch64, x86_64}.

Guided from here: https://docs.python.org/3.11/whatsnew/3.11.html#pyframeobject-3-11-hiding

Thanks,
~Cristian.

",pytorch
80516,lezcano,pr,2022-06-29T15:00:28Z,Correct meta behaviour of prims.resize,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78616
* #78615
* #78350
* #80219
* #80443
* #79941
* __->__ #80516

The previous behaviour did not modify the tensor in-place when it should",pytorch
80519,0x00b1,pr,2022-06-29T15:30:52Z,Unary Special Functions,"Fixes #ISSUE_NUMBER
",pytorch
80601,lezcano,pr,2022-06-30T11:37:26Z,Add support for multiple inputs to out_wrapper and strict dtype checking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78616
* #78615
* #78350
* #80219
* #80443
* __->__ #80601

Reland of https://github.com/pytorch/pytorch/pull/79941",pytorch
80748,eqy,pr,2022-07-01T00:23:18Z,Don't implicitly convert to channels-first in MaxPool3D on CUDA,"MaxPool3D currently converts inputs implicitly to channels-first (via `.contiguous()`) which may yield unexpected regressions in workloads that expect a full channels-last path. This PR preserves the channels-last format in MaxPool3D while attempting to avoid seriously regressing performance.

Currently, typical case (kernel size == 2 == stride) looks good, but larger kernel sizes (>4) or the unusual case of stride 1 can sometimes be slower than converting to channels-first before doing MaxPool3D.

Additionally, this PR adds a test for 64bit-indexing backwards as testing of these changes uncovered an IMA for large tensors when doing the backwards pass with MaxPool3D.

Performance comparison on A6000:


```
[------------------------------------- max_pool3d ---------------------------------------------------------]
                                          |  channels_last=False  |   curr ch_last=True |  new ch_last=True
1 threads: ---------------------------------------------------------------------------- ---------------------
      [64, 256, 32, 32, 32] 4x4 stride 4  |        20093.5        |       34823.4       |       20640.0      
      [64, 256, 32, 32, 32] 4x4 stride 2  |        28623.7        |       42625.6       |       27935.5      
      [64, 256, 32, 32, 32] 4x4 stride 1  |        68177.5        |       79147.2       |       85604.8      
      [64, 256, 32, 32, 32] 2x2 stride 4  |        17237.7        |       32071.3       |       16641.6      
      [64, 256, 32, 32, 32] 2x2 stride 2  |        25252.5        |       39993.2       |       25054.8      
      [64, 256, 32, 32, 32] 2x2 stride 1  |        43185.2        |       58164.6       |       48416.9      
      [64, 256, 16, 16, 16] 4x4 stride 4  |         3017.7        |        3952.4       |        2593.8      
      [64, 256, 16, 16, 16] 4x4 stride 2  |         4581.5        |        5384.3       |        3294.3      
      [64, 256, 16, 16, 16] 4x4 stride 1  |        11334.1        |       11534.7       |        8651.1      
      [64, 256, 16, 16, 16] 2x2 stride 4  |         2346.9        |        3304.6       |        2098.8      
      [64, 256, 16, 16, 16] 2x2 stride 2  |         3550.8        |        4526.5       |        3143.6      
      [64, 256, 16, 16, 16] 2x2 stride 1  |         6898.1        |        7816.0       |        5820.8      
      [64, 256, 4, 4, 4] 4x4 stride 4     |          191.5        |         176.3       |          77.5      
      [64, 256, 4, 4, 4] 4x4 stride 2     |          191.8        |         176.8       |          94.1      
      [64, 256, 4, 4, 4] 4x4 stride 1     |          191.3        |         176.4       |          97.3      
      [64, 256, 4, 4, 4] 2x2 stride 4     |           96.4        |         114.4       |          93.6      
      [64, 256, 4, 4, 4] 2x2 stride 2     |          172.1        |         178.6       |          93.7      
      [64, 256, 4, 4, 4] 2x2 stride 1     |          263.0        |         279.4       |          92.4      
      [64, 64, 32, 32, 32] 4x4 stride 4   |         5033.2        |        7208.3       |        5167.5      
      [64, 64, 32, 32, 32] 4x4 stride 2   |         7216.1        |        9218.7       |        6637.1      
      [64, 64, 32, 32, 32] 4x4 stride 1   |        17192.1        |       18392.9       |       20489.0      
      [64, 64, 32, 32, 32] 2x2 stride 4   |         4318.0        |        6511.2       |        4193.1      
      [64, 64, 32, 32, 32] 2x2 stride 2   |         6324.4        |        8657.7       |        6263.6      
      [64, 64, 32, 32, 32] 2x2 stride 1   |        10855.0        |       13040.2       |       12055.9      
      [64, 64, 16, 16, 16] 4x4 stride 4   |          764.1        |         975.6       |         671.3      
      [64, 64, 16, 16, 16] 4x4 stride 2   |         1163.1        |        1333.4       |         833.6      
      [64, 64, 16, 16, 16] 4x4 stride 1   |         2890.0        |        2898.5       |        2209.8      
      [64, 64, 16, 16, 16] 2x2 stride 4   |          593.5        |         811.2       |         536.3      
      [64, 64, 16, 16, 16] 2x2 stride 2   |          895.9        |        1112.3       |         794.5      
      [64, 64, 16, 16, 16] 2x2 stride 1   |         1742.5        |        1968.0       |        1475.2      
      [64, 64, 4, 4, 4] 4x4 stride 4      |          101.1        |         112.2       |          93.4      
      [64, 64, 4, 4, 4] 4x4 stride 2      |           96.7        |         114.6       |          92.5      
      [64, 64, 4, 4, 4] 4x4 stride 1      |           98.9        |         111.9       |          96.5      
      [64, 64, 4, 4, 4] 2x2 stride 4      |          100.1        |         107.1       |          94.2      
      [64, 64, 4, 4, 4] 2x2 stride 2      |           96.6        |         108.0       |          94.5      
      [64, 64, 4, 4, 4] 2x2 stride 1      |           96.7        |         107.9       |          95.2      
      [64, 3, 32, 32, 32] 4x4 stride 4    |          250.1        |         326.6       |         278.0      
      [64, 3, 32, 32, 32] 4x4 stride 2    |          350.4        |         414.0       |         323.2      
      [64, 3, 32, 32, 32] 4x4 stride 1    |          825.6        |         846.9       |         982.5      
      [64, 3, 32, 32, 32] 2x2 stride 4    |          213.3        |         289.8       |         219.9      
      [64, 3, 32, 32, 32] 2x2 stride 2    |          308.2        |         384.9       |         305.9      
      [64, 3, 32, 32, 32] 2x2 stride 1    |          523.5        |         594.7       |         589.9      
      [64, 3, 16, 16, 16] 4x4 stride 4    |          103.8        |         116.7       |          93.0      
      [64, 3, 16, 16, 16] 4x4 stride 2    |          100.9        |         108.3       |          93.3      
      [64, 3, 16, 16, 16] 4x4 stride 1    |          139.4        |         140.7       |         104.8      
      [64, 3, 16, 16, 16] 2x2 stride 4    |           97.5        |         114.7       |          92.7      
      [64, 3, 16, 16, 16] 2x2 stride 2    |           97.4        |         108.8       |          91.7      
      [64, 3, 16, 16, 16] 2x2 stride 1    |           99.9        |         108.0       |          94.1      
      [64, 3, 4, 4, 4] 4x4 stride 4       |           97.2        |         110.2       |          94.7      
      [64, 3, 4, 4, 4] 4x4 stride 2       |          105.7        |         107.4       |          92.8      
      [64, 3, 4, 4, 4] 4x4 stride 1       |           98.0        |         110.0       |          93.7      
      [64, 3, 4, 4, 4] 2x2 stride 4       |           98.3        |         116.7       |          93.0      
      [64, 3, 4, 4, 4] 2x2 stride 2       |           98.6        |         107.5       |          92.8      
      [64, 3, 4, 4, 4] 2x2 stride 1       |          100.6        |         110.3       |          94.0      
      [16, 256, 32, 32, 32] 4x4 stride 4  |         5034.2        |        8838.0       |        5165.9      
      [16, 256, 32, 32, 32] 4x4 stride 2  |         7236.3        |       10869.9       |        7038.2      
      [16, 256, 32, 32, 32] 4x4 stride 1  |        17385.4        |       21401.6       |       21900.7      
      [16, 256, 32, 32, 32] 2x2 stride 4  |         4318.7        |        8101.2       |        4172.9      
      [16, 256, 32, 32, 32] 2x2 stride 2  |         6324.0        |       10147.5       |        6279.7      
      [16, 256, 32, 32, 32] 2x2 stride 1  |        10899.7        |       14826.0       |       12256.3      
      [16, 256, 16, 16, 16] 4x4 stride 4  |          765.4        |        1012.7       |         675.6      
      [16, 256, 16, 16, 16] 4x4 stride 2  |         1162.8        |        1376.9       |         843.4      
      [16, 256, 16, 16, 16] 4x4 stride 1  |         2928.9        |        2969.8       |        2222.5      
      [16, 256, 16, 16, 16] 2x2 stride 4  |          593.5        |         845.8       |         534.2      
      [16, 256, 16, 16, 16] 2x2 stride 2  |          896.9        |        1152.2       |         796.9      
      [16, 256, 16, 16, 16] 2x2 stride 1  |         1750.2        |        2009.4       |        1481.8      
      [16, 256, 4, 4, 4] 4x4 stride 4     |           96.6        |         107.1       |          92.7      
      [16, 256, 4, 4, 4] 4x4 stride 2     |           97.9        |         114.9       |          93.8      
      [16, 256, 4, 4, 4] 4x4 stride 1     |           98.2        |         115.6       |          94.0      
      [16, 256, 4, 4, 4] 2x2 stride 4     |           97.0        |         106.7       |          93.8      
      [16, 256, 4, 4, 4] 2x2 stride 2     |           96.8        |         108.1       |          93.3      
      [16, 256, 4, 4, 4] 2x2 stride 1     |           95.8        |         120.9       |          95.7      
      [16, 64, 32, 32, 32] 4x4 stride 4   |         1266.4        |        1815.4       |        1312.3      
      [16, 64, 32, 32, 32] 4x4 stride 2   |         1818.5        |        2328.0       |        1678.9      
      [16, 64, 32, 32, 32] 4x4 stride 1   |         4352.9        |        4649.3       |        5204.6      
      [16, 64, 32, 32, 32] 2x2 stride 4   |         1090.0        |        1631.2       |        1060.8      
      [16, 64, 32, 32, 32] 2x2 stride 2   |         1589.4        |        2141.1       |        1576.4      
      [16, 64, 32, 32, 32] 2x2 stride 1   |         2733.5        |        3286.0       |        3041.6      
      [16, 64, 16, 16, 16] 4x4 stride 4   |          201.7        |         259.6       |         175.0      
      [16, 64, 16, 16, 16] 4x4 stride 2   |          301.0        |         350.1       |         226.3      
      [16, 64, 16, 16, 16] 4x4 stride 1   |          740.1        |         748.7       |         570.6      
      [16, 64, 16, 16, 16] 2x2 stride 4   |          156.0        |         214.8       |         140.8      
      [16, 64, 16, 16, 16] 2x2 stride 2   |          232.3        |         292.3       |         208.7      
      [16, 64, 16, 16, 16] 2x2 stride 1   |          449.1        |         504.0       |         382.1      
      [16, 64, 4, 4, 4] 4x4 stride 4      |           97.5        |         111.4       |          94.5      
      [16, 64, 4, 4, 4] 4x4 stride 2      |           98.8        |         111.9       |          94.4      
      [16, 64, 4, 4, 4] 4x4 stride 1      |           98.2        |         112.0       |          95.2      
      [16, 64, 4, 4, 4] 2x2 stride 4      |           99.7        |         111.0       |          94.0      
      [16, 64, 4, 4, 4] 2x2 stride 2      |          100.3        |         110.0       |          93.2      
      [16, 64, 4, 4, 4] 2x2 stride 1      |           97.5        |         107.6       |          93.5      
      [16, 3, 32, 32, 32] 4x4 stride 4    |          100.5        |         117.1       |          95.7      
      [16, 3, 32, 32, 32] 4x4 stride 2    |           97.5        |         121.3       |          92.5      
      [16, 3, 32, 32, 32] 4x4 stride 1    |          216.0        |         227.4       |         258.4      
      [16, 3, 32, 32, 32] 2x2 stride 4    |           97.1        |         109.0       |          91.9      
      [16, 3, 32, 32, 32] 2x2 stride 2    |           95.8        |         108.5       |          92.9      
      [16, 3, 32, 32, 32] 2x2 stride 1    |          139.4        |         161.2       |         157.8      
      [16, 3, 16, 16, 16] 4x4 stride 4    |           96.4        |         113.6       |          91.9      
      [16, 3, 16, 16, 16] 4x4 stride 2    |           97.4        |         108.1       |          93.5      
      [16, 3, 16, 16, 16] 4x4 stride 1    |           99.0        |         107.5       |          92.1      
      [16, 3, 16, 16, 16] 2x2 stride 4    |           96.9        |         118.1       |          93.4      
      [16, 3, 16, 16, 16] 2x2 stride 2    |           97.3        |         106.7       |          95.8      
      [16, 3, 16, 16, 16] 2x2 stride 1    |           98.8        |         109.2       |          93.8      
      [16, 3, 4, 4, 4] 4x4 stride 4       |           97.8        |         108.0       |          94.2      
      [16, 3, 4, 4, 4] 4x4 stride 2       |           92.7        |         108.0       |          93.9      
      [16, 3, 4, 4, 4] 4x4 stride 1       |           97.8        |         107.6       |          93.5      
      [16, 3, 4, 4, 4] 2x2 stride 4       |          100.3        |         107.7       |          94.3      
      [16, 3, 4, 4, 4] 2x2 stride 2       |           97.2        |         107.5       |          96.1      
      [16, 3, 4, 4, 4] 2x2 stride 1       |           98.1        |         111.1       |          93.8      

Times are in microseconds (us).
```

Performance comparison on V100:
(these times have been updated after working around some noisy measurements in my setup)
```
[------------------------------------- max_pool3d ---------------------------------------------------------]  
                                          |  channels_last=False  |  curr ch_last=True |  new ch_last=True
1 threads: -------------------------------------------------------------------------------------------------
      [64, 256, 32, 32, 32] 4x4 stride 4  |        15810.7        |       33807.7      |        16452.9     
      [64, 256, 32, 32, 32] 4x4 stride 2  |        24422.7        |       42515.3      |        27700.3     
      [64, 256, 32, 32, 32] 4x4 stride 1  |        71756.0        |       89916.5      |       106464.0     
      [64, 256, 32, 32, 32] 2x2 stride 4  |        12102.9        |       30210.4      |        11319.8     
      [64, 256, 32, 32, 32] 2x2 stride 2  |        19101.7        |       37210.8      |        20373.3     
      [64, 256, 32, 32, 32] 2x2 stride 1  |        41418.0        |       59650.5      |        53009.2     
      [64, 256, 16, 16, 16] 4x4 stride 4  |         2362.0        |        4210.3      |         2114.0     
      [64, 256, 16, 16, 16] 4x4 stride 2  |         4102.4        |        5897.4      |         3179.7     
      [64, 256, 16, 16, 16] 4x4 stride 1  |        11339.3        |       13116.6      |        10032.6     
      [64, 256, 16, 16, 16] 2x2 stride 4  |         1709.7        |        3506.7      |         1423.6     
      [64, 256, 16, 16, 16] 2x2 stride 2  |         2966.6        |        4760.8      |         2499.3     
      [64, 256, 16, 16, 16] 2x2 stride 1  |         6998.4        |        8797.3      |         6152.0     
      [64, 256, 4, 4, 4] 4x4 stride 4     |          173.0        |         176.3      |          127.9     
      [64, 256, 4, 4, 4] 4x4 stride 2     |          149.1        |         176.3      |          125.5     
      [64, 256, 4, 4, 4] 4x4 stride 1     |          150.0        |         177.2      |          125.6     
      [64, 256, 4, 4, 4] 2x2 stride 4     |          158.0        |         192.7      |          127.9     
      [64, 256, 4, 4, 4] 2x2 stride 2     |          169.7        |         199.2      |          125.3     
      [64, 256, 4, 4, 4] 2x2 stride 1     |          289.6        |         318.2      |          116.5     
      [64, 64, 32, 32, 32] 4x4 stride 4   |         3914.4        |        6993.3      |         4141.4     
      [64, 64, 32, 32, 32] 4x4 stride 2   |         6107.4        |        9186.4      |         6378.5     
      [64, 64, 32, 32, 32] 4x4 stride 1   |        17920.0        |       20993.5      |        23891.1     
      [64, 64, 32, 32, 32] 2x2 stride 4   |         3029.7        |        6112.6      |         2895.6     
      [64, 64, 32, 32, 32] 2x2 stride 2   |         4787.8        |        7870.6      |         4724.8     
      [64, 64, 32, 32, 32] 2x2 stride 1   |        10366.4        |       13446.4      |        12603.8     
      [64, 64, 16, 16, 16] 4x4 stride 4   |          605.8        |         962.9      |          499.7     
      [64, 64, 16, 16, 16] 4x4 stride 2   |         1037.0        |        1394.8      |          791.6     
      [64, 64, 16, 16, 16] 4x4 stride 1   |         2835.4        |        3191.8      |         2484.3     
      [64, 64, 16, 16, 16] 2x2 stride 4   |          438.6        |         795.7      |          368.6     
      [64, 64, 16, 16, 16] 2x2 stride 2   |          749.1        |        1108.0      |          612.0     
      [64, 64, 16, 16, 16] 2x2 stride 1   |         1756.4        |        2112.2      |         1538.5     
      [64, 64, 4, 4, 4] 4x4 stride 4      |          132.6        |         163.9      |          115.4     
      [64, 64, 4, 4, 4] 4x4 stride 2      |          129.3        |         153.7      |          117.8     
      [64, 64, 4, 4, 4] 4x4 stride 1      |          128.0        |         153.8      |          117.6     
      [64, 64, 4, 4, 4] 2x2 stride 4      |          128.2        |         154.1      |          117.5     
      [64, 64, 4, 4, 4] 2x2 stride 2      |          130.5        |         157.3      |          117.6     
      [64, 64, 4, 4, 4] 2x2 stride 1      |          128.8        |         156.4      |          120.6     
      [64, 3, 32, 32, 32] 4x4 stride 4    |          200.4        |         261.0      |          228.8     
      [64, 3, 32, 32, 32] 4x4 stride 2    |          305.3        |         366.5      |          344.4     
      [64, 3, 32, 32, 32] 4x4 stride 1    |          860.9        |         922.1      |         1136.0     
      [64, 3, 32, 32, 32] 2x2 stride 4    |          157.0        |         216.9      |          158.1     
      [64, 3, 32, 32, 32] 2x2 stride 2    |          240.5        |         300.9      |          247.7     
      [64, 3, 32, 32, 32] 2x2 stride 1    |          503.5        |         565.1      |          609.8     
      [64, 3, 16, 16, 16] 4x4 stride 4    |          136.0        |         159.0      |          120.3     
      [64, 3, 16, 16, 16] 4x4 stride 2    |          131.2        |         156.9      |          120.0     
      [64, 3, 16, 16, 16] 4x4 stride 1    |          146.6        |         158.5      |          123.8     
      [64, 3, 16, 16, 16] 2x2 stride 4    |          133.8        |         158.4      |          117.1     
      [64, 3, 16, 16, 16] 2x2 stride 2    |          132.1        |         160.8      |          117.9     
      [64, 3, 16, 16, 16] 2x2 stride 1    |          133.7        |         174.4      |          118.0     
      [64, 3, 4, 4, 4] 4x4 stride 4       |          156.8        |         166.2      |          119.4     
      [64, 3, 4, 4, 4] 4x4 stride 2       |          126.8        |         150.4      |          118.2     
      [64, 3, 4, 4, 4] 4x4 stride 1       |          125.2        |         151.7      |          117.8     
      [64, 3, 4, 4, 4] 2x2 stride 4       |          127.3        |         152.7      |          116.2     
      [64, 3, 4, 4, 4] 2x2 stride 2       |          128.6        |         153.3      |          114.6     
      [64, 3, 4, 4, 4] 2x2 stride 1       |          128.6        |         153.5      |          114.7     
      [16, 256, 32, 32, 32] 4x4 stride 4  |         3921.7        |        8445.7      |         4064.7     
      [16, 256, 32, 32, 32] 4x4 stride 2  |         6111.7        |       10630.0      |         6944.4     
      [16, 256, 32, 32, 32] 4x4 stride 1  |        17938.9        |       22896.8      |        26648.7     
      [16, 256, 32, 32, 32] 2x2 stride 4  |         3029.6        |        7552.7      |         2840.9     
      [16, 256, 32, 32, 32] 2x2 stride 2  |         4788.0        |        9322.1      |         5110.5     
      [16, 256, 32, 32, 32] 2x2 stride 1  |        10363.7        |       14885.9      |        13213.6     
      [16, 256, 16, 16, 16] 4x4 stride 4  |          606.0        |        1059.1      |          535.9     
      [16, 256, 16, 16, 16] 4x4 stride 2  |         1037.5        |        1491.5      |          822.3     
      [16, 256, 16, 16, 16] 4x4 stride 1  |         2835.4        |        3306.8      |         2522.8     
      [16, 256, 16, 16, 16] 2x2 stride 4  |          438.6        |         892.3      |          369.0     
      [16, 256, 16, 16, 16] 2x2 stride 2  |          749.2        |        1203.7      |          638.7     
      [16, 256, 16, 16, 16] 2x2 stride 1  |         1756.1        |        2212.5      |         1547.0     
      [16, 256, 4, 4, 4] 4x4 stride 4     |          159.6        |         187.6      |          117.6     
      [16, 256, 4, 4, 4] 4x4 stride 2     |          161.1        |         185.5      |          117.3     
      [16, 256, 4, 4, 4] 4x4 stride 1     |          160.0        |         148.1      |          117.8     
      [16, 256, 4, 4, 4] 2x2 stride 4     |          123.9        |         148.3      |          117.6     
      [16, 256, 4, 4, 4] 2x2 stride 2     |          126.0        |         151.7      |          117.4     
      [16, 256, 4, 4, 4] 2x2 stride 1     |          127.1        |         152.3      |          117.9     
      [16, 64, 32, 32, 32] 4x4 stride 4   |          983.5        |        1756.7      |         1067.8     
      [16, 64, 32, 32, 32] 4x4 stride 2   |         1542.4        |        2315.2      |         1621.5     
      [16, 64, 32, 32, 32] 4x4 stride 1   |         4498.7        |        5273.4      |         6006.7     
      [16, 64, 32, 32, 32] 2x2 stride 4   |          767.2        |        1543.4      |          736.7     
      [16, 64, 32, 32, 32] 2x2 stride 2   |         1207.8        |        1981.5      |         1197.0     
      [16, 64, 32, 32, 32] 2x2 stride 1   |         2603.3        |        3367.5      |         3161.9     
      [16, 64, 16, 16, 16] 4x4 stride 4   |          169.5        |         264.6      |          142.8     
      [16, 64, 16, 16, 16] 4x4 stride 2   |          274.6        |         368.9      |          216.8     
      [16, 64, 16, 16, 16] 4x4 stride 1   |          723.3        |         820.4      |          643.2     
      [16, 64, 16, 16, 16] 2x2 stride 4   |          131.4        |         216.0      |          116.1     
      [16, 64, 16, 16, 16] 2x2 stride 2   |          199.9        |         295.0      |          166.8     
```
CC @ptrblck 
",pytorch
80767,lezcano,pr,2022-07-01T11:34:12Z,Clean-up error checking in linalg.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80767

This PR does a number of things:
- Fixes https://github.com/pytorch/pytorch/issues/80735 returning early in the most common case (no errors).
- Leaves just one entry to check the errors, so that it's as uniform as it gets (easy to get right, difficult to get wrong)
- Having all the error checking in one function simplifies the code
- We make sure that the `infos_ptr` sent over to LAPACK is always of type `int`, as it should
- We just access `infos.data_ptr<int32_t>` in `_linalg_check_errors`

Edit. I get a consistent x7-10 speed-up with this patch using the script in https://github.com/pytorch/pytorch/issues/80735, so I think we're good here.",pytorch
80841,lezcano,pr,2022-07-04T15:22:14Z,Fix the derivative of `acosh` for complex numbers.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #80841

For a summary of what was going on, see https://github.com/pytorch/pytorch/issues/50692#issuecomment-1173922441

Fixes https://github.com/pytorch/pytorch/issues/50692",pytorch
80879,vfdev-5,pr,2022-07-05T15:32:30Z,Fixed docstring typo,"Just a docsting typo fix
",pytorch
80936,rohan-varma,pr,2022-07-06T05:17:22Z,[Reland][FSDP Optim State] Remove checkpoint prefix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80936

Reland which adds __all__ to torch/utils/checkpoint to silence public binding errors.",pytorch
80987,rohan-varma,pr,2022-07-06T18:57:44Z,Add kwarg support for no_reentrant checkpoint,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80987

Supports kwargs input to function when `torch.utils.checkpoint` with use_reentrant=False. This is required to unblock T5 activation checkpointing and MetaSeq use cases.

Closes https://github.com/pytorch/pytorch/issues/79887",pytorch
80988,rohan-varma,pr,2022-07-06T19:02:48Z,[WIP] Activation checkpoint fix,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80988
* #80987

",pytorch
81113,lezcano,pr,2022-07-08T15:59:03Z,[PrimTorch] Reference for linalg.matrix_norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81763
* #81762
* #81761
* #81765
* #81241
* __->__ #81113

As per title. I corrected a thing or two from my previous implementation
to make for better errors in some weird edge-cases and have a more clear
understanding of when does this function support low_precision types and
when it doesn't.

We also use the optimisation for bfloat16 within `vector_norm` within
this function.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @ezyang @ngimel @peterbell10",pytorch
81139,eqy,pr,2022-07-08T21:02:28Z,[CUDNN] Update tests and dispatching for CUDNN V8 API behavior for `bfloat16` convs,"cuDNN via the V8 API supports `bfloat16` on Ampere (`>= (8, 0)` but not older devices) which might be unexpected given current test settings. This PR fixes some dispatching to check the device capability before dispatching `bfloat16` convs and adjusts the expected failure conditions for the autocast test.

CC @xwang233 @ptrblck ",pytorch
81182,fritzo,pr,2022-07-10T15:28:26Z,Implement .mean property for Relaxed distributions,"Fixes [this issue](https://forum.pyro.ai/t/not-implemented-error-for-relaxed-bernoulli-straight-through-with-auto-hierarchical-normal-messenger/4377/5) on the Pyro forum.

## Tested
- covered by existing distribution tests",pytorch
81241,lezcano,pr,2022-07-11T15:18:41Z,[PrimTorch] Reference for linalg.norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81763
* #81762
* #81761
* #81765
* __->__ #81241
* #81113

After all the work done, this one's just a simple composition of the
others :)",pytorch
81272,crcrpar,pr,2022-07-11T21:00:27Z,Support NCCL Premul Sum,"This PR adds the support for https://docs.nvidia.com/deeplearning/nccl/archives/nccl_21212/user-guide/docs/api/ops.html?highlight=premul#c.ncclRedOpCreatePreMulSum.

The major changes include
- convert enum ReduceOp to struct
- add premul sum specific paths to init.cpp and Ops.cpp.

note:
- For pip wheels / conda binaries to support this, ~~I think https://github.com/pytorch/pytorch/pull/79132 would be needed~~ https://github.com/pytorch/pytorch/pull/82775 landed

The commit titled ""add nccl premul"" whose current hash is https://github.com/pytorch/pytorch/pull/81272/commits/cb99ad67447b5899ecf8c4c3d78deaafa1cc09b8 was authored by @mcarilli and @ptrblck.

cc @ptrblck ",pytorch
81316,lezcano,pr,2022-07-12T10:13:44Z,Small optimisation to linalg.cholesky,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #81316

I figured we could save one kernel call in `linalg.cholesky`",pytorch
81477,r-barnes,pr,2022-07-14T17:49:51Z,Cleaning Indexing.cu,"Differential Revision: D37842740

",pytorch
81486,eqy,pr,2022-07-14T19:01:23Z,[cuDNN] Work-around 32-bit indexing failures in cuDNN batchnorm,"The following workload fails on Ampere:

```
import torch
import torch.nn as nn
bn = nn.BatchNorm2d(128).cuda()
x = torch.randn([256, 128, 294, 294], device='cuda')
x = bn(x)
print(x.shape)
```

This PR adds a magic number in the `use_cudnn` condition to avoid this failure.

CC @ptrblck ",pytorch
81504,crcrpar,pr,2022-07-14T21:39:58Z,Run `test_graph_concurrent_replay` if CUDA>=11.4,"According to #61063, I think it possible to run `test_graph_concurrent_replay` and it actually is on my local environment with CUDA 11.7.

Related: #57556",pytorch
81560,r-barnes,pr,2022-07-15T18:04:46Z,Check all CUDA API calls for errors in torch/,"Summary:
Original commit changeset: 0bb770d2cdb2

Original Phabricator Diff: D35194935 (https://github.com/pytorch/pytorch/commit/79e5b053b690852b21d881357904bc5a4438d95b)

Differential Revision: D35291874



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",pytorch
81576,lezcano,pr,2022-07-15T21:41:28Z,Reduce the boilerplate needed to bind properties,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #81576

We implement a template and we fill it up via CRTP. This heavily reduces
the ammount of repeated code.

Just testing the waters here. If you like this idea, I can easily extend
this idea to cover many of the properties that we currently implement.

N.b. It'd be nice to have proper `if constexpr` support for this one,
but here we are.",pytorch
81596,eqy,pr,2022-07-16T01:36:57Z,"[BC-Breaking] Separate `stream_id`, `device_index`, and `device_type` in `pack` and `unpack` for `Streams`","#75854

A naive attempt at working around the limitations of using a single 64-bit integer to pack `stream_id`, `device_index`, and `device_type`.

Stills needs sanity checks, testing, and minimization of BC-breaking changes.

Currently a Holder for the `StreamData3` struct is used for `IValue` compatibility. While doing this seems to work for `ivalue.h` and `ivalue_inl.h`, this doesn't seem to be naively working for the JIT CUDA stream wrapper? (Something about ambiguous calls if an `intrusive_ptr` to `c10::ivalue::StreamData3Holder` is used as the return type for `pack()`. It turns out that the methods required to access the fields for rematerializing a CUDA Stream are basically already present anyway, so `pack` is simply removed in the wrapper for now and the methods to access the required fields are called directly.

CC @ptrblck 

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",pytorch
81619,ngimel,pr,2022-07-17T21:16:59Z,"make clamp decomps use torch.* calls, move clamp_min/clamp_max to refs","Per title, 
@chillee is anything else necessary to remove decomp other than decorating ref with `register_decomposition`?
",pytorch
81705,crcrpar,pr,2022-07-19T17:00:51Z,[mta] APEX style Fused Adam,"This PR implements an APEX style FusedAdam in PyTorch.
This is different from the APEX one in that this is compatible with `torch.cuda.amp.GradScaler` by setting `_step_supports_amp_scaling` to `True` and unscales gradients inside its CUDA kernel.

related: https://github.com/pytorch/pytorch/issues/68041, https://github.com/pytorch/pytorch/issues/71274, https://github.com/pytorch/pytorch/issues/80167
possibly related to https://github.com/pytorch/pytorch/issues/80595#issuecomment-1178519436

cc @ptrblck @ngimel ",pytorch
81707,tillahoffmann,pr,2022-07-19T17:23:18Z,Ensure `Transform` is pickleable.,"`Transform` is not currently pickleable if the inverse transform cache `_inv` is not `None` because `_inv` is a `weakref` which cannot be serialized by `pickle`. 

The following succeeds.

```python
>>> import torch as th
>>> import pickle

>>> dist = th.distributions.TransformedDistribution(
...     th.distributions.Normal(0, 1),
...     [th.distributions.AffineTransform(2, 3)]
... )
>>> th.save(dist, ""some-file.pt"")
```

But the transformed distribution can no longer be pickled after evaluating `log_prob` (which implicitly creates `_inv`).

```python
>>> dist.log_prob(th.linspace(0, 1, 10))
>>> th.save(dist, ""some-file.pt"")
TypeError: cannot pickle 'weakref' object
```

This PR fixes the issue by setting `_inv` to `None` in `__getstate__`. cc @fritzo, @neerajprad ",pytorch
81761,lezcano,pr,2022-07-20T12:41:47Z,Dispatch torch.norm to linalg.vector_norm and linalg.matrix_norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81763
* #81762
* __->__ #81761

`torch.norm` is very odd. Some notable issues are:

- The default value of `""fro""` in `torch.norm` has an odd behaviour when `dim=None`. This is handled in the new dispatch
- The treatment of the `dtype` argument in `torch.norm` was completely wrong. This should fix it
- Some `out=` variants in the previous implementation were also wrong. This should fix those.
- This new dispatch should make some paths much faster. For example, `torch.norm(x)` where `x` is complex.

I'll try to make the changes in these PRs as incremental as possible as this is a tricky one.

cc @ezyang @gchanan @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @ngimel @fdrocha",pytorch
81762,lezcano,pr,2022-07-20T12:41:53Z,Remove overload at::frobenius_norm(const Tensor&),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81763
* __->__ #81762

This function is an auxiliary function for `torch.norm`. This particular
overload was not even used or tested. I hope it's not used internally
either. If it is, we can simply drop this PR

cc @mcarilli @ptrblck @leslie-fang-intel @jgong5",pytorch
81763,lezcano,pr,2022-07-20T12:41:57Z,Dispatch the auxiliary frobenius_norm and nuclear_norm to better implementations and deprecate them,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81763

These functions will be legacy functions. We deprecate them, but we also
take this chance to dispatch to a more efficient and consistent implementation.
Doing so should help writing a conversion rule for these to be able to
remove them once and for all

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano @ezyang @ngimel @peterbell10

Differential Revision: [D42354776](https://our.internmc.facebook.com/intern/diff/D42354776)",pytorch
81765,lezcano,pr,2022-07-20T13:42:23Z,[PrimTorch] Add reference for torch.norm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #81763
* #81762
* #81761
* __->__ #81765

This ref does more things than `torch.norm`, and it fixes a few bugs
that `torch.norm` has. This implementation and the `torch.norm`
implementation come to terms in the next PR of this stack

We put this PR before, as otherwise `test_decomp.py` was failing.",pytorch
81766,rohan-varma,pr,2022-07-20T14:50:06Z,[Checkpoint] Fix autocasting,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81766

Add support for the correct autocasting in the non-reentrant checkpoint as it exists in the reentrant-version.

This was noticed by @awgu. ",pytorch
81816,r-barnes,pr,2022-07-20T21:31:30Z,Check all CUDA API calls for errors in caffe2/,"Test Plan: Sandcastle

Differential Revision: D35194868

",pytorch
81817,r-barnes,pr,2022-07-20T21:38:26Z,Check all CUDA API calls for errors in benchmarks/cpp/nvfuser (#74920),"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/74920

Test Plan: Sandcastle

Differential Revision: D35194656

",pytorch
82157,b0noI,pr,2022-07-25T20:14:37Z,svekars added as a maintainer to the docs module,Adding Svetlana Karslioglu to the maintainer for the docs module has been reviewed and approved on the latest quarterly sync among core maintainers.,pytorch
82272,ngimel,pr,2022-07-26T23:16:10Z,Fix invalid read in masked softmax,"PEr title, unfortunately testing invalid reads with caching allocator is hard.",pytorch
82274,b0noI,pr,2022-07-26T23:37:13Z,"new doc/tutorial module been added, with the first maintainer svekarsâ€¦",Approved on the core maintainers meeting: https://dev-discuss.pytorch.org/t/first-pytorch-quarterly-maintainers-meeting-minutes-meeting-date-july-22-2022/709,pytorch
82396,powderluv,pr,2022-07-28T05:27:16Z,Update FBGEMM to compile with latest clang-14+,"### Description
FBGEMM depends on asmjit that failed with clang-14+. Update to
the latest FBGEMM SHA which updates asmjit.

### Testing
Builds with latest clang-15+ 
",pytorch
82512,micmelesse,pr,2022-07-29T23:56:09Z,[ROCM] Enable test_memory_format_nn_BatchNorm tests on ROCM,"### Description
This enables some unit tests related to BatchNorm for ROCM.  We make sure that we call the MIOpen library incases where it can handle it and use the default path in other cases. When MIOpen implements this specific case we will file a follow up PR enabling that code path. ",pytorch
82523,crcrpar,pr,2022-07-30T07:50:18Z,[foreach][mta] Inplace `maximum` and `minimum`,"### Description
<!-- What did you change and why was it needed? -->
Implement `torch._foreach_maximum_` and `torch._foreach_minimum_` mainly for `_multi_tensor_adam` and `_multi_tensor_adamw` with `amsgrad=True` to correctly update their `max_exp_avg_sqs`.

### Issue
<!-- Link to Issue ticket or RFP -->
- https://github.com/pytorch/pytorch/issues/78807
- https://github.com/pytorch/pytorch/pull/81894
- https://github.com/pytorch/pytorch/pull/81348
- https://github.com/pytorch/pytorch/pull/81705
- https://github.com/pytorch/pytorch/issues/58833
- https://github.com/pytorch/pytorch/issues/68041

### Testing
<!-- How did you test your change? -->
Updated `test_foreach.py::TestForeach::_minmax_test` to compare the outputs of `_foreach_maximum_` (and `_foreach_minimum_`) against those of `[torch.maximum(a, b) for a, b in zip(tensors1, tensors2)]`

cc @ngimel @albanD @mikaylagawarecki ",pytorch
87,kashif,pr,2015-11-10T09:51:40Z,fixed link to tutorial and some typos,,tensorflow
574,gokceneraslan,pr,2015-12-20T22:54:50Z,Typo fix,"Fix citation to Bahdanau et al. in sequence to sequence documentation
",tensorflow
698,benoitsteiner,pr,2016-01-05T20:48:58Z,Deleted the TF copy of the tensor code,"Since  we're now pulling the tensor code directly from the eigen repository we don't need to keep a local copy anymore.
",tensorflow
711,benoitsteiner,pr,2016-01-07T01:54:38Z,Delete most of the local copy of eigen that is shipped with tensorflow,"TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.
",tensorflow
753,benoitsteiner,pr,2016-01-12T19:19:26Z,Delete the local copy of eigen that is shipped with tensorflow,"TensorFlow now pulls the eigen code directly from the eigen upstream repository. There is no need to keep a local copy anymore.
",tensorflow
814,benoitsteiner,pr,2016-01-19T21:08:49Z,Improved the performance of the contrast adjustment code,"Improved the performance of the contrast adjustment code by enabling nvcc to optimize the broadcast of scalars
",tensorflow
845,benoitsteiner,pr,2016-01-23T01:40:31Z,Improved the performance of convolutions on CPU,,tensorflow
846,benoitsteiner,pr,2016-01-23T02:13:45Z,Improved the performance of spatial convolutions on CPU:,"Moved some checks out of inner loops
Split the mapper in 2: a base mapper, and a sub-mapper. This reduces the number of variables that are contained in the base mapper and helps reduce register spills
",tensorflow
847,benoitsteiner,pr,2016-01-23T03:07:43Z,Ensure the code compiles when AVX2 instructions are enabled.,"This fixes issue https://github.com/tensorflow/tensorflow/issues/580
",tensorflow
891,gokceneraslan,pr,2016-01-26T16:23:59Z,Clone 0.6.0 branch to use 0.6 branch,"Master head is not bazel 0.1.1-compatible any more.
",tensorflow
920,benoitsteiner,pr,2016-01-28T22:07:48Z,Upgraded to a newer version of Eigen that fixes a compilation error when compiling with nvcc and clang,,tensorflow
983,benoitsteiner,pr,2016-02-04T01:16:58Z,Silenced some bogus compilation warning generated by nvcc,,tensorflow
991,benoitsteiner,pr,2016-02-04T17:15:39Z,Made the fixed point code compile when AVX2 instructions ar enabled,,tensorflow
1053,benoitsteiner,pr,2016-02-11T04:58:31Z,Completed the cleanup of the TensorFlow local copy of the Eigen codebase,,tensorflow
1055,benoitsteiner,pr,2016-02-11T07:04:00Z,Upgraded to the latest version of Eigen,"This version provides better support for embedded platforms and optimized implementations of the tanh function amongst many enhancements
",tensorflow
1068,benoitsteiner,pr,2016-02-12T01:09:54Z,Upgraded to the latest version of Eigen,"Upgraded to the latest version of Eigen that introduces a way to conjugate a tensor as well as improved LU decompositions.
",tensorflow
1137,rasbt,pr,2016-02-17T07:09:43Z,Embed __version__ into package,"Hi,
it would be nice to embed a version into the tensorflow package for reproducibility. The common Python convention is to do this via `__version__` attribute in the `__init__.py` file so that one can import the package and verify its version. 

```
import tensorflow as tf
print(tf.__version__)
```

This is very helpful for reproducibility and managing different virtual environments. 
",tensorflow
1371,syed-ahmed,pr,2016-03-03T17:14:11Z,Add check_numerics_op.cc to Android extended operator filegroup.,"The check_numerics_op.cc is needed to load the newer inception model/retrained inception model in the android demo. Refer to the issue #1269 .
",tensorflow
1424,kashif,pr,2016-03-07T22:17:48Z,Logsoftmax kernel,"for issue #443
",tensorflow
1505,rasbt,pr,2016-03-15T03:12:12Z,typos fix and ign temp files in gitignore,"Just wanted to fix a simple typo in the documentation and noticed that some temporary files (e.g. .ipynb checkpoints and Mac DS_Store files for indexing) were not listed in the gitignore, yet, so that I just added them as well.
",tensorflow
1580,rdipietro,pr,2016-03-22T16:30:13Z,Fix docs for cross-entropy loss functions,"Should close https://github.com/tensorflow/tensorflow/issues/1234
",tensorflow
1649,darrengarvey,pr,2016-03-26T00:45:07Z,Fix building android_tensorflow_lib.,"This commit fixes a build time failure of the Android TF library that is
missing some dependencies since they have been made more fine-grained.
For reference, the relevant errors (one per required dep) look something
like:

```
ERROR: tensorflow/tensorflow/core/BUILD:663:1: Couldn't build file tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/softsign_op.o: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/softsign_op.cc':

ERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/maxpooling_op.cc':

ERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/ctc_decoder_ops.cc':

ERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/avgpooling_op.cc':
```
",tensorflow
1834,tillahoffmann,pr,2016-04-09T23:47:04Z,Add polygamma and zeta function to tensorflow,"This PR adds the polygamma and zeta functions as discussed in #1741. It also implements the derivative of the digamma function. Tests are provided and can be run provided `scipy` is installed.
",tensorflow
1847,tillahoffmann,pr,2016-04-11T06:32:26Z,Modified tests for gradients,"While implementing gradients for some new functions, I forgot to propagate the `grad` argument in the definition of the derivative in [`tensorflow/python/ops/math_grad.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py). The wrong implementation passed all the tests because the derivative of the function is evaluated in isolation. 

Multiplying the TensorFlow operation by a constant `a` ensures that such errors are caught by the tests.  This PR implements the modified tests with `a = 1.1`.
",tensorflow
2027,kashif,pr,2016-04-19T22:24:39Z,set cudnnSetFilter4dDescriptor format using filter layout for cudnn_v5,"`CUDNN_TENSOR_NHWC` the only other cudnn filter format is still not supported since `kInputYXOutput` would correspond to `CUDNN_TENSOR_CHWN` which doesnâ€™t exist

for issue #1786 
",tensorflow
2058,kashif,pr,2016-04-21T23:20:05Z,use cudnnGetErrorString to get cudnn error string,"the built-in cudnnGetErrorString does exactly the same as the ToString helper
",tensorflow
2078,rasbt,pr,2016-04-24T04:30:18Z,remove unused array assignment,"The `array` in

```
array = np.random.rand(32, 100, 100)
```

in the context 

```
import numpy as np
array = np.random.rand(32, 100, 100)

def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return tf.matmul(arg, arg) + arg

# The following calls are equivalent.
value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))
value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])
value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))
```

is never used. I suggest removing it for clarity.
",tensorflow
2100,benoitsteiner,pr,2016-04-25T19:28:29Z,Branch 120724543,,tensorflow
2103,benoitsteiner,pr,2016-04-25T22:39:58Z,Branch 120750153,,tensorflow
2112,benoitsteiner,pr,2016-04-26T15:45:21Z,Branch 120798893,,tensorflow
2120,benoitsteiner,pr,2016-04-26T23:49:06Z,Branch 120862895,,tensorflow
2137,zheng-xq,pr,2016-04-27T20:00:16Z,Issue 2066: Fix the conv for stride > ksize case.,"Issue 2066: Fix the conv for stride > ksize case. Passed all the tests and locally verified it fixed the problem. 
",tensorflow
2182,kashif,pr,2016-05-01T07:53:42Z,added cuda/extras and cuda/lib to gitignore,"on OS X these folders need to be gitignored
",tensorflow
2246,zheng-xq,pr,2016-05-06T02:00:16Z,Branch 121636618,"Merge internal changes.
",tensorflow
2250,kashif,pr,2016-05-06T08:46:00Z,fix compile issue on os x,"for issue #2243
",tensorflow
2259,zheng-xq,pr,2016-05-07T00:25:39Z,Branch 121719728,"Merge internal changes
",tensorflow
2346,yaroslavvb,pr,2016-05-13T01:00:54Z,Basic implementation of immediate execution mode for TensorFlow,"This version implements Immediate execution for native TensorFlow ops with graph caching:

IE, you can do

```
    tf = immediate.Env()
    val = np.ones(())
    tensor1 = immediate.Tensor.numpy_to_tensor(env, val)
    tensor2 = immediate.Tensor.numpy_to_tensor(env, val)
    tensor3 = tf.add(tensor1, tensor2)   # executes operation immediately
    tensor3 = tf.add(tensor3, tensor2)   # executes operation without modifying graph
    print tensor3

```

What's not implemented yet (rough implementation detail is in the design doc):
- support for ""tf.nn"" namespace
- support for manually created Python ops
- caching for get_session_tensor and get_session_handle ops

@yuanbyu @keveman 
",tensorflow
2404,ziky90,pr,2016-05-17T14:32:35Z,Updated installation instructions for conda.,"This PR updates documentation of installation instructions for conda users.

The new version was tested so it seems that nobody has changed documentation of installation instructions.
",tensorflow
2442,rdipietro,pr,2016-05-20T17:34:46Z,Add piecewise constant op,"This should close https://github.com/tensorflow/tensorflow/issues/2432

This is more general than for learning-rate decay, but at the same time it didn't feel like it should be a native math op, so I kept it in `tensorflow/python/training/learning_rate_decay.py`.
",tensorflow
2490,jihunchoi,pr,2016-05-24T15:33:54Z,Modify zero_state to work with MultiRNNCell when state_is_tuple is True,"#2463
",tensorflow
2556,kashif,pr,2016-05-28T10:00:34Z,cuda 8 api uses cudaDataType for 16bit float type,,tensorflow
2557,kashif,pr,2016-05-28T10:41:12Z,added Install instructions for OS X GPU,"for issue #2522
",tensorflow
2580,jihunchoi,pr,2016-05-30T13:34:03Z,Update RNN PTB example to use state_is_tuple,"Currently, the PTB example emits warnings that using without `state_is_tuple` would soon be deprecated.
It can confuse newbies (like me), so it would be nice to use the one that makes no warning.
",tensorflow
2581,jihunchoi,pr,2016-05-30T15:50:40Z,Implement bidirectional_dynamic_rnn (#1779),"In #1779, it seems that the problem of implementing bidirectional version of `dynamic_rnn` is that `tf.reverse_sequence` cannot reverse a sequence whose shape is unknown.
However, now it can reverse an unknown-shaped sequence according to #1816, so I implemented the dynamic version of `dynamic_rnn` and its tests.
",tensorflow
2595,yaroslavvb,pr,2016-05-31T19:49:24Z,Immediate Mode execution in tensorflow,"Implementation of immediate mode execution in TensorFlow. The idea is to wrap original tensorflow API, but provide session/run management logic so that commands execute immediately, and graph caching to avoid modifying graph when same op is run repeatedly. Data is kept in TensorFlow runtime whenever possible using persistent tensors and transferred to Python runtime on demand when needed for printing/control flow.

```
import tensorflow as tf
from tensorflow.contrib import immediate

tfi = immediate.Env(tf).tf        # wraps ""tf"" namespace, saves it as ""tfi""
val1 = tfi.ones((3,))             # creates tensor on GPU
val2 = val1 + val1                # runs into tf.add, keeps result on GPU
val3 = tfi.ones((2, 2, 2, 2))
val4 = tfi.nn.conv2d(val3, val3, [1, 1, 1, 1], ""VALID"")   # run CuDNN conv2d
if (tfi.reduce_sum(val3)>0.):     # runs reduce_sum on GPU, transfers bool to CPU
  print(val4)                     # transfers whole tensor to CPU for display

```

This is a single commit rebase of a previous pull request from https://github.com/tensorflow/tensorflow/pull/2346

@keveman 
",tensorflow
2612,orionr,pr,2016-06-01T23:49:22Z,Fix setup for development link step.,"- This was a typo as agreed upon at https://github.com/tensorflow/tensorflow/issues/2591
",tensorflow
2670,yaroslavvb,pr,2016-06-05T17:54:04Z,Add a method to remove entries from registry.,"I'm using this to remove incompatible shape functions from the registry to make ""immediate"" tensors work as a drop-in replacement for regular TensorFlow tensors.

Since ""static shape inference"" doesn't make sense for persistent tensors, I'm returning Dimension(None) for the shape for such tensors. But, there's at least one shape function which doesn't allow `Dimension(None)` -- (`_ReverseShape(op):`), so as a work-around, I'm using `unregister followed by`register(None)` 

@mrry 
",tensorflow
2672,yaroslavvb,pr,2016-06-05T18:46:21Z,Mention fix for building on 16.04 in Troubleshooting,"I ended up using @fayeshine fix from https://github.com/tensorflow/tensorflow/pull/2073 to get tensorflow building on Ubuntu 16.04, add a mention to this work-around under ""Linux Issues""
",tensorflow
2733,MicaelCarvalho,pr,2016-06-08T16:31:56Z,MNIST downloader doesn't work as expected,"See: https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html

> For your convenience, we've included a script which automatically downloads and imports the MNIST dataset. It will create a directory 'MNIST_data' in which to store the data files.

Linked to this file: https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/mnist/input_data.py

Which currently doesn't create any folder.
",tensorflow
2747,yaroslavvb,pr,2016-06-09T02:47:32Z,Immediate execution mode with graph caching.,"Implementation of immediate mode execution in TensorFlow. The idea is to wrap original tensorflow API, but provide session/run management logic so that commands execute immediately, and graph caching to avoid modifying graph when same op is run repeatedly. Data is kept in TensorFlow runtime whenever possible using persistent tensors and transferred to Python runtime on demand when needed for printing/control flow.

```
import tensorflow as tf
from tensorflow.contrib import immediate

tfi = immediate.Env(tf).tf        # wraps ""tf"" namespace, saves it as ""tfi""
val1 = tfi.ones((3,))             # creates tensor on GPU
val2 = val1 + val1                # runs into tf.add, keeps result on GPU
val3 = tfi.ones((2, 2, 2, 2))
val4 = tfi.nn.conv2d(val3, val3, [1, 1, 1, 1], ""VALID"")   # run CuDNN conv2d
if (tfi.reduce_sum(val3)>0.):     # runs reduce_sum on GPU, transfers bool to CPU
  print(val4)                     # transfers whole tensor to CPU for display

```

This is a single commit rebase of a previous pull request from https://github.com/tensorflow/tensorflow/pull/2346

@keveman @yuanbyu 
",tensorflow
2828,MicaelCarvalho,pr,2016-06-13T11:55:50Z,MNIST tutorial incorrectly described a file,"The file ""input_data.py"" from MNIST tutorial does not download any data, as previously stated.

Related to #2733.
",tensorflow
2832,benoitsteiner,pr,2016-06-13T16:31:42Z,Branch 124731683,,tensorflow
2843,benoitsteiner,pr,2016-06-14T01:33:25Z,Branch 124789706,,tensorflow
2856,benoitsteiner,pr,2016-06-14T16:54:12Z,Branch 124848019,,tensorflow
2864,benoitsteiner,pr,2016-06-14T20:01:22Z,Branch 124869789,,tensorflow
2876,benoitsteiner,pr,2016-06-15T01:56:00Z,Branch 124906347,,tensorflow
2886,benoitsteiner,pr,2016-06-15T17:04:28Z,Branch 124956736,,tensorflow
2892,benoitsteiner,pr,2016-06-15T22:14:11Z,Branch 124994976,,tensorflow
2912,benoitsteiner,pr,2016-06-16T20:07:54Z,Branch 125083559,,tensorflow
2918,benoitsteiner,pr,2016-06-17T01:05:17Z,Branch 125120308,,tensorflow
2941,benoitsteiner,pr,2016-06-18T00:53:35Z,Branch 125215862,,tensorflow
2956,benoitsteiner,pr,2016-06-20T04:28:58Z,Branch 125246026,,tensorflow
2978,MicaelCarvalho,pr,2016-06-21T15:17:31Z,CIFAR-10 init routine breaks simple usages,"This is partly breaking the CIFAR-10 example, and it doesn't make sense.

The files `cifar10.py` and `cifar10_input_test.py` already import `cifar10_input`, so the initialization doesn't seem to be needed at all.
If one tries to copy the code of `cifar10.py` and run it, it will break, since `__init__.py` imports cifar10, which defines the flags `batch_size` and `data_dir`, causing an error when your `cifar10.py` tries to redefine them.

To reproduce the problem, create a file exactly like `cifar10.py` and try to run it.
",tensorflow
3010,kashif,pr,2016-06-23T14:55:47Z,Added complex type support to ops for GPU,"Initial `matmul` complex support for issue #2977 
",tensorflow
3448,yaroslavvb,pr,2016-07-21T17:25:35Z,Fix segfault in MacOS when GPU is not available,"Fixes #3435 
",tensorflow
3522,kashif,pr,2016-07-27T09:05:27Z,fix for BM_UNARY test,"here is a fix for the test. Can you kindly review @rryan ?
",tensorflow
3633,zheng-xq,pr,2016-08-03T21:54:49Z,Branch 129255791,"Merging internal changes
",tensorflow
3656,zheng-xq,pr,2016-08-05T01:16:55Z,Branch 129393964,"Merging internal changes.
",tensorflow
3753,benoitsteiner,pr,2016-08-11T19:26:04Z,Branch 130016968,,tensorflow
3755,benoitsteiner,pr,2016-08-11T21:36:59Z,Branch 130028402,,tensorflow
3772,benoitsteiner,pr,2016-08-12T17:03:01Z,Branch 130102794,,tensorflow
3785,benoitsteiner,pr,2016-08-12T23:56:39Z,Branch 130152575,,tensorflow
3792,darrengarvey,pr,2016-08-13T12:43:58Z,Fix Eigen-related compilation error when using -mavx2.,"Since updating to use a newer version of Eigen [1], compilation with
-mavx2 causes an error caused by an undefined type. Eigen removed
the type `scalar_multipl2_op` [2] so `scalar_product_op` should now
be used.

This commit replaces uses of `scalar_multiple2_op` similarly to how
[2] did this.

[1] See chage 127264575, commit 10211a6c.
[2] See https://bitbucket.org/eigen/eigen/commits/03556a17eb548275bc9404d7cda8303ff6ca5c13
",tensorflow
3808,LaurentMazare,pr,2016-08-14T18:46:38Z,Typo,"Fix a typo in some comment.
",tensorflow
3820,benoitsteiner,pr,2016-08-15T17:37:59Z,Branch 130271393,,tensorflow
3830,LaurentMazare,pr,2016-08-15T22:02:01Z,Add a new target including the C api.,"This is to fix issue #3814.
",tensorflow
3832,benoitsteiner,pr,2016-08-16T01:21:41Z,Branch 130325850,,tensorflow
3866,benoitsteiner,pr,2016-08-16T23:23:02Z,Branch 130458942,,tensorflow
3878,benoitsteiner,pr,2016-08-17T18:20:46Z,Branch 130529399,,tensorflow
3884,benoitsteiner,pr,2016-08-17T20:53:10Z,Branch 130554783,,tensorflow
3904,benoitsteiner,pr,2016-08-18T15:56:30Z,Branch 130640571,,tensorflow
3908,benoitsteiner,pr,2016-08-18T20:02:58Z,Branch 130665817,,tensorflow
4198,thuyen,pr,2016-09-05T02:04:10Z,remove redundancy in moment ops,"We subtracted the shift twice for both the first and second moments. Removing one subtraction slightly improves batch_norm performance. 
",tensorflow
4316,gokceneraslan,pr,2016-09-10T13:07:21Z,WMT en-fr training corpus filename change,"`giga-fren.release2.fr.gz` file has been renamed to `giga-fren.release2.fixed.fr.gz`
",tensorflow
4389,thuyen,pr,2016-09-15T04:19:07Z,correct batch_norm docstring,"In response to #4361 
",tensorflow
4572,kashif,pr,2016-09-25T10:08:29Z,upgrade protobuf to 3.1.0,,tensorflow
4598,kashif,pr,2016-09-27T11:22:03Z,get EIGEN_VERSION from workspace.bzl,"perhaps a better fix for #4575
",tensorflow
4702,LaurentMazare,pr,2016-10-01T10:49:09Z,Fix typo in comment.,,tensorflow
4845,benoitsteiner,pr,2016-10-08T14:20:17Z,Fixed a compilation error with gcc 6.2,,tensorflow
4891,thuyen,pr,2016-10-11T11:25:44Z,Add fused_batch_norm layer,"Add data_format option for convolution2d, bias_add, max_pool2d, and avg_pool2d as well. This helps to fully utilize the cudnn speed for normal convnets with 'NCHW' data layout. 
",tensorflow
4892,thuyen,pr,2016-10-11T11:35:37Z,Add fused_batch_norm layer,"Add data_format option for convolution2d, bias_add, max_pool2d, and avg_pool2d as well. This helps to fully utilize the cudnn speed for normal convnets with 'NCHW' data layout.
",tensorflow
4919,thuyen,pr,2016-10-12T18:52:53Z,Fixed fused_batch_norm gradients,"Fixed issues with fused_batch_norm as discussed in #4899, added support to `2D` tensor for fused_batch_norm and added `data_format` option for `bias_add`. 

Also the `nn.fused_batch_norm` op should be able to take minimum `epsilon=1e-5`. This value is set as  mininum value in `cudnn` and has been used in several models in `slim.nets`. At the moment `nn.fused_batch_norm` throw `CUDNN_STATUS_BAD_PARAM` when `epsilon=1e-5`, probably due to the numerical/casting error. This pull added `epsilon=epsilon+1e-12` in `nn.fused_batch_norm` (`1e-5 + 1e-12` works but `1e-5 + 1e-13` doesn't).
",tensorflow
4986,thuyen,pr,2016-10-15T11:23:14Z,Add data_format option to convolution2d_transpose,"Add `data_format` option to `convolution2d_transpose`. 
",tensorflow
5072,mkolod,pr,2016-10-19T19:17:39Z,Use fast IDCT for JPEG decoding by default,"Please refer to issue #4833 for details.
",tensorflow
5164,tillahoffmann,pr,2016-10-24T11:17:45Z,Orthogonal initializer,"This PR adds an orthogonal initializer based on the paper [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/abs/1312.6120).
",tensorflow
5224,zheng-xq,pr,2016-10-26T23:29:24Z,Branch 137329621,"Merging internal changes. 
",tensorflow
5232,kashif,pr,2016-10-27T09:37:55Z,added python tools to pip_package rules,"for issue #5014
",tensorflow
5248,zheng-xq,pr,2016-10-27T23:29:16Z,Branch 137452948,"Merging internal changes.
",tensorflow
5267,benoitsteiner,pr,2016-10-28T23:01:43Z,Initial support for OpenCL,,tensorflow
5270,zheng-xq,pr,2016-10-29T04:17:22Z,Branch 137581645,,tensorflow
5329,tillahoffmann,pr,2016-11-01T15:56:15Z,Hessian with respect to one-dimensional tensors,This PR adds an operation to compute the Hessian with respect to one-dimensional tensors.,tensorflow
5356,tillahoffmann,pr,2016-11-02T17:00:19Z,Add log1p,This PR adds the `log1p` function and partially addresses #3682. It also uses `log1p` in the computation of the sigmoid cross entropy to improve numerical stability (I had issues with rare events).,tensorflow
5359,Randl,pr,2016-11-02T17:53:28Z,Replace expunge with expunge_async in bazel clean,https://github.com/tensorflow/tensorflow/issues/5357,tensorflow
5454,benoitsteiner,pr,2016-11-07T16:24:53Z,Branch 138387835,,tensorflow
5461,benoitsteiner,pr,2016-11-07T19:14:25Z,Branch 138409704,,tensorflow
5469,benoitsteiner,pr,2016-11-08T01:59:48Z,Branch 138426641,,tensorflow
5489,benoitsteiner,pr,2016-11-09T03:02:17Z,Branch 138590602,,tensorflow
5499,benoitsteiner,pr,2016-11-09T17:17:52Z,Branch 138642972,,tensorflow
5508,benoitsteiner,pr,2016-11-10T01:42:07Z,Branch 138702302,,tensorflow
5526,tillahoffmann,pr,2016-11-10T19:49:19Z,Add support for broadcasting shapes.,"This PR adds functionality to `TensorShape` to allow for broadcasting. In particular,

* `TensorShape.broadcast_with(other)` returns the shape resulting from broadcasting a tensor of the given shape with a tensor of shape `other`. It raises a `ValueError` if the shapes are not broadcastable.
* `TensorShape.is_broadcastable_with(other)` determines whether two shapes are broadcastable
* `TensorShape.assert_is_broadcastable_with(other)` asserts that two shapes are broadcastable",tensorflow
5528,gokceneraslan,pr,2016-11-10T20:32:45Z,Add maxout op to tf.contrib.layers,,tensorflow
5529,benoitsteiner,pr,2016-11-10T21:33:23Z,Branch 138793949,,tensorflow
5542,darrengarvey,pr,2016-11-11T11:47:09Z,"word2vec: Create directories for saving summaries, if necessary.",Simple change to allow passing a directory that doesn't exist to `--save_path` in the word2vec examples.,tensorflow
5547,mkolod,pr,2016-11-11T17:25:20Z,optional libjpeg-turbo support,This is in reference to #4807.,tensorflow
5558,thuyen,pr,2016-11-11T23:43:27Z,Add string tensor tags to new summary interface,"Old/Deprecated summary ops can accept `string tensors` for `tags`. The new summary interface only allows `strings` for `names` (subsequently used as tags). This pull adds `prefix` option, which can be a `string tensor`. 

This is important because sometimes we want to define the tags symbolically based on some conditions of input placeholders. ",tensorflow
5560,benoitsteiner,pr,2016-11-12T02:21:50Z,Branch 138939121,,tensorflow
5565,benoitsteiner,pr,2016-11-12T18:24:38Z,Branch 138943977,,tensorflow
5573,b0noI,pr,2016-11-13T03:57:54Z,Checks that quantized Tensor's elements have type tuple.,"Fix for the issue #5568 

",tensorflow
5641,darrengarvey,pr,2016-11-16T13:12:41Z,"Fix projector_api_test, ensuring it runs.","This test was missing a call to tf.test.main(), so the test wasn't being
run.

This commit also fixes the test with s/embedding/embeddings/.",tensorflow
5645,gokceneraslan,pr,2016-11-16T16:48:14Z,contrib/layers: Make integer check Python 3 compatible,Fixes `NameError: name 'long' is not defined` error thrown in Python 3.,tensorflow
5647,benoitsteiner,pr,2016-11-16T17:09:26Z,Improved support for OpenCL devices,,tensorflow
5653,benoitsteiner,pr,2016-11-16T23:56:10Z,Fixed several bugs in the OpenCL code,,tensorflow
5687,b0noI,pr,2016-11-18T04:38:42Z,tf.const now supports verification of a shape of values. ,"If shape that is passed via the argument is not consistent with the value in the shape variable ValueError will be raised. Example:

""Expected Tensor's shape: (2, 1), got (1, 2)""",tensorflow
5689,darrengarvey,pr,2016-11-18T11:18:26Z,Ensure stochastic_tensor_test runs.,"stochastic_tensor_test.py was missing a call to tf.test.main() so the
test wasn't actually running. This commit fixes that and the test
passes.",tensorflow
5712,b0noI,pr,2016-11-19T07:23:54Z,Documentation for NowSeonds and NowMillis methods have been updated.,"Unlike the ""NowMillis"", the new method guarantees to return the time since the epoch. This is the fix for #5682 . Also, the fix should unblock the implementation of the feature request: #2076 .",tensorflow
5727,b0noI,pr,2016-11-20T04:54:04Z,Fix for the code in the tf.cond example.,"Small problem, however without the fix it creates some confusion when copy/pasting code to the datalab Notebook (or any other place).",tensorflow
5750,b0noI,pr,2016-11-21T16:34:03Z,Default optimizer initialization in the LinearRegressor fix,Default optimizer of the LinearRegressor should be set only if the users have not provided custom optimizer.,tensorflow
5778,tillahoffmann,pr,2016-11-22T08:31:19Z,Add maximize method to Optimizer.,This PR adds the convenience method `maximize` to the `Optimizer` class.,tensorflow
5810,benoitsteiner,pr,2016-11-23T14:51:22Z,Improved support for OpenCL,,tensorflow
6009,darrengarvey,pr,2016-12-01T13:49:25Z,Use correct variable scope for weights in rnn_cell._linear.,"If a scope is passed in it should be used for the variable scope
rather than always falling back to the default scope.

This change in behaviour was a consequence of recent cleanup in
92da8abf. The commit may alter the name of ""weights"" variable in saved
checkpoints but is necessary if two `rnn_cell._linear` layers exist in
the same parent variable scope.",tensorflow
6028,alsrgv,pr,2016-12-01T21:09:23Z,slim.learning.train and slim.evaluation.evaluation to handle OutOfRange gracefully,"While doing this fix, I noticed that some of the metric operations don't handle OutOfRange gracefully also.  

For example, streaming mean computation would update total value and count concurrently, possibly updating count before failing to update total value due to OutOfRange. 

I have added fixes for mean metrics, but I am not sure what other places might be handling OutOfRange incorrectly.",tensorflow
6051,benoitsteiner,pr,2016-12-03T00:49:52Z,Branch 140903864,,tensorflow
6066,rdipietro,pr,2016-12-04T11:27:00Z,Fix rnn cell scope bug in _linear,"The optional `scope` argument of `_linear` is currently ignored, as explained here: https://github.com/tensorflow/tensorflow/issues/6065

This pull request should fix that bug.",tensorflow
6161,benoitsteiner,pr,2016-12-07T15:48:53Z,Improved support for OpenCL,,tensorflow
6246,benoitsteiner,pr,2016-12-10T22:14:44Z,Added support for OpenCL to ApplyAdamOp,,tensorflow
6257,yaroslavvb,pr,2016-12-11T23:59:37Z,replace AsProtoField with AsProtoTensorContent for efficiency,This improves throughput of gRPC worker->client fetch about 25% (116 MB/s -> 151 MB/s) as measured by benchmark in https://github.com/tensorflow/tensorflow/issues/6256,tensorflow
6323,benoitsteiner,pr,2016-12-14T23:24:19Z,Added support for AVX512 to fixed point instructions.,,tensorflow
6371,benoitsteiner,pr,2016-12-17T00:31:13Z,Improved support for OpenCL devices,,tensorflow
6403,benoitsteiner,pr,2016-12-19T17:14:02Z,Improved OpenCL support,,tensorflow
6447,benoitsteiner,pr,2016-12-21T23:52:47Z,Improved support for OpenCL,,tensorflow
6485,yaroslavvb,pr,2016-12-24T18:04:59Z,Remove Double from supported types for Conv2D since no double implemeâ€¦,â€¦ntation is registered for any device. Came up in http://stackoverflow.com/questions/41310637/no-opkernel-was-registered-to-support-op-conv2d-with-these-attrs,tensorflow
6516,rasbt,pr,2016-12-27T07:15:54Z,fix spelling/insert correct reference to udacity notebook,This pull request corrects the spelling of the notebook *notmist.ipynb* -> `1_notmnist.ipynb` in notebook `3_regularization.ipynb`.,tensorflow
6526,yaroslavvb,pr,2016-12-27T20:10:05Z,"Update release to document breaking change 141725777, fixes #6202",,tensorflow
6562,tillahoffmann,pr,2016-12-29T16:02:14Z,More numerically stable gradients for division.,"This PR improves the numerical stability of gradients for the division operator. 

Let `f = x / y` such that `df/dy = - x / y ** 2`. But if `y` is of small magnitude, `y ** 2` can underflow, and if `y` is of large magnitude, `y ** 2` can overflow. It is thus better to compute the derivative as `df/dy - (x / y) / y`. The changes are illustrated in the figure below using 32-bit floating point representations.

![divgrad](https://cloud.githubusercontent.com/assets/966348/21547770/0fcae1f8-cde0-11e6-9090-524f24bff1a1.png)

The coverage of the new implementation is strictly greater than the coverage of the old implementation, where coverage is the domain in which the gradient is defined.",tensorflow
6655,yaroslavvb,pr,2017-01-05T04:27:10Z,fix zlib dependency (current one fails with 403 on ./configure),"Looks like 1.2.8 got deleted, ./configure fails with 
ERROR: /local_home/yaroslav/tensorflow_dbg.git/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /local_home/yaroslav/.cache/bazel/_bazel_yaroslav/687fadd894268346c74cc86e6f287d8c/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.
",tensorflow
6657,yaroslavvb,pr,2017-01-05T04:39:03Z,fix zlib dependency (current one fails with 403 on ./configure),"Looks like 1.2.8 got deleted, ./configure fails with
ERROR: /local_home/yaroslav/tensorflow_dbg.git/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /local_home/yaroslav/.cache/bazel/_bazel_yaroslav/687fadd894268346c74cc86e6f287d8c/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.",tensorflow
6710,yaroslavvb,pr,2017-01-07T02:38:10Z,Update jpeg.BUILD,Fixes #6706,tensorflow
6792,kashif,pr,2017-01-11T16:39:14Z,fix documentation of center argument in BN layers,fixes #6736,tensorflow
6810,jihunchoi,pr,2017-01-12T16:20:36Z,Update LSTMBlockCell to use LSTMStateTuple as state,Fixes #6582,tensorflow
6813,yaroslavvb,pr,2017-01-12T17:36:30Z,Fix wrong order for op_name/summary_name in range_input_producer,Fixes #6808 ,tensorflow
6818,mkolod,pr,2017-01-13T00:52:21Z,GPU impl. of hue adjustment op,This is related to issue #6817. Mentioning @zheng-xq since he implemented the fused CPU hue adjustment kernel.,tensorflow
6830,jihunchoi,pr,2017-01-13T13:39:27Z,Update einsum to check whether uncompacted_shape has multiple None values,"Fixes #6824.
It checks `uncompacted_shape`, which contains the 'final' shape of `matmul` of two tensors, if it contains two or more `None` values.
If `uncompacted_shape` contains two or more `None` values, it means that it cannot be directly given to `reshape` as parameter.
This PR resolves this issue, by querying the dimension value using `tf.shape` function for that case.",tensorflow
6835,yaroslavvb,pr,2017-01-13T18:16:43Z,update Defun doc to clarify that definitions are frozen at first .run call,Fixes #6804 ,tensorflow
6853,yaroslavvb,pr,2017-01-14T19:56:09Z,Propagate seed in parallel_read to readers. Fixes #6735,,tensorflow
6864,darrengarvey,pr,2017-01-15T17:44:38Z,Add ProfilerHook for capturing CPU/GPU profiling information with MonitoredSession,"It would be good to see if this is the preferred way to get profiling when using tf.contrib.training. If so, I can add the missing tests and doc updates.",tensorflow
6868,yaroslavvb,pr,2017-01-16T04:01:30Z,Remove overly restrictive check on gradient types.,"Fixes #6858 

Currently gradient computation `input.is_compatible_with(backprop)` for every input. This fails if we have float16 activations and float32 backprops (ie `tf.float32.is_compatible_with(tf.float16)` returns False), this PR removes this check.",tensorflow
6909,yaroslavvb,pr,2017-01-17T19:06:33Z,Use local variable for match_input_once to avoid saving it in checkpoint,Fixes #6786 ,tensorflow
6918,benoitsteiner,pr,2017-01-18T02:58:59Z,Improved support for libxsmm,,tensorflow
6941,mkolod,pr,2017-01-18T21:34:11Z,Add pre-compilation of Pascal binaries (CC 6.0/6.1),"See issue #6914.

Mentioning @vrv since he gave the go-ahead for this.",tensorflow
6960,benoitsteiner,pr,2017-01-19T17:20:06Z,Improved support for libxsmm,,tensorflow
6990,yaroslavvb,pr,2017-01-21T03:11:08Z,make tensorflow/contrib/layers:feature_column_ops_test medium to fix â€¦,"â€¦Jenkins test timeout.

IE, happened when testing PR here
https://github.com/tensorflow/tensorflow/pull/6909",tensorflow
7018,yaroslavvb,pr,2017-01-23T20:07:55Z,"replace is_real with is_floating, remove is_real","@martinwicke 

Haven't tested (because of https://github.com/tensorflow/tensorflow/issues/6911 testing time goes 1 min -> 20 mins every time you switch branch)",tensorflow
7113,benoitsteiner,pr,2017-01-27T17:42:30Z,Upgraded to libxsmm 1.7,,tensorflow
7120,zheng-xq,pr,2017-01-27T23:46:57Z,Branch 145839627,Merging internal changes. ,tensorflow
7153,zheng-xq,pr,2017-01-30T23:15:29Z,Branch 146039928,Merging internal changes.,tensorflow
7155,zheng-xq,pr,2017-01-31T03:48:27Z,Branch 146066214,Merging internal changes,tensorflow
7197,yaroslavvb,pr,2017-02-01T22:06:34Z,Add microsecond timestamps to LOG messages,Fixes #2076,tensorflow
7202,yaroslavvb,pr,2017-02-02T00:30:20Z,increase size of nccl_manager_test,"This test timed out when testing https://github.com/tensorflow/tensorflow/pull/7197 , increasing size small->medium",tensorflow
7265,lezcano,pr,2017-02-05T00:20:01Z,Errata in mnist.py,Part of the images and labels were not selected from the shuffled array.,tensorflow
7279,byronyi,pr,2017-02-06T04:31:25Z,resolve #6762 on ldconfig only available on root PATH,"See Linux man page for [ldconfig(8)](https://linux.die.net/man/8/ldconfig), synopsis section:

```bash
/sbin/ldconfig [ -nNvXV ] [ -f conf ] [ -C cache ] [ -r root ] directory ...
/sbin/ldconfig -l [ -v ] library ...
/sbin/ldconfig -p
```

Former pull request #6848 has been closed as @jowagner has not signed a CLA.",tensorflow
7298,benoitsteiner,pr,2017-02-06T18:39:13Z,Branch 146677928,,tensorflow
7335,yaroslavvb,pr,2017-02-07T19:34:02Z,Fix typos in graph_editor add_control_inputs.,@purpledog,tensorflow
7339,benoitsteiner,pr,2017-02-07T20:55:23Z,Branch 146817322,,tensorflow
7342,mkolod,pr,2017-02-07T21:25:31Z,Float indexing test fix for Numpy >= 1.12+,"This fixes test failure for NumPy 1.12 that does not allow float indexing of arrays. Of course, it makes sense to index arrays using ints anyway, regardless of NumPy support. I assume this wasn't causing problems to people with NumPy up to 1.11.3, but it's better not to assume a specific NumPy version.",tensorflow
7346,benoitsteiner,pr,2017-02-08T00:41:12Z,Branch 146845645,,tensorflow
7387,jihunchoi,pr,2017-02-09T14:21:34Z,Update shape checking logic in einsum,"In #6830, I modified einsum function to check whether new shape has two or more unknown dimensions.
However, according to the comment on #6824, it is proven to be not sufficient to check only the last reshape call.
This PR fixes the issue.",tensorflow
7396,benoitsteiner,pr,2017-02-09T19:08:41Z,Branch 147051664,,tensorflow
7401,benoitsteiner,pr,2017-02-10T00:13:35Z,Branch 147093777,,tensorflow
7418,benoitsteiner,pr,2017-02-10T16:25:11Z,Branch 147149282,,tensorflow
7527,byronyi,pr,2017-02-15T13:25:11Z,Fix #7328 by checking if socket address is valid,"Not sure if it's the most standard compliant way for both platforms, but I've tried my best.

Some references: [POSIX](http://www.unix.com/man-page/POSIX/3posix/getaddrinfo/) and [Windows](https://msdn.microsoft.com/en-us/library/windows/desktop/ms738520(v=vs.85).aspx).",tensorflow
7596,benoitsteiner,pr,2017-02-16T23:45:02Z,OpenCL Improvements,,tensorflow
7676,yaroslavvb,pr,2017-02-19T20:09:11Z,Add .shape property to Variable object,also adjust docstrings for consistency with Tensor.shape/Tensor.get_shape ,tensorflow
7724,malmaud,pr,2017-02-21T04:10:34Z,Add TF_CONTROL_SLOT to c_api,"This lets users of `TF_ImportGraphDefOptionsAddInputMapping` remap control inputs without having to guess the magical value of the `kControlSlot` constant. 

cf #7508",tensorflow
7820,benoitsteiner,pr,2017-02-23T17:21:40Z,Improved support for AVX512.,"Added missing definition for the Packet16q16i.
Fixed a couple of bugs in the implementation of max reductions for
avx512",tensorflow
7888,benoitsteiner,pr,2017-02-25T22:27:01Z,Added missing defines for AVX512 packets,,tensorflow
7903,benoitsteiner,pr,2017-02-26T19:44:53Z,Deleted unused private field.,,tensorflow
8075,rasbt,pr,2017-03-04T06:06:05Z,Update the github docs link in the error msg upon import failure,"Hi,
since the docs have moved from `g3doc` to `docs_src` (and have been refactored), I thought it would be good to update the link in the error message that comes up upon import failures.",tensorflow
8169,MicaelCarvalho,pr,2017-03-07T14:14:10Z,Closes #8165 -- Removing traces of os_setup.md,,tensorflow
8358,MicaelCarvalho,pr,2017-03-13T13:34:44Z,Fixing function path for 'bow_encoder' -- Closes #8353,,tensorflow
8597,yaroslavvb,pr,2017-03-21T20:54:01Z,fix description of tf.switch outputs,,tensorflow
8598,yaroslavvb,pr,2017-03-21T20:57:59Z,"docfix, reverse switch outputs",,tensorflow
8599,yaroslavvb,pr,2017-03-21T21:00:55Z,fix documentation of tf.switch outputs,,tensorflow
8601,benoitsteiner,pr,2017-03-21T21:13:30Z,Import the Ops Research tools in TensorFlow as well as their dependencies,,tensorflow
8728,tillahoffmann,pr,2017-03-26T18:07:43Z,PollZmqOp for network communication,"This PR adds `PollZmqOp` for retrieving data over the network (fixes #4836, ref #7951) . A few points for discussion:

* ZeroMQ is LGPL but has some exceptions which seem to allow static linking. Is that a problem?
* The `Compute` method of the Op is ensured to be thread safe by blocking. This could be improved by having a dictionary of sockets keyed by thread id or a connection pool. However, I have never needed concurrent calls to `Compute` in my work.
* I'm not very familiar with `bazel` so there may be some problems in the dependencies.

This op allows for distributed training data generation to minimize the CPU workload on the training machine. For example, one or more machines can generate training data which can be retrieved by a training machine using the op. An example can be found in the test.",tensorflow
8849,benoitsteiner,pr,2017-03-30T17:28:54Z,Upgraded libxsmm to version 1.8,,tensorflow
8929,benoitsteiner,pr,2017-04-03T16:48:36Z,Enabled the generation of big kernels by libxsmm,,tensorflow
9029,tillahoffmann,pr,2017-04-06T20:23:42Z,CPU kernels for FFT (WIP),"This PR adds CPU kernels for FFTs (cf #386). A few points to note:

* This PR does not yet support RFFTs. These can in principle be implemented by slicing such that the negative frequency components of the FFT are removed. Maybe @benoitsteiner has some better ideas though.
* Does it make sense to split up the code into multiple files to avoid having `#if GOOGLE_CUDA` half way?
* @rryan, because the tensor FFT in Eigen is templated, I couldn't avoid bloating the binary a bit as discussed via e-mail.",tensorflow
9145,darrengarvey,pr,2017-04-11T20:03:49Z,Fix loading metadata in tensorboard embedding projector.,"Fixes loading metadata (eg. vocab files, sprites) for display in
tensorboard's embedding projector plugin.

Since [1], `project_plugin.pbtxt` is looked for under the directory
`$LOGDIR/plugins/org_tensorflow_tensorboard_projector`, different to the
docs and where `projector.visualise_embeddings()` stores the file.
Revert to the previous location.

[1] See #9059, commit 52dcb259",tensorflow
9261,freedomtan,pr,2017-04-17T07:04:03Z,a python implmentation of label_image,"A quick implementation of tensorflow/examples/label_image/main.cc
in python.

With data file described in [1], we can we get reasonable results

$ python3 ./tensorflow/examples/label_image/label_image.py
military uniform 0.834306
mortarboard 0.0218692
academic gown 0.0103579
pickelhaube 0.00800814
bulletproof vest 0.00535088

[1] https://www.tensorflow.org/tutorials/image_recognition",tensorflow
9271,bryant1410,pr,2017-04-17T16:22:54Z,Remove extra space in sample code,,tensorflow
9305,tillahoffmann,pr,2017-04-19T09:44:20Z,Add reqeueue operations.,"This PR adds `requeue` and `requeue_many` operations to `QueueBase`. The operations dequeue an element/elements from the queue, add the element(s) to the queue again, and return an op that evaluates to the dequeued value.

The motivation behind these operations is to be able to reuse training data that is hard to come by after using them in training.",tensorflow
9403,Kongsea,pr,2017-04-24T07:32:05Z,Add 3D operations for layers,"1. Add 3D operations for layers:

- conv3d
- avg_pool3d
- max_pool3d

2.Fix a TODO(jbms):
change `rate` parameter to `dilation_rate` for consistency with underlying op.",tensorflow
9404,Kongsea,pr,2017-04-24T07:51:11Z,"Add 3D operations for layers: conv3d, avg_pool3d, max_pool3d and conv3d_transpose.","1.Fix a TODO(jbms):
change rate parameter to dilation_rate for consistency with underlying op.

2.Add 3D operations for layers:

- conv3d
- avg_pool3d
- max_pool3d
- conv3d_transpose
",tensorflow
9477,Kongsea,pr,2017-04-27T05:33:24Z,"Add 3D operations for layers: conv3d, avg_pool3d and max_pool3d","1.Fix a TODO(jbms):

change 'rate' parameter to 'dilation_rate' for consistency with underlying op.

2.Add 3D operations for layers:

conv3d
avg_pool3d
max_pool3d

3.Replace a deprecated function in layers_test.py.

4.Add unit test for avg_pool3d and max_pool3d.",tensorflow
9531,byronyi,pr,2017-04-29T05:20:42Z,Avoid extra TensorReference allocation in gRPC,The original TODO comments pretty much speak for themselves.,tensorflow
9563,freedomtan,pr,2017-05-01T06:59:52Z,Add a simple BMP decoder and enable it on Android,"Add a simple BMP decoder and enable it on Android so that we can test simple image classification in command line on Android platform. On Android, image decoding is not directly available to NDK C/C++ 
programs. 

### Why bmp?
.bmp can be decoded easily without relying on external libraries.
",tensorflow
9565,freedomtan,pr,2017-05-01T10:06:24Z,fix some trivial typos,Some backslashes should not be there. They are not expected by --transforms.,tensorflow
9733,byronyi,pr,2017-05-07T02:48:08Z,Fix comments in EncodeTensorToByteBuffer,,tensorflow
9773,Kongsea,pr,2017-05-09T01:21:05Z,"Add 3D operations for layers: conv3d, avg_pool3d, max_pool3d and conv3d_transpose","1.Add 3D operations for layers:

conv3d
avg_pool3d
max_pool3d
conv3d_transpose

2.Replace a deprecated function in layers_test.py.

3.Add unit test for avg_pool3d and max_pool3d.",tensorflow
9776,benoitsteiner,pr,2017-05-09T02:32:44Z,Branch 155393864,,tensorflow
9830,freedomtan,pr,2017-05-11T07:17:41Z,There is no --logtostderror,"There is not --logtostderror option. Remove them.
And there should be no backslash in --transforms=...",tensorflow
9840,rasbt,pr,2017-05-11T15:48:37Z,Add a note about how weights are initialized in Dense/dense to the docstrings,"Hi,
this PR adds a note about the default weight initializer to the `tf.layers.Dense` and `tf.layers.dense` docstrings to clarify how the default weights are initialized as discussed in #9744 .",tensorflow
9843,benoitsteiner,pr,2017-05-11T18:19:16Z,Branch 155393864,,tensorflow
9848,benoitsteiner,pr,2017-05-11T21:58:08Z,Branch 155393864,,tensorflow
9850,benoitsteiner,pr,2017-05-12T05:47:21Z,Branch 155393864,,tensorflow
9860,benoitsteiner,pr,2017-05-12T15:23:04Z,Branch 155393864,,tensorflow
9874,benoitsteiner,pr,2017-05-13T01:55:28Z,Branch 155393864,,tensorflow
9877,freedomtan,pr,2017-05-13T10:18:01Z,"gif decoder returns 4-D tensor, remove the first dim","What returned by the gif decoder returns is a 4-D tensor [frames, height, width, channels]. Since only the single frame gif will be used in this label_image, frames should be 1. So we can use Squeeze() to remove it. 

The patch solve https://github.com/tensorflow/tensorflow/issues/8572",tensorflow
10034,freedomtan,pr,2017-05-19T16:10:32Z,Replace use of tensorflow::ops::ReadFile in label_image,"Use tensorflow::Env instead of tensorflow::ops::ReadFile so that
we can avoid including whole_file_read_ops for Android.",tensorflow
10112,benoitsteiner,pr,2017-05-22T19:30:18Z,Updated libxsmm kernels,,tensorflow
10261,taehoonlee,pr,2017-05-28T12:41:41Z,Fix typos,,tensorflow
10271,taehoonlee,pr,2017-05-29T09:14:05Z,Remove r in docstrings that do not have backslashes,,tensorflow
10309,taion,pr,2017-05-31T00:41:18Z,Fix TensorBoard demo data,"https://github.com/tensorflow/tensorflow/commit/d97706437932443ae22082ada49d3ac054b77304 suggests that the demo TensorBoard was supposed to go away entirely, but `tensorboard/DEVELOPMENT.md` still says to use it.

Better that it shows scalars and histograms than not?",tensorflow
10310,taion,pr,2017-05-31T01:07:40Z,Don't unnecessarily reset y domain on line charts,Fixes #8994,tensorflow
10379,malmaud,pr,2017-06-01T19:01:35Z,Fix typo,,tensorflow
10385,freedomtan,pr,2017-06-02T02:40:52Z,make gcc-5 on Ubuntu 16.04 happy,"gcc-5 complains of ambiguity and refuses to go when doing something
like 'bazel build -c opt tensorflow/...'

Error messages:
> ...
> ERROR: /hack/freedom/tensorflow/freedom/tensorflow/tensorflow/core/kernels/BUILD:3854:1: C++ compilation of rule '//tensorflow/core/kernels:mfcc_test' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 136 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
> In file included from external/gmock_archive/googletest/include/gtest/gtest.h:58:0,
>                  from ./tensorflow/core/platform/test.h:35,
>                  from tensorflow/core/kernels/mfcc_test.cc:18:
> tensorflow/core/kernels/mfcc_test.cc: In member function 'virtual void tensorflow::MfccTest_AvoidsNansWithZeroInput_Test::TestBody()':
> tensorflow/core/kernels/mfcc_test.cc:66:29: error: 'isnan' was not declared in this scope
>      EXPECT_FALSE(isnan(value));
>                              ^
> tensorflow/core/kernels/mfcc_test.cc:66:29: note: suggested alternatives:
> In file included from /usr/include/c++/5/complex:44:0,
>                  from ./tensorflow/core/framework/numeric_types.h:19,
>                  from ./tensorflow/core/framework/allocator.h:23,
>                  from ./tensorflow/core/framework/op_kernel.h:23,
>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,
>                  from ./tensorflow/core/kernels/mfcc.h:23,
>                  from tensorflow/core/kernels/mfcc_test.cc:16:
> /usr/include/c++/5/cmath:641:5: note:   'std::isnan'
>      isnan(_Tp __x)
>      ^
> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:536:0,
>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
>                  from ./tensorflow/core/framework/numeric_types.h:21,
>                  from ./tensorflow/core/framework/allocator.h:23,
>                  from ./tensorflow/core/framework/op_kernel.h:23,
>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,
>                  from ./tensorflow/core/kernels/mfcc.h:23,
>                  from tensorflow/core/kernels/mfcc_test.cc:16:
> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GlobalFunctions.h:88:3: note:   'Eigen::isnan'
>    EIGEN_ARRAY_DECLARE_GLOBAL_UNARY(isnan,scalar_isnan_op,not-a-number test,\sa Eigen::isinf DOXCOMMA Eigen::isfinite DOXCOMMA ArrayBase::isnan)
>    ^
> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:371:0,
>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
>                  from ./tensorflow/core/framework/numeric_types.h:21,
>                  from ./tensorflow/core/framework/allocator.h:23,
>                  from ./tensorflow/core/framework/op_kernel.h:23,
>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,
>                  from ./tensorflow/core/kernels/mfcc.h:23,
>                  from tensorflow/core/kernels/mfcc_test.cc:16:
> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/MathFunctions.h:1108:46: note:   'Eigen::numext::isnan'
>  template<typename T> EIGEN_DEVICE_FUNC bool (isnan)   (const T &x) { return internal::isnan_impl(x); }
>                                               ^
> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:411:0,
>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
>                  from ./tensorflow/core/framework/numeric_types.h:21,
>                  from ./tensorflow/core/framework/allocator.h:23,
>                  from ./tensorflow/core/framework/op_kernel.h:23,
>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,
>                  from ./tensorflow/core/kernels/mfcc.h:23,
>                  from tensorflow/core/kernels/mfcc_test.cc:16:
> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/Half.h:372:45: note:   'Eigen::half_impl::isnan'
>  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isnan)(const half& a) {
>                                              ^
> In file included from /usr/include/c++/5/complex:44:0,
>                  from ./tensorflow/core/framework/numeric_types.h:19,
>                  from ./tensorflow/core/framework/allocator.h:23,
>                  from ./tensorflow/core/framework/op_kernel.h:23,
>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,
>                  from ./tensorflow/core/kernels/mfcc.h:23,
>                  from tensorflow/core/kernels/mfcc_test.cc:16:
> /usr/include/c++/5/cmath:641:5: note:   'std::isnan'
>      isnan(_Tp __x)
> ",tensorflow
10386,taehoonlee,pr,2017-06-02T05:01:40Z,Fix typos,,tensorflow
10387,freedomtan,pr,2017-06-02T06:54:26Z,Update the instruction for building a minimal xla benchmark,"1. the instruction for building lstm_layer_inference_benchmark doesn't work.
2. the ""cc_target_os"" in ""android_armeabi"" is an unknown option
",tensorflow
10417,darrengarvey,pr,2017-06-03T15:15:38Z,configure: Fix default path when enabling MPI.,A minor fixup. Correct showing what the default path is when mpi is installed.,tensorflow
10469,fritzo,pr,2017-06-06T17:03:05Z,Export C API symbols in _pywrap_tensorflow_internal.so,"This PR exports C API symbols so that python extension libraries can use the C API.

As noted in #7541, there is currently a conflict between `_pywrap_tensorflow_internal.so` and `libtensorflow.so`. This conflict prevents use of the tensorflow C API and the python API in the same process. This PR attempts to implement a workaround suggested by @jhseu by exporting the C API symbols in the python library, so that the Python and C APIs are both available from the single `_pywrap_tensorflow_internal.so`.",tensorflow
10533,taehoonlee,pr,2017-06-08T13:19:56Z,Fix typos,,tensorflow
10571,wangkuiyi,pr,2017-06-08T17:46:58Z,Remove an unused typedef,,tensorflow
10627,taehoonlee,pr,2017-06-11T05:24:30Z,Fix typos,,tensorflow
10629,byronyi,pr,2017-06-11T07:25:55Z,[WIP] Multi-bus support with NUMA-aware allocator,"As the title suggests, this is a WIP to partially address issues brought by @poxvoculi in #5986. 

In our testbed, we have a multi-bus topology where 4 K40m GPUs are connected by different PCI-e root, as shown with the following command:

```
$ nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	mlx4_0	CPU Affinity
GPU0	 X 	PHB	SOC	SOC	SOC	0-5
GPU1	PHB	 X 	SOC	SOC	SOC	0-5
GPU2	SOC	SOC	 X 	PHB	PHB	6-11
GPU3	SOC	SOC	PHB	 X 	PHB	6-11
mlx4_0	SOC	SOC	PHB	PHB	 X

Legend:

  X   = Self
  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
```

Current TF always allocates the host memory used to transfer from/to GPU on NUMA node 0, which is sub-optimal for GPUs with other CPU affinity. This could be further validated by adjusting `CUDA_VISIBLE_DEVICES` and `numactl -m <numa_node> -N <numa_node>` prepended to TF process.

The basic idea is simple: use ``numa_alloc_onnode`` and other related functions available in [libnuma](https://linux.die.net/man/3/numa) to allocate memory for CUDA host allocator. There are some pieces of code, e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L669), [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L951), and [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L855) to identify bus id and numa node for each device, and I plan to reuse those as much as possible.

I am wondering if anyone in the TF team could give me some advice so I can start to implement it.",tensorflow
10657,taion,pr,2017-06-12T17:31:11Z,Lazily configure TensorFlow logger,Fixes #10498,tensorflow
10666,taehoonlee,pr,2017-06-13T04:33:20Z,Fix typos,"This PR fixes some typos: `the the`, `Classs`, `classs`, `currrently`, and `apppropriate`.",tensorflow
10670,guillaumekln,pr,2017-06-13T09:00:14Z,Fix minor typos in lookup_ops code samples.,,tensorflow
10753,taehoonlee,pr,2017-06-16T02:33:53Z,Fix typos,"This PR revises the sample usages for lookup, and fixes some typos: `contants`, `Opeartor`, `squeee`, `nulll`, `Contant`, and `Tranpose`.",tensorflow
10830,tillahoffmann,pr,2017-06-19T18:11:26Z,Performance improvements for hessians.,"This PR improves the performance of `hessians` by using `while_loop` instead of using a `for` loop in python. This improves both the build time (to create the graph) and the evaluation time. Full details are [here](https://gist.github.com/tillahoffmann/003f3bcb9639a18253ec7854abcbea01).

![image](https://user-images.githubusercontent.com/966348/27299092-06e6dc52-5523-11e7-84ff-5972d7490a89.png)
",tensorflow
10841,taehoonlee,pr,2017-06-20T02:07:27Z,Fix references,This PR fixes deprecated URLs.,tensorflow
10967,taehoonlee,pr,2017-06-22T05:03:44Z,Fix typos,"This PR fixes some typos: `intialized`, `be be`, `by by`, `in in`, `tranformation`, `new new`, `constaint`, and `mean_square_error`.",tensorflow
11050,taehoonlee,pr,2017-06-26T01:10:54Z,Fix typos,"This PR fixes some typos: `as as`, `is is`, `for for`, `not not`, and `are are`.",tensorflow
11079,Kongsea,pr,2017-06-27T08:37:22Z,Fix some typos in doc strings of Conv3D of keras.,,tensorflow
11100,taehoonlee,pr,2017-06-28T06:46:32Z,Fix typos,"This PR fixes some typos: `to to`, `of of`, `that that`, and `this this`.",tensorflow
11125,Kongsea,pr,2017-06-29T01:26:45Z,Fix typos,,tensorflow
11128,taehoonlee,pr,2017-06-29T04:35:57Z,Make consistent author information format,This PR makes formatting consistent. All the other formats for author information are described as `et al.` not `et. al.`.,tensorflow
11129,taehoonlee,pr,2017-06-29T05:01:08Z,Fix typos,"This PR fixes some typos: `succesfully`, `optimzations`, `unecessary`, `accross`, `trainning`, `beacuse`, and `dont`.",tensorflow
11161,Kongsea,pr,2017-06-30T02:21:15Z,Fix an error,Fix an error or the if statement will never be true.,tensorflow
11166,taehoonlee,pr,2017-06-30T06:21:30Z,Fix typos,"This PR fixes some typos: `the the`, `Dont`, `Initalized`, `Dimenson`, and `resuts`.",tensorflow
11167,taehoonlee,pr,2017-06-30T07:57:32Z,Improve docstrings involving package structure,"This PR improves docstrings involving `tf.learn`. In the case of `tensorflow_dataframe.py`, I think it is better to put just `DataFrame` in order to keep consistency within one file. Except for the case, all the `tf.learn`s are replaced by `tf.contrib.learn`s.",tensorflow
11234,guoyejun,pr,2017-07-03T06:40:47Z,add a new config option sycl_nodouble for SYCL build,"When TF is built with SYCL enabled, the SYCL device code is generated
at build time. Currently, all the data types such as float and double
are registered to generate the device code.

The SYCL device code is compiled into SPIR at build time, and then
passed to OpenCL implemenation at runtime. Since double precision is
an optional feature in the OpenCL spec, it is possible that an OpenCL
implemenation does not support double.

To make some platforms without double support work, this new config
option disables double register for SYCL device code.

This patch just changes the cwise_add operation as an example, and
other operations will be changed in future small patches one by one.",tensorflow
11261,byronyi,pr,2017-07-04T04:54:47Z,Fix TODO avoiding serialization in gRPC/GPU path,See #10530 and #10531 for the rationale.,tensorflow
11288,taehoonlee,pr,2017-07-05T05:14:48Z,Fix typos,"This PR fixes some typos: `parition`, `partiton`, `grpah`, `excuted`, `definitons`, and `operaiton`.",tensorflow
11346,taehoonlee,pr,2017-07-07T08:55:52Z,Improve examples for Python 3 compatibility,This PR improves examples in docstrings for Python 3 compatibility.,tensorflow
11392,byronyi,pr,2017-07-09T11:33:03Z,GPUDirect RDMA Out-of-Band Tensor Transport,"Introduction
===

This PR implements GDR out-of-band transport for TensorFlow distributed runtime, complementary to current gRPC transport. It uses gRPC as control plane to setup rendezvous for each tensor transmission, and utilizes [GPU Direct RDMA](https://developer.nvidia.com/gpudirect) whenever possible to transmit tensors in remote GPU memory through network interface card (NIC), bypassing host memory and CPU entirely. It gracefully falls back to ordinary RDMA or even gRPC when GDR is not available.

Design
===

The GDR out-of-band transport is designed to avoid any unnecessary memory copies, especially for large tensors (>100MB). That typically requires registration of tensor buffers to NIC on the fly, which is rather slow as described in the design trade-off of the verbs runtime. The verbs runtime thus chooses to manage its own NIC-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.

We show that, however, such design trade-off is not always relevant. In this patch, we manage both computation and communication buffers in a unified manner. By pre-registration of large buffers to NIC and allocating small tensors from the buffer pool using a BFC allocator, it is possible to avoid both buffer registration on the fly and memory copies all together.

For the actual tensor transport, we rely on gRPC to transmit the remote buffer information. This greatly simplifies our design, and there are only 2 types of RDMA messages: a single READ to retrieve the tensor data (bypassing remote CPU), and another invalidate using WRITE with IMM to release the tensor buffer on the remote side. The remote side will only be polling the invalidate message and `Unref` the tensor buffers that read by its peer.

Environment
===

To fully utilize GDR, the target environment has to meet 3 conditions:

1. There is an RDMA capable device with corresponding [OFED package](https://www.openfabrics.org/index.php/overview.html) installed (detailed information is available from your [Infiniband/RoCE](http://www.mellanox.com/page/products_dyn?product_family=116)/[iWarp](http://www.chelsio.com/gpudirect-rdma/) vendor), which could be verified through `ibv_devinfo`, e.g.

```
$ ibv_devinfo
hca_id:	mlx4_0
	transport:			InfiniBand (0)
	fw_ver:				2.40.7000
	node_guid:			248a:0703:00f6:3370
	sys_image_guid:			248a:0703:00f6:3370
	vendor_id:			0x02c9
	vendor_part_id:			4099
	hw_ver:				0x1
	board_id:			MT_1090110023
	phys_port_cnt:			2
	Device ports:
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		1024 (3)
			sm_lid:			0
			port_lid:		0
			port_lmc:		0x00
			link_layer:		Ethernet

		port:	2
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		1024 (3)
			sm_lid:			0
			port_lid:		0
			port_lmc:		0x00
			link_layer:		Ethernet
```

2. There is a GDR capable GPU, i.e. of Fermi, Kepler or later architecture with [corresponding driver](http://docs.nvidia.com/cuda/gpudirect-rdma/index.html) installed. The PCI-e topology could be confirmed by `nvidia-smi topo -m`. For example, in the following topology, `GPU2` and `GPU3` are adjacent to `mlx4_0`, and tensors on these devices could benefit from GDR in current implementation.

```
$ nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	mlx4_0	CPU Affinity
GPU0	 X 	PHB	SOC	SOC	SOC	0-5
GPU1	PHB	 X 	SOC	SOC	SOC	0-5
GPU2	SOC	SOC	 X 	PHB	PHB	6-11
GPU3	SOC	SOC	PHB	 X 	PHB	6-11
mlx4_0	SOC	SOC	PHB	PHB	 X

Legend:

  X   = Self
  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
```

3. The [`nv_peer_mem`](https://github.com/Mellanox/nv_peer_memory) kernel module is installed.

How to build and run in GDR mode
===

To test it out on a GDR capable environment, choose to enable GDR in your configure script.

```
Do you wish to build TensorFlow with GDR support? [y/N]: y
GDR support will be enabled for TensorFlow.
```

Change your `protocol` to `grpc+gdr` to enable GDR in your deployment.

```
server = tf.train.Server(cluster, job_name=""local"", task_index=0, protocol='grpc+gdr') # default protocol is 'grpc'
```

Currently the out-of-band transport service listens to the same IP and port address as specified in gRPC.

A successful initialization looks like this:

```
2017-08-05 19:10:38.601718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0)
2017-08-05 19:10:38.601728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0)
2017-08-05 19:10:38.601736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0)
2017-08-05 19:10:38.601742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0)
2017-08-05 19:10:39.591026: I tensorflow/contrib/gdr/gdr_memory_manager.cc:235] RDMA server is listening on 10.40.2.200:5001
2017-08-05 19:10:39.591071: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cuda_host_bfc
2017-08-05 19:10:39.591083: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_pool
2017-08-05 19:10:39.591095: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_rdma_bfc
2017-08-05 19:10:39.591278: I tensorflow/contrib/gdr/gdr_memory_manager.cc:78] NUMA node for device: mlx4_0 is 1
2017-08-05 19:10:39.740253: I tensorflow/contrib/gdr/gdr_memory_manager.cc:296] Instrumenting GPU allocator with bus_id 2
```

The last line suggests that the GPUs with bus id 2 (mapped to pci bus id prefixed 0000:8) will benefit from GDR and host memory bypass, which is `/gpu:2` and `/gpu:3` in this case.

Caveats
===

In current implementation, only tensors that reside in host memory or in GPU memory such that the GPU is adjacent to an RDMA capable NIC will use direct RDMA as its transport. When RDMA is available but not GDR, a temporary tensor copy on host memory will be used as RDMA source/destination (and copied from/to the target device). When there is no RDMA device present, it can even fallback to the original gRPC runtime. While it is theoretically possible to mix GDR enabled TF with non-GDR deployments in the same job, make sure the environment is properly setup so the GDR mode is enabled whenever possible (i.e. do not fall back to gRPC when it is not absolutely necessary).",tensorflow
11400,brettkoonce,pr,2017-07-09T20:16:51Z,update folder link in docs,,tensorflow
11402,taehoonlee,pr,2017-07-10T00:13:32Z,Fix typos,"This PR fixes some typos: `Acccumulate`, `represnting`, and `implictly`.",tensorflow
11410,guillaumekln,pr,2017-07-10T15:21:11Z,Fix minor docstring formatting issue,,tensorflow
11432,byronyi,pr,2017-07-11T12:38:48Z,Enable building grpc+verbs runtime on any Linux box,I don't have a Windows machine so it is probably broken on Windows. Currently I don't have an idea on how to achieve cross-platform portability. Any suggestions?,tensorflow
11444,taehoonlee,pr,2017-07-12T05:44:54Z,Fix typos,"This PR fixes some typos: `Compatble`, `objets`, `overriden`, `reseting`, `an an`, `libraryh`, and `a a`.",tensorflow
11515,taehoonlee,pr,2017-07-15T10:35:44Z,Add white spaces,This PR adds white spaces on docstrings.,tensorflow
11545,guoyejun,pr,2017-07-17T07:33:14Z,add nodouble option for all cwise ops,"this is the following patch of ac98d1184008e4 to support nodouble
option for all cwise ops. The macros REGISTER* defined within
__ANDROID_TYPES_SLIM__ should be changed to empty, and it impacts
all the cwise ops, so, all the changes for the cwise ops have to be
in a single patch.

there will be more patches for other ops to support nodouble option.",tensorflow
11566,taehoonlee,pr,2017-07-18T07:10:31Z,Fix typos,"This PR fixes some typos: `tranposed`, `observaiton`, `the the`, `implementaton`, and `concurently`.",tensorflow
11576,hgaiser,pr,2017-07-18T12:50:12Z,Fix typo in exception.,Title says it all.,tensorflow
11622,darrengarvey,pr,2017-07-19T20:41:37Z,Enable passing scope to variable lookups,"NB: Changes the public API, albeit in a backwards-compatible way.

Especially when copying variables between scopes it's handy to be able to grab sets of variables by scope. This allows the much more succinct:

``tf.trainable_variables(""myscope"")``

which is equivalent to:

``tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""myscope"")``

- Also fixup a minor unrelated documentation bug.",tensorflow
11627,taehoonlee,pr,2017-07-20T00:32:44Z,Fix typos,This PR fixes some typos: `partiton` and `executon`.,tensorflow
11694,darrengarvey,pr,2017-07-23T15:26:16Z,"Add `as_default()` to `MonitoredSession`, `MonitoredTrainingSession` and `SingularMonitoredSession`.","Equivalent to `tf.Session.as_default()`, allows refetching the session from elsewhere in code using `tf.get_default_session()`.

Notes:
- (additive) public API change.
- If there is a better way to get the current session when using these `tf.Session` wrappers, this may be unnecessary. I just couldn't find it.
",tensorflow
11703,taehoonlee,pr,2017-07-24T06:12:14Z,Fix missing spaces for several errors,,tensorflow
11800,Kongsea,pr,2017-07-27T05:44:04Z,Refine docstrings,,tensorflow
11895,taehoonlee,pr,2017-07-31T02:04:10Z,Fix typos,"This PR fixes some typos: `squeee`, `argumnet`, `succeeeded`, and `pyton`.",tensorflow
11913,benoitsteiner,pr,2017-07-31T16:10:26Z,Branch 163695881,,tensorflow
11927,taehoonlee,pr,2017-08-01T02:20:18Z,Add bool type supports for GPU kernels,"This PR adds bool type supports for GPU kernels (`reshape`, `concat`, and `stack`). The problem is originally described in #11676. Following is a minimum code snippet, which runs on CPU but not on GPU:

```python
import tensorflow as tf
import numpy as np
a = tf.placeholder(tf.bool, shape=[2, 2])
b = tf.placeholder(tf.bool, shape=[2, 2])
with tf.device('/gpu:0'):
    try:
        with tf.Session() as sess:
            result = tf.reshape(a, [1, 4])
            sess.run(tf.global_variables_initializer())
    except Exception as e:
        print('---reshape---\n' + str(e))
    try:
        with tf.Session() as sess:
            result = tf.concat([a, b], 0)
            sess.run(tf.global_variables_initializer())
    except Exception as e:
        print('---concat---\n' + str(e))
    try:
        with tf.Session() as sess:
            result = tf.stack([a, b], 1)
            sess.run(tf.global_variables_initializer())
    except Exception as e:
        print('---stack---\n' + str(e))
```

As far as I know, there is no reason not to have boolean supports for these operations. Thus I propose the boolean supports.",tensorflow
11932,taehoonlee,pr,2017-08-01T07:27:29Z,Increase docstring consistency,"This PR considers the following six main items:
- Adjusting indents of markdown code blocks
- Adding language identifiers on markdown code blocks
- Replacing `Return` with `Returns` (The frequencies over all python codes: 2 vs. 3369)
- Replacing `Arguments` with `Args`
(`Args` is a consistent word over all python codes except `tensorflow/contrib/keras` and `tensorflow/python/layers`)
- In `tensorflow/python/framework/tensor_shape.py`, removing ` ``` ` for consistency on the file
- In `tensorflow/python/ops/array_ops.py` and `math_ops.py`, revising python code blocks to make them more concrete
  - I think it is better to replace `==>` with `#` because `==>` is not a python keyword.
  - For each operation, two styles (with and without `tf.`) are mixed. I think it is better to make them as `tf.operation` not `operation`.
  - For more concrete examples, I think it is better to write down real tensors not pseudo. (e.g.,`# 'x' is [1, 4]` is replaced by `x = tf.constant([1, 4])`.)",tensorflow
11934,taehoonlee,pr,2017-08-01T08:21:47Z,Fix white spaces,This PR cleans up extra white spaces.,tensorflow
11946,benoitsteiner,pr,2017-08-01T17:49:10Z,Branch 163848365,,tensorflow
11972,benoitsteiner,pr,2017-08-02T16:06:42Z,Branch 163983198,,tensorflow
12020,benoitsteiner,pr,2017-08-03T20:44:14Z,Branch 164170971,,tensorflow
12031,taehoonlee,pr,2017-08-04T08:53:31Z,Fix typos,"This PR fixes some typos: `attempst`, `lenght`, and `acessed`.",tensorflow
12049,benoitsteiner,pr,2017-08-04T21:33:39Z,Branch 164309367,,tensorflow
12067,freedomtan,pr,2017-08-07T00:59:46Z,fix a typo in tf.nn.separable_conv2d's doc,"It's obvious that the right bracket in ""sum_{di, dj, q, r]"" should
be a right brace.",tensorflow
12074,byronyi,pr,2017-08-07T09:27:12Z,Fix segfault when recording raw allocation returns nullptr,See [here](https://github.com/tensorflow/tensorflow/commit/ec1403e7dc2b919531e527d36d28659f60621c9e#commitcomment-23432313) for discussion.,tensorflow
12089,byronyi,pr,2017-08-07T22:21:45Z,"Revert ""Fix segfault when recording raw allocation returns nullptr""","Reverts tensorflow/tensorflow#12074, in favour of a complete fix in 9bb2c8e.",tensorflow
12127,facaiy,pr,2017-08-09T03:30:03Z,tf.reshape accepts Dimension objects for the shape parameter,"The PR aims to fix #11974.

Because it's my first contribution, so the PR is opened early to get feedback from community.

### What changes were proposed in this pull request?

Cast tf.Dimension to int for shape argument.


### How was this patch tested?

+ [x] add a doctest. ",tensorflow
12170,byronyi,pr,2017-08-10T09:10:01Z,Fix unnecessary sync_memops cuda pointer attr in GDR,See the comments [here](https://github.com/tensorflow/tensorflow/pull/11392#discussion_r132026389). I've tested on my local boxes and it seems removing `CU_POINTER_ATTRIBUTE_SYNC_MEMOPS` slightly improves the training throughput while introducing no data race.,tensorflow
12195,taehoonlee,pr,2017-08-11T00:27:47Z,Fix typos,"This PR fixes some typos: `explictly`, `initialised`, `paritally`, `unecessary`, `substitue`, and `Paramterized`.",tensorflow
12200,guoyejun,pr,2017-08-11T03:55:08Z,fix build issue for --config=sycl,,tensorflow
12206,facaiy,pr,2017-08-11T08:27:02Z,BUG: Estimator.eval() runs feature_engineering_fn more than once.,"The PR is aimed to fix #12205.

### What changes were proposed in this pull request?

eliminate the second call of `feature_engineering_fn` in `evaluate` method.


### How was this patch tested?

+ [x] add an unit test.",tensorflow
12207,taehoonlee,pr,2017-08-11T08:27:46Z,Fix typos,This PR fixes some typos.,tensorflow
12212,byronyi,pr,2017-08-11T11:02:20Z, Remove useless RecvTensorResponse allocation in GDR,See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc#L347) for your reference.,tensorflow
12257,Kongsea,pr,2017-08-14T04:48:55Z,Add focal_loss,"Add a new loss function, focal_loss, which was described in [this paper](https://arxiv.org/pdf/1708.02002.pdf).",tensorflow
12398,alanhdu,pr,2017-08-18T16:18:47Z,Better documentation for tf.contrib.layers.optimize_loss,"- `tf.contrib.layers.optimize_loss` references `OPTIMIZER_SUMMARIES` in
   its docstring, so we should expose that in the API
- Correct the documentation for what `summaries` `optimize_loss` uses by default.",tensorflow
12462,byronyi,pr,2017-08-21T22:27:32Z,fix gdr compiling error,,tensorflow
12490,guillaumekln,pr,2017-08-22T12:55:22Z,Expose new CRF classes and function.,#12056 introduced a new tensor-based CRF decoding. This PR exposes the new names at the module level.,tensorflow
12493,bryant1410,pr,2017-08-22T13:48:21Z,Fix code highlight language in profiler README,,tensorflow
12494,bryant1410,pr,2017-08-22T13:52:57Z,Fix images not showing in profiler README,,tensorflow
12495,bryant1410,pr,2017-08-22T15:23:25Z,Fix ProfileContext location in README,,tensorflow
12502,nluehr,pr,2017-08-22T22:12:59Z,Support for CUDA 9.0,"For review by @zheng-xq.

Add explicit __syncwarp to bias_op
 - Makes warp-synchronous code safe on Volta
Add sync mask to __shfl intrinsics
Add libdevice bytecode paths for CUDA 9
 - In CUDA 9, all supported architectures are merged into a single file
Update code gating for CUDA 9
Add sm_70 to the lookup table used by XLA
Change the default sm arch from 20 to 30.
Fix for NVPTX not yet supporting sm_70
Remove unnecessary cuda decorators from defaulted constructors
Use updated NCCL for CUDA 9 fp16 support",tensorflow
12503,nluehr,pr,2017-08-22T23:05:44Z,Add cudnn7 support,"Attention @zheng-xq 

- Account for cudnnSetRNNDescriptor API change
- Add support for CUDNN_TENSOR_OP_MATH in cudnn v7
  - Applies to forward and backward convolutions that have fp16
  input/output. Computations will fall back to pseudo-fp16
  if tensor op math is disabled or not supported.
  - Enabled by default, but can be disabled using the environment
  variable TF_ENABLE_TENSOR_OP_MATH=0.
  - The choice of whether to use tensor op math is included in the
  autotuning of convolutions.",tensorflow
12504,nluehr,pr,2017-08-22T23:14:07Z,NaN propagation for GPU pooling ops,"Attention @zheng-xq

- Changes the custom fwd maxpooling kernel to propagate NaNs. This
  makes it match the behavior of CUDNN, and ensures that CUDNN's
  bwd maxpooling kernel behaves as expected (propagating NaNs).
- Previous behavior can be restored with environment variable
  TF_ENABLE_MAXPOOL_NANPROP=0.
- Changes the GPU bwd maxpool op tests to expect propagated NaNs
  (matching the behavior of the CPU path).",tensorflow
12513,facaiy,pr,2017-08-23T05:25:54Z,make Dimension be compatible with integer,"The PR is another solution to fix #11974 .  I believe that it is better than #12127 .

### What changes were proposed in this pull request?

Make Dimension to be compatible with integer, so `[1, Dimension(2)]` will be casted to `[1, 2]` automatically.

### How was this patch tested?

+ [x] add a doctest. 
+ [x] add an unit test.
+ [x] pass all unit tests.
",tensorflow
12529,vfdev-5,pr,2017-08-23T15:25:55Z,Docs typo fix,"`(key, value pair) -> (key, value) pair`",tensorflow
12554,guillaumekln,pr,2017-08-24T10:03:08Z,Fix deprecation warnings of *_global_step functions.,"In `tensorflow/contrib/layers/python/layers/optimizers.py`, use `*_global_step` functions from `tf.train` instead of `tf.contrib.framework`.",tensorflow
12558,facaiy,pr,2017-08-24T11:12:12Z,convert Dimension to Int for sparse_merge in _IndicatorColumn,"The PR is aimed to fix #12557 .

`sparse_merge` cannot handle `Dimension`,  hence casting Dimension to int when invoked.

### How to test

+ [x] add an unit test.
+ [ ] pass all tests.",tensorflow
12564,byronyi,pr,2017-08-24T18:39:07Z,Partially fixes #10838,"The following warnings seem to print for compiling many ops using gcc-4.9.

For example, in `cuda_solvers_gpu.cu.cc`:

```
INFO: From Compiling tensorflow/core/kernels/cuda_solvers_gpu.cu.cc:
./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/kernel.h(307): warning: variable ""result"" is used before its value is set

./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless

./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/kernel.h(307): warning: variable ""result"" is used before its value is set

./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless
```

In `beam_search_ops_gpu.cu.cc`:

```
INFO: From Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc:
./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/kernel.h(307): warning: variable ""result"" is used before its value is set

./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless

./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/kernel.h(307): warning: variable ""result"" is used before its value is set

./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless
```

I believe this fix will address many (but not all) of the warnings during the build.",tensorflow
12584,facaiy,pr,2017-08-25T05:16:38Z,Fix: indices is out of bounds in _OneHotColumn,"The PR is opened to fix #12583. CF pr #12638.

### What changes were proposed in this pull request?

slice `weighted_column` to get rid of -1 index.

### How was this patch tested?

+ [x] add unit tests.
+ [ ] pass all tests.",tensorflow
12617,facaiy,pr,2017-08-26T07:31:59Z,expose sparse_column_with_vocabulary_file method,"The bug is explained in #12568 . 

I am not familiar with build tool, however, I observe that the method is missing in `__init__.py`.",tensorflow
12624,facaiy,pr,2017-08-27T00:26:36Z,expose build_default_serving_input_fn,"For #12508, I find that `build_default_serving_input_fn` is hidden like #12568 . I don't know whether it is intentional. 

CF pr #12617.",tensorflow
12638,facaiy,pr,2017-08-28T02:16:52Z,Fix: indices is out of bounds in _IndicatorColumn,"The same bug of #12583 exists in `tf.feature_column` as well.  CF pr #12584.

### What changes were proposed in this pull request?

slice weighted_column to get rid of -1 index.

### How was this patch tested?

+ [x] add unit tests.
+ [x] pass all tests.
",tensorflow
12645,facaiy,pr,2017-08-28T07:55:03Z,y can be a dict for numpy_input_fn,"The PR is opened for #12610.

### What changes were proposed in this pull request?

dict is accept for `y` as labels.

### How was this patch tested?

+ [x] add unit test
+ [ ] pass all tests.

",tensorflow
12714,taion,pr,2017-08-30T22:20:40Z,Fix dtype for streaming confusion matrix,"Currently, dependents of `_streaming_confusion_matrix` don't actually work correctly with floating-point weights.",tensorflow
12720,facaiy,pr,2017-08-31T04:03:16Z,PREP: migrate lgamma gradient to c++ side,"It's my first attempt to contribute to C++ side. Hence the PR is opened early to get feedback.

### What changes were proposed in this pull request?

Migrate the implementation of _Lagmma from python side to c++ side, see  #12686.

### How was this patch tested?

+ [x] add unit tests.
+ [ ] pass all tests.",tensorflow
12741,fritzo,pr,2017-09-01T04:40:02Z,Export C API symbols of _pywrap_tensorflow_internal.so on OS X,"This follows up #10469 by exporting C API symbols on OS X.

## How?

This PR widens the `.lds` matching pattern to include the C API symbols as they are mangled on OS X.

Before this PR the C API symbols are exported in Linux but are hidden in OS X:
```sh
(Linux)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession
00000000012d2d40 T TF_CloseSession
00000000010f8e50 t _wrap_TF_CloseSession

(OS X)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession
00000000002be1b0 t _TF_CloseSession
000000000001a0a0 t __Z21_wrap_TF_CloseSessionP7_objectS0_
```
After this PR the C API symbols are exported in both Linux and OS X:
```sh
(Linux)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession
0000000000fa25f0 T TF_CloseSession
0000000000cf02e0 t _wrap_TF_CloseSession

(OS X)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession
0000000000392880 T _TF_CloseSession
000000000001ae50 t __Z21_wrap_TF_CloseSessionP7_objectS0_
```
(notice the change from `t` to `T`, i.e. static to global)

## Why?

This is important for libraries that use multiple client APIs as described in #7541. My use case is [an R package](https://github.com/nimble-dev/nimble) that uses the Python API through [rstudio/tensorflow](https://github.com/rstudio/tensorflow) and also uses the C API library: I build tf graphs using the Python API and run those graphs from R-wrapped C++ code using the C API. This already works in Linux, and should also work in OS X after this PR.

## Tested

Successfully loaded [a shared library](https://github.com/nimble-dev/nimble/tree/devel/packages/nimble/inst/CppCode/tensorflow.cpp) in R that uses symbols from `_pywrap_tensorflow_internal.so`:
- [x] Linux
- [x] OS X",tensorflow
12834,nluehr,pr,2017-09-05T23:50:42Z,Update NCCL and Eigen sources for CUDA9,Also fix presumably outdated manifest_proto_py_pb2 and pandas dependencies.,tensorflow
12839,facaiy,pr,2017-09-06T05:46:13Z,BLD: precheck patch command before invoked,"The PR is opened to fix #12821.

### How to test

+ [ ] I have no idea how to test.",tensorflow
12872,facaiy,pr,2017-09-07T10:29:24Z,PREP: migrate ErfGrad to c++ side,"see #12686.

### How was this patch tested?

+ [x] add unit tests.
+ [x] pass all tests.
",tensorflow
12920,nluehr,pr,2017-09-08T22:34:48Z,Workaround for NVCC 9.0 internal error,Fixes 'Internal Compiler Error (codegen)' encountered when building with nvcc 9.0.,tensorflow
12995,facaiy,pr,2017-09-12T08:53:58Z,boringssl sha256 checksum mismatched,"issue #12986 .

### How to test 

+ [ ] confirmed by #12752 @louiehelm @gunan ",tensorflow
13008,facaiy,pr,2017-09-13T02:28:19Z,EHN: csv supports missing value,"### What changes were proposed in this pull request?

`tf.decode_csv` support NA values, see #13007 

### How was this patch tested?

+ [x] add unit tests.
+ [ ] pass all tests.",tensorflow
13057,facaiy,pr,2017-09-15T05:26:54Z,check invalid string type for dest_nodes in extract_sub_graph,"Fix #13047.

### How to test

+ [x] add an unit test.
+ [ ] pass all tests.",tensorflow
13070,nluehr,pr,2017-09-15T20:13:39Z,Use AllClose instead of AllEqual in layers tests,"- While simple implementations of convolution (e.g., gemm-based) will
  produce exact results in these tests, general implementations
  only guarantee floating-point precision.
- (One of these tests was previously observed to fail with a max error
  of 1e-7).",tensorflow
13107,yaroslavvb,pr,2017-09-17T23:02:48Z,"Add new op BytesInUse, similar to MaxBytesInUse","Adding BytesInUse
This is more useful than MaxBytesInUse for getting peak memory for a given session.run call because the latter gives maximum memory usage over lifetime of allocator, which can span multiple session.run calls/multiple session objects",tensorflow
13110,taehoonlee,pr,2017-09-18T02:08:47Z,Fix typos,"This PR fixes some typos: `Tenor`, `tenor`, and `varibles`.",tensorflow
13123,guillaumekln,pr,2017-09-18T11:32:39Z,Fix argument name mismatch in FinalBeamSearchDecoderOutput docstring,,tensorflow
13140,byronyi,pr,2017-09-19T01:10:24Z,[GDR] Eliminate several unnecessary sync barriers,"Following the plan I mentioned in https://github.com/tensorflow/tensorflow/pull/12361#issuecomment-323101744, I have refactored out the sync wrapper around copy between CPU and GPU.

Now the user need to supply a `StatusCallback` when calling `RemoteMemoryManager:: TransportOptionsFromTensor ` and `RemoteMemoryManager::TensorFromTransportOptions`, in order to prepare for the potential CPU-GPU tensor transfer.",tensorflow
13205,facaiy,pr,2017-09-21T05:50:03Z,"Revert ""Revert ""EHN: csv supports missing value (#13008)"" (#13051)""","issue: #13007
CF PR: #13008

This reverts commit b6f253429c475d1a6f80702feadfff8ff9409156.",tensorflow
13208,guillaumekln,pr,2017-09-21T10:17:47Z,Use Tensors instead of TensorArrays for storing AttentionWrapper's alignment_history,"I would like to propose this fix for #13154.

This is a breaking change as it replaces `TensorArray`s of the `alignment_history` field with batch major `Tensor`s.

Additionally, the helper function used to gather beams in `BeamSearchDecoder` had to be updated to allow keeping the original values dimensions.",tensorflow
13239,tillahoffmann,pr,2017-09-22T17:52:34Z,Add ReceptiveField class and coordinate conversion methods.,This PR adds a `ReceptiveField` class which supports coordinate conversion from the input to the feature space and vice versa while maintaining backwards compatibility.,tensorflow
13252,nluehr,pr,2017-09-22T23:43:08Z,GetConvolve*Algorithms return tensor-op algos,"Attention: @jlebar and @zheng-xq

This is the follow-up on the cudnn7 patch. When enumerating convolution algorithms, it moves the tensor_ops toggle into the GetConvolve*Algorithms functions.

Also tensor_ops are no longer included in the returned algo list if they are not supported by the cuDNN version or GPU compute capability.",tensorflow
13266,facaiy,pr,2017-09-24T00:05:02Z,ENH: row_shape supports unknown dim in Dataset.dense_to_sparse_batch,"see issue: #13216 

### What changes were proposed in this pull request?

eg: `row_shape = [20, -1, 10]`, `row_shape = [20, None, 10]`

### How to test

+ [x] add test cases.
+ [ ] pass all tests.",tensorflow
13268,facaiy,pr,2017-09-24T03:37:11Z,ENH: add Relu6GradGrad,Fix #13144.,tensorflow
13312,guillaumekln,pr,2017-09-26T13:07:16Z,Support TensorArray in BeamSearchDecoder state.,"#13208 attempted to fix #13154 by representing the `alignment_history` field with a `Tensor` instead of a `TensorArray`. However, @ebrevdo pointed out that this approach led to a quadratic time and space overhead.

This PR fixes the issue by directly adding the support for `TensorArray` in the `BeamSearchDecoder` state as proposed by @ebrevdo.

@ebrevdo Let me know what you think of this implementation. Thanks!",tensorflow
13388,nluehr,pr,2017-09-29T21:40:21Z,Add fp16 support to fused batchnorm op,"Attention @zheng-xq 

- This commit adds a mixed-precision fused_batch_norm_v2 op.
  The inputs and outputs are fp16, while the scale, offset, mean
  and variance are kept in fp32.
- The tf.nn.fused_batch_norm op has been modified to use the v2
  fused batchnorm whenever inputs are fp16 (this does not affect
  compatibility because fp16 was not previously supported).
- The high-level layers API has also been updated to store the
  scale, offset, mean and variance variables as fp32.",tensorflow
13414,kashif,pr,2017-09-30T08:51:37Z,updated instructions for libcupti for CUDA Toolkit >= 8.0,,tensorflow
13420,facaiy,pr,2017-09-30T11:27:36Z,Fix bug: variables outside won't update in DNNLinearCombinedRegressor,"This fix is an proposal for #13419 

### What changes were proposed in this pull request?

There are two solutions to fix the problem in my opinion:
+ all variables except of linear is optimized by dnn.
+ which one of optimizers does the variables left (except of linear and dnn) use is selected by user, perhaps a new argument is needed.

The PR is implemented by the first one for simplicity.

### How to test

+ [x] add test case: I have no idea of how to write it, hence any suggestions will be appreciated.
+ [ ] pass all tests.",tensorflow
13427,taehoonlee,pr,2017-10-01T05:35:40Z,Fix typos,"This PR fixes some typos: `correclty`, `lenght`, `identifer`, and `obejct`.",tensorflow
13440,taehoonlee,pr,2017-10-02T07:56:14Z,Fix typos,"This PR fixes some typos: `Explicitely`, `ouput`, `Dimensiton`, `occurences`, `partiton`, `unknwon`, and `Summmary`.",tensorflow
13451,nluehr,pr,2017-10-02T19:48:01Z,Add support for CUBLAS_TENSOR_OP_MATH in fp16 GEMM,"- Applies to matrix multiplications with fp16 input/output.
  Computations will fall back to pseudo-fp16 if tensor op math is
  disabled or not supported.
- Enabled by default, but can be disabled by setting the environment
  variable TF_DISABLE_TENSOR_OP_MATH=1.",tensorflow
13470,nluehr,pr,2017-10-03T17:07:51Z,GetConvolveAlgorithms fixup take 2,"Attention @tfboyd 

Move loop to toggle tensor_ops inside GetConvolveAlgorithms functions. Also
tensor_ops are not included in the returned list if they are not supported by
the cuDNN or GPU architecture versions.

This is a re-submit of PR 13252 which seems to have been accidentally squashed
during the merge at hash 37800b9.",tensorflow
13495,yaroslavvb,pr,2017-10-04T23:17:49Z,Remove unneeded branch check,,tensorflow
13496,alsrgv,pr,2017-10-05T00:48:51Z,Add tf.sysconfig.get_compile_flags() & tf.sysconfig.get_link_flags() for custom operators,"The goal is to make custom operators compilation a breeze.

```
In [1]: import tensorflow as tf
Couldn't import dot_parser, loading of dot files will not be possible.

In [2]: tf.sysconfig.get_compile_flags()
Out[2]:
['-I/home/devadmin/env/lib/python2.7/site-packages/tensorflow/include',
 '-I/home/devadmin/env/lib/python2.7/site-packages/tensorflow/include/external/nsync/public',
 '-D_GLIBCXX_USE_CXX11_ABI=1']

In [3]: tf.sysconfig.get_link_flags()
Out[3]:
['-L/home/devadmin/env/lib/python2.7/site-packages/tensorflow',
 '-ltensorflow_framework']
```",tensorflow
13518,facaiy,pr,2017-10-06T05:49:00Z,WIP: BUG: fix inconsistent scope for `tensorflow.python.layers.base.Layer`,"It is proposed to fix #13429, however, I'm not sure whether it's the best solution. I'll finish the PR when I get back from holiday (next week).

### How to test

+ [ ] add test case.
+ [ ] pass all tests.",tensorflow
13547,taehoonlee,pr,2017-10-07T06:05:41Z,Fix typos,"This PR fixes some typos: `initalizers`, `fileds`, `mutli`, `beacuse`, and `summmary`.",tensorflow
13605,taehoonlee,pr,2017-10-10T13:28:07Z,Fix typos,"This PR fixes some typos: `overriden`, `explictly`, `corresonding`, `accross`, and `contructor`.",tensorflow
13747,taehoonlee,pr,2017-10-16T12:57:56Z,Fix typos,"This PR fixes some typos: `a a`, `for for`, `tranpose`, `from from`, and `to to`.",tensorflow
13845,powderluv,pr,2017-10-20T04:02:27Z,Fix ../makefile/download_dependencies.sh on OSX,"wget expects parameters before the URL on OSX (tested on
version 1.16 and 1.19)

It would fail trying to use -P as a URL

Resolving -p... failed: nodename nor servname provided, or not known.
wget: unable to resolve host address â€˜-pâ€™",tensorflow
13852,facaiy,pr,2017-10-20T08:26:08Z,WIP: ENH: only master is allowed to export model,"Fix #13849 

If the proposition is accepted, I'll add the corresponding test case later.

### How to test

+ [ ] add unit test
+ [ ] pass all tests",tensorflow
13871,powderluv,pr,2017-10-20T22:35:48Z,Remove deprecated 32bit IOS builds,"Apple has stopped supporting 32bit iOS builds staring with iOS11.
Building these binaries are pretty much useless on iOS, but saves
a lot of time by just building arm64 and x86_64.

TEST: build ios libraries with ./build_all_ios.sh",tensorflow
13920,powderluv,pr,2017-10-23T14:54:18Z,Update build_all_ios.sh to support per arch builds,,tensorflow
13923,benoitsteiner,pr,2017-10-23T16:50:46Z,Branch 173127955,,tensorflow
13959,powderluv,pr,2017-10-24T23:19:15Z,Allow build_all_ios.sh to build just one arch,"This change allows build_all_ios.sh to take an -a flag with a
specific arch so you dont have to waste time building unwanted
architectures (32bit etc)

TEST: tensorflow/contrib/makefile/build_all_ios.sh  #builds fat lib
tensorflow/contrib/makefile/build_all_ios.sh -a ""arm64"" #only arm64",tensorflow
13972,taehoonlee,pr,2017-10-25T14:58:17Z,Fix typos,"This PR fixes some typos: `prediciton`, `iteraton`.",tensorflow
13976,benoitsteiner,pr,2017-10-25T18:28:01Z,Branch 173415707,,tensorflow
14005,benoitsteiner,pr,2017-10-26T17:48:11Z,Branch 173553770,,tensorflow
14006,benoitsteiner,pr,2017-10-26T18:08:05Z,Branch 173560463,,tensorflow
14011,powderluv,pr,2017-10-26T22:39:16Z,iOS/RPi Add the ability to choose ANDROID_TYPES_FULL,"Some networks require ""full"" types instead of ""slim"" so remove
the hard coding of SLIM in iOS and RPi. It still defaults to
building SLIM for them if not ENV var is specified but now
you can build with

ANDROID_TYPES=""-D__ANDROID_TYPES_FULL"" \
./tensorflow/contrib/makefile/build_all_ios.sh

TEST: Verify the  -D__ANDROID_TYPES_SLIM__ flag is default and
you can override with an env var",tensorflow
14015,rohan-varma,pr,2017-10-27T04:13:11Z,"Set reuse=False instead of reuse=None, and add suggestion for tf.AUTO_REUSE","As per https://github.com/tensorflow/tensorflow/issues/13887 we'd like to change the message that contains ""reuse=None"" to ""reuse=False"" and suggest the use of tf.AUTO_REUSE.",tensorflow
14046,benoitsteiner,pr,2017-10-27T21:18:41Z,Branch 173716375,,tensorflow
14102,flx42,pr,2017-10-30T18:06:30Z,Dockerfile: do not perform cleanup in a separate RUN statement,"Cleanup must be performed in the same statement, otherwise the build
files are still stored in the upper layer and no space is reclaimed.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>


@gunan
Image size before the patch: 8.21GB
Image size after the patch: 4.99GB
",tensorflow
14120,facaiy,pr,2017-10-31T05:59:40Z, Recover the lost output channel number of atrous convolution for NC format,"Fix #13861.

### How to test

+ [x] add new test case
+ [ ] pass all tests.",tensorflow
14159,alsrgv,pr,2017-11-01T17:30:26Z,Make sure to set _GLIBCXX_USE_CXX11_ABI=0 if it's not defined,This is necessary to make sure we can compile TensorFlow with gcc4 and compile custom operator with gcc5.,tensorflow
14192,flx42,pr,2017-11-02T22:00:43Z,Dockerfile.gpu: use the runtime cuDNN v6 image,"The generated Docker image will be approximately 900 MB smaller.

The Dockerfile switched to the devel image a long time ago to
workaround a bug when looking up CUDA libraries. This problem has been
fixed in the meantime.


@gunan let me know if I'm missing something and there is a good reason to keep the devel image as a base.",tensorflow
14204,taehoonlee,pr,2017-11-03T08:07:40Z,Fix typos,"This PR fixes some typos: `a same`, `specifed`, `signalled`, and `compatiblity`.",tensorflow
14307,alsrgv,pr,2017-11-07T01:20:17Z,Update Custom Op instructions to use tf.sysconfg flags,,tensorflow
14390,facaiy,pr,2017-11-09T03:07:44Z,`variable_scope` use `auxiliary_name_scope` to control whether to create new name scope,"Now `variable_scope` always create a new name scope (side-effect?) when invoked. The behavior will result in name scope collision mentioned in #13429 . Hence we proposed to add a parameter `auxiliary_name_scope` to control whether to create new name scope or not.

If accepted, we can reenter `variable_scope` and its original name scope without side-effect:
```python
with tf.variable_scope(s, auxiliary_name_scope=False) as ss:
     with tf.name_scope(ss.original_name_scope) as n:
          # do something
```
### How to test

+ [x] add unit tests
+ [ ] pass all tests.",tensorflow
14421,powderluv,pr,2017-11-09T21:15:43Z,[iOS] Add optional Selective Registration of Ops,"The current iOS library is huge. Add the ability to selectively
register for the ops the tensorflow library will support. This
greatly reduces resultant binary based on the network.

A ""full"" arm64 build was 122MB on my machine vs one selectively
registered for SSD Mobilenet was only 93MB.

Also fixes a minor bug where the selected arch wasn't being passed
to the compile_ios_protobuf.sh script.

TEST:build_all_ios.sh -a arm64 # generates a fat binary for arm64
     build_all_ios_sh -a arm64 -g ~/Downloads/op_inference_graph.pb
        #generates a binary that is much smaller",tensorflow
14447,boeddeker,pr,2017-11-10T13:05:48Z,PrefetchDataset with buffer_size==0 results in deadlock,"Hey, 

I experimented with `tf.data.Dataset.prefetch` and found that an assert inside the code is missing.
When `buffer_size` is zero, I got a deadlock.
Here is a toy example to reproduce my bug:

```python
import tensorflow as tf
def test(buffer_size):
    ds = tf.data.Dataset.range(5)
    ds = ds.prefetch(buffer_size=buffer_size)
    iterator = ds.make_one_shot_iterator()
    entry = iterator.get_next()
    with tf.Session() as sess:
        try:
            while True:
                print(sess.run(entry))
        except tf.errors.OutOfRangeError:
            pass
        
test(1)
test(0)  # deadlock
```
and here my tensorflow version
```bash
$ pip show tensorflow
Name: tensorflow
Version: 1.4.0
Summary: TensorFlow helps the tensors flow
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: .../lib/python3.6/site-packages
Requires: protobuf, numpy, wheel, tensorflow-tensorboard, absl-py, six, enum34
```
",tensorflow
14499,boeddeker,pr,2017-11-12T14:37:39Z,fix assert_shallow_structure for dicts,"The function `tensorflow.python.data.util.nest.flatten_up_to` used in `tf.data.Dataset.from_generator` does not compare the dict keys (only the length of the dict)
```python
tf_tree = dict(A=tf.placeholder(tf.float32), B=tf.placeholder(tf.float64))
np_tree = dict(A=np.zeros([1], np.float32), C=np.zeros([2], np.float64))
nest.flatten_up_to(tf_tree, np_tree)  # Fails now
```
and ignores nested dict's (Bug, iterate over keys in `tensorflow.python.data.util.assert_shallow_structure`)
```python
tf_tree = dict(A=tf.placeholder(tf.float32), B=dict(C=tf.placeholder(tf.float64)))
np_tree = dict(A=np.zeros([1], np.float32), B=dict(D=np.zeros([2], np.float64)))
nest.flatten_up_to(tf_tree, np_tree)  # Fails now
```

This PR fix `tensorflow.python.data.util.assert_shallow_structure` that is used in `tensorflow.python.data.util.nest.flatten_up_to`, but there may also other function that have such a bug.",tensorflow
14508,facaiy,pr,2017-11-13T05:06:02Z,PrefetchDatasetOp checks buffer_size in c++ side,"alternative pr for #14447

check `buffer_size` in c++ side

### How to test

+ [x] add test case
+ [x] pass all tests

",tensorflow
14510,facaiy,pr,2017-11-13T05:42:37Z,index_to_string_table_from_file can use tf.string as vocabulary file,"Fix #14505 

### How to test

+ [x] add test case
+ [ ] pass all tests",tensorflow
14513,taehoonlee,pr,2017-11-13T07:19:39Z,Add missing conv1d in `tf.contrib.layers`,"Currently, conv1d exists in `tf.layers` but does not in `tf.contrib.layers`.",tensorflow
14516,taehoonlee,pr,2017-11-13T10:58:09Z,Fix typos,"This PR fixes some typos: `mutli`, `accomodate`, `ouput`, `conjuction`, `accross`, `simplifed`, and `seperately`.",tensorflow
14564,brettkoonce,pr,2017-11-14T22:46:08Z,minor spelling tweaks,,tensorflow
14570,brettkoonce,pr,2017-11-15T06:18:41Z,minor spelling tweaks in headers,,tensorflow
14667,facaiy,pr,2017-11-17T20:41:03Z,`mean_relative_error` supports complex label,"Fix #14658 


### How to test

+ [x] add test case
+ [ ] pass all tests",tensorflow
14730,alanhdu,pr,2017-11-20T16:39:32Z,Only install enum34 on Python <3.4 versions,"Python 3.6 sometimes has issues with enum34 because the standard library
relies on enum features not in enum34 (see
https://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36
for more details).

cc @macat",tensorflow
14770,nluehr,pr,2017-11-21T21:35:01Z,Update Eigen hash for fix of fp16 predux bug,"Attention: @benoitsteiner and @zheng-xq 

For Maxwell and earlier GPUs, Eigen was incorrectly casting fp16 values to
unsigned int during some reductions. This causes incorrect results in
Tensorflow's xent and sparse_xent ops when applied to fp16 data.",tensorflow
14808,flx42,pr,2017-11-22T20:16:26Z,Remove useless statements in Dockerfiles,"'CMD [""/bin/bash""]' is not useful since it's already provided by the base ubuntu image.
'RUN [""/bin/bash""]' looks like a typo and just creates an extra empty layer.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>",tensorflow
14862,facaiy,pr,2017-11-24T11:24:50Z,C++ gradient for Select,"Fix #14845 

migrate python implementation to c++ side, source: https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/python/ops/math_grad.py#L919

### How to test

+ [x] add test case
+ [ ] pass all tests",tensorflow
14865,facaiy,pr,2017-11-24T13:27:11Z,Fix bug: divide by zero in embedding_lookup_sparse,"Fix #14851


### How to test

+ [x] add test case
+ [ ] pass all tests.",tensorflow
14874,facaiy,pr,2017-11-25T11:16:49Z,tf.Print converts Variable to mutable Tensor,"The PR is proposed to resolve #14788.

tf.Print converts Variable to mutable Tensor, instead of constant.

```python
v = tf.Variable([99])
# <tf.Variable 'Variable:0' shape=(1,) dtype=int32_ref>

p = tf.Print(v, [v])
# Tensor(""PrintRef:0"", shape=(1,), dtype=int32_ref)
```

### How to test

+ [x] add test case
+ [ ] pass all tests",tensorflow
14893,powderluv,pr,2017-11-27T00:06:18Z,"[XLA] FIX XLA/tfcompile on OSX. #if Guard AVX, SSE and NEON instructions","This fixes XLA / tfcompile on OSX. 

On OSX you currently run into linker errors because unsupported
instructions are registered. Add ifdefs to register only the
supported instructions.

Also include PR#14137 changes for missing __sincos/__sincosf in
XLA on macOS since it was closed without a merge. 

TEST=Build tensorflow/compiler/aot/tests:tfcompile builds
successfully on OSX (10.13.2)",tensorflow
14974,guillaumekln,pr,2017-11-29T12:53:26Z,Support passing layer instances to produce attentional hidden states,"This PR closes #14972.
  ",tensorflow
14976,bryant1410,pr,2017-11-29T13:57:54Z,Add missing <stdio.h> include,"Add missing <stdio.h> include in `tensorflow/contrib/lite/kernels/op_macros.h`. Otherwise, it fails to find `stderr`.

To reproduce it:

```shell
bazel build //tensorflow/contrib/lite/java:tflite_runtime
```",tensorflow
14979,boeddeker,pr,2017-11-29T18:41:02Z,improved estimator.export_savedmodel exception,,tensorflow
15000,facaiy,pr,2017-11-30T11:51:15Z,"WIP: Support ""causal"" padding in tf.layers.convolutional.Conv1D","Fix #14933.

Because we don't see causal padding in other use cases expect of NTC, we choose to modify code at Conv1D, instead of `tf.nn.convolution`.
For simplicity, we hack the Conv1D's `__call__` method, instead of `build`, `call` and `_compute_output_shape`.

### How to test

+ [x] add test case.
+ [ ]  pass all tests.",tensorflow
15005,freedomtan,pr,2017-11-30T15:05:13Z,Make build rule for //tensorflow/contrib/lite/tools:benchmark_model,"Resolve issue https://github.com/tensorflow/tensorflow/issues/14581
Note that this benchmark_model.cc is not completed yet. It doesn't
load and models.

1. With proper Android SDK and NDK settings in WORKSPACE, we can
   build armeabi-v7a or arm64-v8a binary, e.g.,

   > bazel build --cxxopt='--std=c++11' \
     --crosstool_top=//external:android/crosstool \
     --cpu=arm64-v8a \
     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
    //tensorflow/contrib/lite/tools:benchmark_model

2. It's also possible to build this for Linux or OS X, e.g.,

   >  bazel --config opt --cxxopt='--std=c++11' \
     //tensorflow/contrib/lite/tools:benchmark_model",tensorflow
15021,freedomtan,pr,2017-12-01T02:43:37Z,add link to decode_bmp,add link to decode_bmp to image api guide,tensorflow
15022,freedomtan,pr,2017-12-01T04:19:25Z,update label_image.py,"1. add build rule for label_image.py
2. remove extraneous semicolons",tensorflow
15037,facaiy,pr,2017-12-01T11:26:24Z,"Support ""causal"" padding in tf.layers.convolutional.Conv1D","Because #15000 is closed by mistake, hence the PR is reopened to resolve #14933 issue.

Because we don't see causal padding in other use cases expect of NTC, we choose to modify code at Conv1D, instead of tf.nn.convolution.

Ref: [conv1d implementation](https://github.com/fchollet/keras/blob/d956d19fccf6de6344c282218f1b027453785fa9/keras/backend/tensorflow_backend.py#L3141-3145) in keras.

### How to test

+ [x] add test for layers.Conv1D
+ [x] add test for keras.layers.Conv1D
+ [ ] pass all tests.",tensorflow
15042,alanhdu,pr,2017-12-01T16:21:39Z,Only install enum34 on Python <3.4 versions (Take 2),"Python 3.6 sometimes has issues with enum34 because the standard library
relies on enum features not in enum34 (see
https://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36
for more details).

We'll avoid the new versioning syntax in setuptools to allow old versions of setuptools to still work (see #14779)

Do you mind taking a look @gunan @yifeif?",tensorflow
15068,facaiy,pr,2017-12-03T02:43:03Z,tf.Print supports Variable,"CF #14788, #14874

I agree that it might be not a good idea to pass Variable as `input_` for `tf.Print`, and the implementation might be incomplete, incorrect or even potentially dangerous.  So feel free to close the pr.

As we knwon, `tf.Print` is consistent with `tf.identity`, which always return a Tensor with the same type. However, the pr changes `tf.Print`'s behavior as:
+ return Tensor for Tensor.
+ return mutable Tensor for mutable Tensor.
+ return Variable for Variable.

The pr is opposed to #15069.",tensorflow
15069,facaiy,pr,2017-12-03T02:54:23Z,DOC: underline that tf.Print behaves like tf.identity,"As @alextp suggested in #14788, fix the docstring: print should have the same behavior as identity (and it does)

The pr is opposed to #15068 .",tensorflow
15095,freedomtan,pr,2017-12-04T12:30:47Z,add label_image for tflite,"label_image for TensorFlow Lite is inspired by TensorFlow's
[label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image), a command line app to load and run classifier
models.",tensorflow
15098,boeddeker,pr,2017-12-04T14:21:01Z,improve tf.saved_model.loader.load exception,"If the `tags` argument to `tf.saved_model.loader.load` is wrong, the exception does not help.
First: It says use saved_model_cli, but it take a while to figure out that this is a executable and not a python function.
Second: The required information (allowed tags) is known inside `tf.saved_model.loader.load` and now it prints this error
```python
>>> with tf.Session(graph=tf.Graph()) as sess:
>>>    tf.saved_model.loader.load(sess, ['wrong tag'], 'path/to/model')
RuntimeError: MetaGraphDef associated with tags 'wrong tag' could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`available_tags: [{'serve'}]
```",tensorflow
15121,boeddeker,pr,2017-12-05T10:08:33Z,improve py_func,"See #14448
Improved `py_func` to accept nested structures as input and as output like in `tf.data.Dataset.form_generator`

 - relable inp to args and Tout to output_types
 - add arguments kwargs and output_shapes
 - allow args/kwargs/output_types/output_shapes to be a nested structure

Open questions:
 - allow output_types to be callable? (Dynamic output_types inference from args/kwargs)
 - (new) argument names
 - backward compatibility for old names",tensorflow
15127,freedomtan,pr,2017-12-05T13:51:53Z,OS X ld does not have icf,"when using tflite_linkopts_unstripped() or tflite_linkopts() to build stuff, such as https://github.com/tensorflow/tensorflow/pull/15095, on OS X, identical code folding flag is not supported.",tensorflow
15139,alsrgv,pr,2017-12-05T20:22:48Z,Support --config=monolithic in tf.sysconfig.get_link_flags(),"Currently, `tf.sysconfig.get_link_flags()` always adds `-ltensorflow_framework`.  With this change, it would check whether TensorFlow was built with `--config=monolithic`.",tensorflow
15176,freedomtan,pr,2017-12-07T06:35:50Z,fix a problem in tflite custom_operators.md,"s/SinResize/SinPrepare/ 
Obviously, it should be SinPrepare instead of SinResize.",tensorflow
15182,facaiy,pr,2017-12-07T11:58:42Z,GPU: Add Complex kernel for tf.exp(),"Fix #15103.

Because it's my first contribution for GPU kernel, the PR might be not good. Welcome to feedback, and any help will be appreciated. Thanks.

### How to test

+ [x] add test case.
+ [ ] pass all test.",tensorflow
15183,boeddeker,pr,2017-12-07T14:16:33Z,estimator: allow export_outputs to be a Tensor,"When someone wants to use `tf.estimator.Estimator.export_savedmodel` to save the predictions of a model and `tf.contrib.predictor.from_saved_model` to load the prediction function, it is currently necessary to define boilerplate code. This PR reduces the amount of code.

Currently:
```python
...
ret['export_outputs'] = {
        'predictions': tf.estimator.export.PredictOutput(
            {'predictions': ret['predictions']}
        )
    }
tf.estimator.EstimatorSpec(**ret)
```
With this PR the following is enough:
```python
ret['export_outputs'] = {'predictions': ret['predictions']}
ret['export_outputs'] = ret['predictions']
```
The predictor repr for the first example is equal to the original code
```python
SavedModelPredictor with feed tensors {'x': <tf.Tensor 'Placeholder:0' shape=(1, 10) dtype=float32>} and fetch_tensors {'predictions': <tf.Tensor 'dense/Relu:0' shape=(1, 10) dtype=float32>}
```
and for the second the output name changes
```python
SavedModelPredictor with feed tensors {'x': <tf.Tensor 'Placeholder:0' shape=(1, 10) dtype=float32>} and fetch_tensors {'output': <tf.Tensor 'dense/Relu:0' shape=(1, 10) dtype=float32>}
```

When someone wants to verify the code, here a full example:
```python
from types import SimpleNamespace
import numpy as np
import tensorflow as tf
from tensorflow.contrib import predictor

def model_fn(features, labels, mode):
    assert labels is None, labels
    x = features['x']
    y = features['y']
    
    y_hat = tf.layers.Dense(10, tf.nn.relu)(x)
    loss = tf.losses.mean_squared_error(y, y_hat)
    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
    
    ret = dict(
        mode=mode,
        loss=loss,
        predictions=y_hat,
        train_op=train_op,
    )

    # Boilderplate:
    ret['export_outputs'] = {  # <------------------- old
        'predictions': tf.estimator.export.PredictOutput(
            {'predictions': ret['predictions']}
        )
    }
    ret['export_outputs'] = {'predictions': ret['predictions']}  # <------------------- new
    ret['export_outputs'] = ret['predictions']  # <------------------- new alternate
    
    return tf.estimator.EstimatorSpec(**ret)
        
estimator = tf.estimator.Estimator(
    model_fn=model_fn
)

def generator():
    x = np.random.randn(1, 10).astype(np.float32)
    y = np.random.randn(1, 10).astype(np.float32)
    yield {'x': x, 'y': y}

def input_fn():
    ds = tf.data.Dataset.from_generator(generator, {'x': tf.float32, 'y': tf.float32}, {'x': [1, 10], 'y': [1, 10]})
    it = ds.make_one_shot_iterator()
    element = it.get_next()
    return element

estimator.train(input_fn)

def serving_input_receiver_fn():
    x = tf.placeholder(tf.float32, [1, 10])
    y = tf.placeholder(tf.float32, [1, 10])
    
    ret = SimpleNamespace()
    ret.features = {'x': x, 'y': y}  # for graph def
    ret.receiver_tensors = {'x': x}  # for serving
    ret.receiver_tensors_alternatives = None
    return ret


model_path = estimator.export_savedmodel('tmp', serving_input_receiver_fn)

predict_fn = predictor.from_saved_model(model_path)
# predict_fn({'x': np.random.randn(1, 10)})
predict_fn
```",tensorflow
15229,powderluv,pr,2017-12-09T04:54:25Z,[XLA] Fix another XLA/tfcompile compile error on OSX ,"This fixes issue https://github.com/tensorflow/tensorflow/issues/15196 (though that was accidentally closed)

kernel_support_library.cc:99:5: error: no matching function
for call to 'transform' std::transform(function->arg_begin(),..

TEST=build tfcompile on OSX (also requires PR#14893)",tensorflow
15234,facaiy,pr,2017-12-09T13:29:16Z,Fix static shape inference for keras.layers.LSTM,"fix #15165

### How to test

+ [x] add test case.
+ [ ] pass all tests.",tensorflow
15245,facaiy,pr,2017-12-10T06:48:33Z,add c++ gradient for op: Pow,"Fix  #15239.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",tensorflow
15282,nluehr,pr,2017-12-11T21:04:22Z,Fixed memory_stats_ops_test,Added explicit dependency to avoid matrix free prior to stats op execution.,tensorflow
15283,nluehr,pr,2017-12-11T23:10:06Z,Fix minor typo in CUDNN_VERSION check,"Effectively enables CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED in
CudnnSupport::GetConvolveBackwardFilterAlgorithms() for cuDNN v5.1.",tensorflow
15333,taehoonlee,pr,2017-12-13T08:00:25Z,Fix typos,"This PR fixes some typos: `transfered`, `betweeen`, `forwared`, `occuring`, `occured`, `varibale`, `succesfully`, and `depedency`.",tensorflow
15354,facaiy,pr,2017-12-14T04:21:58Z,CPU: Support for DT_STRING type in ScatterNd ,"The PR is proposed to fix #15321.

~~However, the solution seems not idea. Because `ScatterNdOP` use ADD op by default, which doesn't make sense for string type. I prefer to use ASSIGN than ADD, but the modification will change the op's behavior.~~

### How to test 

+ [x] add test.
+ [ ]  pass all tests.",tensorflow
15355,flx42,pr,2017-12-14T05:39:52Z,Dockerfile.devel-gpu: optimize the size of the generated image,"- Use `nvidia/cuda:9.0-base-ubuntu16.04` as the base image to select
  just the CUDA libraries we need.
- Remove the installed static libraries.
- Remove the dependency on openjdk-8 since Bazel ships with a local copy.
- Perform a shallow clone of the repository.

The image is 2.94GB, down from 4.87GB.

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>

See initial discussion here: https://github.com/tensorflow/tensorflow/issues/15284
@gunan @martinwicke @yongtang ",tensorflow
15378,taehoonlee,pr,2017-12-15T02:11:27Z,Fix PEP8,This PR fixes pep8-related issues.,tensorflow
15379,flx42,pr,2017-12-15T04:29:44Z,Only emit PTX code for the most recent CUDA compute architecture,"When there is no gap in the compute architectures you are compiling
against, there is no point in generating PTX for all architectures but
the latest.

See the CUDA documentation:
http://docs.nvidia.com/cuda/volta-compatibility-guide/index.html#building-volta-compatible-apps-using-cuda-9-0

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>

Slightly decreases the size of the generated binaries.",tensorflow
15399,facaiy,pr,2017-12-15T14:43:00Z,Fix bug: leaky relu supports float64,"Fix #15391.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",tensorflow
15407,lutzroeder,pr,2017-12-15T22:13:05Z,Fix tflite models.md title,,tensorflow
15443,facaiy,pr,2017-12-18T11:23:25Z,New metric: Cohen's kappa,"resolve #15285.

Add new metric: `cohen_kappa`, which is equivalent to `sklearn.metrics.cohen_kappa_score`(>=0.19), but the implementation doesn't support weighted matrix yet.

Ref:
+ [Cohen's kappa - Wiki](https://en.wikipedia.org/wiki/Cohen's_kappa)
+ [Cohen's kappa: Index of Inter-rater Reliability](http://psych.unl.edu/psycrs/handcomp/hckappa.PDF)
+ [sklearn.metrics.cohen_kappa_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)

### How to test

+ [x] add test cases.
+ [ ] pass all tests.",tensorflow
15471,facaiy,pr,2017-12-19T05:41:42Z,BUG: fix name scope collision in `tf.layers`,"Fix  #13429.

Since  #14390 has been merged into master branch, we can easily solve the problem with `auxiliary_name_scope=False`.

### How to test

+ [x] add test case.
+ [x] pass all tests.
",tensorflow
15535,powderluv,pr,2017-12-21T01:23:07Z,[iOS/tflite] Add ability to build a specific arch,"Add a flag to build only the arch you want. By default build all.
Also check number of CPUs when invoking the make commands -j

TEST=tensorflow/contrib/lite/build_ios_universal_lib.sh -a arm64",tensorflow
15544,facaiy,pr,2017-12-21T06:29:42Z,support unknown shape for `sparse_multiclass_hing_loss`,"Fix #15480.

### How to test

+ [x] add test case.
+ [ ] pass all tests.",tensorflow
15574,yanboliang,pr,2017-12-22T07:07:18Z,Fix adding elements to collections.deque.,"In some cases, [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L123) will fail at L123 and throw the following error:
```
TypeError: sequence index must be integer, not 'slice'
```
It seams the ```text8``` dataset and parameters will not touch that line code, but I hit this issue when I run this script against my own dataset. This is caused by Python ```collections.deque``` doesn't support index by slice:
```
>>> from collections import deque
>>> d = deque('tensorflow')
>>> d[:]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: sequence index must be integer, not 'slice'
```
This PR fix this issue by using ```deque.extend```, and this is consistent with L114.",tensorflow
15639,freedomtan,pr,2017-12-26T09:53:39Z,fix doc for benchmark_model for android,"after configure.py set framework_shared_object=true,
benchmark_model won't build for Android without tweeks. Add
'--config monolithic' to avoid confusion.",tensorflow
15664,themightyoarfish,pr,2017-12-27T18:54:06Z,Fix small typo in docstring which causes the API doc to render incorrâ€¦,"â€¦ectly
See
![screen shot 2017-12-27 at 19 53 06](https://user-images.githubusercontent.com/11613312/34390366-adc111e8-eb3f-11e7-8d31-bdb0c24c32a9.png)
on https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant",tensorflow
15686,freedomtan,pr,2017-12-28T09:30:22Z,fix description of HASHTABLE_LOOKUP in smartreply doc,HASHTABLE_LOOKUP is a builtin op rather than a custom one.,tensorflow
15807,facaiy,pr,2018-01-03T06:12:08Z,Fix unstable test case for Select op,"Fix #14862. CF: #15764

In the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.

Test passed for 100 times.
```bash
[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test
HEAD is now at c642574... TST: modify unstable test case
INFO: Found 1 test target...
Target //tensorflow/cc:gradients_math_grad_test up-to-date:
  bazel-bin/tensorflow/cc/gradients_math_grad_test
INFO: Elapsed time: 49.959s, Critical Path: 21.29s
//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s
  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s

Executed 1 out of 1 test: 1 test passes.
```

cc @gunan @drpngx 
  ",tensorflow
15825,0x00b1,pr,2018-01-03T21:42:09Z,Update advise.md,,tensorflow
15843,facaiy,pr,2018-01-04T11:08:15Z,WIP: LSTMBlockFusedCell supports Dropout,"Fix #13649.

The work has not been done yet. However I'm not sure whether the solution is approved, I mean, using DropoutWrapper for inner implementation. So I open the PR early to collect feedback. cc @ebrevdo.

I'll add test case later.
  ",tensorflow
15969,facaiy,pr,2018-01-09T05:59:31Z,Fix variable property of DropoutWrapper,Fix #15810.,tensorflow
15993,taehoonlee,pr,2018-01-10T02:59:48Z,Fix typos,"This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.",tensorflow
16060,zheng-xq,pr,2018-01-12T01:17:21Z,Branch 181679271,Merging internal changes,tensorflow
16065,freedomtan,pr,2018-01-12T05:40:40Z,export tflite::Intepreter's  UseNNAPI() and setNumThreads() to java,"Export tflite::Intepreter's UseNNAPI() and SetNumThreads() to Java
and modify the Android TfLiteCameraDemo app to use them.",tensorflow
16092,zheng-xq,pr,2018-01-13T02:23:26Z,Branch 181814918,,tensorflow
16123,facaiy,pr,2018-01-15T06:48:28Z,add rolling window batch operation for tf.data.Dataset ,"Resolve #15044.

### implementation

The PR proposes a `slide` method for Dataset: Groups elements in fixed size blocks by passing a ""sliding window"" over Dataset. It behaves like `batch`, in fact, `batch(n) == slide(n, n)`.

~I failed to move c++ implementation from `core` to `contrib`. Any help will be appreciated.~


### how to test

+ [x] add test case.
+ [x] pass all tests.",tensorflow
16131,bryant1410,pr,2018-01-15T15:59:50Z,Update contrib/HVX readme,"I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.

First, the very simple ""quick start guide"" doesn't work, there's no ""-X"" option (at least publicly) and so you always need to have the SDK installed manually.

Apart from that, some clarifications and rewording were done to help the users understand what's happening.

/cc @satok16 ",tensorflow
16158,bstriner,pr,2018-01-16T11:36:00Z,GAN model: move generated and real operations under discriminator namespace,"Hi everybody,

`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.

* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.
* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.

`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.

Cheers",tensorflow
16175,freedomtan,pr,2018-01-17T04:23:38Z,make applications using TFLite work on ARM64 Linux (non-Android) platforms,"When build TF Lite related stuff such as lable_image for tflite on ARMv64 non-Android environment (I am running Debian on an internal development board). I saw something like:

 ```
 /home/freedom/work/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:264:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:neon_tensor_utils' failed (Exit 1)
  gcc: error: unrecognized command line option '-mfpu=neon'
  gcc: error: unrecognized command line option '-mfloat-abi=softfp'
```

It seems ARM64 falls into the "":arm"" category. After removing it, I can build my tflite-based command line programs without problems.",tensorflow
16183,facaiy,pr,2018-01-17T08:06:26Z,`AssignVariableOp` supports `DT_BFLOAT16`,Fix #16103,tensorflow
16201,benoitsteiner,pr,2018-01-18T00:02:16Z,Branch 182280342,,tensorflow
16206,freedomtan,pr,2018-01-18T02:33:26Z,make label_image for tflite build again,"1. add namespace to label_image.h to make label_image for tflite build again
2. add --config monolithic and mention NDK settings in label_image.md
3. fix a typo in display_usage()",tensorflow
16222,benoitsteiner,pr,2018-01-18T16:57:40Z,Branch 182384458,,tensorflow
16233,benoitsteiner,pr,2018-01-19T00:52:44Z,Branch 182456096,,tensorflow
16245,benoitsteiner,pr,2018-01-19T16:35:22Z,Branch 182511847,,tensorflow
16256,freedomtan,pr,2018-01-20T00:01:05Z,[tflite] make calling NNAPI work again,"calling PrepareOpsAndTensors() before using NN API looks
1. unnecessary
2. will decrease `next_node_to_prepare_ so` that the next
```
   next_node_to_prepare_ == nodes_and_registration_.size()
```
will fail",tensorflow
16275,brettkoonce,pr,2018-01-22T03:39:32Z,minor spelling tweaks for lite docs,,tensorflow
16292,benoitsteiner,pr,2018-01-22T16:41:22Z,Branch 182783262,,tensorflow
16318,cclauss,pr,2018-01-23T09:51:42Z,import tensorflow as tf,These five files do not explicitly `import tensorflow as tf` yet they use they use __tf.__ methods or functions which drives linters like pylint and flake8 crazy unless special directives are put in place.,tensorflow
16322,facaiy,pr,2018-01-23T10:34:23Z,py_func convert unicode string results to bytes for python2,Fix #16320,tensorflow
16349,taehoonlee,pr,2018-01-24T03:36:43Z,Fix typos,"This PR fixes some typos: `intializes`, `nubmer`, `varibale`, `fuction`, `Ouput`, `avaliable`, `ouput`, and `Explictly`.",tensorflow
16355,brettkoonce,pr,2018-01-24T06:35:52Z,minor spelling tweaks for eager execution docs,,tensorflow
16426,cclauss,pr,2018-01-26T01:42:22Z,import contextmanager in side_effect_guards.py,"Resolve the undefined name '__contextmanager__' in __side_effect_guards.py__ via __from contextlib import contextmanager__ and remove the associated pylint directive.

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
tensorflow/contrib/py2tf/converters/side_effect_guards.py:126:17: F821 undefined name 'contextmanager'
          ctx = contextmanager(lambda: (yield))()  # pylint:disable=undefined-variable
                ^
```",tensorflow
16432,taehoonlee,pr,2018-01-26T02:39:25Z,Fix docstrings in `scan`,,tensorflow
16438,cclauss,pr,2018-01-26T03:43:34Z,xrange() was removed in Python 3,Each of these files contains at least one call to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  To each of these files we add the line [__from six.moves import xrange__](https://pythonhosted.org/six/#module-six.moves) for compatibility with both Python 2 and Python 3.,tensorflow
16439,cclauss,pr,2018-01-26T04:04:38Z,long was removed in Python 3,"__long__ was [removed](https://docs.python.org/3/whatsnew/3.0.html#integers) from Python 3 in favor of __int__.  Here we have replaced the tuple __(int, long)__ with [six.integer_types](https://pythonhosted.org/six/#six.integer_types) which does the right thing in both Python 2 and Python 3.",tensorflow
16440,cclauss,pr,2018-01-26T04:25:51Z,raw_input() was removed in Python 3,__raw_input()__ was removed in Python 3 in favor of __input()__.  We __import six__ so that [__six.moves.input()__](https://pythonhosted.org/six/#module-six.moves) works identically in both Python 2 and Python 3.  We also add .strip() to gracefully deal with leading or trailing whitespace in the user input as well as lower() to gracefully deal with capital as well as lowercase letters.,tensorflow
16446,freedomtan,pr,2018-01-26T07:44:36Z,use tflite bilinear op to resize input of label_image,replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator,tensorflow
16449,cclauss,pr,2018-01-26T09:09:47Z,Placate pylint on jupyter_notebook_config.py,"Eliminate the following pylint issues:
```
E: 18, 0: Undefined variable 'c' (undefined-variable)
E: 19, 0: Undefined variable 'c' (undefined-variable)
E: 20, 0: Undefined variable 'c' (undefined-variable)
E: 26, 4: Undefined variable 'c' (undefined-variable)
E: 28, 4: Undefined variable 'c' (undefined-variable)
E: 29, 4: Undefined variable 'c' (undefined-variable)
```",tensorflow
16485,cclauss,pr,2018-01-27T09:07:33Z,resolve undefined name array_ops,"flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/framework/python/ops/accumulate_n_v2.py:94:12: F821 undefined name 'array_ops'
    return array_ops.identity(inputs[0], name=name)
           ^
```",tensorflow
16500,cclauss,pr,2018-01-27T17:58:50Z,contrib/learn: Typo in variable name x_exrta --> x_extra,"flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'
    spir_x = np.append(spir_x, x_extra)
                               ^
```",tensorflow
16501,cclauss,pr,2018-01-27T18:22:13Z,"Define Cr, Fr, Shared, Var to resolved undefined names","flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/specs/python/specs_test.py:202:11: F821 undefined name 'Cr'
      _ = Cr
          ^
./tensorflow/contrib/specs/python/specs_test.py:204:28: F821 undefined name 'Cr'
      self.assertIsNotNone(Cr)
                           ^
./tensorflow/contrib/specs/python/specs_test.py:205:32: F821 undefined name 'Cr'
      self.assertTrue(callable(Cr(64, [3, 3])))
                               ^
./tensorflow/contrib/specs/python/specs_test.py:207:11: F821 undefined name 'Cr'
      _ = Cr
          ^
./tensorflow/contrib/specs/python/specs_test.py:215:13: F821 undefined name 'Var'
        v = Var(""test_var"",
            ^
./tensorflow/contrib/specs/python/specs_test.py:232:13: F821 undefined name 'Shared'
        f = Shared(Fr(100))
            ^
./tensorflow/contrib/specs/python/specs_test.py:232:20: F821 undefined name 'Fr'
        f = Shared(Fr(100))
                   ^
```",tensorflow
16554,yanboliang,pr,2018-01-29T22:10:01Z,Correct argument doc for BasicLSTMCell.call,,tensorflow
16577,bstriner,pr,2018-01-30T09:37:58Z,"By default, only download inception if it doesn't exist already","Hope this saves some bandwidth:

- I updated `get_graph_def_from_url_tarball` to accept a default location and only download a file if the file has not already been downloaded. If you do not give it a default location, it will always download (preserving existing behavior).
- I added a default location for the inception model as part of `_default_graph_def_fn`. This means you only download inception the first time you run `run_inception` instead of every time you start your script.

Cheers",tensorflow
16579,bstriner,pr,2018-01-30T09:55:51Z,Add option to not include histograms,"`add_gan_model_image_summaries` does the work of adding images to summaries, but then it also calls `add_gan_model_summaries` which dumps every trainable variable to histograms. It would be nice to be able to get the image summaries without the histograms.

I would prefer to just delete that line, because it is weird that the two functions are tied. It wouldn't be hard to call both functions in your code if you wanted both.

However, this preserves existing functionality. If you pass `model_summaries=False`, it does not call `add_gan_model_summaries`.

Cheers",tensorflow
16616,Randl,pr,2018-01-31T08:03:48Z,[WIP] More generic configuration,"https://github.com/tensorflow/tensorflow/issues/13851 
@drpngx",tensorflow
16666,cclauss,pr,2018-02-01T17:17:34Z,Typo in variable name: BETA --> self.BETA,"__BETA__ is defined on line 118 as a class member so it can only be accessed via __self__ or via the class name, __ElasticAverageOptimizer__.

flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/opt/python/training/elastic_average_optimizer.py:153:27: F821 undefined name 'BETA'
      self._moving_rate = BETA / communication_period / num_worker
                          ^
```",tensorflow
16668,cclauss,pr,2018-02-01T17:49:41Z,Fix undefined name: import as_str_any for line 35,"flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/python/util/compat_internal.py:33:12: F821 undefined name 'as_str_any'
    path = as_str_any(path.__fspath__())
           ^
```",tensorflow
16711,superbobry,pr,2018-02-02T21:36:52Z,Fixed a typo in `group_by_window` documentation,Nothing else to add :),tensorflow
16802,freedomtan,pr,2018-02-06T15:26:45Z,[tflite] make calling NNAPI work again (resend),"for the previous one (https://github.com/tensorflow/tensorflow/pull/16256) somehow reverted/overwritten

calling PrepareOpsAndTensors() before using NN API looks
  1. unnecessary
  2. decrease next_execution_plan_index_to_prepare_ so that
     the logic check in the next line
     `next_execution_plan_index_to_prepare_ == execution_plan_.size`
     will fail",tensorflow
16811,brettkoonce,pr,2018-02-06T23:21:29Z,spelling fixes for contrib docs,,tensorflow
16830,freedomtan,pr,2018-02-07T12:48:06Z,[tflite] fixed label_image resize bilinear problems,"1. Interpreter does not need delete anymore, so cannot use `std::unique_ptr<>`, otherwise there will be double free
2. ResizeBilinear need the `align_corners` parameter after https://github.com/tensorflow/tensorflow/commit/1a0b637df8d082301118dd0f85ec63704f862aeb",tensorflow
16897,chrisyeh96,pr,2018-02-09T19:31:38Z,Improve formatting of Tensor shapes in tf.losses,"Updating the documentation of Tensor shapes in `tf.losses` to match the documentation guide at 
https://www.tensorflow.org/community/documentation",tensorflow
16902,chrisyeh96,pr,2018-02-10T00:14:40Z,Add reduction parameter to mean_pairwise_squared_error loss,"- add reduction parameter to `tf.losses.mean_pairwise_squared_error` to make it consistent with all of the other loss functions
- increased clarify of the documentation for the function
- use the `axis` parameter instead of `reduction_indices` for `math_ops.reduce_sum()` because `reduction_indices` is deprecated",tensorflow
16921,chrisyeh96,pr,2018-02-10T20:57:19Z,Improve formatting of shapes in tf.losses documentation,,tensorflow
16922,chrisyeh96,pr,2018-02-10T21:10:04Z,Add reduction parameter to mean_pairwise_squared_error loss,Also increased clarify of the documentation for the function,tensorflow
17010,byronyi,pr,2018-02-14T13:30:17Z,Fixes #16976,,tensorflow
17027,nluehr,pr,2018-02-15T02:51:11Z,Fix __shared__ complex<T> undefined behavior,"std::complex<T> has a non-empty constructor (zero assignment) that is not
compatible with CUDA __shared__ memory. This fixes current reliance on
undefined behavior (and removes an unnecessary run-time initialization).",tensorflow
17083,facaiy,pr,2018-02-17T04:47:38Z,tf.tile gradient supports IndexedSlice,Fix #16930,tensorflow
17086,facaiy,pr,2018-02-17T06:43:24Z,tf.Dimension raises TypeError for tf.DType,fix #17079.,tensorflow
17094,freedomtan,pr,2018-02-17T15:44:01Z,make the TfLiteCameraDemo.apk built with bazel work again,"Current bazel build rule needs ""labels.txt"". Build with current head, you get ""Uninitialized Classifier or invalid context"" as shown in the figure below. ![failed](https://user-images.githubusercontent.com/3395998/36347403-69fb75cc-1491-11e8-81df-822c27e162f9.png)
",tensorflow
17125,chrisyeh96,pr,2018-02-19T10:53:04Z,Remove extraneous check for Eager mode,The check is already made once at the start of the method,tensorflow
17126,chrisyeh96,pr,2018-02-19T11:57:22Z,Cleaner documentation for tf.confusion_matrix,Also use `tf.stack` with `axis=1` instead of a stack + transpose.,tensorflow
17152,facaiy,pr,2018-02-20T14:15:13Z,docker ci allows bad name user,"Our server maintainer creates user name like ""xxx.yyy"", which is invalid for `adduser` and so docker ci failed. I hope tensorflow could provide a way to loose the restriction:
1. use `--force-badname` for `adduser`.
2. or, we can provide `CI_BULID_USER` for `ci_builder.sh`.",tensorflow
17224,cclauss,pr,2018-02-23T17:51:42Z,TF Lite: from six.moves import xrange for Python 3,Lines 1785 and 1818 contain calls to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  This PR adds the line [__from six.moves import xrange__](http://six.readthedocs.io/#module-six.moves) for compatibility with both Python 2 and Python 3.,tensorflow
17225,cclauss,pr,2018-02-23T18:19:23Z,Change unicode() --> six.text_type() for Python 3,__unicode()__ was removed in Python 3 because all str are Unicode so this PR changes four calls to __unicode()__ into calls to [__six.text_type()__](http://six.readthedocs.io/#six.text_type).,tensorflow
17230,nkreeger,pr,2018-02-23T21:08:48Z,Ship TF Eager header with libtensorflow tarballs.,"The TF Eager headers are getting excluded from nightly tarballs because they both have the same filename and output destination. Looks like the core TF c header wins. This PR introduces shipping the eager header in the correct location.

Take a look at one of the hosted tarballs here: http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow/TYPE=cpu-slave/",tensorflow
17280,nkreeger,pr,2018-02-26T19:16:20Z,Upgrade Jenkins/Docker build scripts to Bazel 0.11.0.,"The 0.10.0 bazel has problems with static-linking on linux:
https://github.com/bazelbuild/bazel/issues/4474. This PR bumps to the
latest bazel that produces proper binaries w/o the linking issue.",tensorflow
17338,alanhdu,pr,2018-03-01T00:46:04Z,Don't use NCHW or NHCW for conv1d,Fixes deprecation warning when using tf.layers.conv1d,tensorflow
17371,brettkoonce,pr,2018-03-02T05:03:11Z,minor spelling tweaks for contrib/verbs docs,,tensorflow
17395,facaiy,pr,2018-03-03T09:04:21Z,add AdaMax optimizer,"Fix #17104. The PR implements the AdaMax optimizer: https://arxiv.org/pdf/1412.6980v8.pdf

Please pay attention to three points when reviewing:
1. To avoid division by zero, we add `epsilon` for `v_t`, which is slightly different from original paper section 7.1. And we explained the change in the document.
2. Contrast to the sparse implementation of [`tf.train.AdamOptimizer`](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/AdamOptimizer), the AdaMaxOptimizer only updates variable slices and corresponding `m_t`, `v_t` terms when that part of the variable was used in the forward pass.
3. Because AdaMax is quite similar with Adam, we refactor the Adam code in c++ side to reuse its route. Not sure whether the solution sounds good.

The PR is still very rudimentary implementation, any feedback/review would be appreciated.",tensorflow
17455,alanhdu,pr,2018-03-06T00:13:53Z,Don't use NCHW or NHCW in tf.layers.conv1d,Fixes deprecation warning when using tf.layers.conv1d.,tensorflow
17480,facaiy,pr,2018-03-06T12:57:25Z,add assert_element_shape method for tf.contrib.data,"fix #16052. 

```python
shapes = [tf.TensorShape([16, 256]), tf.TensorShape(None)]
result = dataset.apply(tf.contrib.data.assert_element_shape(shapes))
print(result.output_shapes)  # ==> ""((16, 256), <unknown>)""
```",tensorflow
17531,brettkoonce,pr,2018-03-08T02:12:26Z,contrib/lite: spelling/code tweaks,,tensorflow
17545,facaiy,pr,2018-03-08T08:24:07Z,add reflexive method for Dimension,Fix #17482,tensorflow
17591,guillaumekln,pr,2018-03-09T13:19:55Z,Support other dtypes in BeamSearchDecoder initialization,The `BeamSearchDecoder` initialization failed when other dtypes were used (e.g. `tf.float16`). This PR correctly converts the scalar values to tensors with the current dtype.,tensorflow
17597,brettkoonce,pr,2018-03-09T18:39:48Z,"SECURITY.md: minor sp, permisisons->permissions",,tensorflow
17637,freedomtan,pr,2018-03-12T06:01:03Z,make benchmark_model for TFLite build,It doesn't build for Android and Ubutu box. Make it build.,tensorflow
17661,brettkoonce,pr,2018-03-13T00:03:34Z,contrib/quantize: minor spelling,,tensorflow
17700,freedomtan,pr,2018-03-14T03:16:57Z,make TFLite kernel tests work,"make
```
bazel build --config opt //tensorflow/contrib/lite/kernels:all
```
work. And then
```
bazel test --config opt //tensorflow/contrib/lite/kernels:all
```
works.",tensorflow
17701,brettkoonce,pr,2018-03-14T03:23:35Z,contrib/tpu: minor spelling tweaks,,tensorflow
17706,superbobry,pr,2018-03-14T09:44:50Z,Changed sparse_column_with_vocabulary_file to estimate vocab_size,"Prior to this change, the function required vocab_size to be explicitly specified by the user which made the API inconsistent with categorical_column_with_vocabulary_file.",tensorflow
17723,flx42,pr,2018-03-14T23:49:45Z,Pin the version of cuDNN used in Dockerfile.gpu,"Related: #17566
Fixes: #17431

Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>
",tensorflow
17726,brettkoonce,pr,2018-03-15T01:15:17Z,contrib/gan: minor spelling tweaks,,tensorflow
17732,brettkoonce,pr,2018-03-15T04:42:28Z,contrib/boosted_trees: minor spelling tweaks,,tensorflow
17788,brettkoonce,pr,2018-03-17T00:00:33Z,contrib: minor spelling tweaks,"packages:
  model_pruning
  rnn
  solvers
  tensorrt",tensorflow
17815,brettkoonce,pr,2018-03-19T02:06:24Z,contrib: minor spelling tweaks,,tensorflow
17871,benoitsteiner,pr,2018-03-20T22:48:12Z,Branch 189819449,,tensorflow
17885,freedomtan,pr,2018-03-21T06:25:50Z,make toco build for android,"for ARMv8
`
bazel build --config android_arm64 --cxxopt=-std=c++11 --linkopt=""-llog"" --linkopt=-pie //tensorflow/contrib/lite/toco:toco   --config monolithic
`
for ARMv7a
`
bazel build --config android_arm --cxxopt=-std=c++11 --linkopt=""-llog"" --linkopt=-pie //tensorflow/contrib/lite/toco:toco   --config monolithic
`",tensorflow
17899,benoitsteiner,pr,2018-03-21T16:15:20Z,Branch 189913309,,tensorflow
17935,benoitsteiner,pr,2018-03-22T23:15:55Z,Branch 190141732,,tensorflow
17940,freedomtan,pr,2018-03-23T02:05:46Z,[tflite] fix number of gemmlowp threads problem,"Recent changes moved setting of the number of gemmlowp threads to
Init() of conv operation so that invoking `Interpreter::SetNumThreads()`
after nodes created doesn't change of number of threads.

Note that the number of Eigen threads is also problematic, but I don't have an easy fix yet. In current [`tflite::eigen_support::IncrementUsageCounter()` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/eigen_support.cc#L27), it assumes that `Eigen::setNbThreads()` changes the number of Eigen thread. But current [multithreaded conv](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h) actually has its own Eigen threadpool.",tensorflow
17992,brettkoonce,pr,2018-03-25T18:14:22Z,contrib/factorization: minor spelling tweaks,,tensorflow
18010,brettkoonce,pr,2018-03-26T23:15:01Z,Seq2seq minorsp,"can squash to be more like #17815 if desired, let me know!",tensorflow
18048,taehoonlee,pr,2018-03-28T13:57:54Z,Fix typos,"This PR fixes some typos: `parition`, `avaliable`, `exection`, `ouput`, and `suports`.",tensorflow
18138,brettkoonce,pr,2018-03-30T23:50:00Z,Distribute sp,,tensorflow
18162,brettkoonce,pr,2018-04-01T19:23:31Z,contrib/image: minor spelling tweaks,,tensorflow
18284,brettkoonce,pr,2018-04-06T02:56:14Z,contrib/autograph: minor spelling tweaks,,tensorflow
18305,facaiy,pr,2018-04-07T01:38:15Z,remove the misleading n_class information,"Fix #15800 

`tf.estimator.DNNClassifier` expects a `[D0]` or `[D0, 1]` labels, where `expected_labels_dimension` is not equal to `n_classes`.  Hence the error information below is totally misleading.
> Classifier configured with n_classes=1. Received 4.",tensorflow
18330,brettkoonce,pr,2018-04-08T19:21:13Z,contrib: minor spelling tweaks,,tensorflow
18475,taehoonlee,pr,2018-04-12T23:47:28Z,Fix typos,"This PR fixes some typos: `wihtin`, `seperately`, `paramaters`, `Additonal`, `transformaitons`, `varibale`, and `folow`.",tensorflow
18545,eqy,pr,2018-04-16T06:53:37Z,fix command line example package path,,tensorflow
18651,facaiy,pr,2018-04-18T12:19:11Z,tf.keras: Warn user when they mix up `sparse_categorical_crossentropy` and `categorical_crossentropy`,"Sometimes we convert one-hot labels to integer labels, and forget to change the loss function `categorical_crossentropy` to `sparse_categorical_crossentropy`.

Because `categorical_crossentropy` doesn't check the shape of `target` and `output`, tf.keras works well except a totally wrong result.

The PR only checks the last dimension of static shape for efficient, perhaps we'd better to check whole shape with help of `tf.contrib.framework.with_same_shape`?",tensorflow
18727,freedomtan,pr,2018-04-20T09:15:08Z,[tflite] add profiling to label_image for tflite ,"use the profiling mechanism in dfae914b

build with something like:
```
bazel build --config android_arm64 \
  --cxxopt=-std=c++11 \
  --cxxopt=-DTFLITE_PROFILING_ENABLED \
  //tensorflow/contrib/lite/examples/label_image:label_image
```

run `label_image` will get something like:
```
./label_image -p 1
Loaded model ./mobilenet_quant_v1_224.tflite
resolved reporter
invoked
average time: 67.227 ms
    13.349, Node   0, OpCode   3, CONV_2D
     6.024, Node   1, OpCode   4, DEPTHWISE_CONV_2D
    11.847, Node   2, OpCode   3, CONV_2D
     3.927, Node   3, OpCode   4, DEPTHWISE_CONV_2D
     1.905, Node   4, OpCode   3, CONV_2D
     3.573, Node   5, OpCode   4, DEPTHWISE_CONV_2D
     2.344, Node   6, OpCode   3, CONV_2D
     0.964, Node   7, OpCode   4, DEPTHWISE_CONV_2D
     1.224, Node   8, OpCode   3, CONV_2D
     1.846, Node   9, OpCode   4, DEPTHWISE_CONV_2D
     2.181, Node  10, OpCode   3, CONV_2D
     0.454, Node  11, OpCode   4, DEPTHWISE_CONV_2D
     0.997, Node  12, OpCode   3, CONV_2D
     0.865, Node  13, OpCode   4, DEPTHWISE_CONV_2D
     1.844, Node  14, OpCode   3, CONV_2D
     0.753, Node  15, OpCode   4, DEPTHWISE_CONV_2D
     1.724, Node  16, OpCode   3, CONV_2D
     0.803, Node  17, OpCode   4, DEPTHWISE_CONV_2D
     1.698, Node  18, OpCode   3, CONV_2D
     0.794, Node  19, OpCode   4, DEPTHWISE_CONV_2D
     1.754, Node  20, OpCode   3, CONV_2D
     0.798, Node  21, OpCode   4, DEPTHWISE_CONV_2D
     1.704, Node  22, OpCode   3, CONV_2D
     0.204, Node  23, OpCode   4, DEPTHWISE_CONV_2D
     0.983, Node  24, OpCode   3, CONV_2D
     0.373, Node  25, OpCode   4, DEPTHWISE_CONV_2D
     1.791, Node  26, OpCode   3, CONV_2D
     0.067, Node  27, OpCode   1, AVERAGE_POOL_2D
     0.388, Node  28, OpCode   3, CONV_2D
     0.001, Node  29, OpCode  22, RESHAPE
     0.035, Node  30, OpCode  25, SOFTMAX
0.600: 458 bow tie
0.365: 653 military uniform
0.008: 835 suit
0.008: 611 jersey
0.004: 514 cornet
```",tensorflow
18771,facaiy,pr,2018-04-22T13:16:53Z,make safe_embedding_lookup_sparse method public and clean duplicate codes,"In #17417, I found that same `safe_embedding_lookup_sparse` code exists in both embedding_ops.py and feature_column.py.

To resolve circular dependency, I create an `embedding_ops_py` target. The method looks not so good, any suggestion would be appreciated.",tensorflow
18833,nehaljwani,pr,2018-04-24T13:37:08Z,Fix typo in CMakeLists.txt,,tensorflow
18851,facaiy,pr,2018-04-25T05:33:38Z,BUG: fix unmatched dtype between y_train and y_test,"y_train_np is uint8, while y_test_np is int64:

```python
from tensorflow.python.keras.datasets import cifar10
(x_train_np, y_train_np), (x_test_np, y_test_np) = cifar10.load_data()
print(y_train_np.dtype, y_test_np.dtype)
# uint8 int64
```",tensorflow
18948,facaiy,pr,2018-04-28T14:52:05Z,DOC: add more explanation for auxiliary_name_scope,"Fix #18781.

update the documentation of `auxiliary_name_scope` making the intentions clear. cc @lukaszkaiser  ",tensorflow
18953,nehaljwani,pr,2018-04-29T01:01:45Z,Update is_windows() to allow mingw shells,,tensorflow
18954,nehaljwani,pr,2018-04-29T01:02:03Z,Escape regex pattern properly before using it,"Without this patch, the function `_ignore_file_path` throws a traceback:
```
  File ""C:\xxx\lib\site-packages\tensorflow\contrib\tensorboard\plugins\trace\trace.py"", line 156, in _ignore_file_path
    if re.search(regex_pattern, fname):
  File ""C:\xxx\lib\re.py"", line 182, in search
    return _compile(pattern, flags).search(string)
  File ""C:\xxx\lib\re.py"", line 301, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""C:\xxx\lib\sre_compile.py"", line 562, in compile
    p = sre_parse.parse(p, flags)
  File ""C:\xxx\lib\sre_parse.py"", line 855, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File ""C:\xxx\lib\sre_parse.py"", line 416, in _parse_sub
    not nested and not items))
  File ""C:\xxx\lib\sre_parse.py"", line 502, in _parse
    code = _escape(source, this, state)
  File ""C:\xxx\lib\sre_parse.py"", line 401, in _escape
    raise source.error(""bad escape %s"" % escape, len(escape))
sre_constants.error: bad escape \p at position 11
```",tensorflow
18955,brettkoonce,pr,2018-04-29T03:14:20Z,contrib: minor spelling tweaks,,tensorflow
18972,bstriner,pr,2018-04-30T05:51:43Z,Use MKLROOT,"Search ENV{MKLROOT} in addition to MKL_HOME. MKLROOT is the standard environment variable set by Intel scripts.

Should make install a little easier. No need to set MKL_HOME if you run the Intel environment setup.

Cheers",tensorflow
18973,bstriner,pr,2018-04-30T06:30:28Z,Fix MSVC openmp flag,"CMake is checking the flag ""-fopenmp"" but the flag is ""/openmp"" at least on my setup (MSVC 2015 and 2017 with Intel compilers_and_libraries_2018.2.185).

This PR checks for both ""-fopenmp"" and ""/openmp"" if WIN32.",tensorflow
19096,bstriner,pr,2018-05-04T22:49:55Z,Include ThenBlasGemm,"Hi everybody,

I'm getting an unresolved external symbol `ThenBlasGemm` when compiling for GPU with CMake using MSVC 2015. This function isn't picked up by the script to create the def file for the DLL, so it isn't being exported. This PR just adds that function to the regex so the build works. Linker error below.

Does Windows CMake GPU not go through CI? I'm curious why this hasn't come up before.

Cheers

```
[D:\Projects\tensorflow\tensorflow\contrib\cmake\buildout\_gru_ops.vcxproj]
   199>blas_gemm.obj : error LNK2019: unresolved external symbol ""public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class stream_executor::DeviceMemory<double> const &,int,class stream_executor::DeviceMemory<double> const &,int,double,class stream_executor::DeviceMemory<double> *,int)"" (?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11NAEBV?$DeviceMemory@N@2@H2HNPEAV52@H@Z) referenced in function ""public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)"" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z) 
```",tensorflow
19097,bstriner,pr,2018-05-04T22:59:19Z,Use FindOpenMP instead of custom code,"As suggested by @fo40225 , this switches from custom code to FindOpenMP. Seems to have been in CMake since at least 3.0. FindOpenMP would be preferred to anything custom. However, this might break the build for anyone still on CMake 2. Is that a supported configuration or something to not worry about?

@gunan This also adds a flag to enable or disable OpenMP. Default is ON but it will warn and disable if not found. This maintains current functionality for existing builds that expect OpenMP but gives the option of disabling.

@fo40225 , I saw you had a couple other ideas about changes to CMake. Were you going to put together a PR with your other changes?",tensorflow
19098,bstriner,pr,2018-05-04T23:04:26Z,Use Configuration instead of CMAKE_BUILD_TYPE in MSVC,"This uses the `Configuration` passed to MSBuild instead of `CMAKE_BUILD_TYPE`. You can actually omit `CMAKE_BUILD_TYPE` during configuration, and the `Configuration` selected during build is used.

Several of the CMake scripts used `Configuration`, several used `CMAKE_BUILD_TYPE`, so this is just making things consistent for MSVC.

One less parameter to worry about during configuration and makes it possible to build debug and release from a single configuration.",tensorflow
19099,bstriner,pr,2018-05-05T00:10:09Z,Add clcache to CMake CI build,"Locally, my GPU build seems to run about 20 mins faster using clcache. I'm curious whether you see a difference in the runtime of the CI build.

This change installs clcache each build, which is a little silly, but should be enough to see if the build time changes. If it does, maybe we can just install ahead of time or add a check to see if already installed.",tensorflow
19105,facaiy,pr,2018-05-05T11:30:23Z,unsafe_div op for division by zero ,"Fix #15706, #17350, #20091. 

This is a summary of discussion in #15706:

+ op name: div_no_nan
+ op behavior: return 0 when denominator is zero
+ designed for float value only: float32, float 64
+ op kernel: CPU for now (I'd like to add GPU kernel later, is it OK? cc @asimshankar)",tensorflow
19148,facaiy,pr,2018-05-08T14:29:06Z,BUG: keras.callbacks.TensorBoard raises an exception for non_trainale_weights,"The class seems not to exclude non_trainable_weight (moving_variances and moving_mean in BatchNormalization) correctly, see the script and its logs below for more details:


```python
# TF 1.8, CPU
import tensorflow as tf
import numpy as np

x = np.ones((32, 10, 10, 3))
y = np.ones((32,))
y = tf.keras.utils.to_categorical(y)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.BatchNormalization(input_shape=(10, 10, 3)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(2, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
model.summary()

cbks = [tf.keras.callbacks.TensorBoard(histogram_freq=1, batch_size=2, write_grads=True)]

model.fit(
  x,
  y,
  batch_size=4,
  validation_data=(x, y),
  callbacks=cbks,
  epochs=2,
  verbose=0)
```

```
Traceback (most recent call last):
  File ""test.py"", line 24, in <module>
    verbose=0)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1232, in fit
    validation_steps=validation_steps)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training_arrays.py"", line 156, in fit_loop
    callbacks.set_model(callback_model)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py"", line 70, in set_model
    callback.set_model(model)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py"", line 716, in set_model
    grads = model.optimizer.get_gradients(model.total_loss, weight)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/optimizers.py"", line 115, in get_gradients
    raise ValueError('An operation has `None` for gradient. '
ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
```",tensorflow
19183,ziky90,pr,2018-05-09T16:12:18Z,Enable OrderedEnqueuer from keras in tf.keras,"Goal of this PR is to enable usage of `tf.keras.utils.OrderedEnqueuer` which has the very same functionality `keras.utils.OrderedEnqueuer`, but for some reason was not available in `tf.keras`",tensorflow
19205,benoitsteiner,pr,2018-05-10T19:00:50Z,Branch 196146716,,tensorflow
19212,facaiy,pr,2018-05-11T02:45:51Z,Add `AppendFloat16ArrayToTensorProto` to acclerate `tf.constant` for float16,"Related with #19180.
It seems that cython doesn't support `np.float16_t` by now, we use `np.uint16` instead. 

The conversion still be time-consuming (25 times slower than float32), however it is better than original slow implementation (260 times slower than float32).

Performance comparison:
```
before:
float32: 0.0535 sec
float16: 13.7567 sec

after:
float32: 0.0496 sec
float16: 1.0815 sec
```

script:
```python
import time
images = np.random.rand(128, 100, 100, 3)
imagesFloat16 = images.astype(np.float16)
imagesFloat32 = images.astype(np.float32)

start = time.time()
constant_op.constant(imagesFloat32)
end = time.time()
print(""float32: {0:.4f} sec"".format(end - start))

start = time.time()
constant_op.constant(imagesFloat16)
end = time.time()
print(""float16: {0:.4f} sec"".format(end - start))
```",tensorflow
19278,Randl,pr,2018-05-14T19:30:00Z,Fix fake quantization link,,tensorflow
19311,zheng-xq,pr,2018-05-16T05:10:21Z,Branch 196777020,Merging internal changes.,tensorflow
19336,superbobry,pr,2018-05-16T21:38:17Z,Fallback to dynamic loader even if HADOOP_HDFS_HOME is not defined,"Prior to this commit HadoopFileSystem required HADOOP_HDFS_HOME to be
defined to initialize the filesystem, even if libhdfs.so is located
outside of the standard location. This limitation is unnecessary and
can be safely removed.

As a nice side-effect, the error message is now more informative.

Before:

    Environment variable HADOOP_HDFS_HOME not set

After:

    libhdfs.so: cannot open shared object file: No such file or directory

Change-Id: Ief6a8679d7ef353003aa387f7767ebaa8ef290ce",tensorflow
19345,zheng-xq,pr,2018-05-17T05:36:15Z,Branch 196939548,Merging internal changes.,tensorflow
19409,zheng-xq,pr,2018-05-19T00:07:03Z,Branch 197218170,Merging internal changes,tensorflow
19553,silvasean,pr,2018-05-25T07:27:56Z,Fix broken link,"Also, as a side note, I found this link confusing. I was expecting it to take me to a list of pre-made estimators. Not a definition of what a pre-made estimator is. (maybe the glossary definition should link to a list of pre-made estimators?)",tensorflow
19609,boeddeker,pr,2018-05-29T07:06:45Z,Add a note that stop_gradient in moments does not change the gradient,"In https://github.com/tensorflow/tensorflow/commit/eccd162119675d0bf5bc6f8e6a93dcda7ab6db4a#diff-ef8609a43751227afcaacc838670a96f @sguada added support for sample mean in `moments`. He added a `stop_gradient` to the mean in the variance calculation. Since it took me some time to figure out that this `stop_gradient` has no effect (except a reduced computation time), I think a note to the `stop_gradient` could be usefull.

PS: This PR is a suggestion for such a note.",tensorflow
19636,flx42,pr,2018-05-29T22:34:49Z,docker: update cuDNN to 7.1.4.18,"Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>

@gunan lightly tested on my side, so it will have to pass CI.",tensorflow
19663,facaiy,pr,2018-05-31T09:58:12Z,Add gradient for operation 'SparseSlice' ,Fix #19373.,tensorflow
19682,yanboliang,pr,2018-06-01T05:43:21Z,Unify learning rate as normal Tensor for tf.contrib.layers.optimize_loss,"In ```tf.contrib.layers.optimize_loss```, we handle ```learning_rate``` differently. 
* If it's a Tensor, then keep intact.
* If it's a float, then we wrap it as TF Variable, thus be stored in checkpoint.

It will throw exception if we train a model with constant learning rate(for example ```0.2```), save to checkpoint, and then continue train with a Tensor learning rate(for example ```tf.constant(0.2)```). This exceptions was reported at (here](https://github.com/tensorflow/tensor2tensor/issues/809)

Meanwhile, I think we don't need to store ```learning rate``` in checkpoint. If the checkpoint is used for inference only, no learning rate is needed. If the checkpoint is used for further training, we specify constant learning rate, or learning rate decay function(which takes constant ```learning_rate``` and ```global_step``` variable as input).

Further more, if we export ```learning rate``` as variable, when continuing training a model with constant ```learning rate```, we can't change it to different value. But there is need to use a new ```learning rate``` to continue training.

To sum up, I would suggest to unify ```learning rate``` as normal Tensor regardless of it's Tensor or float input. I'm glad to hear TF core developers' opinion. Thanks.

",tensorflow
19730,freedomtan,pr,2018-06-04T02:01:01Z,[tflite] make benchmark_model tflite build,"//tensorflow/contrib/lite/tools:benchmark_model doesn't build
for either x86 or android targets. With this, something like

```
bazel build --config opt \
//tensorflow/contrib/lite/tools:benchmark_model
```
or
```
bazel build --config android_arm64 --config monolithic \
--cxxopt=-std=c++11 --linkopt=-llog \
//tensorflow/contrib/lite/tools:benchmark_model
```
works",tensorflow
19736,freedomtan,pr,2018-06-04T05:23:21Z,[tflite] label_image for tflite in Python,"With model (mobilenet_v1_1.0_224_quant.tflite), input image
(grace_hooper.bmp), and labels file (labels.txt) in /tmp.
Run

```
bazel run --config opt //tensorflow/contrib/lite/examples/python:label_image
```

We can get results like

```
0.470588: military uniform
0.337255: Windsor tie
0.047059: bow tie
0.031373: mortarboard
0.019608: suit
```",tensorflow
19783,superbobry,pr,2018-06-05T19:26:37Z,[RFC] Added support for sequence_numeric_column to bucketized_column,"This is a work in progress.

Caveats:
* the current implementation only works when the bucketized_column is used in the dense context;
* sequence columns are >=2 dimensional, so I had to drop the 1-D constraint from `bucketized_column`.

Closes #18975",tensorflow
19784,superbobry,pr,2018-06-05T19:29:41Z,Replaced explicit __metaclass__ assignment with @six.add_metaclass,"`__metaclass__ `only works for Python 2. In Python 3 it has no effect since the metaclass syntax is

    class A(meta=abc.ABCMeta):
        ...

`@six.add_metaclass` covers both variants.",tensorflow
19882,facaiy,pr,2018-06-10T00:31:15Z,Merge weight column of duplicated indices for indicator_column,"Fix #19876 . cc @ispirmustafa 

",tensorflow
19932,facaiy,pr,2018-06-12T08:15:09Z,support run_options and run_metadata for tf.keras.function,"Fix #19911. 
cc @fchollet ",tensorflow
19976,guillaumekln,pr,2018-06-13T07:32:33Z,BestExporter cherry-pick request for r1.9: Only calls compare function if values were read from event file,"I would like to suggest this cherry-pick for r1.9.

Fixes #19877.",tensorflow
20044,brettkoonce,pr,2018-06-15T02:34:15Z,contrib: autograph/constrained_optimization: minor spelling tweaks,,tensorflow
20223,facaiy,pr,2018-06-22T14:11:06Z,Allow stride > window_size for sliding_window_batch,Fix #20191.,tensorflow
20244,facaiy,pr,2018-06-23T14:10:41Z,register float16 for gpu kenel of scatter and scatter_nd functions,"Fix #20219.

I compile the codes and run those related gpu test cases. All seems passed.",tensorflow
20296,freedomtan,pr,2018-06-26T01:59:48Z,fix tests in lite/kernels/internal,"make
```
bazel test --config opt //tensorflow/contrib/lite/kernels/internal:all
```
work. So that
```
bazel test --config opt //tensorflow/contrib/lite/kernels/...
```
works.


This is a sequel to https://github.com/tensorflow/tensorflow/pull/17700",tensorflow
20330,freedomtan,pr,2018-06-27T05:30:09Z,make benchmark_model for tflite build,benchmark_model doesn't build for desktop as reported at https://github.com/tensorflow/tensorflow/issues/20313,tensorflow
20331,freedomtan,pr,2018-06-27T06:01:04Z,update protobuf requirement to 3.6.0 for pip,"since protobuf dependence was update 3.6.0 in 3bfd3aeb (this was actually a part of @gunan's 3bfd3aeb. Probably, it's reverted when merging 1.9.0 back to master), there are incompatible
problems on machines with 3.4.0 < protobuf version < 3.6.0. 

E.g., I got something like
```
Traceback (most recent call last):
  File ""/tmp/foobar.py"", line 26, in <module>
    from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/node_def_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as3bfd3aeb tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __new__() got an unexpected keyword argument 'serialized_options'
```",tensorflow
20641,taehoonlee,pr,2018-07-09T08:28:10Z,Fix typos,"This PR fixes some typos: `unecessary`, `parition`, `seperately`, `exectutes`, `ouputs`, `funcion`, `funtions`, and `corresonding`.",tensorflow
20665,guillaumekln,pr,2018-07-10T09:51:49Z,Fix masking of beam ids in gather_tree_from_array,"The `sequence_length` argument that is passed to the function is the
lengths of the **reordered** predictions and was incorrectly used to
mask beam ids *before* reordering. Instead, we can reorder beam ids
without caring about out of range steps and only select the reodered
ids that are in bounds.

The added test covers a beam trajectory that previously produced an
out of range error because `gather_tree` returned `end_token` (here
`beam_width + 1`) for some steps.",tensorflow
20746,nluehr,pr,2018-07-12T16:56:13Z,Merge crop_and_resize with resize_bilinear_op internals,Improves performance of crop_and_resize.,tensorflow
20875,freedomtan,pr,2018-07-17T09:39:18Z,make TFLite kernel tests work again,"The `pow_test.cc` introduced in (51c80b60) doesn't build with
```
bazel test --config opt //tensorflow/contrib/lite/kernels:all
```

s/int32/int32_t/ to make it build and run",tensorflow
20988,freedomtan,pr,2018-07-20T06:29:01Z,[tflite label_image] get output size from output tensor ,"1. get output size from the output tensor
2. add a command line option to specify number of results

address the issue https://github.com/tensorflow/tensorflow/issues/20485 found by @jbuisson1",tensorflow
21005,yanboliang,pr,2018-07-20T21:42:33Z,Numpy ndarray should be serialized as Python list,"Pick the fix at https://github.com/keras-team/keras/pull/10727 to ```tf.keras```.
This PR can fix #22062 and https://github.com/keras-team/keras/issues/11023 as well.",tensorflow
21073,yanboliang,pr,2018-07-23T21:45:02Z,Network.to_json should handle numpy.ndarray correctly,Pick the fix at https://github.com/keras-team/keras/pull/10754 to ```tf.keras```.,tensorflow
21078,facaiy,pr,2018-07-24T03:21:45Z,Fix:  Validate variable dtype before restoring checkpoint,Fix #20487 ,tensorflow
21086,taehoonlee,pr,2018-07-24T11:34:28Z,Fix typos,"This PR fixes some typos: `not not`, `fuction`, `constrast`, `be be`, `is is`, `by by`, `Arithmatic`, `of of`, `Parition`, `Instatiate`, `orignal`, `to to`, and `mutliple`.",tensorflow
21087,taehoonlee,pr,2018-07-24T11:36:13Z,Replace `NWHC` with `NHWC`,"This PR replaces `NWHC` with `NHWC`. The `array_ops.transpose(x, [0, 3, 1, 2])` performs `NHWC` to `NCHW`.",tensorflow
21109,freedomtan,pr,2018-07-25T01:37:55Z,make build_pip_package work again on rpi3,"make something like
```
bazel build --config opt --local_resources 1024.0,0.5,0.5 --cxxopt=-mfpu=neon-vfpv4 --cxxopt=-ftree-vectorize --cxxopt=-funsafe-math-optimizations --cxxopt=-ftree-loop-vectorize --cxxopt=-fomit-frame-pointer //tensorflow/tools/pip_package:build_pip_package
```
work.

When building tensorflow pip package natively on RPI 3 (yes, I know it's slow, not recommended), I saw error message like
```
ERROR: /home/pi/work/tensorflow/tensorflow/BUILD:589:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/pi/.cache/bazel/_bazel_pi/fab1708ba000bf373adf42719f003e8e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE

```

Demangling with `c++filt`
```
$ c++filt _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE 
void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)
```
we can see that we need instantiation for `ConcatCPU<bfloat16>(.....)`.

Tested on Raspbian Jun-2018.",tensorflow
21119,bstriner,pr,2018-07-25T06:20:34Z,Add PATCH_COMMAND to eigen.cmake (fix #19198),"This PR adds the ability to apply a patch file to eigen as part of the build. Currently, Eigen has at least one issue under certain configurations. This PR is not to address any specific issue but to make the build more flexible. I linked a patch file below to fix the specific issue in #19198.

Just add `-Deigen_PATCH_FILE=path/to/patch.txt` to cmake and it will run the patch file after downloading eigen. Defaults to `OFF` (no patch file).

Bug I was getting:
- http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1526
- https://github.com/tensorflow/tensorflow/issues/19198

Patch file in gist below. Based on the bug above with some path and line number changes.
- https://gist.github.com/bstriner/a7fb0a8da1f830900fa932652439ed44

This way no one is waiting on a third-party to make a new release. Should be easy enough to make other patches as necessary.

This does assume that `patch` command is on `PATH`. An alternative would be to let the user specify the entire patch command instead of just the patch file but that might unnecessarily complicate things.

Cheers",tensorflow
21201,bstriner,pr,2018-07-28T07:46:45Z,[CMAKE] Add MKL to PATH,"Currently, instead of adding `mkl_BIN_DIRS` to the path, the path is replaced by `mkl_BIN_DIRS`.  I was getting weird errors from the generated command because my `PATH` was now gone. This PR adds the MKL binary path to the front of `PATH`  instead of replacing `PATH` .

Brief discussion of why this wasn't done before is in the CMake file comments. Refer to those comments for details. It appears there were some issues with having more than one path in `PATH`. I've tested this on Windows to generate the correct syntax, but haven't tested on a Linux build.

@LoSealL @mrry It looks like both of you worked on that section.  What kinds of issues were happening with multiple items in `PATH`? With only MKL on the path, generated commands fail for me at importing numpy.

If the command fails it shows up in the build log as `error MSB6006: ""cmd.exe"" exited with code 1.`

BTW, the other minor changes are from tab to space.

Cheers",tensorflow
21202,bstriner,pr,2018-07-28T07:59:48Z,Quick Fix for Python 3.7,"Fix for 3.7 I made before I found https://github.com/tensorflow/tensorflow/pull/20766

Just built CUDA 9.2 CuDNN 7 compute 6.1 python 3.7 on VS2017 using CMake.

That seems to be the place for working on what will be the final version, but I'm just putting this out there if anyone wants a quick fix in the meantime (currently that PR conflicts). This version casts away consts, so it isn't exactly safe but should be fine if you just want things up and running. Tensorflow overuses const_cast but that is an issue for another day.

Just merging this branch into your own work is an easy option if you need to build for 3.7. I'll close this PR when the other branch gets finished.

Related reading materials:
- https://github.com/tensorflow/tensorflow/issues/20517
- https://github.com/tensorflow/tensorflow/issues/17022
- https://github.com/tensorflow/tensorflow/issues/20690
- https://github.com/tensorflow/tensorflow/pull/20766

Cheers",tensorflow
21215,facaiy,pr,2018-07-29T05:43:26Z,use np.array to avoid copy behavior of index tensor for extract_image_patches,"Related to #20146.

It's slow to construct gradient of tf.extract_image_patches: ~ 136s for @dbbert 's example. 
And I find the performance bottlenecks are:
1. construct `idx` array: ~22s
2. copy from list to tf.int64 index_tensor when constructing `sp_mat`: ~110s

In the PR, we pre-allocate a np.int64 array for `idx`. I think the result gets better, although it's still time consuming: ~ 38s
1. construct `idx` array: ~30s
2. copy: ~6s",tensorflow
21218,facaiy,pr,2018-07-29T09:34:16Z,Faster implementation for constructing gradient of extract_image_patches,"Fix #20146.

In the PR, we created indices matrix for input tensor, and reuse `extract_image_patches` to calculate `idx` (the mapping table for indices: input -> output). 

The list below is the performance comparison based on the example of @dbbert in #20146.

Before:
+ constructing time: ~138.4s
+ execute time: ~33.7s

After:
+ constructing time: ~3.6s
+ execute time: ~20.3s",tensorflow
21231,freedomtan,pr,2018-07-30T03:30:26Z,enable fp16 for Batch Matmul,"since there is relevant code in #18436, should be safe to enable it",tensorflow
21356,facaiy,pr,2018-08-03T07:38:42Z,register float64 GPU kernel for Conv3d,Fix #21295.,tensorflow
21428,facaiy,pr,2018-08-07T05:38:34Z,tf.clip_by_global_norm raise exception when use_norm = inf,Fix #21363,tensorflow
21462,yanboliang,pr,2018-08-07T22:32:36Z,RNN.call should get initial state from full input spec,"Pick the fix at https://github.com/keras-team/keras/pull/10845 to ```tf.keras```, to fix a critical bug when running ```RNN``` with ```multi_gpu_model```.",tensorflow
21509,facaiy,pr,2018-08-09T07:15:35Z,Calculate feature_importances for BoostedTreesRegressor and BoostedTreesClassifier,"Fix #21204.

~~The PR has not been done yet~~, and we create it early to collect feedback.

I have some questions about design:
1. The PR is to add a method `compute_feature_importances` for estimator. I think it's convenient for user, however we have to construct graph and restore metadata from checkpoint. Does the solution sound reasonable?
2. If yes, for `_read_tree_ensemble_from_checkpoint` method in the PR, is it necessary to replace `Session` by `MonitoredSession` with `ChiefSessionCreator`?
3. Is it good to add feature_num attribute to TreeEnsemble protobuf file?
4. As for unit test, because it is easier to construct TreeEnsemble than estimator.  How about only adding test cases for function 
 `def compute_feature_importances(tree_ensemble, num_features, normalize=True): ` next? I mean, like `ModelFnTests`.

cc @nataliaponomareva @martinwicke @dvdbisong",tensorflow
21529,brettkoonce,pr,2018-08-09T22:07:02Z,docs_src: minor spelling tweaks,,tensorflow
21536,facaiy,pr,2018-08-10T06:29:37Z,Support placeholders without shape in build_raw_serving_input_receiver_fn,Fix #21178.,tensorflow
21621,facaiy,pr,2018-08-15T04:43:40Z, add `tf.div_no_nan` op for division by zero,"Fix #15706, #17350, #20091. 
Ref #19105.

This is a summary of discussion in #15706:

+ op name: div_no_nan
+ op behavior: return 0 when denominator is zero
+ designed for float value only: float32, float 64
+ op kernel: CPU for now  (GPU kernel will be add later).",tensorflow
21628,byronyi,pr,2018-08-15T09:13:34Z,Add grpc as default rpc layer for distribute coordinator server,Would you like to comment? @yuefengz ,tensorflow
21650,guillaumekln,pr,2018-08-16T07:53:54Z,"Update fold{l,r} examples to run on TensorFlow 1.9+",See #21340.,tensorflow
21662,superbobry,pr,2018-08-16T16:25:16Z,Fixed bytes/str issue in get_temp_export_dir,"Prior to this commit on Python 3 get_temp_export_dir produced directories
of the form

    /foo/bar/temp-b'1534435836'

because basename is bytes and str(bytes) is b'...'.",tensorflow
21664,yanboliang,pr,2018-08-16T22:19:06Z,GlobalAveragePooling1D supports masking,"Pick https://github.com/keras-team/keras/pull/10913 to ```tf.keras```, to make ```GlobalAveragePooling1D``` support masking.",tensorflow
21666,freedomtan,pr,2018-08-17T01:57:03Z,make build_rpi_lib.sh work,"running `./tensorflow/contrib/lite/tools/make/build_rpi_lib.sh` showed something like

```
/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model
arm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/audio_spectrogram.cc -o /hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/audio_spectrogram.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/detection_postprocess.cc -o /hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/detection_postprocess.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/mfcc.cc -o /hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/mfcc.o
tensorflow/contrib/lite/kernels/mfcc.cc:16:61: fatal error: include/flatbuffers/flexbuffers.h: No such file or directory
compilation terminated.
tensorflow/contrib/lite/tools/make/Makefile:159: recipe for target '/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/mfcc.o' failed
make: *** [/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/mfcc.o] Error 1
make: *** Waiting for unfinished jobs....
tensorflow/contrib/lite/kernels/detection_postprocess.cc:18:61: fatal error: include/flatbuffers/flexbuffers.h: No such file or directory
compilation terminated.
tensorflow/contrib/lite/tools/make/Makefile:159: recipe for target '/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/detection_postprocess.o' failed
make: *** [/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/detection_postprocess.o] Error 1
tensorflow/contrib/lite/kernels/audio_spectrogram.cc:25:61: fatal error: include/flatbuffers/flexbuffers.h: No such file or directory
compilation terminated.
tensorflow/contrib/lite/tools/make/Makefile:159: recipe for target '/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/audio_spectrogram.o' failed
make: *** [/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/kernels/audio_spectrogram.o] Error 1

```

Some kernels use `flatbuffers/flexbuffers.h`, others use `include/flatbuffers/flexbuffers.h`",tensorflow
21672,freedomtan,pr,2018-08-17T06:54:30Z,tflite label_image: add -llog and update doc,"1. -llog needed for Android after 2c623eaa
2. update label_image.md
   a. reflect changes
   b. where to get test model and data
   c. describe --cxxopt=-DTFLITE_PROFILING_ENABLED",tensorflow
21681,nluehr,pr,2018-08-17T15:04:55Z,Faster bilinear resize for x86,"@alextp This is a rework of our earlier bilinear resize PR (#20746).

Performance for TF master prior to this PR (run on DGX-1 with Volta):
```
Benchmark                                           Time(ns) Iterations
-----------------------------------------------------------------------
BM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    5097690        100  241.1M items/s
BM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512    3449158        202  118.8M items/s
BM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     7290470        100  168.5M items/s
BM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     4387491        159  93.4M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    4273981        161  287.5M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512    3053134        231  134.2M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7          85094       6208
```

Performance benchmark including this PR:
```
Benchmark                                           Time(ns) Iterations
-----------------------------------------------------------------------
BM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    1097690        581  1119.4M items/s
BM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512     503559       1400  813.4M items/s
BM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     2726747        249  450.6M items/s
BM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     1069408        650  383.0M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    1251613        496  981.8M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512     448486       1576  913.3M items/s
BM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7          81488       6333
```",tensorflow
21702,facaiy,pr,2018-08-18T10:21:26Z,assert_element_shape support partial shapes,Fix #16052.,tensorflow
21712,cclauss,pr,2018-08-19T11:42:51Z,ci_build: Upgrade the Python 'six' compatibility module,,tensorflow
21757,facaiy,pr,2018-08-21T10:53:02Z, Fix bug: divide by zero in embedding_lookup_sparse,Fix #14851,tensorflow
21768,superbobry,pr,2018-08-21T19:40:12Z,Fixed mode in load_inputs_from_input_arg_string,"NPY files are binary and should be opened with mode ""rb"".",tensorflow
21784,facaiy,pr,2018-08-22T04:36:56Z,register gpu kernel for div_no_nan,This is a follow up PR for #21621.,tensorflow
21798,facaiy,pr,2018-08-22T13:26:00Z,"Clean all those safe_div, _safe_div methods","tf.div_no_nan was introduced by #21621, which is designed to replace all those safe_div methods here.

Ref: #21784

~~@martinwicke please add a API design label, because we add a new argument: negative_to_zero.~~",tensorflow
21827,facaiy,pr,2018-08-23T12:19:31Z,add div_no_nan in the math section of user_guide,Add document for #21784 #21621 .,tensorflow
21865,facaiy,pr,2018-08-25T03:01:07Z,initializer should support TensorShape,Fix #21838.,tensorflow
21970,superbobry,pr,2018-08-30T10:05:00Z,tf.errors.OpError and its subclasses are pickleable,"The builtin `BaseException` class defines a generic `__reduce__` method which assumes that the `args` attribute corresponds to constructor arguments. See `BaseException_reduce` in [Objects/exceptions.c](https://github.com/python/cpython/blob/master/Objects/exceptions.c#L129). `tf.errors.OpError` keeps args empty and therefore cannot be pickled via `BaseException.__reduce__`. This commit fixes  the issue by defining `OpError.__reduce__`.

Side question: is there a reason `tf.errors.OpError` does not pass its arguments to the `Exception` constructor to make them available via `args` and in the repr?",tensorflow
22006,facaiy,pr,2018-09-01T13:41:02Z,Append error msg to exception for test.assert_* method,Fix #21988.,tensorflow
22073,yanboliang,pr,2018-09-04T23:04:23Z,Fix ReLU layer serialization bug,"This bug is reported at https://github.com/keras-team/keras/issues/11023, but actually ```keras-team/keras``` handles this correctly, ```tf.keras``` has the bug. You can also use the following code snippet to reproduce:
```
from tensorflow.python.keras.layers import Input, ReLU
from tensorflow.python.keras.models import Model, save_model, load_model
import numpy as np

input = Input(shape=(5, 6, 3))
output = ReLU(6)(input)
model = Model(inputs=input, outputs=output)
model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

save_model(model, ""/tmp/test1"")
load_model(""/tmp/test1"")
```
This is the exception:
```
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    load_model(""/tmp/test1"")
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/saving.py"", line 229, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/saving.py"", line 306, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1209, in from_config
    process_layer(layer_data)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1195, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/serialization.py"", line 64, in deserialize
    printable_module_name='layer')
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 175, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1553, in from_config
    return cls(**config)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/advanced_activations.py"", line 302, in __init__
    self.max_value = K.cast_to_floatx(max_value)
  File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/backend.py"", line 216, in cast_to_floatx
    return np.asarray(x, dtype=_FLOATX)
  File ""/Library/Python/2.7/site-packages/numpy/core/numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
TypeError: float() argument must be a string or a number
```
The bug is caused by the type of ```self.max_value``` is ```np.ndarray``` after ```K.cast_to_floatx```, so we should force to convert it to float when serializing it, otherwise, it would be serialized as ```np.ndarray``` and throws exception when loading back. This fix is following other layers in ```advanced_activations.py```, such as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/advanced_activations.py#L63), they handle similar issue correctly.",tensorflow
22083,facaiy,pr,2018-09-05T08:09:08Z,add gradient for broadcast_to,Fix #21901,tensorflow
22206,freedomtan,pr,2018-09-11T06:43:20Z,[tflite] make build_rpi_lib.sh work again,"fix undefined referneces introduced by 1a25a8e6. Functions
defined and implemented in `neon_tensor_utils.h`, but not
included anywhere so that there are undefined references.

error messages:
```
....
layer_norm_lstm.cc:(.text+0x20f6): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2104): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2110): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2138): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2146): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2152): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x216c): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x217c): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2188): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x21ea): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x21fa): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2206): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x229e): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x22ac): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x22b8): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x22d2): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x22e0): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x22ec): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2306): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2316): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2322): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x23f4): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2402): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x240e): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2428): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2438): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2444): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(layer_norm_lstm.o): In function `tflite::ops::custom::layer_norm_lstm::LayerNormLstmStep(float const*, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, signed char const*, float, float const*, float, float, TfLiteFusedActivation const&, int, int, int, int, float*, float*, float*, float*, float*, float*, float*, signed char*, signed char*, signed char*, float*, float*, float*)':
layer_norm_lstm.cc:(.text+0x2f06): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2f14): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2f20): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2f40): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2f4e): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2f5a): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2f76): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2f84): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x2f90): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x2ff0): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x2ffe): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x300a): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x326a): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x3278): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x3284): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x32c0): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x32ce): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x32da): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x32f6): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x3304): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x3310): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x337c): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x338a): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x3396): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x33b6): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x33c4): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x33d0): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x35d6): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x35e4): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x35f0): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
layer_norm_lstm.cc:(.text+0x3610): undefined reference to `tflite::tensor_utils::MeanStddevNormalization(float const*, float*, int, int, float)'
layer_norm_lstm.cc:(.text+0x361e): undefined reference to `tflite::tensor_utils::VectorBatchVectorCwiseProduct(float const*, int, float const*, int, float*)'
layer_norm_lstm.cc:(.text+0x362a): undefined reference to `tflite::tensor_utils::VectorBatchVectorAdd(float const*, int, int, float*)'
collect2: error: ld returned 1 exit status
tensorflow/contrib/lite/tools/make/Makefile:191: recipe for target '/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/bin/minimal' failed
make: *** [/hack/freedom/tensorflow/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/bin/minimal] Error 1
make: *** Waiting for unfinished jobs....

```",tensorflow
22265,yanboliang,pr,2018-09-14T01:41:40Z,Conv2DTranspose supports dilation,"Fix #21598 
I have implemented this function at https://github.com/keras-team/keras/pull/11029 , just pick it to ```tf.keras```.",tensorflow
22296,yanboliang,pr,2018-09-16T04:08:25Z,Raise warning once only if trainable_weights and _collected_trainable_weights are inconsistent,"Fix #22012
If ```trainable_weights``` and ```_collected_trainable_weights``` are inconsistent, raise warning only once, not every batch(```train_on_batch```) which would flood the screen. ```train_on_batch``` inside loop is not efficient as each batch would rebuild the ```train_function```, but it's the most simple way to train model like GAN in #22012 . This may not a perfect fix, but at least, don't flood my screen. The updated behavior would be the same as ```keras-team/keras```.",tensorflow
22308,bstriner,pr,2018-09-17T06:46:39Z,[WIP] Variable Length CuDNN LSTMs/RNNs,"Hi! Started working on implementing variable sequence lengths in cudnn. Getting close, but also need to implement Pack/Unpack operations to make things useful. Hopefully will have this working within a week or so. Should be a huge performance difference.

If anyone else is interested in the subject and wants to help, please let me know!

Cheers",tensorflow
22334,freedomtan,pr,2018-09-18T02:44:00Z,[tflite] fix calculating of output pixels,fix an issue reported by issue #22310,tensorflow
22337,byronyi,pr,2018-09-18T05:38:30Z,Support scoped_allocator_ops for renamed device.,"This fixes #22274.

Signed-off-by: Bairen Yi <byi@connect.ust.hk>",tensorflow
22363,yanboliang,pr,2018-09-18T22:53:37Z,Fix Embedding layer to check invalid inputs,"Bug: If users set ```input_length``` for ```Embedding``` layer incorrectly, it should throw exception(existing not). Otherwise, the downstream layers compute shape incorrect and throw confused exception.
I have fixed this at https://github.com/keras-team/keras/pull/11091, but it's different from ```tf.keras```. ```keras-team/keras``` makes this check in ```compute_output_shape``` which was executed always. But ```tf.keras``` only calls ```compute_output_shape``` in deferred mode, so we need to move this check to other place. Actually it makes more sense to move it to ```_assert_input_compatibility``` which is used to check the inputs legal, and would be run in all mode.
",tensorflow
22392,yanboliang,pr,2018-09-19T19:54:27Z,Fix bug in tf.keras.metrics.sparse_categorical_accuracy,"Fix #22190

For the input of ```tf.keras.metrics.sparse_categorical_accuracy```, the shape of ```y_true``` can be ```(num_samples, 1)``` or ```(num_samples,)```, see #22190 for detail. The existing code assume the shape of ```y_true``` is ```(num_samples, 1)```, always reduce in the last dimension which leads the incorrect output. Actually we should check the shape of ```y_true``` and squeeze if applicable.
Meanwhile, I also fix ```sparse_top_k_categorical_accuracy``` which has the same issue.",tensorflow
22521,freedomtan,pr,2018-09-26T05:26:39Z,fix unbalanced delimiter in benchmark_model doc,"as reported in https://github.com/tensorflow/tensorflow/issues/22499, there is unbalanced delimiter `""`",tensorflow
22549,cbalint13,pr,2018-09-26T20:43:05Z,Add abseil_cpp cmake dependence.,"This patch enhance cmake build and add proper dependency to both **external** path (by fetching and compiling from scratch) and **system** path (existing library on the system).


  **Abseil_CPP** is required and referenced in tensorflow code:

```
tensorflow]$ find . -name '*.h' -exec grep -H '#include ""absl/' {} \; | wc -l
214
```

---

Few outlines:

***New module***

*  **1** New specialized ```modules/FindAbseilCpp.cmake``` module was added in new folder. The ```abseil_cpp``` upstream project doesn't expose cmake or pkgconfig helpers so a custom module capable of searching it system wide was added. More modules like this for all existing packages (that have poor/buggy/lacks support in its cmake exposure) will be proposed. The module is reliable on mswin and linux too. It has minimal lines of code that can be maintained.

* **2** ```-DABSEIL_CPP_INCLUDE_DIR_HINTS``` and ```-DABSEIL_CPP_LIBRARIES_DIR_HINTS``` can be used as extra path hints for the search task (e.g. in case of very obscure places). These two VARS will be default feature for any future modules like this.

* **3** The proposed module can search through specified list of COMPONENTS, otherwise will export all available components from **system**. It happens that in the case of abseil_cpp we have more libraries as components (quite a lot).

***Rule change proposal***

* **4** ```ExternalProject_Add(abseil_cpp_build)``` notice the **_build** suffix of label. More changes (appending **_build**) for all existing module will be proposed. The reason is that in the case of **system** libraries, the very project name e.g. **re2** will conflict inside cmake with the library name **re2** appended into ```tensorflow_EXTERNAL_DEPENDENCIES```. Thus extra discrimination e.g. **_build** tag is required otherwise cmake will throw error on it (it will appear only in **system** scenario).

* **5** ```tensorflow_EXTERNAL_LIBRARIES``` and ```tensorflow_EXTERNAL_DEPENDENCIES``` plus  ```include_directories()```  are updated right in this related module ```external/abseil_cpp.cmake``` and not from very main ```CMakeLists.txt```. More changes like this will come for all rest of packages.

@perfinion ,

  If this is passed and rule changes agreed, very next PR will update the rest of existing modules. Then will continue with PR series relating other issues.


Thank you !",tensorflow
22559,byronyi,pr,2018-09-27T09:40:15Z,Fix grpc+gdr compile error introduced in 33170cc6.,"@poxvoculi Most CPU tensors in PS are still managed by cpu_allocator, and I have added a separate path to copy them back to a CPU tensor managed by ProcessState::GetCPUAllocator.

I believe there is no harm (maybe a little bit larger page table on NIC?) to register the same memory region multiple times, so it is not required for the memory managers to be singletons.",tensorflow
22562,superbobry,pr,2018-09-27T11:42:05Z,[WIP] Load CUDA libraries dynamically,"Prior to this commit, TensorFlow was built in two flavours: CPU-only and CPU/GPU. The latter version was dynamically linked with CUDA and therefore required CUDA libraries to be installed in the host system. This change replaces dynamic linking with dynamic loading for all of the CUDA libraries but cudart. This would allow to ship a single TensorFlow build which works for both cases.

Note that TensorFlow already used dynamic loading for CUDA libraries prior to version 1.1.0. In 1.1.0 however, dynamic loading has been changed to dynamic linking, see commit 191658d54f90ac03c15b339326129cd52d1f56a3.

Closes #16184",tensorflow
22566,superbobry,pr,2018-09-27T14:57:54Z,Added chief to the default device_filters for /job:ps,"This behaviour matches the description in the _get_default_session_config_distributed docstring, and restores the symmetry of the default device_filters.",tensorflow
22640,nehaljwani,pr,2018-10-01T04:45:01Z,Fix warning for format specifier,"tensorflow/core/util/command_line_flags.cc:73:37: warning:
format specifies type 'long *' but the argument has type 'int64_t *' (aka 'long long *') [-Wformat]
    if (sscanf(arg.data(), ""%ld%c"", &parsed_int64, &extra) != 1) {
                            ~~~     ^~~~~~~~~~~~~
                            %lld
1 warning generated.",tensorflow
22716,nehaljwani,pr,2018-10-04T04:16:01Z,"Static cast size_t to int in arguments 1,2 to forward_input_or_allocate_output()","Static cast size_t to int in arguments 1,2 to forward_input_or_allocate_output()

This fix resolves the following compiler error:

tensorflow/core/kernels/mkl_relu_op.cc(1028): error C2398: Element '1':
conversion from 'const std::size_t' to 'int' requires a narrowing conversion",tensorflow
22791,nehaljwani,pr,2018-10-06T06:56:35Z,Add alternate clock_gettime() implementation for macOS < 10.12,"clock_gettime is not available in macOS SDK < 10.12

xref: https://github.com/tensorflow/tensorflow/issues/22636#issuecomment-426488627",tensorflow
22792,nehaljwani,pr,2018-10-06T07:04:12Z,Add (init) symbol names prefixed with an underscore,"The command: `python -c 'from tensorflow.contrib import tensorrt as trt'` fails
with:
```
File ""/xxx/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/wrap_conversion.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_conversion', fp, pathname, description)
ImportError: dynamic module does not define init function (init_wrap_conversion)

âžœ  1534993632 nm /xxx/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/_wrap_conversion.so  | grep wrap_conversion
0000000000003670 t _init_wrap_conversion <---------- symbol defined, but local, not exposed
                 U init_wrap_conversion
                 I init_wrap_conversion (indirect for init_wrap_conversion)
```
Happens because the linker version script has: `init_wrap_conversion` instead of `_init_wrap_conversion`

xref: https://github.com/tensorflow/tensorflow/issues/21818
xref: https://stackoverflow.com/a/37534357",tensorflow
22856,freedomtan,pr,2018-10-10T02:41:13Z,[aarch64] make aws sdk work on aarch64,"`bazel build //tensorflow/tools/pip_package:build_pip_package`
requires AWS SDK by default. but platform part was not built
on aarch64",tensorflow
22929,facaiy,pr,2018-10-12T07:14:46Z,remove support for TFOptimizer in optimizer.get,"Fix #22780.

I think we should forbid user to use TFOptimizer directly, which is an inner implementation for keras.",tensorflow
22968,facaiy,pr,2018-10-14T08:38:27Z,remove noisy warning in StepCounterHook,fix #21523,tensorflow
23004,facaiy,pr,2018-10-16T02:02:50Z,support dynamic shape for conv3d,"Fix #16834, #15572, #15655, #15696, #16834, #20379, #22771.

Related PR: #21610, #15595

Because #22127 has been merged (thank @yongtang), we can fix the dynamic shape issue easily here.",tensorflow
23079,cbalint13,pr,2018-10-18T19:41:54Z,Enhance CUDA detection.,"This proposal enhance cmake CUDA related dependency on all cmake compatible platforms.

1. Use cmake internals (only) to **find cuda libraries** and **detect cuda arch**
2. Libraries are picked up as **shared** in case of tensorflow_BUILD_SHARED_LIB else **static**
3. **-DCUDA_ARCH_NAME** can be selected as **Auto** or **All** or **custom** as per cmake docs
4. Get rid of any hardcoded versions and strings, all details are automatically detected and generated

The cmake's internal CUDA module is mature enough and well supported on all platforms, desired and preferable instead of hardcoded or custom local functions.

Additionally **contrib/data** OPS is gone, so for updated compilation flow is removed from cmake too. OPS and library submodules as **contrib** will be addressed in separate PR hopefully with better way than is now.

Added @perfinion to this.

",tensorflow
23370,facaiy,pr,2018-10-30T08:30:22Z,WIP: Fix max_pool_with_argmax behavior on GPU,The PR is still not done yet.,tensorflow
23480,byronyi,pr,2018-11-04T03:56:25Z,Quick fix for 462a79b that breaks GDR.,"Please see https://github.com/tensorflow/tensorflow/issues/22081#issuecomment-435316069 for the context.

@wangshuaizs Could you please check if this patch solves your problem?

This patch should fix #22081.",tensorflow
23567,supriyar,pr,2018-11-06T22:42:13Z,Add a new Batched NMS OP,"This change adds a new Batched NMS operation (called NonMaxSuppressionLite). The entire NMS operation for all batches and across all classes is performed in one kernel operation.
The change also adds unit tests for the new kernel operation.",tensorflow
23829,freedomtan,pr,2018-11-18T05:36:43Z,[TFLite] Add options for benchmark_model and label_image to allow fp16 for fp32 models,Add options for benchmark_model and label_image to allow running fp32 models with fp16. Useful when testing NNPAI accelerators with fp16 capability.,tensorflow
23880,ziky90,pr,2018-11-20T12:29:07Z,[tf.keras] Enable tf.keras.callbacks.CallbackList API for tf.keras,In keras I'm used to use `keras.callbacks.CallbackList` I'd like to be able to use it also in the tf.keras this PR should enable the API `tf.keras.callbacks.CallbackList`.,tensorflow
23993,facaiy,pr,2018-11-27T08:31:58Z,"For max_pool_with_argmax op, GPU kernel keeps consistent with the behavior of CPU","Fix #22025

Respect with [ tf.nn.max_pool_with_argmax: API](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax) , now GPU implementation returns flattened index `((b * height + y) * width + x) * channels + c`.",tensorflow
23994,freedomtan,pr,2018-11-27T08:39:26Z,[tflite] python script for object detection w/ SSD,Add a TFLite Python script that uses SSD to detect objets.,tensorflow
24038,mpjlu,pr,2018-11-29T06:02:51Z,Fix mkl_softmax integration error when the input tensor is mkl format,"Bugs of master:
The method to get input_dims is wrong when input is mkl tensor.
The layout_type is wrong when input is mkl tensor.

This PR fix these two bugs",tensorflow
24057,mpjlu,pr,2018-11-30T03:24:57Z,[Intel MKL]Fix mkl_softmax integration error when the input tensor is mkl format,"Bugs of master:
The method to get input_dims is wrong when input is mkl tensor.
The layout_type is wrong when input is mkl tensor.

This PR fix these two bugs",tensorflow
24058,byronyi,pr,2018-11-30T04:07:51Z,Implement async TensorFromTransportOptions for GDR,"Instead of blocking on completion of an RDMA op, RecvTensor client will now post a work request to the NIC send queue and return immediately. 

The GDR background polling thread will handle the callback after the corresponding RDMA op is completed, i.e. polled from the completion queue on NIC. The old epoll based mechanism is removed to trade higher CPU usage for improved throughput and lower latencies for RDMA ops.

The maximum numbers of work request (WR) in the send/recv queues on NIC are increased to entertain the increased number of concurrent RDMA ops. The threshold of tensor size below which we pass the tensor content in metadata is also increased to reduce the pressure to send/recv queues on NIC.

This fixes #23933.

Signed-off-by: Bairen Yi <byronyi@clustar.ai>",tensorflow
24114,byronyi,pr,2018-12-03T05:19:11Z,Fix version number in Bazel downgrade warning,"I have upgraded to Bazel 0.20:

```bash
$ bazel version
INFO: Invocation ID: 00114105-2567-451d-b22d-bb21e2d14b11
Build label: 0.20.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Nov 30 14:39:01 2018 (1543588741)
Build timestamp: 1543588741
Build timestamp as int: 1543588741
```

Bazel version check is introduced in e7a123f4b361b8104b783d8b1407b45dc087491c. 
Before this patch:

```bash
$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: b6a7b9a2-db26-4777-a4bb-f5a668e52e82
You have bazel 0.20.0 installed.
Please downgrade your bazel installation to version 0.15.0 or lower to build TensorFlow!
Configuration finished
```

After this patch:

```bash
$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 34b75ed8-1359-4fa7-ba71-b67fd887ee3a
You have bazel 0.20.0 installed.
Please downgrade your bazel installation to version 0.19.2 or lower to build TensorFlow!
Configuration finished
```",tensorflow
24261,b0noI,pr,2018-12-10T05:04:46Z,Update for documentation for TF Micro,"Proposal includes:
* new line on how to install arm compiler on Mac OS
* fix for the build on Mac OS
* instructions how to train new model in the Cloud with Deep Learning images",tensorflow
24301,duncanriach,pr,2018-12-11T22:58:13Z,Add cuDNN deterministic env variable (only for convolution),"This change is a major component of the recipe for making TensorFlow training reproducible on GPUs.

Setting the environment variable TF_CUDNN_DETERMINISTIC=1 (or true) will ensure that both forward and  backwards convolution algorithms are both fixed and deterministic. It overrides autotune and selects deterministic back-prop algorithms.",tensorflow
24355,duncanriach,pr,2018-12-14T01:13:09Z,Add cuDNN deterministic env variable (only for convolution),"This change is a major component of the recipe for making TensorFlow training reproducible on GPUs.

Setting the environment variable TF_CUDNN_DETERMINISTIC=1 (or true) will ensure that both forward and backwards convolution algorithms are both fixed and deterministic. It overrides autotune and selects deterministic back-prop algorithms.

Attention @azaks2

Note: This pull request was [previous issued incorrectly](https://github.com/tensorflow/tensorflow/pull/24301) based on r1.12.",tensorflow
24413,facaiy,pr,2018-12-18T04:10:52Z,The gradient op of bias_add supports 3/4/5D NCHW format,"Ref: #22127, #23004",tensorflow
24551,vishwakftw,pr,2018-12-24T16:01:13Z,Close code environment in docstring for adjoint,"This PR adds the delimiter for the code environment in the docs for tf.linalg.adjoint. Without this, the docs were rendered inappropriately as shown below:

![image](https://user-images.githubusercontent.com/23639302/50403262-2d355d80-07c3-11e9-80af-9de4b0c83bff.png)
",tensorflow
24747,duncanriach,pr,2019-01-07T21:42:28Z,Add cuDNN deterministic env variable (only for convolution),"This change is a component of the recipe for making TensorFlow training reproducible on GPUs.

Setting the environment variable TF_CUDNN_DETERMINISTIC=1 (or true) will ensure that both forward and backwards convolution algorithms are both fixed and deterministic. It overrides autotune and selects deterministic back-prop algorithms.

This pull request has two previous abandoned versions:
* [pr/24301](https://github.com/tensorflow/tensorflow/pull/24301) was issued incorrectly based on r1.12.
* [pr/24355](https://github.com/tensorflow/tensorflow/pull/24355) was temporarily closed and could not be re-opened after a force-push to the branch.

This pull request is different from 24355 in the following ways:

1. It caches the environment variable using a more favorable, pre-existing pattern.
2. It uses tensor op math, if it's available.
3. It factors the logic that decides if tensor op math is available into a separate inline function.
4. If TF_CUDNN_DETERMINISTIC is set, the code that accumulates the list of algorithm options is skipped.

Attention @azaks2 @timshen91 ",tensorflow
24788,mpjlu,pr,2019-01-09T02:47:10Z,[Intel MKL] SparseAdam: an optimizer to improve the accuracy of LazyAdam ,"
",tensorflow
24791,facaiy,pr,2019-01-09T06:00:31Z,"ENH: more robust leaky_relu, for alpha > 1 or < 0",Fix #24566,tensorflow
24864,byronyi,pr,2019-01-11T23:47:09Z,Add support of collective ops for GDR,"Fixes #18232.

Signed-off-by: Bairen Yi <byronyi@clustar.ai>",tensorflow
24878,kashif,pr,2019-01-13T14:41:22Z,Replace deprecated Cuda driver API,"Replaced deprecated cuda driver API calls with recommended ones. Note these are deprecated since CUDA 5.0 (2013) so should be safe to remove.

- [x] [` cuDeviceComputeCapability`](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html#group__CUDA__DEVICE__DEPRECATED_1gdc50ce6a6e0a593158d4ccb3567e0545)
- [x] [`cuDeviceGetProperties `](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html#group__CUDA__DEVICE__DEPRECATED_1ged20a6d946d0217b3b1e0a40df6a43a6)",tensorflow
25008,musikisomorphie,pr,2019-01-17T19:29:39Z,add tensor[tensor > a] syntax for issue #24133,"Add tensor[tensor > a] syntax for issue #24133.
Hi @alextp, could you take a look my changes?
 ",tensorflow
25121,freedomtan,pr,2019-01-23T07:39:23Z,[tflite] fix the location of build_ios_universal_lib.sh in TFLite bechmark doc,fix the location of build_ios_universal_lib.sh in TFLite benchmark for iOS README.md,tensorflow
25241,facaiy,pr,2019-01-28T07:50:59Z,Add include_batch_in_index for CPU kernel of max_pool_with_argmax,"Related issue: #24067, PR: #23993 

Note include_batch_in_index attribute defaults to False, which breaks CPU behavior.",tensorflow
25269,duncanriach,pr,2019-01-29T00:24:58Z,Add deterministic cuDNN max pooling,"When the TF_CUDNN_DETERMINISTIC environment variable is set to 1/true, cuDNN max pooling is performed using a deterministic algorithm.

This current pull request follows-on from pull request [24747](https://github.com/tensorflow/tensorflow/pull/24747) which dealt only with the convolution algorithms.

This current pull request completes the functionality controlled by TF_CUDNN_DETERMINISTIC, so the TODO from the previous pull request is also removed.",tensorflow
25404,duncanriach,pr,2019-02-01T01:34:22Z,Improves docstring for tf.contrib.nn.conv1d_transpose,This was mainly prompted by noting that the documented valid `data_format` options would raise an exception.,tensorflow
25510,cclauss,pr,2019-02-05T06:50:44Z,ci_build: Upgrade the Python 'six' compatibility module,"https://github.com/benjaminp/six/blob/master/CHANGES Fixes a metaclass bug and adds __six.ensure_binary()__, __six.ensure_text()__, and __six.ensure_str()__.",tensorflow
25515,cclauss,pr,2019-02-05T09:56:25Z,install_pip_packages.sh: 'pep8' is now 'pycodestyle',"As discussed at https://pep8.readthedocs.io

https://pypi.org/project/pep8/ says:
### Changelog
### 1.7.1 (2017-10-22)
Changes:
* Prominently note via warning message that the tool is no longer released as pep8 and will only be fixed in the pycodestyle package


",tensorflow
25529,ziky90,pr,2019-02-05T17:23:07Z,Fix/Enable keras.utils.metrics_utils AUCCurve api,"PR enabling AUCCurve API that is needed by https://github.com/tensorflow/estimator, particularly PR https://github.com/tensorflow/estimator/pull/27",tensorflow
25748,freedomtan,pr,2019-02-14T07:59:01Z,[tflite] export SetNumThreads to TFLite Python API,"export SetNumThreads() and add an option to set the number of threads in the `label_image.py` for TFLite. On [PYNQ-Z1](https://www.xilinx.com/products/boards-and-kits/1-hydd4z.html), a board with 2xCA9,
```
$ python tensorflow/lite/examples/python/label_image.py --num_threads 1
0.415686: 653:military uniform
0.352941: 907:Windsor tie
0.058824: 668:mortarboard
0.035294: 458:bow tie, bow-tie, bowtie
0.035294: 835:suit, suit of clothes
time:  0.691580057144

$ python tensorflow/lite/examples/python/label_image.py --num_threads 2
0.415686: 653:military uniform
0.352941: 907:Windsor tie
0.058824: 668:mortarboard
0.035294: 458:bow tie, bow-tie, bowtie
0.035294: 835:suit, suit of clothes
time:  0.412344932556
```",tensorflow
25796,duncanriach,pr,2019-02-16T08:00:47Z,Add tests for TF_CUDNN_DETERMINISTIC,"Some of the tests for the environment variable `TF_CUDNN_DETERMINISTIC`, which was added in pull requests [24747](https://github.com/tensorflow/tensorflow/pull/24747) and [25269](https://github.com/tensorflow/tensorflow/pull/25269).",tensorflow
25897,ngimel,pr,2019-02-19T17:13:14Z,correctly initialize scratch descriptor for gemm autotuning,"Fixes #25761. Scratch should be initialized with the same descriptor as output_matrix, and not assume that output_matrix is column major. ",tensorflow
25941,bharatr21,pr,2019-02-20T16:10:14Z,Fix #24818 by updating linalg_ops.py,"Fix #24818 by replacing the circular reference to `tf.linalg.eigvalsh`  with `tf.linalg.eigh`  [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L338) which also returns two outputs.
I believe that `tf.linalg.eigh` is the correct equivalent to resolve:
1. Circular Reference
2. Two outputs instead of one output, highlighted as ""(possibly ignoring the second output)"" in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L338)",tensorflow
26156,freedomtan,pr,2019-02-27T05:45:10Z,remove HAVE_XLOCALE_H in hwloc BUILD.bazel,xlocale.h is not a standard header and is not shipped with newer glibc.,tensorflow
26180,alanhdu,pr,2019-02-27T17:15:27Z,Update link in log message,The contrib README just redirects you to the other README.,tensorflow
26230,nluehr,pr,2019-02-28T22:54:09Z,Cuda 10.1 support,"Enables building TensorFlow with CUDA 10.1.
Addresses #26150",tensorflow
26264,musikisomorphie,pr,2019-03-01T20:21:51Z,Merge pull request #1 from tensorflow/master,aa,tensorflow
26269,musikisomorphie,pr,2019-03-01T23:21:39Z,Data augmentation for N-D tensor #25295,,tensorflow
26308,duncanriach,pr,2019-03-04T03:59:05Z,Update username in TODO,"As [recommended by GitHub](https://github.community/t5/Support-Protips/Using-one-account-for-all-your-projects/ba-p/5509), I merged my personal and professional GitHub accounts, and I changed my username. I don't anticipate my username ever changing again.",tensorflow
26345,neerajprad,pr,2019-03-05T04:57:30Z,Fix AttributeError on LocalBuffer.__delete__ call in python xla client,"While playing around with some custom JAX primitives, I was noticing the python xla client throwing exceptions like below towards the end (probably when LocalBuffer is gc'd or in a separate cleanup operation). The exceptions are ignored, but they are logged many times in the code that I am working with. This adds a small check (taken from the Executable class) to ensure that `c_api` hasn't already been freed in the `LocalBuffer.delete()` method.

```
Exception ignored in: <bound method LocalBuffer.__del__ of <jaxlib.xla_client.LocalBuffer object at 0x1267a55c0>>
Traceback (most recent call last):
  File ""/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py"", line 393, in __del__
  File ""/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py"", line 377, in delete
  File ""/Users/npradhan/miniconda3/envs/numpyro/lib/python3.6/site-packages/jaxlib/xla_client.py"", line 104, in delete_buffer
AttributeError: 'NoneType' object has no attribute 'DeleteLocalShapedBuffer'
```

cc. @hawkinsp ",tensorflow
26562,facaiy,pr,2019-03-11T06:35:11Z,`max_pool_with_argmax` GPU kernel supports `include_batch_in_index`,"Fix #22025, #24067

Related PR: #25241, #23993",tensorflow
26723,facaiy,pr,2019-03-15T01:33:13Z,"BUG: fix, layer_test doesn't accept layer_cls key",The bug is reported on https://github.com/tensorflow/addons/pull/92,tensorflow
26847,bharatr21,pr,2019-03-18T17:50:12Z,[TF 2.0 API Docs] Added usage and examples to tf.compat.path_to_str,"Fix #25826 by adding usage and examples to `tf.compat.path_to_str`

Here I have also commented the OS used (since `Path` is OS dependent)
If `tf` uses a differently flavored Markdown or my PR is not up-to-standard, kindly excuse me as I'm not used to the Tensorflow Doc Generation process, also I'd be happy to learn more about it!",tensorflow
26926,byronyi,pr,2019-03-20T08:45:43Z,Fix typo in doc,Nit,tensorflow
27389,musikisomorphie,pr,2019-04-01T20:23:04Z,Add cuda implementation of roll_op  #23590,"@alextp, I tested my changes for tensorflow 1.12, since our cluster only supports cuda9 right now.
",tensorflow
27454,byronyi,pr,2019-04-03T08:54:59Z,grpc+seastar: add grpc+seastar protocol which uses Seastar as RPC for WorkerService,"This PR serves as a placeholder for contribution from @liutongxuan and his colleagues in Alibaba. 

Since TF is going to have yet another (hopefully last) release before 2.0 (r1.14 to be cut on April 15), I am not sure if we have enough time (or incentive) to push this feature into the main repo. 

[tensorflow/networking](https://github.com/tensorflow/networking) should serve as its final target after 2.0. As we are still transitioning and @annarev is working on the networking C API, I would still like to submit this PR against the main repo, mainly for the convenience of reviewing. When we feel like ready, we shall have another PR in https://github.com/tensorflow/networking for refactoring it as a standalone plugin.

Ping @jbedorf @poxvoculi; if you have time feel free to join the reviewing process.",tensorflow
27462,freedomtan,pr,2019-04-03T13:30:15Z,[tflite] make metal delegate code build,"changes to build libtflite_gpu_metal.so and libmetal_delegate.a.

```
bazel build --apple_platform_type ios --cpu=ios_arm64 --cxxopt=-std=c++14 \
tensorflow/lite/delegates/gpu:libtflite_gpu_metal.so
```
and
```
bazel build --apple_platform_type ios --cpu=ios_arm64 --cxxopt=-std=c++14 \
tensorflow/lite/delegates/gpu:metal_delegate
```
build.

https://github.com/tensorflow/tensorflow/issues/27325",tensorflow
27464,freedomtan,pr,2019-04-03T13:59:28Z,[tflite] add GPU Delegate to label_image on Android,`label_image -g 1 -m your_floating_point_model` will be delegated to GL GPU delegate on Android,tensorflow
27814,bharatr21,pr,2019-04-13T10:51:04Z,[TF 2.0 API Docs] Added Documentation to tf.keras.activations.elu,Fix #27652 by adding required information to tf.keras.activations.elu [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py) including an image for visuals and example usage from the Intro to CNNs tutorial as suggested.,tensorflow
27837,bharatr21,pr,2019-04-14T17:36:51Z,[TF 2.0 API Docs] Added Documentation to tf.keras.activations.selu,"Fix #27657 by adding required information to tf.keras.activations.selu [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py) and added examples of FNN instead of ConvNets because SELU finds better use there. Also added an image of the function, hoping the markdown renders properly. ",tensorflow
28011,freedomtan,pr,2019-04-21T03:52:04Z,[tflite] enable using the new NNAPI delegate in Java,"1. add a wrapper for NNAPI delegate can be accessed in Java
2. modify the demo app to use it

enable using the new NNAPI delegate in Java so that we can avoid situation described in https://github.com/tensorflow/tensorflow/issues/27983",tensorflow
28210,chrisyeh96,pr,2019-04-27T10:07:36Z,tf.image.per_image_standardization respects input dtype,Make it clear that it can operate on batches of image,tensorflow
28340,Randl,pr,2019-05-02T16:57:22Z,Initial implementation of general eigenvalue decomposition,,tensorflow
28392,freedomtan,pr,2019-05-04T14:26:59Z,[aarch64] make TensorFlow build for aarch64 linux natively with bazel,"Tested on Coral Dev Board running Mendel Linux chef release
```
bazel --host_jvm_args=-Xms128m --host_jvm_args=-Xmx2048m \
build --config opt --local_resources 1024,1,1 \
//tensorflow/tools/pip_package/build_pip_package
```",tensorflow
28471,byronyi,pr,2019-05-07T11:33:12Z,verbs: fix compilation error,"The forwarded declarations in the header were removed in e7d82e218d538c83cea259cf0f834d406350c911, causing errors in the header file.",tensorflow
28502,bharatr21,pr,2019-05-08T03:30:47Z,Added usage and examples to tf.compat.path_to_str,Fix #25826 (continuing from #26847 which was closed due to CLA issues),tensorflow
28603,nluehr,pr,2019-05-10T15:09:42Z,Remove cudaGraph API from cuda_runtime_10_0,"cudaGraph APIs are not currently used in TensorFlow. This is the simplest fix to
make TensorFlow's cuda runtime wrappers compatible with both CUDA 10.0 and 10.1.

Attention @chsigg ",tensorflow
28618,freedomtan,pr,2019-05-11T01:47:20Z,[tflite] make GPU delegate elementwise ops work,fix the problem in https://github.com/tensorflow/tensorflow/issues/28606,tensorflow
28627,byronyi,pr,2019-05-11T17:26:21Z,Fixes broken GPU build,"```
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 1244
		_create_local_cuda_repository(repository_ctx)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 960, in _create_local_cuda_repository
		_get_cuda_config(repository_ctx)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 686, in _get_cuda_config
		find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 666, in find_cuda_config
		auto_configure_fail((""Failed to run find_cuda_config...))
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 274, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Failed to run find_cuda_config.py: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 493, in <module>
    main()
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 485, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 454, in find_cuda_config
    if tuple(cuda_version.split(""."")) < (10, 1):
TypeError: unorderable types: str() < int()

WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 1244
		_create_local_cuda_repository(repository_ctx)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 960, in _create_local_cuda_repository
		_get_cuda_config(repository_ctx)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 686, in _get_cuda_config
		find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 666, in find_cuda_config
		auto_configure_fail((""Failed to run find_cuda_config...))
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 274, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Failed to run find_cuda_config.py: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 493, in <module>
    main()
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 485, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/org_tensorflow/third_party/gpus/find_cuda_config.py"", line 454, in find_cuda_config
    if tuple(cuda_version.split(""."")) < (10, 1):
TypeError: unorderable types: str() < int()

INFO: Elapsed time: 17.630s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```",tensorflow
28640,byronyi,pr,2019-05-12T13:28:03Z,verbs: fixes GPU build broken by 55a7da3,"Use the default argument sync_dst_compute=true as in DeviceContext::CopyCPUTensorToDevice.

Signed-off-by: Bairen Yi <byi@connect.ust.hk>",tensorflow
28641,byronyi,pr,2019-05-12T13:40:45Z,verbs: fixes GPU build broken by 4c3a1fb,"Use the default argument as in DeviceContext::CopyCPUTensorToDevice.

@poxvoculi is probably the best person to review this, but he's currently on leave.

Ping @dubey if you have time reviewing this. As r1.14 is to be recut, in case we missed it, ping @bananabowl to see if we can cherry-pick this PR.

Thanks!",tensorflow
28686,ThisIsIsaac,pr,2019-05-14T02:36:45Z,BUGFIX:,"Fixed the bug of the kernel not fully processing all the items when the batch * height * width > number of threads spawned by adding a layer of for-loop

**two spaces for minor performance increase:**
1. instead of taking `hue_delta` as a global memory, take in the value inside `hue_delta` in order to eliminate unnecessary global memory read
2. make the copying performed in this conditional: `if (!AdjustHue && !AdjustSaturation && !AdjustV)` access global memory with coalesced accesses",tensorflow
28756,byronyi,pr,2019-05-16T03:38:58Z,build: install TensorRT directly in dockerfiles,"@angersson I see TRT available in [https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1804/x86_64/](https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1804/x86_64/) so there is no need to install a local repo.

Unfortunately, the case is not true for ppc64le. Ping @tjakob to double-check.",tensorflow
28758,ThisIsIsaac,pr,2019-05-16T04:40:25Z,added `ResizeBilinearKernel_faster`,"`ResizeBilinearKernel_faster` is 30 ~ 50% faster than `ResizeBilinearKernel`. It eliminates redundant computation by only spawning at most 8 threads to iterate through the channel dimension and having each thread iterate through the batch dimension.

prerequisite is:
1. channels must be a multiple of 4
2. `sizeof(T) == sizeof(float)`

When the prerequisite is not met, `ResizeBilinearKernel` is used.",tensorflow
28875,nluehr,pr,2019-05-20T19:49:04Z,Fix bazel dependencies for cublas 10.1,"Attention @chsigg

These three dependency tweaks are needed to build against CUDA 10.1.",tensorflow
28930,facaiy,pr,2019-05-22T13:52:03Z,BUG: tf.function cannot handle exception raised by custom op,"Fix https://github.com/tensorflow/addons/issues/260

Hi, would you mind telling me where to add unit test?",tensorflow
29118,bharatr21,pr,2019-05-29T08:26:08Z,Update broken Dockerfile links on CONTRIBUTING.md,Fix #29105 to fix broken Dockerfile links in `CONTRIBUTING.md`,tensorflow
29197,alanhdu,pr,2019-05-31T06:02:43Z,[TF 2] Expose tf.layers as alias for tf.keras.layers,cc #26176 ,tensorflow
29561,byronyi,pr,2019-06-08T18:07:53Z,Fixes include path broken by virtual pip packaging,"Ping @mihaimaruseac for review. Ping @lark as this is a work-related contribution.

This fixes https://github.com/horovod/horovod/issues/1129 in our in-house nightly build.",tensorflow
29667,duncanriach,pr,2019-06-11T21:24:57Z,Add release note about TF_CUDNN_DETERMINISTIC,"This is the release note associated with:

- PR [24747](https://github.com/tensorflow/tensorflow/pull/24747): Add cuDNN deterministic env variable (only for convolution)
- PR [25269](https://github.com/tensorflow/tensorflow/pull/25269): Add deterministic cuDNN max-pooling
- PR [25796](https://github.com/tensorflow/tensorflow/pull/25796): Added tests for TF_CUDNN_DETERMINISTIC",tensorflow
29708,duncanriach,pr,2019-06-12T20:06:30Z,Fix docstring for tf.nn.bias_add(),,tensorflow
29807,alsrgv,pr,2019-06-14T20:19:24Z,Hide hwloc symbols in libtensorflow_framework.so,"Currently, `hwloc_*` symbols are being exported by libtensorflow_framework.so, which could conflict with `hwloc_*` symbols used by MPI.

Fixes horovod/horovod#1123
cc @gunan @byronyi ",tensorflow
30043,freedomtan,pr,2019-06-22T14:42:33Z,fix metal build,there is no message(). use error_message() instead,tensorflow
30067,freedomtan,pr,2019-06-24T02:34:25Z,make rocm compile,3e3b9156 changed `CHECK_ERR()` to `PCHECK()`.,tensorflow
30090,freedomtan,pr,2019-06-24T13:23:34Z,support max_profiling_buffer_entries,"```
bazel build --config android_arm64 \
//tensorflow/lite/examples/label_image:label_image \
--copt=-DTFLITE_PROFILING_ENABLED
```
doesn't work after fbda4a1c5540b81dc6fd4f720e5e1353e08c44e6, which changed `profiling::Profiler` constructor",tensorflow
30168,byronyi,pr,2019-06-26T09:45:37Z,grpc+seastar: add grpc+seastar protocol,"Re-submission of #27454 as the previous PR is messed up.

This PR serves as a placeholder for contribution from @liutongxuan and his colleagues in Alibaba.

Since TF is going to have yet another (hopefully last) release before 2.0 (r1.14 to be cut on April 15), I am not sure if we have enough time (or incentive) to push this feature into the main repo.

tensorflow/networking should serve as the final target after 2.0. As we are still transitioning and @annarev is working on the networking C API, I would still like to submit this PR against the main repo, mainly for the convenience of reviewing. When we feel like ready, we shall have another PR in https://github.com/tensorflow/networking for refactoring it as a standalone plugin.

Ping @jbedorf @poxvoculi @liutongxuan @lilbedwin @shanshanpt @YongCHN @swpnlptl.

I believe there are a few review comments not fully addressed:

- [ ] https://github.com/tensorflow/tensorflow/pull/27454#discussion_r296912350
- [x] https://github.com/tensorflow/tensorflow/pull/27454#discussion_r296911797
- [ ] https://github.com/tensorflow/tensorflow/pull/27454#issuecomment-504677772 (Optional)

Boost is now not a direct dependency for this patchset, but still a transitive one for seastar.",tensorflow
30193,duncanriach,pr,2019-06-27T04:23:08Z,Fix URL in docstring,The URL was out-of-date.,tensorflow
30209,nluehr,pr,2019-06-27T19:57:51Z,Add amp and TRT changes to 1.14 release notes,"Attn: @tfboyd
 ",tensorflow
30210,freedomtan,pr,2019-06-28T02:34:18Z,make label_image.py v2 compatible,Update `tensorflow/examples/label_image/label_image.py` to make it V2 compatible.,tensorflow
30213,themightyoarfish,pr,2019-06-28T07:18:13Z,Fix macos makefile,,tensorflow
30214,themightyoarfish,pr,2019-06-28T07:19:32Z,Fix macos makefile build,"- Adding `CoreFoundation` to `HOST_LDFLAGS` is necessary as the library target uses that variable
- Adding `CoreFoundation` to `LIBS` is necessary as the benchmark target uses that variable",tensorflow
30243,freedomtan,pr,2019-06-29T03:28:25Z,[mlir] make mlir related stuff build on macos,"`bazel build --config opt tensorflow/compiler/mlir/...` doesn't work on macOS.
1. `exp10()` is not a standard C function, macos has `__exp10()`
2. include `""mlir/StandardOps/Ops.h""` earlier to avoid `TRUE` and`FALSE` being defined in `<mach/boolean.h>`",tensorflow
30247,freedomtan,pr,2019-06-29T14:40:02Z,[tflite] include headers to fix undeclared PROT_READ on macos,nnapi delegate doesn't compile on macOS after https://github.com/tensorflow/tensorflow/commit/19f417d905980cb7f2098e48c147cb301c087573. Add ` #include <sys/mman.h>` to fix it.,tensorflow
30279,themightyoarfish,pr,2019-07-01T14:10:31Z,Fix macos makefile build,"- Adding `CoreFoundation` to `HOST_LDFLAGS` is necessary as the library target uses that variable
- Adding `CoreFoundation` to `LIBS` is necessary as the benchmark target uses that variable",tensorflow
30300,freedomtan,pr,2019-07-02T05:34:33Z,[mlir] mlir stuff doesn't build after MLIR rev updated,mlir stuff doesn't build after 99d4a961210c7d800f4ccaf9a9b0a5c3bfc799b6 because of dependency problems,tensorflow
30424,taehoonlee,pr,2019-07-05T04:58:08Z,Fix typos,This PR fixes several typos in the warning and the error messages.,tensorflow
30588,freedomtan,pr,2019-07-11T05:54:43Z,[mlir] fix two trivial typos in comments,,tensorflow
30631,JerryShih,pr,2019-07-12T05:20:32Z,[LITE] Add the missed header for osx platform in micro_speech example for makefile.,"When we use the target specific ""audio_provider.cc""[1], we should also add
its header dependency[2] in makefile.

[1]
tensorflow/lite/experimental/micro/examples/micro_speech/osx/audio_provider.cc
[2]
tensorflow/lite/experimental/micro/examples/micro_speech/simple_features/simple_model_settings.h",tensorflow
30654,freedomtan,pr,2019-07-12T14:31:08Z,[mlir] enable Affine Dialect in tf-opt,"`tf-opt --help` shows Affine dialect passes, but it doesn't
recognize the Affine dialect because AffineDialectRegistration
is not linked in.",tensorflow
30785,freedomtan,pr,2019-07-17T02:18:16Z,[tflite] make coco_object_detection:run_eval build,"fix a namespace problem so that we can build
```
//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval
```
(because Google's internal environment is different from public TensorFlow repo?)
@srjoglekar246 ",tensorflow
30911,tillahoffmann,pr,2019-07-21T22:23:08Z,tensorflow-gpu without nvidia-runtime.,"This PR [symlinks](https://en.wikipedia.org/wiki/Symbolic_link) the required CUDA library stubs to the location where tensorflow expects to find them. That means `tensorflow-gpu` can be used even if a container is started based on the image without using the [nvidia runtime](https://github.com/NVIDIA/nvidia-docker). When the nvidia runtime is used, the stubs are overwritten by the real libraries and one can access the GPU as expected.

The issue with library paths already seems to be fixed within tensorflow for `tensorflow-gpu>=1.14`. This patch would support using `tensorflow-gpu<1.14` using a universal docker image.

cc @angersson.",tensorflow
30917,freedomtan,pr,2019-07-22T06:42:16Z,[mlir] make `//tensorflow/compiler/mlir/...` build again,"Except syncing up with MLIR repo, there are two problems:
1. there is no `absl/base/integral_types.h` in public absl (is this a google internal one?), so remove it, and s/int64/int64_t/
2. this is not `::testing::EqualsProto` in public gtest (is this a google internal one?): need to disable using of `EqualsProto` or implement it

@jpienaar: FYR",tensorflow
30921,freedomtan,pr,2019-07-22T08:27:40Z,[tflite] write debug info only when not empty,this fixed https://github.com/tensorflow/tensorflow/issues/30851,tensorflow
30923,freedomtan,pr,2019-07-22T09:16:27Z,[tflite] fix a typo in evaluation tool doc,a trivial error becasue of copy & paste?,tensorflow
31025,ilhamfp,pr,2019-07-25T09:47:27Z,Fix inconsistent default learning rate in tensorflow.keras.optimizers.Adadelta(),Fixing [#31024 issue](https://github.com/tensorflow/tensorflow/issues/31024) of inconsistencies between tensorflow.keras.optimizers.Adadelta() and keras.optimizers.Adadelta().,tensorflow
31036,ilhamfp,pr,2019-07-25T16:12:44Z,Override enumerate on AutoGraph,This pull request address [#30802](https://github.com/tensorflow/tensorflow/issues/30802) issue.,tensorflow
31038,ilhamfp,pr,2019-07-25T17:05:51Z,Override enumerate on AutoGraph,This pull request address #30802 issue.,tensorflow
31157,JerryShih,pr,2019-07-30T08:32:28Z,[LITE] Create the objects to hold the tensor data in elementwise_test and recognize_commands_test.,"The elementwise_test and recognize_commands_test use the deleted temporary object. That might cause the wrong result. I can't pass these two tests locally.

This pr creates some local variables to hold the tensor data.",tensorflow
31159,byronyi,pr,2019-07-30T10:17:33Z,gdr: Fix build error introduced by commit ea4cbee,Ping @dubey to review.,tensorflow
31198,freedomtan,pr,2019-07-31T13:49:06Z,[tflite] fix allow fp16 in tflite benchmark_model,"interpreter_->SetAllowFp16PrecisionForFp32() should be called
before setting delegate. Otherwise, it won't work.",tensorflow
31223,ThisIsIsaac,pr,2019-08-01T06:36:55Z,added `__restrict__` tag to all `__global__`kernels,"@chsigg 

I have added `__restrict__` tag to all `__global__` kernels.

However, I have left `GpuDeviceArrayStruct<T*>` alone, such as:

```cpp
__global__ void SplitVOpKernel_fixed(const T* __restrict__ input, int32 prefix_dim_size,
                                     int32 suffix_dim_size,
                                     GpuDeviceArrayStruct<T*> output_ptr_data) {
```",tensorflow
31238,guillaumekln,pr,2019-08-01T15:21:51Z,Use where_v2 in lookup_ops to silence warnings,"Without this change, the following deprecation warning is raised:

W0801 19:32:15.921911 140481946597120 deprecation.py:323] From [...]/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1159: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where",tensorflow
31263,kashif,pr,2019-08-02T07:45:10Z,Fixed cuda runtime deprecation warning,"In cuda 10.0 `memoryType` attribute is deprecated in favour of `type`. 

See Runtime Documentation [here](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaPointerAttributes.html#structcudaPointerAttributes_191ef95dac597cc710558dabbc8b9ae38).",tensorflow
31268,ilhamfp,pr,2019-08-02T10:23:57Z,Raise error on to_code when entity didn't contain __code__ attr,This PR address #30895 issue.,tensorflow
31290,ilhamfp,pr,2019-08-02T19:53:06Z,Override zip on AutoGraph ,"This pull request address what @mdanatg said on #31038 PR about overriding zip on AutoGraph. The current implementation override zip when all of the input was dataset. But when not all of it was a dataset, it will execute the built-in python zip. Is this the correct behaviour?",tensorflow
31375,nluehr,pr,2019-08-06T18:21:51Z,Release notes for AMP and TF-TRT precision_mode,@mihaimaruseac These are NVIDIA's final tweaks for the TF 1.14.1 release notes.,tensorflow
31389,duncanriach,pr,2019-08-07T00:30:34Z,Enhance release notes related to TF_CUDNN_DETERMINISTIC,Follow-up to [PR 29667 (merged)](https://github.com/tensorflow/tensorflow/pull/29667). @mihaimaruseac please merge before v1.14.1 tag.,tensorflow
31465,duncanriach,pr,2019-08-09T01:45:58Z,Add GPU-deterministic tf.nn.bias_add,"After addressing cuDNN non-determinism in TensorFlow (PRs [24747](https://github.com/tensorflow/tensorflow/pull/24747), [25269](https://github.com/tensorflow/tensorflow/pull/25269), [25796](https://github.com/tensorflow/tensorflow/pull/25796), [29667](https://github.com/tensorflow/tensorflow/pull/29667), [31389](https://github.com/tensorflow/tensorflow/pull/31389)), `tf.nn.bias_add` is the next-most-common source of GPU non-determinism for deep learning models. After this PR, I intend to address other sources of non-determinism before returning to implement a deterministic `bias_add` solution at the CUDA kernel level for higher performance TF determinism when running on GPUs. What this PR does:

* Add (temporary, but not bad) deterministic `tf.nn.bias_add` solution.
* Add `TF_DETERMINISTIC_OPS` environment variable that enables that solution and also enables cuDNN determinism.
* Tests for the above.",tensorflow
31534,marload,pr,2019-08-12T09:27:29Z,Correct typo: ']' has been written twice.,"An error has been found and corrected in the annotation.

If you have a problem, let me know.

Thank you.",tensorflow
31834,facaiy,pr,2019-08-21T09:38:34Z,bugfix: use experimental_ref in ExponentialMovingAverage instead,Fix #31582,tensorflow
31841,hgaiser,pr,2019-08-21T14:07:07Z,Add skip_mismatch argument to load_weights.,"This PR is analogous to https://github.com/keras-team/keras/pull/8462 in Keras.

With it, you can load weights into models that have a mismatch, but use the same names. A practical application would be for transfer learning, where the number of classes influences the size of the weights. It can still be useful to load the weights that are not changed in shape to provide a better starting point.

Note: I don't have a TPU, I am relying on the unit tests to tell me if it is correctly implemented or not.",tensorflow
32046,guillaumekln,pr,2019-08-28T13:39:41Z,Fix deprecation warning when calling boolean_mask,"Running the examples from the [`tf.boolean_mask`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/boolean_mask) documentation prints the following deprecation warning:

> W0828 15:35:30.538258 140631514593024 deprecation.py:323] From .../lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1486: where (from tensorflow.python.ops.array_os) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.where in 2.0, which has the same broadcast rule as np.where

This PR fixes that.",tensorflow
32080,guillaumekln,pr,2019-08-29T09:39:05Z,[r2.0:Cherrypick] Remove tensor input shape from function signature.,This commit solves a performance issue with `tf.function` and variable input shapes. See #29075.,tensorflow
32121,ilhamfp,pr,2019-08-30T18:50:32Z,Override map on AutoGraph,This pull request address #31823 issue. The current implementation override map when all of the input was dataset.,tensorflow
32155,guillaumekln,pr,2019-09-02T12:32:23Z,Use where_v2 to silence deprecation warning,"Running this function currently prints the following deprecation warning:

> W0902 14:30:32.759225 140344378930944 deprecation.py:323] From /lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

This PR fixes that.",tensorflow
32160,byronyi,pr,2019-09-02T18:24:17Z,[tf.data] Increase default buffer size to 256 MB,A larger buffer size brings better throughput at the cost of buffer bloat.,tensorflow
32195,suphoff,pr,2019-09-04T00:17:12Z,Adjust audio_provider for the sparkfun_edge for production boards.,"Temporary gain emulation to deal with too-quiet audio on prototype boards
together with fluctuating 'zero' level offset causes oversaturation on
production boards.

cc: @petewarden ",tensorflow
32451,nluehr,pr,2019-09-12T00:36:00Z,Move AutoMixedPrecision ahead of GenericLayoutOptimizer,"The layout optimizer grappler pass sets the data layout based on the dtype of the op.
The mixed precision optimizer needs to proceed the layout optimizer because it will
change dtypes from float32 to float16.",tensorflow
32513,nluehr,pr,2019-09-13T23:27:38Z,Update release notes for tensorrt and mixed precision,,tensorflow
32605,freedomtan,pr,2019-09-18T03:41:46Z,[xla] fix xla build on cuda devices without nccl,"Some cuda devices, such as Jetson devices, do not support NCCL.
Building `@local_config_nccl//:nccl` on such kind of devices
will cause problem.",tensorflow
32773,guillaumekln,pr,2019-09-24T09:33:47Z,Track initializer in StaticVocabularyTable,Fixes #32770.,tensorflow
32979,duncanriach,pr,2019-10-02T03:17:15Z,Fix typo in release note,This fixes a small typo in the release notes in the r2.0 branch with the assumption that the change will propagate into the master branch and/or later releases from the r2.0 branch.,tensorflow
33091,brettkoonce,pr,2019-10-06T19:14:21Z,mlir: minor spelling tweaks,,tensorflow
33097,OverLordGoldDragon,pr,2019-10-07T02:55:31Z,Fix grad check compatibility,"Current code uses a conflicting check on `grads` during optimizer compilation - the fix addresses it. Below is a minimal reproducible example + full error trace. Full details at [SO question](https://stackoverflow.com/questions/58261348/valueerror-tried-to-convert-y-to-a-tensor-and-failed-error-none-values-not).

<hr>

**DOESN'T WORK**:

```python
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.optimizers import Nadam
import numpy as np

ipt = Input(shape=(4,))
out = Dense(1, activation='sigmoid')(ipt)

model = Model(ipt, out)
model.compile(optimizer=Nadam(lr=1e-4), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,2,(32,1))
model.train_on_batch(X,Y)
```

**WORKS**: remove `.python` from above's imports. 

<hr>

**Full error trace:**

```python
  File ""<ipython-input-1-2db039c052cf>"", line 20, in <module>
    model.train_on_batch(X,Y)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1017, in train_on_batch
    self._make_train_function()
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 653, in get_updates
    grads = self.get_gradients(loss, params)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 92, in get_gradients
    if None in grads:
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3626, in equal
    name=name)
  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))
 
ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```",tensorflow
33141,ziky90,pr,2019-10-08T11:27:57Z,Fix/_map_graph_network for multiple concatenation operations,"PR that fixes bug, checking graph connectivity when concatenation is used multiple times for the same layer.
Fixes https://github.com/tensorflow/tensorflow/issues/30357 and https://github.com/tensorflow/tensorflow/issues/30355",tensorflow
33221,r-barnes,pr,2019-10-10T21:19:34Z,Update api_def_Unique.pbtxt,The documentation of `tf.unique` is unclear because it shows the input as an already sorted list. A result for a non-sorted list should be shown to clarify the operation.,tensorflow
33240,byronyi,pr,2019-10-11T09:19:51Z,build: Support Bazel 1.0,Ping @gunan @seanpmorgan @dslomov; is there any known blockers?,tensorflow
33284,Randl,pr,2019-10-12T12:12:34Z,Fix android demo build,Seems like the gcc version is too old to support `-std=c++14` option,tensorflow
33318,OverLordGoldDragon,pr,2019-10-14T04:05:20Z,Update outdated K.eval,"`K.eval` for K per `import tensorflow.keras.backend as K` fails for a `tensorflow.python.framework.ops.Tensor` tensor for operations involving `tensorflow.python.ops.resource_variable_ops.ResourceVariable`. Minimal reproducible example for a custom optimizer below; replacing `K.eval` with `K_eval` fixes the problem. `eval_fn` code taken from [Keras' backend](https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L908)

(_Another_ problem does remain for `.save()`, but one down)

<hr>

**Proposed fix**:

```python
def K_eval(x):
    try:
        return K.get_value(K.to_dense(x))
    except:
        eval_fn = K.function([], [x])
        return eval_fn([])[0]
```
<hr>

**UPDATE**: problem is also reproducible in `tensorflow.keras` optimizers & layers, TensorFlow 2.0.0, when eager execution is disabled - overcoming the following error upon `K.eval`:

**Minimal reproducible example 2**:

```python
import tensorflow as tf
import tensorflow.keras.backend as K
tf.compat.v1.disable_eager_execution()

var = K.variable([2.], name='var')

try:
    print(""K.get_value"", K.get_value(var))
except:
    try:
        print(""K.eval"", K.eval(var))
    except:
        try:
            print(""K.eager(K.get_value)"", K.eager(K.get_value)(var))
        except:
            try:
                print(""K.eager(K.eval)"", K.eager(K.eval)(var))
            except:
                print(""K_eval"", K_eval(var))
```
```python
K_eval [2.]
```
<hr>

**Minimal reproducible example**:

```python
from tensorflow.python.keras.optimizers import Optimizer
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
import tensorflow.keras.backend as K
import numpy as np

ipt   = Input(shape=(4,))
out   = Dense(1,  activation='sigmoid')(ipt)
model = Model(ipt, out)
model.compile(SGD(lr=1e-2), loss='binary_crossentropy')

X = np.random.randn(32,4)
Y = np.random.randint(0,3,(32,1))
model.train_on_batch(X,Y)

model.save(""path.h5"")
```

<hr>

**Custom optimizer**:

```python
class SGD(Optimizer):
    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False,
                 total_iterations=100, eta_min=0, eta_max=1,
                 t_cur=0, init_verbose=True, **kwargs):
        eta_t = kwargs.pop('eta_t', 1.)
        super(SGD, self).__init__(**kwargs)
        with K.name_scope(self.__class__.__name__):
            self.iterations = K.variable(0, dtype='int64', name='iterations')
            self.lr = K.variable(lr, name='lr')
            self.momentum = K.variable(momentum, name='momentum')
            self.decay = K.variable(decay, name='decay')
            self.eta_min = K.constant(eta_min, name='eta_min')
            self.eta_max = K.constant(eta_max, name='eta_max')
            self.eta_t = K.variable(eta_t, dtype='float32', name='eta_t')
            self.t_cur = K.variable(t_cur, dtype='int64', name='t_cur')

        self.initial_decay = decay
        self.nesterov = nesterov
        self.total_iterations = total_iterations

    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        self.updates = [state_ops.assign_add(self.iterations, 1)]
        self.updates.append(state_ops.assign_add(self.t_cur, 1))

        lr = self.lr

        # momentum
        shapes = [K.int_shape(p) for p in params]
        moments = [K.zeros(shape) for shape in shapes]
        self.weights = [self.iterations] + moments

        self.eta_t = _compute_eta_t(self)

        for p, g, m in zip(params, grads, moments):
            v = self.momentum * m - lr * g  # velocity
            self.updates.append(state_ops.assign(m, v))

            if self.nesterov:
                p_t = p + self.momentum * v - lr * g
            else:
                p_t = p + v

            new_p = p_t

            # Apply constraints.
            if getattr(p, 'constraint', None) is not None:
                new_p = p.constraint(new_p)

            self.updates.append(state_ops.assign(p, new_p))
        return self.updates

    def get_config(self):
        config = {
            'lr': float(K.get_value(self.lr)),
            'momentum': float(K.get_value(self.momentum)),
            'decay': float(K.get_value(self.decay)),
            'nesterov': self.nesterov,
            'total_iterations': int(self.total_iterations),
            'eta_t': int(K.eval(self.eta_t)),
            't_cur': int(K.get_value(self.t_cur)),
            'eta_min': int(K.get_value(self.eta_min)),
            'eta_max': int(K.get_value(self.eta_max)),
        }
        base_config = super(SGD, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

def _compute_eta_t(cls):
    PI = 3.141592653589793
    t_frac = K.cast(cls.t_cur / cls.total_iterations , 'float32')
    eta_t = cls.eta_min + 0.5 * (cls.eta_max - cls.eta_min) * \
        (1 + K.cos(PI * t_frac))
    return eta_t
```

<hr>

**Full error trace**:

```python

  File ""<ipython-input-7-7b8a41253feb>"", line 1, in <module>
    model.save(""path.h5"")

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 975, in save
    signatures, options)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\save.py"", line 112, in save_model
    model, filepath, overwrite, include_optimizer)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\hdf5_format.py"", line 99, in save_model_to_hdf5
    model_metadata = saving_utils.model_metadata(model, include_optimizer)

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\saving\saving_utils.py"", line 202, in model_metadata
    'config': model.optimizer.get_config()}

  File ""<ipython-input-3-5d1359d04ea5>"", line 70, in get_config
    'eta_t': int(K.eval(self.eta_t)),

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 1271, in eval
    return get_value(to_dense(x))

  File ""D:\Anaconda\envs\tf2_env\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3267, in get_value
    return x.numpy()

AttributeError: 'Tensor' object has no attribute 'numpy'
```",tensorflow
33362,freedomtan,pr,2019-10-15T01:47:37Z,[tflite] update label_image for GPU Delegate V2,Update label_image to be GPU Delegate V2 compatible.,tensorflow
33483,duncanriach,pr,2019-10-17T19:41:39Z,Fix small typo in v2.0.0 release note,"This fix was attempted in the r2.0 branch with [PR 32979](https://github.com/tensorflow/tensorflow/pull/32979), which is now closed.",tensorflow
33518,nkreeger,pr,2019-10-18T17:18:01Z,Fix micro SVDF unit test for hybrid-quantized scratch tensor dims.,"This patch fixes incorrect logic setup for hybrid-quant tests of SVDF
for TF Micro. The previous logic incorrectly set the hybrid-quant
scratch input tensor to have the wrong dimensions. This patch fixes that
problem and cleans up variable names in the test.",tensorflow
33647,brettkoonce,pr,2019-10-23T20:12:18Z,mlir: tweak function input types,,tensorflow
33750,nkreeger,pr,2019-10-26T21:15:37Z,Initial optimized fully-connected kernel for Xtensa HiFi3 (int8/uint8),"This PR introduces an optimization for the Xtensa HiFi3 platform for TF Micro's fully connected kernel. This PR demonstrates where to place optimized TF Micro kernels in the source tree. To trigger a build and utilize the intrinsics - this command can be made to the Makefile:

```sh
make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_kernel_fully_connected_test TAGS=xtensa-hifi3 TARGET=xtensa-xpg
```

With this first pass at optimization - the FC unit tests is sped up 20%. The remaining increase in speed will be swapping out the gemmlow methods with optimized Xtensa bits. I'll tackle that in a future PR.",tensorflow
33803,duncanriach,pr,2019-10-29T00:22:32Z,Enable tf.nn.bias_add python op tests to work in eager mode (as well as graph mode),This current PR is a follow-on from [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465) (Add GPU-deterministic tf.nn.bias_add). This current PR enhances the testing of the tf.nn.bias_add op to include eager as well as graph mode.,tensorflow
33808,Randl,pr,2019-10-29T06:33:20Z,An implementation of eigendecomposition gradients,"As for now, some tests yet failing, but I thought may be someone can help me by pointing out what my mistake is.",tensorflow
33860,duncanriach,pr,2019-10-31T01:22:15Z,Remove duplicated name in 2.0.0 release note thanks section,"The only change is from ""Sami Kama, Sami Kama"" to ""Sami Kama"". GitHub (and git) does not show the exact difference in this long line. [This online diff checker](https://www.diffchecker.com/) works.",tensorflow
33900,duncanriach,pr,2019-11-01T01:16:18Z,Address problems with use_deterministic_cudnn test decorator,"There is something not right here, and I'm trying to figure out how best to fix it.

`use_deterministic_cudnn` in `tensorflow/python/framework/test_util.py` is a test decorator that was added in [this commit](https://github.com/tensorflow/tensorflow/commit/c27909ea80e8823dbf4f7176ab69991a630356a1) on 2019-03-13.

The only place it's used is to decorate `layer_test` in `tensorflow/python/keras/testing_utils.py`, which is called (only) from a bunch of test files in `tensorflow/python/keras/layers`.

I initially noticed that the docstring for `use_deterministic_cudnn` does not match what the decorator actually does, and I intended to fix that. The decorator sets `TF_CUDNN_DETERMINISTIC=true`, and that not only disables cuDNN auto-tuning (from an end-user perspective), but also ensures the deterministic selection of deterministic algorithms for the back-prop of cuDNN convolution and max-pooling.

But the problems go deeper. This decorator intends to enable `TF_CUDNN_DETERMINISTIC` only for the decorated test. However, `TF_CUDNN_DETERMINISTIC` is currently implemented so that its value is cached in a static variable the first time it's used. This means that the implied promise of the decorator code to restore the `TF_CUDNN_DETERMINISTIC` setting cannot be fulfilled. Worse still, if an earlier-running test causes `TF_CUDNN_DETERMINISTIC` to be cached (as ""0"" or ""false"") then the decorator will have no effect whatsoever.

Additionally, the underlying `TF_CUDNN_DETERMINISTIC` functionality is currently implemented in a way that leads to the caching of the chosen algorithms (for any given layer configuration), even if the environment variable itself were not cached.

In this current pull request, I'm proposing changing the decorator to use `TF_CUDNN_USE_AUTOTUNE=false` (instead of `TF_CUDNN_DETERMINISTIC=true`), which both makes the code match the docstring and works in terms of environment variable caching: I believe that `TF_CUDNN_USE_AUTOTUNE` is freshly re-evaluated every time a cuDNN convolution is launched.

Whether this proposed change is appropriate or not depends on what is meant by ""Some tests want to base assertions on a graph being _isomorphic with a copy_."" (from the docstring). What is meant by the graph being isomorphic with a copy? Is this requiring bit-exact determinism from the cuDNN convolution and max-pooling back-prop algorithms? If yes, then this per-test-decorator-based solution is probably not appropriate and `TF_CUDNN_DETERMINISTIC` should be set to ""true"" in a broader scope. An example approach is shown [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/kernel_tests/cudnn_determinism_test.py#L102).

I would like to work with the right person/people to help resolve this issue.",tensorflow
33955,suphoff,pr,2019-11-04T00:33:48Z,comp:micro - Set g_pdm_dma_error_reporter pointer to the error_reporter passed to InitAudioRecording(),"Fix for micro_speech example on apollo3evb boards:

The pointer g_pdm_dma_error_reporter is used to report DMA errors on apollo3evb boards.
However currently it is always set to a nullptr due to an initialization bug.
This causes crashes on DMA errors like fifo overflows.
 
Please add the following labels:
**comp:lite**
**comp:micro**",tensorflow
33964,freedomtan,pr,2019-11-04T09:53:51Z,[tflite] expose more NNAPI Delegate to Java,"1. use StatefulNnApiDelegate instead of the deprecated `tflite::NnApiDelegate()`, see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.h#L173-L181)
2. make it possible to specify NNAPI [execution preference](https://developer.android.com/ndk/reference/group/neural-networks#preferencecode) in Java.",tensorflow
34509,byronyi,pr,2019-11-22T05:30:10Z,Add networking related headers in pip wheel,"This is for building the networking plugins in https://github.com/tensorflow/networking that sub-class from the gRPC-base distributed runtime in TF core. Currently all distributed runtime headers are missing from the PyPI wheels, and we will need to download and build TF core from source every time we build a networking plugin. This is a subsequent change after 11385c9cf21cf9c098040075fe82a4667906155b.

Ping @mrry @dubey for review.",tensorflow
34537,freedomtan,pr,2019-11-23T04:00:03Z,[tflite] add int8 input/output to label_image,"More and more models, such as [MobilenetV3's EdgeTPU ones](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet), are using [post-training full integer quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations). With this patch, I can get reasonable results.

```
./label_image_int8 -m mobilenet_edgetpu_224_1.0_int8.tflite
Loaded model mobilenet_edgetpu_224_1.0_int8.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked
average time: 15.363 ms
0.867188: 653 military uniform
0.0390625: 835 suit
0.015625: 458 bow tie
0.0078125: 907 Windsor tie
0.00390625: 716 pickelhaube
```",tensorflow
34601,duncanriach,pr,2019-11-26T02:14:04Z,Relocate comment in resize_bilinear kernel,"This comment applies to both branches, the older legacy code and the newer half-pixel code.

I hope I'm not being too pedantic with pull-requests like this. I see something wrong, and I want to minimize confusion for anyone else who might pass this way.",tensorflow
34627,duncanriach,pr,2019-11-26T23:52:46Z,Improve resize_bilinear CPU back-prop kernel comment,I believe that the proposed change is both easier to understand and more correct.,tensorflow
34792,cbalint13,pr,2019-12-03T11:33:04Z,Update systemlibs protobuf vars,"Small patch to enable latest master builds using external ```protoc```

```REPOSITORY_NAME``` and ```PACKAGE_NAME``` are no longer valid in newer bazel.

Bringing the attention of @perfinion .
",tensorflow
34818,freedomtan,pr,2019-12-04T08:51:55Z,[tflite] make TransposeConv on NNAPI work,"NNAPI TransposeConv op should take tensor inputs from TFLite node.
This actually is from a0ae68c, which was overwritten by 22fcf5f.

Without this patch, delegating transpose convolution to NNAPI will fail.",tensorflow
34855,freedomtan,pr,2019-12-05T07:46:13Z,[tflite] fix a dilated dw conv problem in tflite_convert,"In tflite_convert/toco, it's assumed that there is a BiasAdd
before or after a BatchToSpaceND. However, in some frozen
graph_def files, e.g., DeepLab V3 mobilenet v2 coco
checkpoints [1], it's a fused BatchNormalization rather than
a BiasAdd.

This patch adds the BatchNorm case.

[1] https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md",tensorflow
34869,freedomtan,pr,2019-12-05T13:30:33Z,[tflite] bump SPLIT op ver from 1 to 3 in NNAPI delegate,"I need SPLIT op version 3. Since it's supported by TFLite and
NNAPI 1.2. It should be safe to bump the op version so that
I can delegate SPLIT ops to accelerators.",tensorflow
34870,hgaiser,pr,2019-12-05T14:01:39Z,Use _get_distribution_strategy only when it is available.,"The changes introduced in https://github.com/tensorflow/tensorflow/commit/06d8f77bcc43826eb2b30209a73f3c833079edbc are not compatible with standalone `Keras` (they are compatible with `tf.Keras`). a `keras.Model` does not have a `_get_distribution_strategy` method, which is now assumed for the `Tensorboard` callback.

These kind of differences are usually not an issue, but Keras is using [`tf.keras.callbacks.Tensorboard`](https://github.com/keras-team/keras/blob/master/keras/callbacks/tensorboard_v2.py#L18) instead of implementing its own.

This PR checks if `_get_distribution_strategy` exists and uses it if it does, making it compatible with a `keras.Model`.",tensorflow
34887,duncanriach,pr,2019-12-06T02:01:57Z,Add info about TF_DETERMINISTIC_OPS to version 2.1 release notes,"**This pull request is specifically targeted at the r2.1 branch.**

This adds information about the environment variable `TF_DETERMINISTIC_OPS`, which is first introduced in version 2.1, to the associated release notes.

For more information, see [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465) that implemented this functionality.",tensorflow
34951,duncanriach,pr,2019-12-09T04:27:35Z,Add multi-algorithm deterministic cuDNN convolutions,"This current pull request intends to address a bug in the functionality of the environment variable `TF_CUDNN_DETERMINISTIC` (see [PR 24747](https://github.com/tensorflow/tensorflow/pull/24747)) and also the environment variable `TF_DETERMINISTIC_OPS` (see [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465)).

The current implementation (before application of this current pull request) of deterministic cuDNN convolution in TensorFlow chooses, for any layer configuration, one fixed deterministic algorithm for each of the forward and two backward propagation paths.

I have since come to appreciate that each algorithm is not guaranteed to work on all layer configurations. The solution represented by this current PR addresses that problem. It uses the existing auto-tuning mechanism to attempt to use all of the available deterministic algorithms, and then, instead of choosing the fastest one (as regular auto-tuning does), this solution chooses the first deterministic algorithm that works. It does this in a deterministic way, based on its index in a intentionally ordered array. ",tensorflow
35006,duncanriach,pr,2019-12-10T21:58:02Z,Fix version 2.1 release note regarding TF_DETERMINISTIC_OPS,"**This current pull request is intentionally aimed at the r2.1 branch.**

This current pull request is a follow-up to [PR 34887](https://github.com/tensorflow/tensorflow/pull/34887). Unfortunately, I didn't review the rendered markdown for those changes before submitting the PR.

This PR:
  * Changes the indent of the note about `TF_DETERMINISTIC_OPS` so that it doesn't appear to be related to TensorRT.
  * Escapes the use of \* twice in the markdown, which was previously interpreted as intending to signal emphasis.
  * Adds some improvements to clarity.

Requesting review by @sanjoy, who reviewed the original PR.",tensorflow
35123,suphoff,pr,2019-12-14T23:05:06Z,lite/micro: Fix bug in tensor lifetime calculation.,"Fix for issue #35121

The tensor lifetime may be incorrectly calculated in MicroAllocator::FinishTensorAllocation() if the same sensor is used multiple times as inputs to different operations or is used as input/output or variable of the graph.",tensorflow
35125,suphoff,pr,2019-12-15T00:21:45Z,lite/micro: Add feature buffer to micro_speech example.,"This fixes #35117

Accumulate feature slices in separate buffer.
The input tensor is not suitable for keeping state across interference
as it has limited lifetime and the buffer space may be reused.",tensorflow
35215,freedomtan,pr,2019-12-18T06:34:06Z,[tflite] fix handling of avgpool in NNAPI delegate,"After NNAPI 1.2, the h * w <= 256 limit is gone, see its CPU avgpool kernel [code](https://android.googlesource.com/platform/external/tensorflow/+/3589bbfbc68ddb8a4c87f90bf7c8dc87c5a50556%5E%21/tensorflow/lite/kernels/internal/optimized/optimized_ops.h) here",tensorflow
35294,suphoff,pr,2019-12-20T03:11:02Z,lite/micro: Add tensor allocation tests for #35121,"Add a second operation to the MockModel sharing inputs with the first
operation to catch #35121 (Tensor lifetime incorrectly calculated on
multiple use) and verify the fix.

Both micro_allocator_test and micro_interpreter_test will fail with this change until  #35123 is merged.",tensorflow
35458,duncanriach,pr,2019-12-28T06:56:10Z,Fix formatting of tf.xla.experimental.jit_scope examples,"The examples for `tf.xla.experimental.jit_scope` contained in its docstring are not rendering correctly (as pre-formatted text) to the TensorFlow [API docs](https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope?version=stable). This current pull request intends to fix that.

I also want to also bring attention to the fact that the argument list is not rendering correctly. I don't know why that's happening, nor how to fix it.",tensorflow
35505,duncanriach,pr,2019-12-30T22:34:53Z,Improve docstrings for tf.config.optimizer.get_jit and .set_jit,,tensorflow
35613,suphoff,pr,2020-01-06T17:05:31Z,Fix lite/micro cmsis-nn kernels that were not refactored in #27019,Fix for #35612 ,tensorflow
35782,gaurav1086,pr,2020-01-12T01:44:24Z,tensor_jni memory cleanup,Delete dims[] before return.,tensorflow
35786,gaurav1086,pr,2020-01-12T03:51:47Z,Some C++ fixes,,tensorflow
35845,gaurav1086,pr,2020-01-14T05:12:17Z, TensorT* nullptr check to avoid segfault,Check TensorT* tensor for nullptr before accessing tensor->shape[channel_dim_index] to avoid segfault.,tensorflow
35882,gaurav1086,pr,2020-01-14T23:51:07Z,[SEGFAULT]: root_node nullptr check,Accessing root_node after it may possibly still be nullptr.,tensorflow
35918,gaurav1086,pr,2020-01-15T23:34:59Z,Minor optimization,"1. Remove the unused variable tensor_ids.
2. Made the member_function const.",tensorflow
35945,byronyi,pr,2020-01-16T15:32:25Z,Fix broken numa build,"After 37f0ac13bdaf6f5c0015885ab70d49b4094feca5 my build was broken with the following error:

```
[0 / 21] [Prepa] BazelWorkspaceStatusAction stable-status.txt
ERROR: missing input file '//third_party/hwloc:include/private/autogen/config.h.in'
ERROR: /tensorflow_src/third_party/hwloc/BUILD.bazel:212:1: //third_party/hwloc:include_private_hwloc_autogen__config_h: missing input file '//third_party/hwloc:include/private/autogen/config.h.in'
ERROR: /tensorflow_src/tensorflow/cc/BUILD:507:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command 
  (cd /home/byronyi/.cache/bazel/_bazel_byronyi/8c9ffbe253caf5ffa98d6d412a0cbaa1/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/bin:/usr/bin:/bin \
  /home/byronyi/.cache/bazel/_bazel_byronyi/install/84defa6eb1e9416bf92d6f89ab2d4f31/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15
ERROR: /tensorflow_src/third_party/hwloc/BUILD.bazel:212:1 1 input file(s) do not exist
INFO: Elapsed time: 39.257s, Critical Path: 1.42s
INFO: 4 processes: 4 remote cache hit.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

This PR fixes the error above.",tensorflow
35967,gaurav1086,pr,2020-01-17T04:15:25Z,Removed redundant nullptr check,Removed duplicate check.,tensorflow
35974,freedomtan,pr,2020-01-17T07:57:39Z,[tflite] add Hexagon delegate to label_image,"add Hexagon Delegte support to label_image and remove unnecessary
depependency in //tensorflow/lite/examples/label_image/BUILD",tensorflow
36050,gaurav1086,pr,2020-01-20T04:51:57Z,[lite] fix ConvBuffer1x1 check,,tensorflow
36051,gaurav1086,pr,2020-01-20T05:06:41Z,Optimize expression,Minor expression optimization: A || (!A && B) <=> A || B,tensorflow
36052,gaurav1086,pr,2020-01-20T05:12:15Z,[grappler]:optimize_expression,Minor optimization.,tensorflow
36171,gaurav1086,pr,2020-01-24T03:17:52Z,[core] Sparse tensor valid index check,,tensorflow
36211,gaurav1086,pr,2020-01-25T23:30:43Z,[grappler] mutable graph: can_dedup_control,,tensorflow
36218,gaurav1086,pr,2020-01-26T06:37:35Z,[core] Added null check for output buffer,Check output != nullptr before calling output->clear(),tensorflow
36324,gaurav1086,pr,2020-01-29T23:31:26Z,[compiler] Simplify the expression for check batch size modified,"Logical expression reduction.

(!batch_size_is_defined ||  (batch_size_is_defined && size[0] != input_dims[0]));
is logically equivalent to => 
(!batch_size_is_defined || size[0] != input_dims[0]);

like !A || (A && B) <=> !A || B

Same for the other change.",tensorflow
36325,gaurav1086,pr,2020-01-30T01:46:22Z,[core] scatter_op: Remove duplicate check for IsScalar(),Remove duplicate check.,tensorflow
36350,nluehr,pr,2020-01-30T20:36:16Z,Add NVTX Ranges,"- Adds NVTX ranges around ops executed by eager and graph executors.
- Useful for NVIDIA Nsight Systems or DLProf profiling.
- All ranges are added to the 'tensorflow-core' domain to avoid
  conflicts with potential user-defined ranges.
- Can be disabled with TF_DISABLE_NVTX_RANGES environment variable.

Attn @jbaiocchi and @qiuminxu ",tensorflow
36397,freedomtan,pr,2020-02-01T04:17:54Z,[tflite] enable INT8 for Java binding,"some models created by full-integer post training quantization,
e.g., the mobilenet v3 edgetpu one [1], have INT8 input and
output tensors.

See also https://github.com/tensorflow/models/issues/7887

[1] https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_1.0.tgz",tensorflow
36403,gaurav1086,pr,2020-02-01T11:26:34Z,[core] Avoid HashNode lookup of a null nodedef,Avoid HashNode lookup of a null nodedef,tensorflow
36405,gaurav1086,pr,2020-02-01T11:42:59Z,[core] check index channel before accessing center_frequencies_,Check index channel before accessing center_frequencies_ since the order of execution for the expression is always left to right.,tensorflow
36406,gaurav1086,pr,2020-02-01T11:58:10Z,[lite] check index channel before accessing center_frequencies_,Check index channel before accessing center_frequencies_ since the order of execution for the expression is always left to right.,tensorflow
36408,gaurav1086,pr,2020-02-01T12:05:29Z,[lite] pass array_names by const ref,[Efficiency] Avoid extra copy for read-only. pass array_names by constant reference instead of by value.,tensorflow
36409,gaurav1086,pr,2020-02-01T12:18:44Z,[lite] Remove possible nullptr sum_op,op_producing_add_input can still be null  while sum_op is not null - due to multiple 'continue' statements within the for loop.,tensorflow
36410,gaurav1086,pr,2020-02-01T12:28:45Z,[lite] Remove possible redundant/nullptr add_op,op_producing_add_input can still be null while add_op is not null - due to multiple 'continue' (error cases) statements within the for loop.,tensorflow
36468,byronyi,pr,2020-02-04T17:14:12Z,Add block cache for low level table library,"This is part of a patch series aiming to improve the performance of on-disk dataset.cache() (CacheDatasetV2).

Currently CacheDataset uses core/util/tensor_bundle to cache dataset elements on disks. It uses sorted string table (SST) to index dataset elements. Unlike checkpoints which do not have a great number of tensors, caching a large dataset may incur a greater number of tensors as well as index blocks.

If the index block is present in an in-memory LRU block cache, fetching a dataset element only needs 1 round trip instead of 2. This is particularly useful when CacheDataset are read from remote file system at a higher latency such as HDFS and GCS.

Almost all code are imported from the LevelDB project, in particular the hash function to shard LRU cache. Currently using Hash32 in core/lib/hash fails the EvictionPolicy test.

I only make 2 modifications to the original cache:

1. Alias leveldb::Slice to tensorflow::StringPiece, which transitively aliases to absl::string_view.
2. Switch to tensorflow::mutex for all mutexes.

Ping @jsimsa to review.",tensorflow
36589,bharatr21,pr,2020-02-09T08:33:42Z,[DOC]: Update the docs regarding Variable.assign,Fix #35667 and Update the docs regarding `Variable.assign` and mention the fact that `variable_shape=False` is not enough and the `tf.Variable` has to be initialized with `shape=TensorShape(None)`,tensorflow
36617,marload,pr,2020-02-10T09:18:59Z,Remove Meaningless Reversed(),"The existing code will reverse the list containing the constant value. Reversed list is always the same list, so overhead occurs.

Thank you for your efforts to create the best project, TensorFlow.",tensorflow
36716,suphoff,pr,2020-02-13T00:34:10Z,TFLu: Port SetCancellationFunction() from TFL and add test cases.,"@petewarden : First draft of porting the SetCancellationFunction() from TFL.
Functionality should be fine - but need to check style in unit test. ",tensorflow
36749,freedomtan,pr,2020-02-14T12:20:35Z,[tflite] add xnnpack delegate to label_image,"The [XNNPACK Delegate](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack) uses [XNNPACK](https://github.com/google/XNNPACK), a fairly optimized floating point library, to run some inference operators (see the delegate's [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) for currently supported ops). With XNNPACK, I was able to get performance numbers similar to what @Maratyszcza described at [XNNPACK's readme](https://github.com/google/XNNPACK/README.md). I also got good numbers on Pixel 4 and Oppo Reno 3. Numbers on x86 machines are also good.

- **Single-threaded**, `label_image -m MODEL_NAME -x 1 -c 50 -t 1`

Model |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms |
-- | --: | --: | --:
MobileNet v1 1.0X |  88.02 | 29.95 | 36.23 |
MobileNet v2 1.0X |  55.13 | 19.13 | 21.69 |
MobileNet v3 Large |  44.84 | 15.69 | 17.65 |
MobileNet v3 Small |   14.23 | 5.58 | 5.66|

- **multi-threaded**, the number of threads is set to be the number of big cores. `label_image -m MODEL_NAME -x 1 -c 50 -t NUMBER_OF_BIG_CORES`

Model |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms|
-- | --: | --: | --:
MobileNet v1 1.0X |  46.00 | 10.60 | 13.23 |
MobileNet v2 1.0X |  28.57 | 6.89 | 7.56 |
MobileNet v3 Large |  24.62 | 6.52 | 6.88 |
MobileNet v3 Small |   8.24 | 2.47 |  2.37 |

",tensorflow
36843,OverLordGoldDragon,pr,2020-02-18T00:11:52Z,Pass full input_shape to cell.build(),"`step_input_shape` unnecessarily deprives `Cell` instances of potentially necessary information - namely, custom architectures may require the layer's `timesteps` at build time (before `call()`). There are no performance or memory gains from trimming a list by one, nor really in clarity as this isn't being passed to `call` where the input shape does omit the time dimension (and in that case `input_shape` as opposed to `step_input_shape` is misleading anyway).

This commit does not require changing any other code in `recurrent` or `recurrent_v2`, as `input_shape` is indexed via `[-1]` - though unsure for the rest of the repository; either way, nothing is lost with this commit.",tensorflow
37033,freedomtan,pr,2020-02-25T02:38:40Z,[tflite] make gpu delegate build,"This resolves https://github.com/tensorflow/tensorflow/issues/36967.

It seems this line was forgotten when adding the space_to_depth kernel.
Without this, all the programs use the GPU delegeate don't build.

@impjdi: it seems you checked in the space_to_depth kernel.",tensorflow
37069,byronyi,pr,2020-02-25T23:48:32Z,tensor_bundle: Supply size hint to input buffer,"This improves performance for random reads on string and variant tensors. Both rely on input buffering currently. If the buffer size is too large, it ends up reading too much data and throw most of them away doing next out-of-range Seek.

This is part of a patch series aiming to improve the performance for CacheDatasetV2. It does not support external shuffle or random reads now, but we plan to add it in the future (subject to the API review).",tensorflow
37074,zhuzilin,pr,2020-02-26T03:00:28Z,Add different pattern to show op in timeline,"This is a PR from JIZHI, the AI platform in Tencent.

When using timeline in tensorflow, we often observe large blanks in the ""/job"" row, which should be showing the consecutive execution of operators. 

The following are timelines for transformer and transformer using XLA:
Transformer:
![transformer](https://i.ibb.co/NVPV5W6/transformer-before.png)
XLA:
![XLA](https://user-images.githubusercontent.com/10428324/75305719-e0935980-5881-11ea-81a4-f1dda66c2767.png)
The timeline is somehow confusing and those blanks may leads to misunderstanding that  there are gpus hanging freely.

The reason for this issue is that the ""/job"" row only shows the scheduling time for each op. As a result, the async kernels may not start to compute by the time the scheduling of its op is over. For the transformer example, the matmul in embedding takes long and blocks other kernels, which results in the large blank in the middle. And for XLA, those fused kernels are scheduled early but have to wait for execution, which results in the large gap between the ending in ""/job"" and ""/stream:all"".

Therefore, we propose 2 new pattern to show the op execution time.
- `""gpu""` pattern will align op with the execution span of all its kernels
- `""all""` pattern will only change the ending time of the op to  the ending of its last kernel. 

we added a new argument `op_time` to function `generate_chrome_trace_format` to let user select how op execution time will be shown. The default value is `""schedule""` which behaves the same as before. And other possible values are `""gpu""` and `""all""` as explained above.

```python
  def generate_chrome_trace_format(self, show_dataflow=True, show_memory=False, op_time=""schedule""):
```

The result of ""gpu"" pattern is:
Transformer:
![transformer](https://i.ibb.co/m4ztqsm/transformer-gpu.png)
XLA:
![XLA](https://i.ibb.co/Tgf2wC6/xla-gpu.png)

And the result of ""all"" pattern is
Transformer:
![transformer](https://i.ibb.co/4Jpfkg9/transformer-all.png)
XLA: 
![XLA](https://i.ibb.co/71FxsmF/XLA-all.png)
Notice that the above illustrations have only shown part of the ""all"" pattern since it may induce large parallel (many op are waiting to be executed at the same time.)

Additionally, the kernel name of an XLA fusion kernel is not parsed correctly, and because we do not have plans on changing the C++ part of the profiler, we reparsed it using the timeline_label attribute provided in RunMetadata.

Thank you for your time on this review.",tensorflow
37080,jaketae,pr,2020-02-26T06:03:56Z,Update docstring for Conv3DTranspose,"As mentioned in #29841, `dillation_rate` is not a supported feature for `Conv3DTranspose` as of yet. Until this feature is supported, it is misleading to show `dilation_rate` as an available parameter in the docstring.",tensorflow
37082,jaketae,pr,2020-02-26T07:12:08Z,Merge (#1),"* Raise error if we hit an unknown exception while reading file from s3

* Add forbidden error check

* Added relevant changes for retrying operations in the s3 file system
Add header as in master for building the test, and change error type for delete dir
Register retrying file system for s3
Compile retrying s3 file system
Add forbidden errors, add mtime to the stat function, add if check around creating file in createDir
Move read error bug to separate PR for easier reviews

Lint fix with buildifier

* Refactored error handling to separate methods for reuse

* Make function static

* Fix link failure for tests

* update ragged_string_ops.py comment

* [Intel Mkl] Upgrade Sqlite3 to fix CVE-2019-19880 CVE-2019-19244 and CVE-2019-19645

* Use CreateStatusFromAwsError for the read error

* make gpu delegate build

it seems this line was forgotten when adding space_to_depth

* Depthwise convolution 3x3 per-channel int8 for dot-product ARM (17x).

Disable dot-product path temporarily.

PiperOrigin-RevId: 297134519
Change-Id: Ib00acb656471143fd31ca39e6762d46e9dec994a

* Add method that returns the version for hexagon_interface.
This will be used to compare version between hexagon_interface and hexagon_skel libraries.

PiperOrigin-RevId: 297138592
Change-Id: I23e1279fa5baa9cb262308f00d5f7f55f1256221

* Use strategy properties to check if a worker should checkpoint instead of use the TF_CONFIG. Using TF_CONFIG will cause errors if we use a cluster resolver to initialize the strategy.

PiperOrigin-RevId: 297139161
Change-Id: I7f13c7b7ea6588e020011569b26eb0bf49893bbf

* Enabling non trivial fast tuning for all vendors.

PiperOrigin-RevId: 297142417
Change-Id: I948e0ae458fbf59841d83d3c087c35c286daf269

* Update Eigen to:
https://gitlab.com/libeigen/eigen/-/commit/52a2fbbb008a47c5e3fb8ac1c65c2feecb0c511c

PiperOrigin-RevId: 297148140
Change-Id: I71fa876fcf492dc0051be3c3eecd12407770220c

* Qualify uses of std::string

PiperOrigin-RevId: 297148457
Change-Id: Id91f1da7d30e9a9880d8610c36ef8657f12817f0

* Improving snapshot read performance (under snappy compression) by reducing the
number of copies by 2.

We do the following in this CL.

1) Get rid of the snappy input and output buffers that were previously being
used to do the compression. This saved one copy.
2) Directly decompress the compressed bytes into the TensorBuffer for simple
types (not string, variant, resource). This saves another copy during Tensor
creation. For complex types, we still continue to use the TensorProto encoding
and pay a copy there.
3) As a result, we end up changing the on-disk format for Snapshot. For a group
of tensors that make up one element of an IteratorGetNext output, we first
write out a metadata proto that describes the types, shapes and sizes of
tensors. After that we lay out the Tensor data (TensorBuffers for simple types
and TensorProtos serialized for complex ones) and compress them via snappy.
4) Add a version to the SnapshotMetadata. If it isn't set its assumed to be 0
and the old code path runs. We now set it to 1 while writing so that all new
snapshots are written in this data format.

PiperOrigin-RevId: 297149479
Change-Id: I2c9a35c5a254189a5fad946b2995f25cdc452308

* update_bazel_macos is not used any more and has been replaced with install_bazelisk

PiperOrigin-RevId: 297149906
Change-Id: Ibcc1a114468a57cdf5314a7499e5a4ad32b8e941

* Replaced tools with exec_tools in genrules.

PiperOrigin-RevId: 297150637
Change-Id: I4b807e07a6b9072d53191ee7c2c3634cc0ca8e98

* Check for TENSORFLOW_LITE_PROTOS instead of __ANDROID__ when checking whether the full/lite protobuf implementation is being used.

PiperOrigin-RevId: 297150726
Change-Id: I5b7930811ef4410bfb85465736749558d6d630f1

* [tfdbg2] Fix a copy-paste error in unit test code

Fixes https://github.com/tensorflow/tensorflow/issues/36934

PiperOrigin-RevId: 297162227
Change-Id: Idcc10fdf84cfeabd6f136725923ef39ea27c4e04

* Fix an issue with out of order execution on a corner cases when there is multiple remote pending requests and remote functions with remote inputs.

Specifically:
1. We keep track theÂ last device used to execute a remote function with remote inputs
2. Before executing a remote function, if the remote device is different than the last tracked device, we insert a sync point.
PiperOrigin-RevId: 297163787
Change-Id: I944669b92dd1b50cdf213c73e1cdf990c9011e00

* Fix the deprecation_test for py3.8

PiperOrigin-RevId: 297168125
Change-Id: I441c6cd7d85fc8e8f8aedb53d1b2ec69594d9bcd

* Add tf.MatrixBandPartOp ODS definition and corresponding verifier.

PiperOrigin-RevId: 297174451
Change-Id: I77400245d526dc647dd17f532b935bf6f942db62

* Fix device placement logic in ConvertToEagerTensor.

The fix consists in always creating a host tensor: the inputs to
ConvertToEagerTensor are host Python objects, so it makes sense that
the created tensor should be a host tensor too. The user can control
GPU copies by using tf.identity.

PiperOrigin-RevId: 297174740
Change-Id: I01f2aa9be3eb29fd49c7d81823e044db292b2d7c

* *In dilated conv pass, set shape for expand_dims result correctly.
*Put more checks on input conv2d's dilation attributes, padding type, etc.

PiperOrigin-RevId: 297174875
Change-Id: Ibc59cc80a091e98d3eb1f7a855a4ee08402d7d96

* Experimental prototype of BertTokenizer  for tflite support library

PiperOrigin-RevId: 297176134
Change-Id: I4fea84735d1686f1ae1e98f4433bd0e29a642d35

* Remove ops.device(None).__enter__ from eager:core_test.

Those statements are not necessary for the test to consistently pass.

PiperOrigin-RevId: 297180465
Change-Id: I7dd6798b56fa7f80b8170ddc92ed811370bffd2f

* Bump open source llvm revision to fee41517fe0f7ff9f0e204dd9200ebf32ca03cb8

PiperOrigin-RevId: 297184332
Change-Id: Ide9c91a6171350d9eadb406f3ed028bcdbc33cc9

* Add `shape_signature` to visualize.py.

PiperOrigin-RevId: 297194460
Change-Id: I9b08d418bf75b6b5f4e2e2f7b05c84932d8bab46

* Expose quantization parameters in the TFLite Java API

PiperOrigin-RevId: 297207151
Change-Id: Iff5a1e720df750d0c90710e5209bcabc53054150

* Go: Update generated wrapper functions for TensorFlow ops.

PiperOrigin-RevId: 297207526
Change-Id: I92b95973a96d2c8641b5c13d4c8addf7687df1c9

* Update hexagon_nn_headers to v1.10.3.1.3
Changes Includes:
* Support soc_id:371
* New method exposed that returns the version of hexagon_nn used in libhexagon_interface.so

PiperOrigin-RevId: 297212018
Change-Id: I5c3396f04b305ea6197e8c693c5cd06c789e8d8d

* Qualify uses of std::string

PiperOrigin-RevId: 297212802
Change-Id: Ic65150e7ab418be034f48d45ce25ef5d19105836

* Internal refactoring for metrics.

PiperOrigin-RevId: 297214293
Change-Id: I9ef1510677669bc9c21de5839fde7ed42421fb6d

* Add test for connecting to v2-32 Cloud TPU.

PiperOrigin-RevId: 297219348
Change-Id: I6072853736fc98badbca598850f850b9980cc7b0

* Fix Concat behavior in Inception V3 quant

PiperOrigin-RevId: 297220987
Change-Id: Icd045f9dac3a7cd6aa42bf5070c206bc7b375046

* Minor change in update script

PiperOrigin-RevId: 297221831
Change-Id: Ibc864c21ad7bd277ffe00caecb9751629f89d97b

* Extend the optimize global tensors pass with resource analysis

Inter-procedural resource analysis for identifying resource as mutable or not.

PiperOrigin-RevId: 297225621
Change-Id: I12690ddab584660792c216347efa7a265fa6ffee

* Add check for hexagon version between interface and skel.
libhexagon_interface and libhexagon_nn_skel should be used with matching versions. Users shouldn't use mixed versions as it will not work correctly.

PiperOrigin-RevId: 297239641
Change-Id: I4e82626fc395eaba23fd51f84fbb402e24b2fd8a

* Improves documentation for keras.preprocessing.pad_sequence

PiperOrigin-RevId: 297247197
Change-Id: I4c85e8ba6d4ae43d4c249442ef9c47bb6a5805c6

* Go: Update generated wrapper functions for TensorFlow ops.

PiperOrigin-RevId: 297248087
Change-Id: Ibc3efb37f0f855fb86f8d0915c17c5bfb26847d3

* Upgrade nanopb to latest release.

Fixes security vulnerability reported in #37011

PiperOrigin-RevId: 297248686
Change-Id: Ib24b109c8a36b8673842b2f2ebb771406e5fa022

* Split the source files into ESP IDF components by their paths.

PiperOrigin-RevId: 297249461
Change-Id: I16c81c416ab803d4ba82cc4ae801b35a00be1acc

* Special-case variable reads in auto control deps.
1. Variable reads, with no write in between, are no longer performed in a sequence. This included reads happening inside functional ops e.g. nested functions or control flow.
2. Variable reads are no longer added to the function's control outputs.

PiperOrigin-RevId: 297253117
Change-Id: I833891f644cf6f8c1b733401ed2e3de3adb9ef0e

* Add GPU kernel stats to cloud tools.

PiperOrigin-RevId: 297267405
Change-Id: Ia3f0ac52ccb1db07f0b6230f9b181e8f2e316c04

* Set model.history after `on_epoch_end` in `History` class.

PiperOrigin-RevId: 297267664
Change-Id: I0cf97412913bdfe3599d74f162d172f02e7ceac9

* Switch ConcatOp, PackOp, SelectOp, and SelectV2Op to access inputs by index (not name).

These kernels are heavily used, and the name resolution on each invocation causes non-trivial overhead.

PiperOrigin-RevId: 297267975
Change-Id: Id69de0e2cff3622e992389c16e020a5da3141462

* Add an experimental_hints to batch all reduce

This contains all performance hints to the API. Currently there's only bytes_per_pack, which splits large batches into multiple packs allows overlapping communication and computation.

Currently we can only pack if all Tensors in the batch have known shapes.

PiperOrigin-RevId: 297269428
Change-Id: Iaf7d7d3adf7c6cad59aa6079fbcd36b31e92c4b5

* Set resource as mutable when unknown op is encountered in the optimize global tensors pass

PiperOrigin-RevId: 297274143
Change-Id: Ia33df38d23875a1a4021c52e70fca17b8aeb9c96

* [OpKernel] Implement `OpKernelContext::output_required(int)`.

This long-pending feature enables kernels to specialize for the case where not all of their outputs may need to be produced. As an example, this change uses `OpKernelContext::output_required()` in `SparseFillEmptyRowsOp`, which often produces unused outputs for the purpose of backpropagation; using `output_required()` we can save the related allocations in inference workloads.

I did consider implementing this as a Grappler rewrite pass, but the example of SparseFillEmptyRowsOp convinced me that (given the present state) a runtime test was better. The rewrite-based alternative would require me to have four similar op registrations for SparseFillEmptyRows (since we now have two optional outputs that may be present or absent, so there are 2^2 possible signatures), and to devise a mapping/registration scheme that the rewriter might use to substitute different implementations. By contrast, the runtime check is (i) pretty cheap compared to op dispatch, and (ii) easy to implement gradually and locally.

PiperOrigin-RevId: 297274989
Change-Id: I23b5207017921ba118bd4fc31dc54ff53fe4332d

* Include FreeRTOS.h before other dependencies demending this prior inclusion.

PiperOrigin-RevId: 297275456
Change-Id: I624bc648276a4cad3ca481cc2433fd29754ddfa6

* Inject keras.preprocessing modules into the doctest.

PiperOrigin-RevId: 297276571
Change-Id: I76c18e90f02ea6d0ef7ad69dae16e231734efc8d

* Fix the BatchNorm v2 performance issue wrt ""training"".

Change the logic to skip the math_ops.logical_and() when layer.trainable is True, since the ""training"" value is only going to be changed if layer.trainable is False.

This apparently fix the regression issue (1.33x). I think the root cause might be the logical_and() with a value that is not the GPU device (layer.trainable).

PiperOrigin-RevId: 297277503
Change-Id: I9e3b6650bfa58c3d2ea8e3f023a3bd1a5160cc8d

* Add warmup in keras tb callback to improve profiling accuracy.

PiperOrigin-RevId: 297277543
Change-Id: Ic78a286400fffa73d583f3bb27bfce0dc1e275dc

* Go: Update generated wrapper functions for TensorFlow ops.

PiperOrigin-RevId: 297278005
Change-Id: I7ae629b04ffb9c44c732dac221b0821f86b282e8

* Consolidate via CallOpInterface the various TF call ops handling in optimize global tensors pass

PiperOrigin-RevId: 297283013
Change-Id: Ibd5a306b662d61ef2644a6cf59f78ac0119aa02e

* Add go_backwards support for keras fused lstm

PiperOrigin-RevId: 297287353
Change-Id: Idaebe3d0c84fc8be03651233a4af7c9cd46a23ca

* Go: Update generated wrapper functions for TensorFlow ops.

PiperOrigin-RevId: 297288470
Change-Id: I86fd6955e320399697a6f1082c8a98efdd09d639

Co-authored-by: Rahul Huilgol <rahulhuilgol@gmail.com>
Co-authored-by: HUAN-PING SU <pingsutw@gmail.com>
Co-authored-by: Clayne Robison <clayne.b.robison@intel.com>
Co-authored-by: ""freedom"" Koan-Sin Tan <koansin.tan@gmail.com>
Co-authored-by: TensorFlower Gardener <gardener@tensorflow.org>
Co-authored-by: Alex Stark <539273+jalexstark@users.noreply.github.com>
Co-authored-by: Karim Nosseir <44206880+karimnosseir@users.noreply.github.com>
Co-authored-by: anj-s <32556631+anj-s@users.noreply.github.com>
Co-authored-by: Rohan Jain <rohan100jain@gmail.com>
Co-authored-by: Brian Atkinson <nairb774@gmail.com>
Co-authored-by: Mihai Maruseac <mihai.maruseac@gmail.com>
Co-authored-by: Shanqing Cai <cais@google.com>
Co-authored-by: Bramandia Ramadhana <bramandia@gmail.com>
Co-authored-by: lucyrfox <lucyfox@google.com>
Co-authored-by: Haoliang Zhang <haoliang@google.com>
Co-authored-by: thuang513 <556234+thuang513@users.noreply.github.com>
Co-authored-by: River Riddle <riddleriver@gmail.com>
Co-authored-by: Nupur Garg <nupurgarg@gmail.com>
Co-authored-by: lu-wang-g <47436172+lu-wang-g@users.noreply.github.com>
Co-authored-by: Revan Sopher <rsopher@gmail.com>
Co-authored-by: Sachin Joglekar <srjoglekar@google.com>
Co-authored-by: sun51 <yanh.sun@gmail.com>
Co-authored-by: Ashwin Murthy <ashwinm@google.com>
Co-authored-by: FrÃ©dÃ©ric Rechtenstein <fred.rec@gmail.com>
Co-authored-by: Saurabh Saxena <saxenasaurabh@users.noreply.github.com>
Co-authored-by: Situ Yi <60493024+yisitu@users.noreply.github.com>
Co-authored-by: Yash Katariya <yashkatariya@google.com>
Co-authored-by: Derek Murray <derek.murray@gmail.com>
Co-authored-by: Ran Chen <crccw@google.com>
Co-authored-by: Qianli Scott Zhu <scottzhu@google.com>
Co-authored-by: renjie-liu <36247193+renjie-liu@users.noreply.github.com>",tensorflow
37087,janosh,pr,2020-02-26T10:39:56Z,docs: add tip to prefer tf.shape(x) over x.shape in custom layers/models,See #36991 for details.,tensorflow
37090,jaketae,pr,2020-02-26T11:15:50Z,Update docstring for consine_similarity,"Addresses some of the issues discussed in tensorflow#33820 by editing the docstring for `consine_similarity`. The edited docstring correctly indicates the `y_true` and `y_pred` need not be normalized, and that passing a zero vector as one of the arguments will return 0 regardless of the proximity between true and predicted labels.",tensorflow
37120,jaketae,pr,2020-02-27T07:13:52Z,"Edited docstring, y_pred for SparseCategoricalAcc","1. The `y_pred` example was previously given as `[0.1, 0.9, 0.8]`, which is odd given that the probabilities should sum up to one after passing through softmax. Edited to `[0.1, 0.6, 0.3]` to maintain integrity of example while posing a probable result.
2. The docstring for master was not up-to-date with that of Tag 2.1.0. Made changes so that they are even.

Relates to #36844.",tensorflow
37121,jaketae,pr,2020-02-27T08:21:23Z,Added see also in array_ops,Added see also for a few similar functions in array_ops.py. Partially addresses #36786 ,tensorflow
37146,jaketae,pr,2020-02-27T22:38:02Z,Added ResNext models,"Added ResNext models. It seems like the implementation of ResNext models was dropped, despite them being available options in `WEIGHT_HASHES`. The implementation can simply be achieved by using `stack3`, which is also a function that was declared and not used.

Addresses tensorflow/models#1248, tensorflow/models#6764. ",tensorflow
37163,byronyi,pr,2020-02-28T12:24:55Z,Fix test that should only be running in v1,"After https://github.com/tensorflow/tensorflow/commit/84f2ec1d60b5bb14a59ccef8f8fa7eb5a1096e8f#diff-42d3946a49d3adc16b4a37e844f986e0R469-R473 the test `testBatchNormGradInferenceShape1 ` is incorrectly enabled in v2, causing the following error:

```
[  FAILED  ] BatchNormalizationTest.testBatchNormGradInferenceShape1
======================================================================
ERROR: testBatchNormGradInferenceShape1 (__main__.BatchNormalizationTest)
testBatchNormGradInferenceShape1 (__main__.BatchNormalizationTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_fused_batchnorm_test.py"", line 471, in testBatchNormGradInferenceShape1
    self._runtests(x_shape, is_training=False, gradient_test=True)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_fused_batchnorm_test.py"", line 407, in _runtests
    exponential_avg_factor=exponential_avg_factor)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_fused_batchnorm_test.py"", line 263, in _test_gradient
    x_shape)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_fused_batchnorm_test.py"", line 203, in _compute_gradient_error_float16
    x, x_shape, y, y_shape, delta=1e-3, x_init_value=x_init_val)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gradient_checker.py"", line 332, in compute_gradient
    dx, dy = _compute_dx_and_dy(x, y, y_shape)
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gradient_checker.py"", line 202, in _compute_dx_and_dy
    with x.graph.as_default():
  File ""/run/bazel-buildfarm/default/operations/9d248bd8-a122-4dee-8f35-b5c55d793f8f/bazel-out/k8-opt/bin/tensorflow/python/nn_fused_batchnorm_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1118, in graph
    ""Tensor.graph is meaningless when eager execution is enabled."")
AttributeError: Tensor.graph is meaningless when eager execution is enabled.
```",tensorflow
37197,OverLordGoldDragon,pr,2020-02-29T19:38:35Z,Correct phrasing on CuDNN conditions,"As-is, the condition means ""inputs are not masked"" AND ""inputs are not strictly right-padded"", which isn't the actual condition, and is thereby misleading.",tensorflow
37213,jaketae,pr,2020-03-01T21:52:22Z,Fix plot_model for .pdf files,"If `plot_model` is asked for a .pdf file in Jupyter via the command `plot_model(model, to_file='model.pdf')`, a value error is raised:

```
ValueError: Cannot embed the 'pdf' image format
```

This is presumably because Jupyter Notebook does not support .pdf format. This PR fixes this issue by adding an additional check for the `extension` argument.",tensorflow
37214,jaketae,pr,2020-03-01T22:40:01Z,Fix np.squeeze call with dimensionality check,"Calling `np.squeeze` is required only if `preds.shape[-1]` is equal to one.. Otherwise, the prediction can be returned as is. Added a conditional to check for this condition.",tensorflow
37218,byronyi,pr,2020-03-02T02:32:12Z,Fix flaky host_tracer_test with data race,"In test `CollectsTraceMeEventsAsXSpace` added in 883b5becaced22f7dd9e3c23d9d259f55e087cb5, `thread_id` is not properly synchronized between threads, causing the following test flakiness:

```
tensorflow/core/profiler/internal/cpu/host_tracer_test.cc:150: Failure
Expected equality of these values:
  line.id()
    Which is: 3350296320
  thread_id
    Which is: -944670976
```",tensorflow
37244,jaketae,pr,2020-03-02T23:24:48Z,Added reference papers in docstring,"Added reference papers in docstring for models in `keras.appliactions`. Some models had reference papers, while others did not. Edited the docstrings such that the descriptions would appear consistent throughout all models.",tensorflow
37249,zhuzilin,pr,2020-03-03T07:30:37Z,Fix typo: delete redundant quote in tf_executor dialect,"I deleted the unnecessary quote for better highlighting. Otherwise, most text editors will regard the rest of the code as string.",tensorflow
37255,jaketae,pr,2020-03-03T11:11:32Z,Fixed inline code markdown for  in docstring,Very minor fix from 'antialias' to `antialias` in docstring for proper inline code markdown notation. ,tensorflow
37377,duncanriach,pr,2020-03-06T03:16:39Z,[XLA] follow-up on GPU-deterministic reductions,"GPU-deterministic `tf.nn.bias_add` (enabled with `TF_DETERMINISTIC_OPS`) and its testing was introduced via [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465). Operation on XLA:GPU was found to be non-deterministic at that time.

In response to [a conversation](https://github.com/tensorflow/tensorflow/pull/34887#discussion_r356259683) on [PR 34887](https://github.com/tensorflow/tensorflow/pull/34887) (Add info about `TF_DETERMINISTIC_OPS` to version 2.1 release notes), @cheshire committed a [change](https://github.com/tensorflow/tensorflow/commit/e31955d9fb34ae7273354dc2347ba99eea8c5280) that implemented deterministic reduction functionality when using XLA:GPU. He then committed [another change](https://github.com/tensorflow/tensorflow/commit/8b7a3db0b6e09415b5640be4986fb4d7c6e5209a) that caused this functionality to be enabled by `TF_DETERMINISTIC_OPS`. Both of these changes are in the `r2.2` branch.

This current pull-request is a [follow-up](https://github.com/tensorflow/tensorflow/pull/34887#discussion_r382293594) to that conversation. It does two things:

1. Enable the deterministic testing for `tf.nn.bias_add` to run on the XLA:GPU.
2. Modify the way that `tensorflow/compiler/xla/service/gpu/gpu_compiler.cc` listens to `TF_DETERMINISTIC_OPS` so that it's the same as all other uses (it caches the value).

It would be ideal to cherry-pick these changes into the `r2.2` branch so that they can accompany the rest of this feature implementation.",tensorflow
37383,freedomtan,pr,2020-03-06T09:09:53Z,[tflite] NNAPI Pow op allows scalar input,resolve the `NN API returned error ANEURALNETWORKS_BAD_DATA`  problem in https://github.com/tensorflow/tensorflow/issues/36645,tensorflow
37400,lithuak,pr,2020-03-06T20:57:41Z,Add support for any Tensor type describable by TensorSpec to tf.data.Dataset.from_generator,"This PR addresses the #35342 issue which proposes to add the support for different types of tensors to tf.data.Dataset.from_generator.

Initially `from_generator` had two parameters: output_types and output_shapes (one mandatory and the other one optional) that allowed user to specify the dtypes and optionally shapes of the output objects, but not the types of the objects themselves thus making it impossible for a generator function to output more complicated types of objects like tf.RaggedTensor or tf.SparseTensor.

I followed here the advice from @jsimsa and added the output_spec parameter that, being specified, overrides the output_types and output_spec parameters and allows uses to specify the nested 
structure of tf.TypeSpec objects thus opening the way for any output objects describable by TypeSpec.

Here is the exact new semantics of the function that I've copied from the new docstring:

```
   There are three ways to specify the output format:

    * Using only `output_types` argument. In this case the output of the
    function will be assumed to consist of `tf.Tensor` objects with the unknown
    shapes and with the types defined by `output_types`.

    * Using both `output_types` and `output_shapes` arguments. In this case the
    output will be assumed to consist of `tf.Tensor` objects with the shapes
    and types defined by these two arguments together.

    * Using `output_spec` argument. In this case the output will be assumed to
    consist of objects with the classes, shapes and types defined by
    `tf.TypeSpec` objects from `output_spec` argument.

    One of the `output_types` and `output_spec` arguments must be specified.
    If used together, `output_spec` will override both `output_types` and
    `output_shapes`.

```
Note:
I've also had to add ""dtype"" property to RaggedTensorSpec to make internal implementation possible. This change looked natural, since RaggedTensorSpec really had the ""dtype"", like all the other TensorSpecs (SparseTensorSpec, TensorArraySpec, etc.). It was also that the Structure module internally relied on the assumption that incoming objects would have the ""dtype"" property (see get_flat_tensor_types function).
I would've commited it as a standalone PR, but I'm new to Tensorflow contribution process and I wasn't sure what is the best way to do things, since this PR whould rely on the previous one.
",tensorflow
37438,jaketae,pr,2020-03-09T09:51:06Z,Minor refactoring and robust mode check for imagenet_utils.py,Minor refactoring of code for better readability of conditional statements (using `elif` instead of multiple `if`s if the statements check for conditions on the same object). Also added robust `mode` argument check to prevent silent failure.,tensorflow
37502,freedomtan,pr,2020-03-11T07:12:05Z,[tflite] add int8 input/output to label_image,"More and more models, such as MobilenetV3's EdgeTPU ones, are using post-training full integer quantization. With this patch, I can get reasonable results.
```
./label_image_int8 -m mobilenet_edgetpu_224_1.0_int8.tflite
Loaded model mobilenet_edgetpu_224_1.0_int8.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
invoked
average time: 15.363 ms
0.867188: 653 military uniform
0.0390625: 835 suit
0.015625: 458 bow tie
0.0078125: 907 Windsor tie
0.00390625: 716 pickelhaube
```

This PR is to reopen #34537, which was merged and reverted because of Google's internal sanity test.",tensorflow
37503,freedomtan,pr,2020-03-11T08:42:44Z,[tflite] allow passing quantized ReLU to NNAPI,Standalone quantized ReLU is not delegated to NNAPI. Version check modified to allow delegating.,tensorflow
37560,freedomtan,pr,2020-03-13T06:19:03Z,[tensorrt] add missing dependency to make it build,resolve https://github.com/tensorflow/tensorflow/issues/37523,tensorflow
37566,guillaumekln,pr,2020-03-13T09:26:16Z,Do not densify sparse gradients in LossScaleOptimizer,The method `get_unscaled_gradients` is currently converting sparse gradients to dense gradients. We can avoid this implicit conversion to save on memory.,tensorflow
37583,jaketae,pr,2020-03-13T22:34:25Z,Added drop_first feature to to_categorical,"Pandas offers a `drop_first` argument as part of its [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function, which is comparable to TensorFlow's `to_categorical`. `drop_first` allows users to drop the first column of their one-hot vectorized binary matrix, since the first dimension can be considered a form of extraneous information. Added this feature into the current version of `to_categorical`, alongside an example in the docstring.",tensorflow
37585,jaketae,pr,2020-03-13T23:31:52Z,Improve preprocessing text docs,Added docstrings from the original keras_preprocessing repo to provide better readability of docs. Also included examples to help demonstrate how each function can be used.,tensorflow
37788,lithuak,pr,2020-03-21T20:52:45Z,Change the author of PR #37400 to original contributor,,tensorflow
37789,musikisomorphie,pr,2020-03-21T20:58:29Z,add convex hull cpu implmentation,"@alextp,  I wrote the cpu version of convex hull to address the issue #31067, tfxla version will be implemented later.  Can someone review my PR?",tensorflow
37813,zhuzilin,pr,2020-03-23T02:25:49Z,[Autograph] Fix loop else support and add loop integration test,"This is a PR from JIZHI, the AI platform in Tencent.

Right now, the autograph does not support loop else syntax like
```python
while x > 2:
  x /= 2
else:
  x += 1
```
In fact, it would just neglect the else part, because the tf functional while op does not support ""else"" (which, in my opinion, is good, since the loop else is a rare syntax sugar that probably only appears in python). Therefore, this pr changed the loop from
```python
while test:
  body
else:
  orelse
```
to
```python
while test:
  body
orelse
```
And for those loops that have `break` statements involve
```python
var_name = False
while ag__.and_(lambda: test, lambda: ag__.not_(var_name)):
  body
if ag__.not_(var_name):
  orelse
```
For the test, it turns out that the UT for break statement only could not detect this problem. In fact, the `test_loop_orelse` in `break_statement_test.py` does not work in current autograph but have passed the UT. Therefore, we added a integration test for the loop syntax, which will run the break_statement, continue_statement and control_flow converters one by one.

Thank you for your time on this review.",tensorflow
37843,powderluv,pr,2020-03-23T23:08:18Z,Remove redundant cusolverDnIRSInfosGetNiters,"It seems to be defined twice.

TEST:build tensorflow with cuda 10.2",tensorflow
37851,zhuzilin,pr,2020-03-24T03:07:20Z,[Autograph] Fix Autograph '\' error,"This is a PR from JIZHI, the AI platform in Tencent.

This PR solves the backslash continuation error mentioned in [#35765](https://github.com/tensorflow/tensorflow/issues/35765)

To illustrate this fix, let's take this function as an example:
```python
# some indentation...
  def f():
    a = \
    1
    return a

```
The problem is that the `new_code` after process will be
```python
def f():
  a =1
  return a
```
The number of lines does not match. This PR fixes it by align the ""a = \"" and ""1"" with ""a =1"".",tensorflow
37890,freedomtan,pr,2020-03-25T05:26:22Z,[tflite] add xnnpack delegate to label_image,"rebase and resubmit #36749 to see if it works.

The [XNNPACK Delegate](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack) uses [XNNPACK](https://github.com/google/XNNPACK), a fairly optimized floating point library, to run some inference operators (see the delegate's [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) for currently supported ops). With XNNPACK, I was able to get performance numbers similar to what @Maratyszcza described at [XNNPACK's readme](https://github.com/google/XNNPACK/README.md). I also got good numbers on Pixel 4 and Oppo Reno 3. Numbers on x86 machines are also good.

- **Single-threaded**, `label_image -m MODEL_NAME -x 1 -c 50 -t 1`

Model |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms |
-- | --: | --: | --:
MobileNet v1 1.0X |  88.02 | 29.95 | 36.23 |
MobileNet v2 1.0X |  55.13 | 19.13 | 21.69 |
MobileNet v3 Large |  44.84 | 15.69 | 17.65 |
MobileNet v3 Small |   14.23 | 5.58 | 5.66|

- **multi-threaded**, the number of threads is set to be the number of big cores. `label_image -m MODEL_NAME -x 1 -c 50 -t NUMBER_OF_BIG_CORES`

Model |  Pixel 3a, ms | Pixel 4, ms| Oppo Reno 3, ms|
-- | --: | --: | --:
MobileNet v1 1.0X |  46.00 | 10.60 | 13.23 |
MobileNet v2 1.0X |  28.57 | 6.89 | 7.56 |
MobileNet v3 Large |  24.62 | 6.52 | 6.88 |
MobileNet v3 Small |   8.24 | 2.47 |  2.37 |
",tensorflow
37893,freedomtan,pr,2020-03-25T08:28:40Z,[tflite] make label_image build,"label_image depends on old `//tensorflow/lite/delegates/gpu:gl_delegate`, which no long built.
update the dependency to use `//tensorflow/lite/delegates/gpu:delegate`",tensorflow
37903,mpjlu,pr,2020-03-25T16:24:16Z,[Grappler]Fuse Conv2D + BiasAdd + Relu on GPU.,"Author: Dr. Peng Meng from Tencent Cloud TI-ML team.
Grappler:remapper Conv2D + BiasAdd + Relu OP fuse doesn't work on GPU.
To fuse these three OP,  FindContractionWithBiasAndActivation needs to know the InputProperties:
![image](https://user-images.githubusercontent.com/13826327/77559148-c38c8d80-6ef6-11ea-8d7e-f2f5ee70be70.png)
TF master code  doesn't call graph_properties.InferStatically() before calling FindContractionWithBiasAndActivation,  FindContractionWithBiasAndActivation always return false, so Conv2D + BiasAdd + Relu cannot be fused.

This PR fixes this bug. After this fix, the WDSR model inference latency improves 20% on Nvidia T4 GPU.",tensorflow
37929,freedomtan,pr,2020-03-26T07:10:04Z,[tflite] fix hexagon delegate profiling unit,"it seems the unit of the number returned by GetCycles() is
microsecond instead of millisecond. Dividing it by 1,000 so
that we can get more reasonable numbers when doing
something like

```
./benchmark_model --graph=... --use_hexagon=1 --hexagon_profiling=1
```",tensorflow
37965,zhuzilin,pr,2020-03-27T06:41:14Z,Fix Bug in MixedPrecisionLossScaleOptimizer,"This is a PR from JIZHI, the AI platform in Tencent.

This pr mainly fixed the error that when disable eager execution and use the `MixedPrecisionLossScaleOptimizer` as the opt for keras model will trigger
```bash
FailedPreconditionError: Error while reading resource variable current_loss_scale from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/current_loss_scale/N10tensorflow3VarE does not exist.
	 [[{{node training/ReadVariableOp_1}}]]
```
The error is able to be replicated in tf-nightly, see gist [here](https://colab.research.google.com/gist/zhuzilin/f116e9689e2611333719fa753af31599/amp-opt-bug.ipynb)

The reason for this error is that the `MixedPrecisionLossScaleOptimizer` failed to pass the variables defined in its base optimizer and loss scale to keras backend and initialize them.

This PR fixed it by overwriting the `variables()` function, which is the function called to initialize variables.

Thank you for your time on reviewing this PR.",tensorflow
38089,duncanriach,pr,2020-03-31T18:54:12Z,Add reminder to test deterministic cuDNN CTC loss,"@sanjoy added deterministic cuDNN CTC loss, enabled via `TF_DETERMINISTIC_OPS`, with [this commit](https://github.com/tensorflow/tensorflow/commit/9e096debc4a0909deb69970f38bee7b77e5e5f7d). This current pull request places a reminder in `cudnn_deterministic_base.py` for me to add a test for it.",tensorflow
38256,joker-eph,pr,2020-04-06T01:05:15Z,NFC change to test presubmits,,tensorflow
38291,joker-eph,pr,2020-04-07T01:19:32Z,"Revert ""[MLIR][XLA] Buffer Assignment""","Reverts tensorflow/tensorflow#37212

This lacks test coverage, it was merged incorrectly without addressing comments.

",tensorflow
38292,zhuzilin,pr,2020-04-07T02:27:31Z,r1.15 cherry-pick: solve memory leak due to defer host callbacks,"This is a PR from JIZHI, the AI platform in Tencent.

This is a cherry-pick pr that merge commit 80851c0ad to r1.15 to solve the memory leak because of the accumulated defered callbacks.

The two merge conlicts are in
- `BUILD`, where the `XLA_OPS_DEPS` does not exist in r1.15
- `cudnn_batchnorm_thunk.cc`, where the `stream` is an object instead of a pointer.

@hawkinsp Thank you for your commit to solve this memory leak problem! Could you please have a look at this cherry-pick?

Thank you for your time on reviewing this PR.",tensorflow
38299,marload,pr,2020-04-07T05:58:15Z,refactoring: 'if' syntax deduplication,"I changed the existing code with the phrase if else to one line.
Thank you for creating a great project.",tensorflow
38301,marload,pr,2020-04-07T06:19:14Z,refactoring: early return 'if else' -> 'if',"For early return, I think 'only if' syntax is better than 'if elif' syntax.",tensorflow
38398,gaurav1086,pr,2020-04-09T16:49:46Z,[compiler] Correct dims_rhs constraint,Correct dims_rhs constraint,tensorflow
38479,freedomtan,pr,2020-04-13T01:02:05Z,[lite] fix nnapi:nnapi_delegate_verbose_validation,"without `@com_google_absl//absl/strings`, when building `//tensorflow/lite/delegates/nnapi:nnapi_delegate_verbose_validation`, I met

```
ERROR: /hack/freedom/tensorflow/tf-clean/tensorflow/lite/delegates/nnapi/BUILD:45:1: undeclared inclusion(s) in rule '//tensorflow/lite/delegates/nnapi:nnapi_delegate_verbose_validation':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/delegates/nnapi/quant_lstm_sup.cc':
  'external/com_google_absl/absl/strings/string_view.h'
  'external/com_google_absl/absl/base/internal/throw_delegate.h'
Target //tensorflow/lite/delegates/nnapi:nnapi_delegate_verbose_validation failed to build
```",tensorflow
38497,freedomtan,pr,2020-04-13T12:42:51Z,[tflite] fix missing dep for preprocess_coco_minival,"add missing dep for
```
//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival
```

`preprocess_coco_minival` uses `evaluation_stages`. `evaluation_stages` uses
`preprocessing_steps` which is not generated.

Add the build rule and corresponding dependency so that
```
bazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \
  --images_folder=/path/to/val2014 \
  --instances_file=/path/to/instances_val2014.json \
  --whitelist_file=/path/to/minival_whitelist.txt \
  --output_folder=/path/to/output/folder
```
will work again.",tensorflow
38509,duncanriach,pr,2020-04-13T20:56:17Z,List deterministic op functionality bug fixes in version 2.2 release notes,"The changes in this current PR refer to the following commits and PR:

* Commit [e31955](https://github.com/tensorflow/tensorflow/commit/e31955d9fb34ae7273354dc2347ba99eea8c5280): [XLA/GPU] Convert reduction into tree reduction using padding + bitcast-reshape
* Commit [8b7a3](https://github.com/tensorflow/tensorflow/commit/8b7a3db0b6e09415b5640be4986fb4d7c6e5209a
): [XLA] Respect TF_DETERMINISTIC_OPS environment variable for reductions
* PR [34951](https://github.com/tensorflow/tensorflow/pull/34951): Add multi-algorithm deterministic cuDNN convolutions

Thanks to @goldiegadde for getting `RELEASE.md` committed in the `r2.2` branch so that I could submit this current PR.",tensorflow
38527,zhuzilin,pr,2020-04-14T09:43:25Z,pass name to GraphExecutionFunction,,tensorflow
38562,zhuzilin,pr,2020-04-15T07:10:55Z,Fix only pass RunOptions to keras will trigger core,"This is a PR from JIZHI, the AI platform in Tencent.

Right now, keras will trigger segmentation fault when only pass `RunOptions` but no `RunMetadata`. This bug can be replicated in tf-nightly in this [gist](https://colab.research.google.com/gist/zhuzilin/a6f71f2240a60769e1a6e054e8733145/untitled3.ipynb)

If you cannot open the gist, the code to replicate the bug is
```python
import tensorflow as tf
mnist = tf.keras.datasets.mnist

tf.compat.v1.disable_eager_execution()

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'],
              options=run_options)

model.fit(x_train,
          y_train,
          epochs=2)
```

The reason for this problem is that keras use the `TF_SessionRunCallable` api instead of `TF_Run`. While the former was using an `unique_ptr` to represent the `run_metadata` and will only new one when `RunMetadata` was passed. However, the `RunOption` will need an `RunMetadata` for tracing. This PR modified the implementation of `TF_SessionRunCallable` to that of `TF_Run` and fix this bug.

Thank you for your time on reviewing this PR.",tensorflow
38587,marload,pr,2020-04-16T04:16:45Z,refactoring: early return 'if else' syntax -> 'if' syntax,"For early return, I think ""if syntax"" is better than ""if else syntax"".
Thx!",tensorflow
38687,marload,pr,2020-04-19T12:07:39Z,Some refacotoring,Improved readability of code and remove code duplication. Thank you.,tensorflow
38697,marload,pr,2020-04-20T04:00:20Z,Some Refactoring,I performed some refactoring. Thank you!,tensorflow
38802,nluehr,pr,2020-04-22T17:27:27Z,Remove calls to deprecated cusparse APIs,"This PR replaces calls to cusparse calls that are deprecated in CUDA 10.2.

CC @pankajkgupta and @sanjoy ",tensorflow
38833,freedomtan,pr,2020-04-23T12:18:54Z,[tflite] make evaluation tasks build,"Fix dependency problem. Not all linkers can check functions in previous libraries. The `//tensorflow/lite/tools/evaluation/tasks:task_executor_main` needs `tflite::evaluation::CreateTaskExecutor(int*, char**)` which is in `:run_eval_lib`. We need `:run_eval_lib` after `task_exceutor_main`. Otherwise we'll see error like

```
ERROR: /Volumes/Seagate5T/tf-clean/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/BUILD:41:1: Linking of rule '//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval' failed (Exit 1)
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/tools/evaluation/tasks/libtask_executor_main.a(task_executor_main.o): In function `main':
/private/var/tmp/_bazel_freedom/adcaa36b2ac85ab2f5a2434777d27f58/execroot/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/task_executor_main.cc:21: undefined reference to `tflite::evaluation::CreateTaskExecutor(int*, char**)'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.753s, Critical Path: 1.15s
INFO: 3 processes: 3 local.
FAILED: Build did NOT complete successfully
```
when building with something like
```
build --config android_arm64 \
  tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval
```",tensorflow
38853,zhuzilin,pr,2020-04-24T01:46:53Z,Fix error in deleting servers because module may be garbage collected ahead,"When deleting grpc server at the end of the program, the `errors` module may already be garbage collected and trigger:
```python
Exception ignored in: <bound method Server.__del__ of <tensorflow.python.training.server_lib.Server object at 0x7f61a00e4668>>
Traceback (most recent call last):
  File ""/usr/local/lib64/python3.6/site-packages/tensorflow/python/training/server_lib.py"", line 161, in __del__
AttributeError: 'NoneType' object has no attribute 'UnimplementedError'
```
This pr will just check if `errors` is none ahead.

Thank you for your time on this review.",tensorflow
38854,zhuzilin,pr,2020-04-24T02:55:11Z,Fix MirroredStrategy cannot output RunMetadata,"This is a PR from JIZHI, the AI platform in Tencent.

Currently, we cannot get `run_metadata` (or timeline) in graph mode when using `MirroredStrategy` and keras together. The reason is that the current implementation failed to pass `run_options` and `run_metadata` to the distributed model. This pr is trying to fix it.

The main changed we did are:
- Retain the kwargs in `self.session_kwargs` in `GraphExecutionFunction`, because it is used as the input of  the distributed `K.function` in `_make_graph_execution_function`.
- Update the unwrapping of grouped kwargs to make them support `run_metadata` and `run_options`.

Thank you for your time on reviewing this pr.",tensorflow
38864,zhuzilin,pr,2020-04-24T09:35:59Z,Fix timeline fail to parse long kernel names when op_time is set,"This is a follow-up pr on #37074 .

I just found out that `re.match` is not good for multi-line text, while many XLA kernels do have a really long name or timeline_label. Therefore, this pr changed the regular expression in to manually parsing. 
Also, the format of `timeline_label` seems to be changed in the lastest version of tf, and I change a little of the parsing rule according to the new format.

Thank you for your time on reviewing this pr and I'm sorry that the origin pr on this did not resolve these problems.",tensorflow
38911,zhuzilin,pr,2020-04-26T10:33:08Z,[XLA] Remove cross device nodes from clusters,"This is a PR from JIZHI, the AI platform in Tencent.

This pr removes the the cross device nodes (nodes that have input or output from
other device) from the compilation candidates in `MarkForCompilation`. The reason for this change is that if the cross device nodes are introduced into clusters, the following 2 situation may happen and largely harm the performance.
```
  device0:  op1 ---    op2        cluster(op1, op2) ---
                  |          ===>                     |
  device1:        ---> op3                            ---> op3

  or

  device0:  op1   ---> op2              ---> cluster(op1, op2)
                  |          ===>       |
  device1:  op3 ---               op3 ---
```
XLA will delay the execution of an op when one of its predecessors is merged in the middle of a cluster. This delay will be ok when the op and the cluster are on the same device, because their kernels are executed sequentially. But when using multiple devices, this delay will make one device wait. For example, this is the timeline of a transformer model:
- Before removing cross device nodes:
![image](https://user-images.githubusercontent.com/10428324/80304808-105fce80-87eb-11ea-845d-473d9397c421.png)
- After removing cross device nodes:
![image](https://user-images.githubusercontent.com/10428324/80304824-29687f80-87eb-11ea-8712-bca355333d52.png)

The alternative way to solve this problem is to add more condition in `TryToContract`. However,  that will result in larger modification because we need to save the device information of all nodes in a cluster. If you prefer this way, we are also glad to help.

Thank you for your time on reviewing this pr.",tensorflow
39027,byronyi,pr,2020-04-29T13:20:14Z,Fix build failures for python 3.8,"According to https://bugzilla.redhat.com/show_bug.cgi?id=1718837:

> In Python 3.8, the reserved ""tp_print"" slot was changed from a function pointer to a number, `Py_ssize_t tp_vectorcall_offset`.

This fix our nightly Python 3.8 builds. 

cc @mihaimaruseac 

",tensorflow
39104,gaurav1086,pr,2020-05-02T18:51:33Z,[Lite] data: Fix memory leak,Free dynamically data before return.,tensorflow
39187,byronyi,pr,2020-05-05T14:24:38Z,Fix a flaky test when pretty print function keyword arguments in Python 3.5,"I observed the following error with Python 3.5 on `//tensorflow/python/eager:function_test`:

```
FAIL: testPrettyPrintedSignature (__main__.FunctionTest)
testPrettyPrintedSignature (__main__.FunctionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/bazel-buildfarm/default/operations/83398799-fc19-4863-a3e0-99fb3531f4ad/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py"", line 3625, in testPrettyPrintedSignature
    c3_summary + '\n' + c3_details)
AssertionError: Regex didn't match: ""func\\(x, kangaroo=None, octopus=7\\)\n  Args:\\n    x: {'a': <1>, 'b': \\[<2>, <3>\\]}\\n      <1>: int32 Tensor, shape=\\(\\)\\n      <2>: RaggedTensorSpec\\(.*\\)\\n      <3>: RaggedTensorSpec\\(.*\\)\\n  Returns:\\n    {'a': <1>, 'b': \\[<2>, <3>\\]}\\n      <1>: int32 Tensor, shape=\\(\\)\\n      <2>: RaggedTensorSpec\\(.*\\)\\n      <3>: RaggedTensorSpec\\(.*\\)"" not found in ""func(x, kangaroo=None, octopus=7)\n  Args:\n    x: {'b': [<2>, <3>], 'a': <1>}\n      <1>: int32 Tensor, shape=()\n      <2>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\n      <3>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\n  Returns:\n    {'b': [<2>, <3>], 'a': <1>}\n      <1>: int32 Tensor, shape=()\n      <2>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)\n      <3>: RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)""
```

It appears that when enumerating the `kwargs` dictionary, the order is only deterministic for Python >=3.6 (so the pre-submit didn't catch it). See https://mail.python.org/pipermail/python-dev/2017-December/151263.html

cc @edloper ",tensorflow
39225,freedomtan,pr,2020-05-06T12:14:30Z,make `bazel build` work on aarch64/arm linux,"make
```
bazel build //tensorflow/tools/pip_package:build_pip_package
````
work again on aarch64 (e.g., EdgeTPU Dev board and Jetson boards)
and arm (e.g, Raspberry Pi 3/4 boards) Linux platforms

without this patch, I saw

```
...
ERROR: While resolving toolchains for target //tensorflow:libtensorflow_framework.so.2.1.0: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: No matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type. Maybe --incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions.
```",tensorflow
39243,duncanriach,pr,2020-05-07T04:10:18Z,GPU-deterministic tf.image.resize (bilinear),"This pull request extends the deterministic functionality that is enabled by `TF_DETERMINISTIC_OPS`.

Prior to the changes delivered by this pull-request, the CUDA back-prop kernels for `tf.image.resize` with `method=ResizeMethod.BILINEAR` introduced uncontrollable noise.

After application of this pull request, setting the environment variable `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" selects a new, deterministic CUDA back-prop kernel for this op.

The exact performance of the deterministic kernels relative to the pre-existing, non-deterministic kernels has not yet been tested. However, the performance is expected to be similar for pass-through (one-to-one re-sampling) and also for down-sampling (output has less pixels than input). The performance should degrade linearly in each dimension (horizontal and vertical) as that dimension is up-sampled (output has more pixels than input). For example, the back-prop for a 1:2 deterministic up-sample in one dimension could take up to twice as long. The slowdown is also proportional to the product of the up-sampling in the two dimensions. For example, the back-prop for a 1:2 deterministic up-sample in both dimensions could take up to four times (2*2) as long.

This pull request also significantly improves the test coverage of the existing, non-deterministic functionality of `method=ResizeMethod.BILINEAR`, and fixes one of the tests for `method=ResizeMethod.BICUBIC`.",tensorflow
39399,marload,pr,2020-05-11T06:39:25Z,Refactoring: Format String -> Format Method,"I think this method is more pythonic.

Thx",tensorflow
39404,freedomtan,pr,2020-05-11T09:21:01Z,[tflite] reformat/cleanup label_image readme.md,,tensorflow
39444,freedomtan,pr,2020-05-12T03:25:22Z,[tflite] add fp32->fp16 relaxation option to NNAPI delegate,"Most NN accelerators on Android devices don't support FP32. To evaluation their capabilities, we should call `Interpreter::SetAllowFp16PrecisionForFp32(True)` so that NNAPI delegate will call the corresponding [ANeuralNetworksModel_relaxComputationFloat32toFloat16()](https://developer.android.com/ndk/reference/group/neural-networks#group___neural_networks_1gab822719f98f0c92e5da3684cdaca6ba0) when used.

This patch adds `fp16` related support so that we can do something like
```
./imagenet_accuracy_eval \
--model_file=tflite-models-new/float/mobilenet_v1_1.0_224.tflite \
--ground_truth_images_path=ilsvrc_images \
--ground_truth_labels=ilsvrc_validation_labels.txt \
--model_output_labels=labels.txt \
--output_file_path=accuracy_outputs/mobilenet_v1_1.0_224_float_nnapi.txt \
--delegate=nnapi \
--allow_fp16=1 \
--num_images=0
```
on Android devices",tensorflow
39480,zhuzilin,pr,2020-05-13T01:18:09Z,Fix typo in gradient readme,,tensorflow
39482,marload,pr,2020-05-13T01:39:45Z,Refactoring: Format String -> Format Method,I think this method is more pythonic. Thank you.,tensorflow
39527,freedomtan,pr,2020-05-14T01:59:14Z,[tflite] Java binding for fp16 in NNAPI delegate,"Add Java binding to use the newly added `allow_fp16` in NNAPI delegate.
This is a follow-up of https://github.com/tensorflow/tensorflow/pull/39444",tensorflow
39576,nluehr,pr,2020-05-15T13:07:07Z,Enable build with CUDA 11,Attn: @reedwm This enables building TensorFlow against the upcoming CUDA 11.0 toolkit.,tensorflow
39577,nluehr,pr,2020-05-15T13:15:40Z,CUDNN v8 support,"Enables building against the CUDNN v8 library.

Attn: @reedwm 

Note this PR can be applied separately from the cuda 11 PR (39576), but both modify find_cuda_config.py. As a result, they will conflcit on find_cuda_config.py.gz.base64 and that file will need to be regenerated via compress_find_cuda_config.py when the second PR is merged.",tensorflow
39698,freedomtan,pr,2020-05-20T03:08:11Z,[tflite] nnapi delegate max_delegated_partitions=0,"make it possible to specify `max_delegated_partitions=0`. In the
original code, when `--max_delegated_partitions=0` (or less than 0)
is specified, it's ignored. That is, the maximum number of
partitions is set to 3 (nnapi delegate's default number).",tensorflow
39764,nluehr,pr,2020-05-21T19:03:25Z,Tf32,"API to enable and disable TensorFloat32 execution on Nvidia Ampere GPUs.

We also remove the following FP16 TensorCore opt-out environment variables:
* TF_DISABLE_CUBLAS_TENSOR_OP_MATH
* TF_DISABLE_CUDNN_TENSOR_OP_MATH
* TF_DISABLE_CUDNN_RNN_TENSOR_OP_MATH

Attn: @reedwm ",tensorflow
39821,gaurav1086,pr,2020-05-23T19:15:15Z,[Lite]: Fix memory leak from model,"Signed-off-by: Gaurav Singh <gaurav1086@gmail.com>
Delete model to avoid memory leak.",tensorflow
39929,zhuzilin,pr,2020-05-28T02:50:29Z,[tf.data] skip concat when origin ds has infinity cardinality,"This pr will skip `a.concatenate(b)` when `a` already has infinity cardinality.

This may also be achieved by removing the `ConcatenateDatasetOp` from the graph when `to_concatenate` has infinity cardinality with grappler, in that way `to_concatenate` and its prior ops may be pruned in graph optimization. If you prefer this way, I'd love to contribute.

Thank you for your time on reviewing this pr.",tensorflow
40118,zhuzilin,pr,2020-06-03T10:06:11Z,Add shape and type check for IteratorGetNextOp and ToSingleElementOp,"This pr adds the same shape and type check to IteratorGetNextOp and ToSingleElementOp as IteratorGetNextAsOptionalOp.

Thank you for your time on reviewing this pr.",tensorflow
40134,ssnl,pr,2020-06-04T00:12:32Z,Fix missing tick in BatchNorm doc,,tensorflow
40158,gaurav1086,pr,2020-06-04T16:24:52Z,[compiler] Fix segmentation fault in segment graph,"Signed-off-by: Gaurav Singh <gaurav1086@gmail.com>

The function: graph->FindNodeId(i) can return a nullptr for invalid i (node_id)
```
  SimpleNode* FindNodeId(int node_id) {
    if (node_id < 0 || node_id > static_cast<int>(nodes_.size())) {
      return nullptr;
    }    
    return nodes_[node_id];
  }
```
The returned node is dereferenced multiple times in the caller thereby causing segmentation fault. To fix this, add the null check and if null/invalid, continue/skip the node right away",tensorflow
40177,zhuzilin,pr,2020-06-05T02:56:39Z,[tf.data] Return unknown cardinality when preserve_cardinality is false,"Fix bug mentioned in #40107 .

cc @aaudiber 

Thank you for your time on reviewing this pr.",tensorflow
40223,zhuzilin,pr,2020-06-06T15:48:07Z,[grappler] Convert identity ConjugateTranspose to Conj instead of removing it,"This pr fixes the bug in #27500 where the identity conjugate transpose op was wrongly removed.

Thank you for your time on reviewing this pr.",tensorflow
40235,cclauss,pr,2020-06-07T10:57:14Z,Undefined name: import sys for line 56,`sys` is neither defined nor imported which leads to an _undefined name_ which has the potential to raise NameError at runtime.,tensorflow
40240,cclauss,pr,2020-06-07T14:00:22Z,Fix SyntaxWarnings on Python >= 3.8,"Avoid [SyntaxWarnings on Python >= 3.8](https://docs.python.org/3/whatsnew/3.8.html#porting-to-python-3-8).

% `python3.8`
```
>>> 0 is 0
<stdin>:1: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
>>> () is ()
<stdin>:1: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
```",tensorflow
40241,cclauss,pr,2020-06-07T14:06:46Z,xrange() was removed from Python on 1/1/2020,,tensorflow
40290,cclauss,pr,2020-06-08T19:45:57Z,xrange() was removed from Python on 1/1/2020,Like #40241,tensorflow
40335,nluehr,pr,2020-06-09T18:59:57Z,Remove cudnn RNN algo APIs from cudnn 8 inc file,"Remove references to some cudnn APIs that were deprecated in cudnn 7 and will be removed in the final cudnn 8 release.

attn: @reedwm ",tensorflow
40393,nluehr,pr,2020-06-11T21:03:07Z,Relax stub include version checking.,"Remove upper bound on version check for latest inc files.

Attn: @sanjoy ",tensorflow
40405,zhuzilin,pr,2020-06-12T10:45:41Z,[tf.data] Add grappler pass to hoist data-discarding ops,"This is a PR from JIZHI, the AI platform in Tencent.

This pr adds a tf.data grappler pass which is used to hoist the data-discarding ops like `shard`, `skip` and `take`. In this way there will be less unnecessary calculation or cache.
For example, this pass will turn this code:
```python
def parse_and_preprocessing(x):
  # very slow

ds = tf.data.Dataset.TFRecordDataset(""example.tfrecord"")
ds = ds.map(parse_and_preprocessing, num_parallel_calls=10)
ds = ds.cache()
ds = ds.skip(100)
ds = ds.take(1000)
ds = ds.repeat()
```
into
```python
# ...

ds = tf.data.Dataset.TFRecordDataset(""example.tfrecord"")
ds = ds.skip(100)
ds = ds.take(1000)
ds = ds.map(parse_and_preprocessing, num_parallel_calls=10)
ds = ds.cache()
ds = ds.repeat()
```
This transformation will enable the `cache` and avoid some unnecessary background calculation in `map`. Also, this pass will help to make a consistently processing map possible (in other words, let the `MapDataset` continue to process whenever there is a empty thread instead of scheduling a element only after a `GetNext` call.). Both of these problems are partially discussed in #39992 . 

The design of this pass is basically the same as `noop_elimination` or `inject_prefetch` and its inner logic is:
1. Find a node that will discard data. (Right now there are `take`, `skip` and `shard`).
Let's use the above code as an example:
```
TFRecord -> ParallelMap -> Cache -> | Skip | -> Take -> Repeat
```
2. Find the chain of ops that will not change the order of the input and is connected to the node in step 1. (Right now there are `map`, `prefetch` and `cache`.)
```
TFRecord -> { ParallelMap -> Cache } -> | Skip | -> Take -> Repeat
```
3. Move the node from the end of the chain to the start.
```
TFRecord -> | Skip | -> { ParallelMap -> Cache } -> Take -> Repeat
```
4. If there is an update of the graph, repeat 1 - 3. (For the example, `take` will be moved in the second round).

gently ping @jsimsa 

Thank you for your time on reviewing this pr.",tensorflow
40410,guillaumekln,pr,2020-06-12T13:22:44Z,Export tf.math.reduce_all first,This makes it consistent with other `reduce_*` symbols.,tensorflow
40464,lutzroeder,pr,2020-06-15T06:06:23Z,Fix Keras documentation,"@qlzh727 8ff354d254dc786dea9fc1f9d3d7cc39df25e39f
@omalleyt12 b4db28ca0df3b4938a923ffac1fc2085062dbe9d",tensorflow
40473,guillaumekln,pr,2020-06-15T13:53:59Z,Fix docstring format of tf.executing_eagerly,,tensorflow
40474,guillaumekln,pr,2020-06-15T14:00:04Z,Copy reverse_sequence docstring to _v2 to remove deprecation notices,The [documentation](https://www.tensorflow.org/api_docs/python/tf/reverse_sequence?version=nightly) is currently showing deprecation warnings for arguments that were never included in V2.,tensorflow
40531,freedomtan,pr,2020-06-17T01:31:10Z,[tflite] make label_image build on linux and macOS,"label_image doesn't build on Linux and macOS platforms

```
bazel build --config opt //tensorflow/lite/examples/label_image:label_image
```
shows something like
```
ERROR: /home/freedom/work/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: undeclared inclusion(s) in rule '//tensorflow/lite/examples/label_image:label_image':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/examples/label_image/label_image.cc':
  'external/com_google_absl/absl/strings/string_view.h'
  'external/com_google_absl/absl/base/internal/throw_delegate.h'

```

Add `""@com_google_absl//absl/strings""` to deps",tensorflow
40624,nluehr,pr,2020-06-19T22:15:38Z,TF32 Support for NVIDIA A100,"This is an update of https://github.com/tensorflow/tensorflow/pull/39764

Attn: @reedwm ",tensorflow
40654,cbalint13,pr,2020-06-22T06:52:59Z,Fix GCC 10.1 compile error.,"This PR fix compile error using gcc 10.1

**Description**

There is an ```auto``` inference failure error fixed by this PR:

```
~/rpmbuild/BUILD/tensorflow/_bazel_cbalint/4c79ce0d14678d18eb8640cac68aaf03/execroot/org_tensorflow ~/rpmbuild/BUILD/tensorflow/tensorflow
tensorflow/python/lib/core/bfloat16.cc: In function â€˜bool tensorflow::{anonymous}::Initialize()â€™:
tensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  664 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
tensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  668 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
tensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  671 |   if (!register_ufunc(""less"", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {
      |                                                                             ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
tensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  675 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
tensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  679 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
tensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to â€˜(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)â€™
  683 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: â€˜tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>â€™
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from â€˜<unresolved overloaded function type>â€™ to â€˜PyUFuncGenericFunctionâ€™ {aka â€˜void (*)(char**, const long int*, const long int*, void*)â€™}
```

* Relevant toolchain versions:

```
$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/10/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,objc,obj-c++,ada,go,d,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --enable-cet --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 10.1.1 20200618 (Red Hat 10.1.1-2) (GCC) 
```
```
$ python --version
Python 3.9.0b3+
```
",tensorflow
40675,zhuzilin,pr,2020-06-22T14:03:25Z,[doc] Fix broken figures in tiled_layout.md,"This pr changed the figures in a style that https://stackoverflow.com/a/12118349/5163915 suggests so that  they can be shown correctly on github.

Thank you for your time on reviewing this pr.",tensorflow
40749,cbalint13,pr,2020-06-24T09:14:14Z,[EXT-SYSLIB] Add absl_py logging submodule to build flow.,"Enable proper build when ```absl_py``` is external ```LOCAL_LIBS```.

```
ERROR: /home/cbalint/rpmbuild/BUILD/tensorflow/tensorflow/tensorflow/python/distribute/BUILD:1733:11: no such package '@absl_py//absl/logging': BUILD file not found in directory 'absl/logging' of external repository @absl_py. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/python/distribute:multi_process_runner'
```

Cc @perfinion 

Thank You !
",tensorflow
40861,zhuzilin,pr,2020-06-27T07:58:52Z,Change GetMatchingPaths to avoid traversing unnecesscary paths,"This is a PR from JIZHI, the AI platform in Tencent.

This pr updates the function `GetMatchingPaths`.

The old code will collect all possible path before using any wildcard characters, which will be really slow and memory demanding when matching patterns like ""/*"". Similar issue was stated in #40553 . The updated function will match the directory while gradually traverse the possible path.

Thank you for your time on reviewing this pr.
",tensorflow
40963,zhuzilin,pr,2020-07-01T03:38:40Z,[tf.data] Add SkipNext interface to iterator,"This is a PR from JIZHI, the AI platform in Tencent.

This pr adds a `SkipNext` interface to `IteratorBase` and use this method in `SkipDatasetOp` and `ShardDatasetOp`.

If this interface is added, we can gradually implement it for all dataset ops so that some unnecesscary calculation can be avoided.

Thank you for your time on reviewing this pr.",tensorflow
40964,zhuzilin,pr,2020-07-01T04:56:58Z,[tf.data] Use output_shapes from python for batch dataset,"This is a PR from JIZHI, the AI platform in Tencent.

This pr is related to #40938 . It removes the output shape calculation in C++ for `BatchDatasetOp` and `PaddedBatchDatasetOp` and use the shapes passed from python instead.

Thank you for your time on reviewing this pr.",tensorflow
41052,zhuzilin,pr,2020-07-03T06:13:10Z,[XLA] Make postorder stack adds a channel once for all predecessors,"This is a PR from JIZHI, the AI platform in Tencent.

This pr slightly changes `ComputeInstructionPostOrder` and make the stack for post order traversal not add the whole channel for every predecessor. And example is the `InstructionPostOrderWithAllReduce` test case in `hlo_computation_test.cc`:
```
HloModule Module

add {
  lhs = f32[] parameter(0)
  rhs = f32[] parameter(1)
  ROOT add = f32[] add(lhs, rhs)
}

ENTRY entry {
  param = f32[128] parameter(0), sharding={maximal device=0}
  crs0 = f32[128] all-reduce(param),
    replica_groups={{0}}, channel_id=1, to_apply=add,
    sharding={maximal device=0}
  crs1 = f32[128] all-reduce(param),
    replica_groups={{0}}, channel_id=1, to_apply=add,
    sharding={maximal device=1}
  add = f32[128] add(crs0, crs0), sharding={maximal device=0}
  ROOT t = (f32[128], f32[128]) tuple(add, crs1)
})
```
The `add` instruction would add `crs0` and `crs1` twice in the old implementation. And this pr avoids that.

Thank you for your time on reviewing this pr.",tensorflow
41222,zhuzilin,pr,2020-07-09T07:11:26Z,Add SkipRecords to RecordReader,"This is a PR from JIZHI Team & TaiJi AI platform in Tencent.

This pr add `SkipRecords(uint64* offset, int num_to_skip)` to `RecordReader`, which will skip `num_to_skip` number of records without reading them (using `SkipNBytes`). It will help to implement the `Skip` interface mentioned in #40963, where we are hoping to avoid unnecessary data IO and transformation in ops like `tf.data.Dataset.shard` and `tf.data.Dataset.skip`.

Thank you for your time on reviewing this pr.

FYI, @aaudiber ",tensorflow
41261,zhuzilin,pr,2020-07-10T03:38:44Z,Remove repeated call in FindKernelDef,Thank you for your time on reviewing this pr.,tensorflow
41412,zhuzilin,pr,2020-07-15T11:35:34Z,Reuse HumanString in HumanStringWithLayout,Thank you for your time on reviewing this pr.,tensorflow
41762,zhuzilin,pr,2020-07-27T06:06:09Z,[doc] Enrich doc for jit_scope for clarification,"This pr adds more doc to `tf.xla.experimental.jit_scope` to clarify possibly confusion mentioned in #41626.

Thank you for your time on reviewing this pr.

Gently ping @cheshire :)",tensorflow
41981,lithuak,pr,2020-08-02T09:36:11Z,Add support for any TensorSpec-describable object to tf.data.Dataset.from_generator,"This PR addresses the #35342 issue which proposes to add the support for different types of tensors to tf.data.Dataset.from_generator. It also finalizes the development effort started at PR #37400.

Initially `from_generator` had two parameters: `output_types` and `output_shapes` (one mandatory and the other one optional) that allowed user to specify the dtypes and optionally shapes of the output objects, but not the types of the objects themselves thus making it impossible for a generator function to output more complicated types of objects like `tf.RaggedTensor` or `tf.SparseTensor`.

We add new parameter `output_signature` which describes the expected output of `from_generator` in terms of [TypeSpecs](https://www.tensorflow.org/api_docs/python/tf/TypeSpec). With this change a user can emit any object from a generator function they decide to pass to the `from_generator` given that such object is describable by the corresponding `TypeSpec` specification.

We leave the `output_types` and `output_shapes` parameters in place for now but mark them as deprecated with the intention to remove them in one of the upcoming TF releases and make `output_signature` the only way to specify the output for `from_generator`.
",tensorflow
41995,lithuak,pr,2020-08-03T06:07:05Z,Don't check for attribute `is_tensor_like` in `is_tensor`,"`is_tensor` implementation says that an object is Tensor if it matches at least one of three independent checks.

One of them is an object having attribute `is_tensor_like`.

But nobody seems to use it throughout the code anymore.

From the history we can see that this check [was added specifically to support the same name property of the `DistributedValues`](https://github.com/tensorflow/tensorflow/commit/89e06304aad35bfb019a8c10f39fc1ead83e0f99) class, but again from the history we see that the property [was later removed from `DistributedValues`](https://github.com/tensorflow/tensorflow/commit/6b68396a8279e00676c75d685ada2f74398d3c08).

Let's remove checking for `is_tensor_like` attribute from `is_tensor`.",tensorflow
42089,guillaumekln,pr,2020-08-06T09:41:14Z,Fix example formatting in tf.profiler.experimental.server.start,,tensorflow
42179,cbalint13,pr,2020-08-10T07:34:28Z,[EXT-SYSLIB] Add runtime_py for external flatbuffer.,"Fix proper build when ```flatbuffer``` is external ```LOCAL_LIBS```.

```
ERROR: /home/cbalint/rpmbuild/BUILD/tensorflow/tensorflow/tensorflow/lite/python/BUILD:206:11: no such target '@flatbuffers//:runtime_py': target 'runtime_py' not declared in package '' (did you mean 'runtime_cc'?) defined by /home/cbalint/rpmbuild/BUILD/tensorflow/_bazel_cbalint/4c79ce0d14678d18eb8640cac68aaf03/external/flatbuffers/BUILD.bazel and referenced by '//tensorflow/lite/python:util'
```

Cc @perfinion, @mihaimaruseac 

Thank You !",tensorflow
42230,freedomtan,pr,2020-08-11T13:08:44Z,[tflite] remove duplicate num_threads flag in benchmark_model,"When running benchmark_model, we saw message like `Duplicate flags: num_threads`
because there is num_threads flag in benchmark_tflite_model.cc",tensorflow
42365,freedomtan,pr,2020-08-14T13:02:16Z,[tflite] make `//tensorflow/lite/kernels:all` build,"make
```
bazel build --config android_arm64 //tensorflow/lite/kernels:all
```
work.

I want to run all test cases on an Android devices, but  there are some problems why trying to build `//tensorflow/lite/kernels:all`. All of them are libm related problems.",tensorflow
42400,freedomtan,pr,2020-08-16T01:47:57Z,[tflite] Make --nnapi_accelerator_name work in kernel tests,"Originally, when `--nnapi=true`, always test `nnapi-reference`, the NNAPI CPU reference implementation. Now when `--nnapi_accelerator_name` is specified, set it as is.

@multiverse-tf: my colleagues and me want to use these kernel tests to test NNAPI drivers other than `nnapi-reference`.",tensorflow
42404,freedomtan,pr,2020-08-16T06:52:36Z,[tflite] allow gpu delegate to run quant model in tflite demo,GPU delegate can handle quantized model since 40088b87ee0,tensorflow
42434,cclauss,pr,2020-08-17T17:05:44Z,pip3.7 --> 3.8 to align with the previous line,,tensorflow
42543,freedomtan,pr,2020-08-21T01:31:22Z,[tflite] enable passing int32 elementray arith to NNAPI,"NNAPI 1.3 (Android API level 30) [1] supports int32 add, sub, mul, and div. This patch enables passing +-*/ with int32 tensors to NNAPI. With that, we can do something like
```
./mul_test   --gtest_filter=IntegerMulOpTest.NoActivation --use_nnapi=1
```
on Android R devices, such as Pixel 4 running Android R beta.

[1] https://developer.android.com/ndk/reference/group/neural-networks",tensorflow
43109,nkreeger,pr,2020-09-10T13:15:18Z,Add test to ensure that MicroInterpreter.tensors_size() works.,"This was broken and not caught when switching to TfLiteEvalTensors. This
test ensures that the API properly reports the size. MicroInterpreter is
the source of truth for TfLiteContext values - I'm setting this field
here (it used to be set in MicroAllocator).

Fixes https://github.com/tensorflow/tensorflow/issues/42964",tensorflow
43474,freedomtan,pr,2020-09-23T07:31:14Z,avoid using TRUE and FALSE in enum,"this addresses a recent failed-to-build problem on Mac.
see #43421",tensorflow
43488,freedomtan,pr,2020-09-23T13:13:01Z,[tflite] fix missing labels.txt in model maker tutorial,fix the problem of missing `labels.txt` in #43463 and #41470,tensorflow
43524,JerryShih,pr,2020-09-24T07:16:57Z,Fix the wrong cli parsing value for xnnpack_delegate option.,"The original code uses a ptr address as the ""xnnpack_delegate"" setting value. It's incorrect.",tensorflow
43715,nluehr,pr,2020-10-02T01:13:00Z,Eager loading of CUDNN sub-libraries.,"Improves accuracy of TF autotuning by ensuring first calls to cudnn kernels run at full speed.

Attention @sanjoy ",tensorflow
43716,nluehr,pr,2020-10-02T01:27:18Z,Cuda 11.1 cudart SO name fix,"In CUDA 11.1 the soname of libcudart remains versioned as 11.0 for better backward comparability. This PR updates the configure scripts accordingly.

This should also address issue #43689 

Attn @sanjoy ",tensorflow
43823,freedomtan,pr,2020-10-06T15:05:26Z,fix grappler/costs:op_performance_data dependency,"1. when tensorflow is used as third party, there is a dependency
   problem when building on macOS. E.g.,
   https://github.com/mlperf/mobile_app/issues/90

2. op_performance_data.proto doesn't really need all the
   tf_additional_all_protos()",tensorflow
43837,freedomtan,pr,2020-10-07T06:55:53Z,[coreml delegate] fix usage of coremltools header,"1. including ""external/...."" will cause problems when using tensorflow
   as a third party / external package
2. there are some redundant includes",tensorflow
44066,mkuchnik,pr,2020-10-15T22:18:51Z,Fix expensive decompression in JPEG GetImageInfo,"The current implementation of GetImageInfo starts a libjpeg decompression pass over input images. Certain images (e.g., progressive JPEG) trigger a full image decompression, resulting in a performance degradation. This commit switches to an equivalent but cheaper libjpeg call for evaluating image dimensions.

The end result is performance is currently slowed down on these images by over **50x** when using ExtractJpegShape.

To reproduce, download input image as 'test_img.jpg':
![test_img](https://user-images.githubusercontent.com/12423239/96190717-ac53d280-0f10-11eb-9bb9-ee1c3301c49e.jpg)

Convert it to progressive jpeg:
`jpegtran -progressive test_img.jpg > test_img_progressive.jpg`

And benchmark using test_img_progressive.jpg with `use_shape=False` and `use_shape=True`. On my machine, the first finishes in 0.12 seconds and the second in 8.85 seconds (roughly 70x performance degradation).

```python3
import tensorflow as tf
import numpy as np

import timeit

# Inputs
#filenames = [""test_img.jpg""]
filenames = [""test_img_progressive.jpg""]
#use_shape = False
use_shape = True
# End Inputs

def read_path(file_path):
    img = tf.io.read_file(file_path)
    return img

def jpeg_to_shape(image_buffer):
  shape = tf.image.extract_jpeg_shape(image_buffer)
  return shape

def fake_jpeg_to_shape(image_buffer):
  """"""Pretend we read shape using extract jpeg shape""""""
  shape = np.array([1920, 1080, 3], dtype=np.int32)
  return shape

def create_dataset():
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.map(read_path)
    dataset = dataset.cache()
    if use_shape:
        dataset = dataset.map(jpeg_to_shape)
    else:
        dataset = dataset.map(fake_jpeg_to_shape)
    return dataset

def noop():
    return None

def run_loop(dataset):
    for x in dataset:
        noop()

def main():
    dataset = create_dataset()
    dataset = dataset.repeat(1000) # run 1e3 times
    timeit_results = timeit.timeit(lambda: run_loop(dataset),
                                   number=1
                                   )
    print(""Elapsed time:\n{}"".format(timeit_results))

if __name__ == ""__main__"":
    main()
```

Inspecting profiling results, the _data decoding_ part of the decompression pass is being performed on the JPEG. In other words, the JPEG is being decompressed at near the full cost of decompression rather than simply extracting the shape (a _metadata_ operation).
![get_shape_profiler](https://user-images.githubusercontent.com/12423239/96191390-fc7f6480-0f11-11eb-9943-4fd9d7fe70a0.png)

This PR replaces the `jpeg_start_decompress` call with a call to `jpeg_calc_output_dimensions`, which fills out the required `cinfo` fields without decompressing the data.
",tensorflow
44101,TylerADavis,pr,2020-10-16T19:15:42Z,Remove Python 2 instructions from TF Lite demo README,"Python 2 is not supported in TensorFlow after TF 2.1, so having Python 2 instructions in a demo's README in 2.4.0 is misleading.",tensorflow
44102,TylerADavis,pr,2020-10-16T19:57:26Z,Rename Raspbian to Raspberry Pi OS,"This May, Raspbian was renamed to Raspberry Pi OS, as visible on the [official Raspberry Pi downloads page](https://www.raspberrypi.org/downloads/raspberry-pi-os/). In order to maintain consistency and reduce confusion for new users, we should replace `Raspbian` with `Raspberry Pi OS` in our docs wherever it makes sense.",tensorflow
44382,marload,pr,2020-10-28T13:22:50Z,[TF-numpy] `imag` and `real` returns scalar when `val` is a scalar.,"`np.imag` and `np.real` returns scalar when `val` is a scalar.

```python
# Example
import tensorflow as tf
np = tf.experimental.numpy

np.real(5)
# output -> 5
```",tensorflow
44383,marload,pr,2020-10-28T13:32:29Z,[TF-numpy] `np.size` always returns `int`,"`np.size` of original numpy always returns `int`. But, Sometimes `np.size` of tensorflow returns 'float'

**before**
```python
np.size(np.array(5))
# output -> 1.0
```

**after**
```python
np.size(np.array(5))
# output -> 1
```",tensorflow
44384,marload,pr,2020-10-28T13:59:23Z,[TF-numpy] More efficient if condition,"If `axis` is None, We don't need assign `axisa`, `axisb`, `axisc`.",tensorflow
44474,marload,pr,2020-10-31T04:02:43Z,[TF-numpy] Implementation `np.random.standard_normal`,https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html,tensorflow
44475,marload,pr,2020-10-31T04:31:56Z,[TF-numpy] Implementation `np.random.poisson`,https://numpy.org/doc/stable/reference/random/generated/numpy.random.poisson.html?highlight=poisson#numpy.random.poisson,tensorflow
44477,marload,pr,2020-10-31T07:24:50Z,[TF-numpy] Add docstring in `np.random`,Thank you,tensorflow
44523,mkuchnik,pr,2020-11-02T16:48:47Z,[tf.data] Add more precise processing accounting,"The execution time of a synchronous captured function is currently approximated by the total elapsed time (including queueing time) rather than only measuring execution time. Time in queue is ~10ns per op or more and may increase under load. Mirroring the asynchronous captured function implementation, this commit pushes the accounting into the executor. Processing time now only measures time spent executing.",tensorflow
44679,SamuelMarks,pr,2020-11-08T04:43:07Z,[keras/optimizer_v2/ftrl.py] Reflect defaults in docstring,"I'm writing an open-source typed wrapped for TensorFlow [and other ML frameworks], to enable good errors; parameter optimisation and analysis; over other interfaces like CLI, GUIs and decoupled frontends, databases, RPC, and REST.

The docstrings must reflect the defaults in the `__init__` function, otherwise I'll need to extend my side to parse & merge the defaultsâ€”e.g., from [`inspect.Signature`](https://docs.python.org/3/library/inspect.html#inspect.signature) or from [`ast`](https://docs.python.org/3/library/ast.html)â€”with the `class`'s docstring.

So this'll probably be the first of many trivial change PRs I'll be sending over :+1: ",tensorflow
44717,duncanriach,pr,2020-11-10T05:17:06Z,Add to release notes: deterministic tf.image.resize (bilinear),"This current pull request (into the r2.4 branch) adds information to the version 2.4.0 release notes about the changes made by pull request [39243](https://github.com/tensorflow/tensorflow/pull/39243), which was merged into the master branch on 2020-09-22.",tensorflow
44731,cloudhan,pr,2020-11-10T14:57:37Z,Fix tpu_executor_dlsym_initializer windows build error,"This PR enables me build jax natively on windows, https://github.com/google/jax/pull/4843",tensorflow
44778,mkuchnik,pr,2020-11-11T20:30:22Z,Fix TFRecord uncompressed test cases,"Two of the test cases were testing the same code path. A TFRecord test case was testing the GZIP (compressed) path twice (case 2 and 3) rather than using one test case (case 3) to test the uncompressed path. This PR changes the latter test to test the uncompressed code path, as was intended.",tensorflow
44786,mkuchnik,pr,2020-11-11T23:58:14Z,Fix slim record_writer compression type,The current code does not compile if the corresponding ifdef branch corresponding to SLIM is used.,tensorflow
44803,cloudhan,pr,2020-11-12T15:43:35Z,correctly handle nccl for nvidia_gpu_device,"This is an ongoing effort of https://github.com/google/jax/pull/4843

nccl is not available on windows, `if_cuda` is not reliable.",tensorflow
44809,nluehr,pr,2020-11-12T18:36:44Z,Move cudnn library preloading into Get*Algorithms,"Allows only required libs to be loaded into device memory while still
preloading libs to avoid impacting autotuning kernel timings.

Attn: @sanjoy ",tensorflow
44810,nluehr,pr,2020-11-12T18:47:37Z,Use fixed arch-specific device memory reserve,"Replaces current 6% policy for device memory not claimed by BFC allocator
with fixed reservation size.

Default system_memory reserve sizes are:
*  500MB for Compute Capability <= 6.x (Pascal and earlier)
* 1050MB for Compute Capabilities of 7.x (Volta, Turing)
* 1536MB for Compute Capability >= 8.x (Ampere and later)

Also adds TF_DEVICE_MIN_SYS_MEMORY_IN_MB env var to directly override the
above defaults. This is needed in cases where external packages require
significant device memory allocations.

Using a fixed-sized system memory reserve can reduce the amount of device memory available to TF models on small-memory GPUs, such as Geforce cards. But it should eliminate the majority of out-of-memory cuda library initialization errors such as https://github.com/tensorflow/tensorflow/issues/44072, https://github.com/tensorflow/tensorflow/issues/43764, https://github.com/tensorflow/tensorflow/issues/42163, https://github.com/tensorflow/tensorflow/issues/41377, and https://github.com/tensorflow/tensorflow/issues/41196. 

Attn: @sanjoy ",tensorflow
44985,yaochengji,pr,2020-11-18T16:44:36Z,xrt ops use bfc_allocator instead,"XRT ops use `cuMemAlloc` directly, which will hurt the performance a lot.

This PR changes the cuda allocator to bfc allocator.

For Resnet-50 fp16 training with batchsize 256 in Tesla V100 GPU, the performance could increase from `730 images/s` to `1290 images/s`. Please refer to the [torch/xla issue](https://github.com/pytorch/xla/issues/2614) for more details.

",tensorflow
45296,freedomtan,pr,2020-12-01T08:16:42Z,[tflite] map leaky relu to prelu to use NNAPI,"Leaky ReLU is one of the widely used activation functions. However it's not supported by NNAPI, so it's unlikely to
accelerate a model with many Leaky ReLU ops. Fortunately, NNAPI 1.2 supports PReLU, which is a general case of Leaky
ReLU. So we can map Leaky ReLU to PReLU. With that, we can fully delegate models, such as the [TFLite super
resolution example,](https://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution/android) to NNAPI.",tensorflow
45420,SamuelMarks,pr,2020-12-05T01:14:39Z,"[*] Rename ""Arguments:"" to ""Args:""","I've written custom parsers and emitters for everything from docstrings to classes and functions. However, I recently came across an issue with the TensorFlow codebase: inconsistent use of `Args:` and `Arguments:` in its docstrings:

```sh
(tensorflow#d393534)$ for name in 'Args:' 'Arguments:'; do
    printf '%-10s %04d\n' ""$name"" ""$(rg -IFtpy --count-matches ""$name"" | paste -s -d+ -- | bc)""; done
Args:      5333
Arguments: 0820
```

It is easy enough to extend my parsers to support both variants, howevever it looks like `Arguments:` is wrong anyway, as per:

  - https://google.github.io/styleguide/pyguide.html#doc-function-args @ [`ddccc0f`](https://github.com/google/styleguide/blob/ddccc0f/pyguide.md)
  - https://chromium.googlesource.com/chromiumos/docs/+/master/styleguide/python.md#describing-arguments-in-docstrings @ [`9fc0fc0`](https://chromium.googlesource.com/chromiumos/docs/+/9fc0fc0/styleguide/python.md)
  - https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html @ [`c0ae8e3`](https://github.com/sphinx-contrib/napoleon/blob/c0ae8e3/docs/source/example_google.rst)

Therefore, only `Args:` is valid. This PR replaces them throughout the TensorFlow codebase.

PS: The trackbacks automatically appearing below are sending the same changes to other repositories in the [TensorFlow](https://github.com/tensorflow/tensorflow) and [keras-team](https://github.com/keras-team) organisations.",tensorflow
45427,SamuelMarks,pr,2020-12-05T22:28:50Z,[pylintrc] Enforce Google docstring format,"Related: #45420 
Requested by: @bhack",tensorflow
46300,nluehr,pr,2021-01-08T21:58:17Z,Tf32 fixes,"Currently, enable_tensor_float_32_execution(False) does not fully disable TensorFloat32 evaluation in RNNs. This PR fixes this and also disabled TF32 execution for a few additional tests. 

Attn: @reedwm ",tensorflow
46331,zhuzilin,pr,2021-01-11T08:27:50Z,Remove cusparseDnVecDescr_t from cusparse_10_1.inc,"This pr removes `cusparseDnVecDescr_t` and `cusparseSpVecDescr_t` relevant functions in `cusparse_10_1.inc` because those types were introduced in cuda 10.2.

This pr also removes some functions in `cusparse_10_1.inc` that would trigger errors like:

```
Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:
./tensorflow/stream_executor/cuda/cusparse_10_1.inc: In function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)':
./tensorflow/stream_executor/cuda/cusparse_10_1.inc:7932:1: error: conflicting declaration of C function 'cusparseStatus_t cusparseDnMatGetStridedBatch(cusparseDnMatDescr_t, int*, int64_t*)'
 cusparseDnMatGetStridedBatch(const cusparseDnMatDescr_t dnMatDescr,
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

Thank you for your time on reviewing this pr.",tensorflow
46358,zhuzilin,pr,2021-01-12T08:51:02Z,[tf.data] Implement SkipInternal for TFRecordDataset,"This is a PR from JIZHI Team & TaiJi AI platform in Tencent.

This PR is a follow-up of #40963 and #41222. In those 2 PRs, I added the `Skip` method to `IteratorBase` and `RecordReader` so that we can potentially reduce the IO when user is using `tf.data.Dataset.shard()` method on samples instead of filenames.

This PR adds the `SkipInternal` for `TFRecordDatasetOp`. The implementation is very similar to `GetNextInternal`. And to test the new method, I added the helper functions and macros for `Skip`, which are also very similar to that of `GetNext`.

I'm not sure the proper way to deal with `metrics::GetTFDataBytesReadCounter` in `SkipInternal`, so it is omitted for now. It would be great if you could give me some hints on it.

Sorry for this late follow-up, I would try to add the meaningful `SkipInternal`s in the coming months.

cc @aaudiber @jsimsa ",tensorflow
46364,zhuzilin,pr,2021-01-12T14:16:59Z,add support for concatenated gzip in ZlibInputStream,"This PR will fix #45137.

To support concatenated gzip, we need to call `inflateReset()` when `inflate()` returns `Z_STREAM_END`, as the zlib's doc explained:
> Unlike the gunzip utility and gzread() (see below), inflate() will not automatically decode concatenated gzip streams. inflate() will return Z_STREAM_END at the end of the gzip stream. The state would need to be reset to continue decoding a subsequent gzip stream.

from https://github.com/madler/zlib/blob/cacf7f1d4e3d44d871b605da3b647f07d718623f/zlib.h#L868

Thank you for your time on reviewing this pr.",tensorflow
46446,zhuzilin,pr,2021-01-15T07:19:09Z,Optimize RunLineHelper in BufferedInputStream,"The old implementation of `ReadLineHelper` will append chars one at a time to the tstring result, which may cause multiply memcpy or reinitialization. This PR change from using
```c++
tstring& append(size_t n, char c)
```
to 
```c++
tstring& append(const char* str, size_t len)
```
to append multiple chars once.

This optimziation will help `TextlineDataset` which iteratively calls `ReadLineHelper`.

Thank you for your time on reviewing this PR.",tensorflow
46448,zhuzilin,pr,2021-01-15T08:00:40Z,Add SkipLine to BufferedInputStream,"This PR adds a `SkipLine` method to `BufferedInputStream` which will help implement the `SkipInternal` method of `TextLineDataset`, like #41222 to `TFRecordDataset`

Thank you for yor time on reviewing this PR.",tensorflow
46466,nluehr,pr,2021-01-15T19:28:48Z,Add cuda 11.2 driver and runtime inc files,CC @sanjoy ,tensorflow
46510,zhuzilin,pr,2021-01-18T09:22:00Z,Add tf.io.encode_raw,"This PR implemented `tf.io.encode_raw` as a counterpart for `tf.io.decode_raw` as requested in #46493.

The interface of `encode_raw` is simply:
```python
def encode_raw(tensor, name=None):
```
And the function will return a Tensor with type `tf.string`. Because the `decode_raw` api already supports different host encoding method with `little_endian`, I think it's not necessary to have one for `encode_raw`. And it doesn't seem reasonable to have a `fixed_length` arg for `encode_raw` as well.

To make sure the `encode_raw` is the reverse operation of `decode_raw`, it will regard the first dimension of the input tensor as batch size and encode the input tensor into a batch of string tensor. In this way, we can do the following check:
```python
val == tf.io.decode_raw(tf.io.encode_raw(val), val.dtype)
```
(which is also how I implemented the test in `encode_raw_op_test.py`).

Thank you for your time on reviewing this PR :).
",tensorflow
46550,elfringham,pr,2021-01-20T10:20:48Z,Allow environment variables to execution of flatc command,"Allow setting of LD_LIBRARY_PATH to reach execution environment of flatc so it can load correct version of libstdc++ and so avoid build failure when built with non-system gcc
Fixes #46549 ",tensorflow
46694,elfringham,pr,2021-01-26T15:42:25Z,At least read the BogoMIPS figure on ARM,Use the capitalization of BogoMIPS that is used by ARM architecture CPUs.,tensorflow
46877,AdityaKane2001,pr,2021-02-03T10:27:45Z,Update README.md :Added TensorFlow: Advanced Techniques Specialization to resources tab,Added TensorFlow: Advanced Techniques Specialization to resources tab. Extremely helpful specialization by Laurence Moroney for more in depth knowledge of TensorFlow.,tensorflow
47317,JerryShih,pr,2021-02-22T18:01:10Z,Add cmake build for tflite label_image example.,Add label_image build for cmake.,tensorflow
47343,AdityaKane2001,pr,2021-02-23T14:00:55Z,Assert if y_pred and y_true have same shape in categorical_accuracy(),Refer this [issue](https://github.com/tensorflow/tensorflow/issues/46953) for details. ,tensorflow
47397,guillaumekln,pr,2021-02-25T13:53:20Z,Add missing closing backtick,,tensorflow
47410,TylerADavis,pr,2021-02-25T20:00:02Z,Add support for probability/thresholds in meanIoU,"Fixes #39173

Currently the meanIoU function requires that if there are `num_classes` in the model's output, each value in `y_pred` must be an integer in the range `[0, num_classes-1]`. This is a problem because most ML models will generate outputs of `num_classes` probabilities in the range `[0-1]` (shape `[dim1, dim2, ..., num_classes]` or `[dim1, dim2, ..., num_classes -1 ]`) , not outputs of shape `[dim1, dim2, ...]` with integer class values. #39173 discusses this further. 

Adding support for probabilities and thresholding will require changes to the API, and I would love to hear your thoughts on the best way to accomplish this.

## This PR adds the following:
* New optional `threshold` argument in the `meanIoU` initializer. Single configurable threshold shared across all predicted classes, used to turn 0-1 probabilities into 0 or 1 integer predictions
* Support for passing in probabilities instead of integer class labels
* New tests


## TODO:
* Add some sort of toggle to select between `classID` mode and `probability` mode. This is needed because if there are more than two classes and `y_pred` is in the current format, values will include integers `[0, 1, 2, ... n]`, and the probability mode would turn all values over the threshold to `1`, giving incorrect results
    * What is the best way to do this? One option would just be to add a `from_probability` field to `meanIoU` that defaults to false, but I'm not sure if there's a better way. #45690 is working towards adding support for logits, so we may need a way to switch between class labels, probabilities, and logins.
* Ensure that results are correct when `num_classes > 2` (no tests at present)
* When in probability mode, ensure that the last dimension of `y_pred` and `y_true` is either `num_classes` or `num_classes - 1`",tensorflow
47499,zhuzilin,pr,2021-03-02T08:07:21Z,"[Grappler] Add dataset.shard(1, 0) to noop_elimination pass",Thank you for your time on reviewing this PR. There are some addition to the test as `ShardDataset` is not a unary dataset op.,tensorflow
47519,byronyi,pr,2021-03-03T03:44:53Z,Fix API compatibility test under Python 3.9,"Thread.isAlive removed in Py 3.9: https://bugs.python.org/issue37804

Thanks @mdanatg for the pointer.

See https://github.com/tensorflow/tensorflow/issues/44485#issuecomment-789360728 for the context.",tensorflow
47594,freedomtan,pr,2021-03-05T16:00:28Z,make native bazel build work on Apple Silicon,"make `bazel --config opt //tensorflow/tools/pip_package:build_pip_package` work
1. include AArch64 stuff for XLA
2. don't build mkl_dnn

With recent bazel arm64 binary (tested with [1]), it's possible to build pip wheel.
Previously, you need x86_64 bazel, see #45404. 

[1] https://github.com/bazelbuild/bazel/commit/492829",tensorflow
47605,freedomtan,pr,2021-03-06T13:39:21Z,[tflite] Enable CoreML/GPU delegate on m1,"To run TFLite models on M1 devices, we need CoreML delegate to offload computation to Apple's Neural Engine.

```
bazel build //tensorflow/lite/tools/benchmark:benchmark_model \
  --macos_cpus arm64
```
    
```
./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \
  --use_coreml=1 \
  --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite
```
works as expected
",tensorflow
47617,JerryShih,pr,2021-03-07T08:36:59Z,Skip the default delegate if we already have set up one delegate.,"This pr try to fix the issue at:
https://github.com/tensorflow/tensorflow/issues/42757

When we use the xnnpack delegate, we will see the following error message:
```
/tensorflow/tensorflow/lite/build$ ./examples/label_image/label_image -m ~/mobilenet_v1_1.0_224.f32.tflite -l ~/labels.txt -i /tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp -p 1 -x 1
INFO: Loaded model /home/jerrys/mobilenet_v1_1.0_224.f32.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Use XNNPACK acceleration.
INFO: Applied XNNPACK delegate.
ERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.
ERROR: Ignoring failed application of the default TensorFlow Lite delegate indexed at 0.
```

The messages comes from the following ModifyGraphWithDelegate() and AllocateTensors() calls.
https://github.com/tensorflow/tensorflow/blob/fb7d34de0a7a892d070104aa4e385159817e7ef5/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L635
https://github.com/tensorflow/tensorflow/blob/fb7d34de0a7a892d070104aa4e385159817e7ef5/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L725


When we setup xnnpack delegate in ModifyGraphWithDelegate(), the graph will become immutable. Then, we call AllocateTensors() and apply the default delegate in ""lazy_delegate_providers_"" again. That will show the error message that we are trying to update the immutable graph using the delegate in ""lazy_delegate_providers_"".
So, I just clear the ""lazy_delegate_providers_"" if we already set up a delegate.",tensorflow
47639,freedomtan,pr,2021-03-08T09:57:01Z,[tflite] use newer xnnpack related source for M1 bazel build,"with newer related code, we can build benchmark_model with xnnpack, gpu, and coreml delegates.

On M1 machines,

Either
```
bazel-3.7.2-arm64 tensorflow/lite/tools/benchmark:benchmark_model --config macos_arm64  --macos_cpus arm64
```
or
```
bazel-4.0-arm64 tensorflow/lite/tools/benchmark:benchmark_model --macos_cpus arm64
```
works.",tensorflow
47664,zhuzilin,pr,2021-03-09T03:07:35Z,[tf.data] Add SkipInternal for flat_map and interleave,"This is a PR from JIZHI Team & TaiJi AI platform in Tencent.

This is a follow up of #46358. I added the `SkipInternal` for `flat_map` and `interleave` so that user could utilize the speed up of `TFRecordDataset` with `tf.data.TFRecordDataset`.

Also, I hope we could discuss about how to deal with the `SkipInternal` for `ParallelInterleaveDataset`, as the output of it could be nondeterministic.

Thank you for your time on reviewing this PR :).

gently ping @aaudiber ",tensorflow
47754,JerryShih,pr,2021-03-12T08:36:12Z,Support tflite kernel unittest in cmake.,"We don't have the kernel test case with cmake build.
This pr try to port the bazel setting into cmake for the kernel tests.

Here are the output message for the tests:
```
/tensorflow/tensorflow/lite/build$ ctest

Test project /tensorflow/lite/build
        Start   1: internal-averagepool_quantized_test
  1/137 Test   #1: internal-averagepool_quantized_test ......................   Passed    0.18 sec
        Start   2: internal-batch_to_space_nd_test
  2/137 Test   #2: internal-batch_to_space_nd_test ..........................   Passed    0.00 sec
        Start   3: internal-conv_per_channel_quantized_16x8_test
  3/137 Test   #3: internal-conv_per_channel_quantized_16x8_test ............   Passed    1.24 sec
  ....
        Start 137: test_util_test
137/137 Test #137: test_util_test ...........................................   Passed    0.00 sec

100% tests passed, 0 tests failed out of 137
```
",tensorflow
47762,elfringham,pr,2021-03-12T15:05:31Z,Fix build failure undefined reference to hwloc_linuxio_component on aarch64,"This is the same fix as previously in the file but for aarch64 instead of ppc64le, for the same reason.",tensorflow
47772,duncanriach,pr,2021-03-12T23:30:10Z,[determinism] Add segment reduction op exceptions for GPU determinism,"## High-Level Summary

This current PR adds and tests the following functionality:

When the environment variable `TF_DETERMINISTIC_OPS` is set to `""true""` or `""1""`, an attempt to run the following ops on a GPU will throw `tf.errors.UnimplementedError` (with an understandable message) when `data` is a floating-point type, including complex types (if supported).

`tf.math.segment_prod`
`tf.math.segment_sum`
`tf.math.unsorted_segment_mean`
`tf.math.unsorted_segment_sqrt_n`
`tf.math.unsorted_segment_prod`
`tf.math.unsorted_segment_sum`

`tf.convert_to_tensor`, when running on a GPU and when `value` is of type `tf.IndexedSlides` will also throw `tf.errors.UnimplementedError` (because it uses `tf.math.unsorted_segment_sum`). This is confirmed by a test included in this PR.

The output of `tf.gather`'s backprop is of type `tf.IndexedSlices`. Therefore, when running on a GPU, if the output of `tf.gather`'s backprop is passed to `tf.convert_to_tensor`, such as when updating a word embedding, then that will also cause `tf.errors.UnimplementedError` to be thrown. This is confirmed by a test included in this PR.

Please see _RFC: Enhancing determinism in TF_ (being added via tensorflow/community PR [346](https://github.com/tensorflow/community/pull/346)).

## Additional Notes

### Sorted Segment Mean

`tf.math.segment_mean` is currently implemented on the CPU only, operates deterministically, and will not throw `tf.errors.UnimplementedError`. The tests included in the current PR confirm that it does not throw an exception.

### Unsorted Segment Mean and Square Root

`tf.math.unsorted_segment_mean` and `tf.math_unsorted_segment_sqrt_n` are both currently implemented on top of `tf.math.unsorted_segment_sum`. Running these two ops on the GPU, with floating-point-based data types, will cause `tf.errors.UnimplementedError` to be thrown as if by `tf.math.unsorted_segment_sum`.

### Min and Max Segment Reductions

The following ops will not throw `tf.errors.UnimplementedError`, even for floating-point-based types, because min and max operations are associative (ignoring potential non-commutativity and non-associativity of math corner cases). That they do not throw exceptions when run on GPU is confirmed by the tests included in this current PR.

  * `tf.math.segment_max`
  * `tf.math.segment_min`
  * `tf.math.unsorted_segment_max`
  * `tf.math.unsorted_segment_min`

### Data Types

Where GPU implementations of the sorted segment (non-min/max) reduction ops exist, they support the following data types: `tf.float16`, `tf.float32`, and `tf.float64`. The tests confirm that `tf.errors.UnimplementedError` is thrown for all of these, regardless of the integer index data type.

Where GPU implementations of the unsorted segment (non-min/max) reduction ops exist, they support the following data types: `tf.float16`, `tf.float32`, `tf.float64`, and `tf.int32` (except `tf.math.unsorted_segment_sqrt_n`, which does not support `tf.int32`).  The tests confirm that `tf.errors.UnimplementedError` is thrown for all ops on all the floating-point types and not for any of the ops with `int32`, regardless of the integer index data type.

The GPU implementation of `tf.math_unsorted_segment_sum` (and therefore also `tf.math.unsorted_segment_mean` and `tf.math_unsorted_segment_sqrt_n`) supports `tf.complex64` and `tf.complex128`. The tests confirm that `tf.errors.UnimplementedError` is thrown for these ops and datatypes, regardless of the integer index data type.

### Graph Mode vs Eager Mode Testing

All the tests run in eager mode and some tests run in graph mode as well. I was not able to get all the test to work in graph mode, for reasons that seemed unrelated to the actual intentions of the tests. I didn't want to spend more time on it, and I felt that enough of the exception throwing had been shown to translate, as expected, to graph mode.

### XLA

The tests that run in graph mode would also end up getting run with the XLA auto-jit enabled. This would cause tests to fail where the expected exceptions were not generated by the XLA implementations of the ops. For this reason, in this PR, XLA auto-jit is disabled. I don't know if the XLA implementations of these ops are deterministic, but I imagine that they probably would be. I propose that when we implement deterministic versions of these ops, the determinism tests (that actually check determinism) should also be run on the XLA functionality.

### Dense Image Warp

`tfa.image.dense_image_warp`'s backprop to `image` (but not `flow`) is nondeterministic on a GPU because [it uses `tf.gather`](https://github.com/tensorflow/addons/blob/d26e2ed5f68092aed57016a7005ce534b1be3dce/tensorflow_addons/image/dense_image_warp.py#L171). Therefore, after this PR, enabling determinism should cause `tfa.image.dense_image_warp` to throw an exception up from `tf.math.unsorted_segment_sum` if its backprop is run on a GPU. This behavior is not confirmed by the tests included with this current PR.",tensorflow
47850,JerryShih,pr,2021-03-17T00:34:03Z,Support ccache in cmake build.,Use ccache to speed up the compiling time.,tensorflow
47925,duncanriach,pr,2021-03-19T18:22:59Z,[determinism] Add softmax/cross-entropy op exceptions for GPU determinism,"## High-Level Summary

This current PR adds and tests the following functionality:

When the environment variable `TF_DETERMINISTIC_OPS` is set to `""true""` or `""1""`, an attempt to run the following ops on a GPU will throw `tf.errors.UnimplementedError` (with an understandable message).

`tf.nn.softmax_cross_entropy_with_logits`
`tf.nn.sparse_softmax_cross_entropy_with_logits`

Please see _RFC: Enhancing determinism in TF_ (being added via tensorflow/community PR [346](https://github.com/tensorflow/community/pull/346)).

## Additional Notes

### Data Types

The exceptions will be thrown for all currently GPU-supported data types for the `logits` input: `tf.float16` and `tf.float32` for both ops, and, additionally, `tf.float64` for `tf.nn.softmax_cross_entropy_with_logits`.

Exception-throwing for all combinations of relevant data types for `logits` and `labels` (`tf.int32` and `tf.int64`) are tested in both eager and graph mode when the op is used in the forward direction.

### Forward vs Backward

It is currently suspected that the introduction of random noise into the gradients passed backwards from this op actually originate in the forward path algorithm, but the backward path algorithm might add additional noise. However, the backprop path for this op is not, and cannot, be used without the forward path algorithm also being used (due to this being a loss function). Therefore, the presence of exception-throwing on the backward path specifically is not necessary and is not implemented or tested by this current PR.

When these ops have a fully deterministic mode of operation, the bit-exact reproducibility of the outputs of both the forward and backward paths of the ops should be verified.

### XLA

The tests will not be run with XLA auto-jit enabled because any XLA implementation of these ops will not throw these exceptions.

When a fully deterministic mode for these ops is implemented, the bit-exact reproducibility of the outputs of both the forward and backward paths of the ops should be verified both with and without XLA auto-jit enabled.",tensorflow
48000,AdityaKane2001,pr,2021-03-23T04:59:36Z,"Assert shapes in categorical_accuracy(), resolve local_test.py errors",Refer this [issue](https://github.com/tensorflow/tensorflow/issues/46953) for details. Refer this [PR](https://github.com/tensorflow/tensorflow/pull/47343) for discussions.,tensorflow
48020,JerryShih,pr,2021-03-23T17:07:48Z,Use reference_ops::Softmax() for RunSoftmaxFloatReference().,"If we use TFLITE_SOFTMAX_USE_UINT16_LUT table look up optimization
for softmax[1], the float look up table will not be initialized.
The optimized_ops::Softmax<float, float>() call will crash.
This commit turns to use reference_ops::Softmax<float, float>()
instead. That reference call should always be available.

[1]
https://github.com/tensorflow/tensorflow/blob/8f8b15c9cdd4f549354cc0e97c81ed9ab6096f67/tensorflow/lite/kernels/activations.cc#L602-L619",tensorflow
48097,JerryShih,pr,2021-03-26T03:13:41Z,Updated xnnpack to commit f0cb70a1ae68c4b99047b7c5d26c40fa3a65f0e4 for cmake build.,Update for fp16 downloading path.,tensorflow
48099,JerryShih,pr,2021-03-26T06:00:01Z,Add riscv platform support for tflite with cmake.,"This pr try to add the riscv platform support with cmake.
It includes:
1. prebuilt binary download script for riscv clang compiler and qemu.
2. the riscv cmake toolchain file which contains the toolchain and compiling options related setting for riscv platform
3. the howto document for all build steps.",tensorflow
48399,freedomtan,pr,2021-04-08T09:21:32Z,[tflite] make build of tflite cmd line tools work again,"This is to revert 5ba9567, which removed a workaround in absl to
make cross-building android stuff on macOS work. Without this,
when building `benchmark_model` with
```
bazel build --config android_arm64 \
  //tensorflow/lite/tools/benchmark:benchmark_model
```
I saw

```
/bin/external/com_google_absl/absl/base/liblog_severity.a bazel-out/arm64-v8a-opt/bin/external/com_google_absl/absl/numeric/libint128.a bazel-out/arm64-v8a-opt/bin/external/ruy/ruy/profiler/libprofiler.a bazel-out/arm64-v8a-opt/bin/external/ruy/ruy/profiler/libinstrumentation.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -latomic -Wl,--no-export-dynamic -Wl,--gc-sections -Wl,--as-needed -s -pie -lm '-Wl,--rpath=/data/local/tmp/' -lm -ldl -ldl -ldl -ldl -lEGL -lGLESv2 -ldl -lm -pthread -framework Foundation -pthread -pthread -pthread -pthread -lm -llog -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -llog -pthread -pthread -pthread -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target aarch64-none-linux-android -no-canonical-prefixes -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a '--sysroot=external/androidndk/ndk/platforms/android-28/arch-arm64')
ERROR: /Users/freedom/work/tf-py3/tensorflow/lite/tools/benchmark/BUILD:30:10: Linking of rule '//tensorflow/lite/tools/benchmark:benchmark_model' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/tensorflow/lite/tools/benchmark/benchmark_model ... (remaining 397 argument(s) skipped)
external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: -f may not be used without -shared
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.293s, Critical Path: 0.04s
INFO: 2 processes: 2 internal.
```",tensorflow
48491,AdityaKane2001,pr,2021-04-12T18:21:37Z,Remove deprecated function call in local.py,Please refer #48486 ,tensorflow
48581,duncanriach,pr,2021-04-17T02:32:02Z,[determinism] Update release notes in branch r2.5,"This current pull request updates the release notes for version 2.5 of TensorFlow with respect to deterministic op functionality.

cc @sanjoy",tensorflow
48588,freedomtan,pr,2021-04-17T11:35:31Z,make `bazel build //tensorflow/tools/pip_package:build_pip_package` work on Jetson devices,"Fixed two problems stopping `bazel build //tensorflow/tools/pip_package:build_pip_package` on Jetson devices
1. CUDA 10.2 doesn't work well with absl CORD
2. mlir needs AArch64 codegen

Fixes #48468",tensorflow
48601,zhuzilin,pr,2021-04-18T14:34:26Z,Add elastic training for workers in PSv2,"We are glad to have PSv2 last year. The single-client distributed training model is much simpler to use and more intuitive to reason able. We especially love the fault tolerance capability of the new strategy, as most industrial scenarios of the asynchronous training with parameter server usually involve dozens of workers and it's crucial to make sure the job keeps running.

In this PR, we will push the fault tolerance capability a step further -- we are adding elastic training for workers in PSv2, which means we could change the number of the workers during training. This would allow us to adjust the worker size by need and make better usage of the computational resources.

For this purpose, this PR mainly add 2 methods to the `CoordinatorCluster`:

- `remove_worker(self, address)`: Remove a `Worker` from `coordinator._cluster` by its address.
- `add_worker(self, address)`: Add a new `Worker` to `coordinator._cluster` by its address.

Notice that we are leaving the start and stop of the worker servers to the plaform, for instance, KubeFlow.

Before discussing about the underlying mechanism, we'd like to split all `Worker`s in the `coordinator._cluster` to 2 states: running and stopped.

- A running `Worker` is one that keeps on grabbing `Closure`s from `_closure_queue` to process. In other words, a `Worker` is running if its processing thread is running. A typical running `Worker` is one that is just initialized.

- A stopped `Worker` is one that stops getting `Closure`s and whose processing thread has stopped (or `join`ed).

We could call `worker.stop()` to turn a running `Worker` to stopped. And to make a stopped `Worker` back to running, we added a `restart` method to `Worker` class. In `restart` method, the `Worker` will recreate a processing thread if it is stopped.

```
                     ----- stop ----->
__init__ ->  running                   stopped
                     <--- restart ---- 
```

Let's come back to the mechanism of elastic training. In fact, we are just using the `connect_to_cluster` function. This great function could recreate the topology of the server in runtime. But because every `connect_to_cluster` will clean all server caches, which introduce overhead, instead of calling `connect_to_cluster` in every `remove_worker` and `add_worker`,

- in `remove_worker`, we will only stop the `Worker` (by running `worker.stop()`) and add the stopped worker to a dict in `coordinator` called `_stopped_workers`, which is a map from the address of the stopped worker to its index;

- in `add_worker`, if the address is in the `_stopped_workers`, restart the worker to running state (by running `worker.restart()`). Otherwise, we need to use `connect_to_cluster` to recreate the cluster. In this way, we will first remove the stopped workers from the cluster spec and clean the `_stopped_workers`. Then, add the new worker to the cluster spec and call `connect_to_cluster`. Finally, create a new `Worker` and append it to `coordinator._cluster`.

We add some auxiliary functions as well, for example, `add_task` and `remove_task` in `ClusterSpec`. And to support sparse worker_index (worker indices like [0, 2, 5]), we changed the `PerWorkerValues` to hold a dict.

As for the dataset part, we choose to create new iterator after `add_task`. This may be oversimplied. But the dataset part seems to be relatively isolated with the coordinator and can be updated in maybe another PR.

A typical use case of the new apis is:

```python
...
per_worker_train_ds = coordinator.create_per_worker_dataset(per_worker_dataset_fn)
per_worker_train_iter = iter(per_worker_train_ds)

for epoch in range(EPOCHS):
    ...
    if epoch == 0:
        coordinator.remove_worker(""localhost:2101"")

    if epoch == 1:
        coordinator.add_worker(""localhost:2102"")
        per_worker_train_ds = coordinator.create_per_worker_dataset(per_worker_dataset_fn)
        per_worker_train_iter = iter(per_worker_train_ds)

    for i in range(step):
        coordinator.schedule(step_fn,
                              args=(per_worker_train_iter,))
    coordinator.join()
    ...
```

Any discussion on the design or any details of this PR is welcomed :). It will be great if you can give us some suggestion on the limitation of this design.

As for the test, we have tested this PR in multiple elastic scenarios. And we hope to discuss with you on how to integrate test for the elastic training as well.

Thank you for your time on reviewing this PR!

Gently ping @yuefengz @rchao.

cc @zw0610
",tensorflow
48610,AdityaKane2001,pr,2021-04-19T04:13:10Z,Check if all dimensions in output are non-zero,See #48589 ,tensorflow
48688,duncanriach,pr,2021-04-22T03:23:05Z,[determinism] Add CPU-focused tests for fused softmax/cross-entropy ops,"This current PR is a follow-up to PR [47925](https://github.com/tensorflow/tensorflow/pull/47925) (Add softmax/cross-entropy op exceptions for GPU determinism), and specifically to [this interaction](https://github.com/tensorflow/tensorflow/pull/47925#discussion_r599219389) between @sanjoy and myself.

This current PR adds determinism tests for the CPU implementations of `tf.softmax_cross_entropy_with_logits` and `tf.sparse_softmax_cross_entropy_with_logits`. When deterministic GPU implementations are added for these ops, the tests can be used to demonstrate, confirm, and ensure that the functionality is, and stays, deterministic.",tensorflow
48800,nkreeger,pr,2021-04-28T15:52:48Z,Add initial docs for online memory planning in TensorFlow Lite Micro.,,tensorflow
48803,byronyi,pr,2021-04-28T16:49:48Z,Fix build failure with CUDA 11.3,"Fix a typo introduced in e07069218c39cbfc4bbad79fc50c83d64b0546af to strip NCCL's relocatable device code.

/cc @chsigg ",tensorflow
48832,yaochengji,pr,2021-04-29T21:27:25Z,fix xrt cuda memory allocator [reopen],reopened for [pr](https://github.com/tensorflow/tensorflow/pull/44985),tensorflow
48905,duncanriach,pr,2021-05-04T21:31:10Z,"[determinism] Add GPU excepts, CPU d9m, and tests to crop_and_resize","This current PR adds the following functionality, plus associated tests:

When `TF_DETERMINISTIC_OPS` is set to `""true""` or `""1""` (when op-determinism is expected),

1. the gradient w.r.t `image` of `tf.image.crop_and_resize` when running on CPU will be bit-exactly reproducible (from run-to-run), and
2. an attempt to use the GPU kernels for gradient w.r.t either `image` or `boxes` of `tf.image.crop_and_resize` will cause a `tf.errors.UnimplementedError` to be thrown along with an understandable message.

This current PR is associated with [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md).

cc @sanjoy, @reedwm, @nluehr ",tensorflow
49178,duncanriach,pr,2021-05-14T04:49:51Z,[determinism] Add non-sparse softmax/xent GPU-determinism,"This PR adds and tests deterministic forward and backward operation of `tf.nn.softmax_cross_entropy_with_logits` when running on a GPU.

Note that there are changes and enhancements to the existing tests that may be obscured by the restructuring of the test files.

Thanks to @reedwm for providing support and guidance on this PR, including looking into the arithmetic equivalence of the forward and backward operation of the python-level solution.

Note that a naive implementation of softmax followed by cross-entropy is not as numerically stable as the version implemented here (and in the existing Eigen-based/C-level implementation) in which the log in the cross-entropy function is moved back into the softmax, changing it into a log-softmax and changing the cross-entropy function into a dot-product.  log-softmax does not demand as large dynamic ranges as softmax.

Note that the following tests do not pass on this deterministic implementation (and have been disabled):

  * Backprop to logits when there is only a single class (the forward path passes). See `testSingleClass`.
  * Backprop to logits when labels are broadcast (the forward path passes). See `testLabelsBroadcast`.

I have not yet been able to determine the reason for this, and I don't know if it's because the existing functionality is incorrect or if the new, deterministic functionality is incorrect. For the single class case, for example, it seems to me that the correct gradients should all be zero (which is what the new, deterministic implementation provides). It seems as though the above two use cases  (single class and broadcast labels) would rarely be used; it's not obvious to me what the applications of these use cases would be, and these functionalities are also not documented. I have added TODO comments for me to look into this more deeply. @reedwm, feel free to explore.

UPDATE: After further investigation, it has been revealed that the gradients only mismatch between the nondeterministic and deterministic implementations when the labels vector is not a valid probability distribution, as required (but not enforced) by the API. See [this comment](https://github.com/tensorflow/tensorflow/pull/49178#discussion_r641061999) for more information.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent).

cc @sanjoy @nluehr ",tensorflow
49226,freedomtan,pr,2021-05-17T07:07:29Z,[nnapi delegate] fix a trivial redundant comment and a typo,remove a redundant comment `// When using NN API ...` and a typo (?) `: -> .`,tensorflow
49974,SamuelMarks,pr,2021-06-02T09:24:34Z,Ensure default types are accurate: fix `int` given for `float` args,"I'm writing a parser/emitter that goes from Python to Python (and also OpenAPI). I found some type inconsistencies, when I would run `type(func_argument)` it would not match the argument type referenced in the docstring. This PR solves one such problem.",tensorflow
50018,SamuelMarks,pr,2021-06-03T05:49:05Z,[tensorflow/python/ops/init_ops.py] RandomUniform docstring says `maxval` should default to 1â€¦ now it does,,tensorflow
50070,duncanriach,pr,2021-06-04T05:27:18Z,[determinism] Add sparse softmax/xent GPU-determinism,"This PR adds and tests deterministic forward and backward operation of `tf.nn.sparse_softmax_cross_entropy_with_logits` when running on a GPU. This PR is a follow-on from PR [49178](https://github.com/tensorflow/tensorflow/pull/49178) (Add non-sparse softmax/xent GPU-determinism).

Note that there are significant changes and enhancements to the existing tests that may be obscured by the restructuring of the test files.

This implementation uses `tf.gather` in a way that does not exercise the nondeterminism that is currently possible through its backprop path on GPU: in each batch slot, it gathers only once from the correct logit, so it backprops (using `tf.math.unsorted_segment_sum`) from only one gradient value (associated with the loss) per batch slot. Therefore, this solution is deterministic. However, due to the d9m-unimplemented exception-throwing that was added to `tf.math.unsorted_segment_sum` via PR [47925](https://github.com/tensorflow/tensorflow/pull/47925) (Add softmax/cross-entropy op exceptions for GPU determinism) use of `tf.gather`'s backprop on a GPU while op-determinism is enabled will cause a `tf.errors.UnimplementedError` to be thrown. For this reason, the test currently disables the exception by setting `TF_DISABLE_SPARSE_SOFTMAX_XENT_WITH_LOGITS_OP_DETERMINISM_EXCEPTIONS` to `'1'` and this current PR should not be merged until a deterministic GPU implementation for `tf.math.unsorted_segment_sum` has been merged into the top-of-tree.

The pre-existing, nondeterministic op enforces validity of the `labels` input by raising an exception when running on CPU, or injecting NaNs into the forward and backwards paths associated with the illegal entries in `labels` when running on a GPU. This functionality is [documented](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). The initial state of this PR emulates the original on-GPU functionality (injecting NaNs) when the new deterministic functionality is running on either a CPU or a GPU. There is also the beginnings of a python-level emulation of the on-CPU functionality (throwing an exception) in case we decide to complete that implementation. Emulating the on-CPU functionality at the python-level seems very hacky (the way I started doing it) and I'm not sure that's it's possible. The emulation of the on-GPU functionality at the python-level is solid but adds a reasonable amount of compute. Let's discuss.

Note that there is currently no `tf.float64` GPU implementation of this op.

I was not able to make the determinism test exercise the pre-existing nondeterminism in the forward direction for this op when `logits` is of type `tf.float16` because of the way `logits` is up-sampled to 32-bits in `nn_ops.py`. I'm not sure if it's ever possible to exercise that potential nondeterminism. In other words, the pre-existing GPU implementation may always operate deterministically in the forward direction when `logits` is of type `tf.float16`. This is mostly moot, of course (at least for training), since the op is a loss function and the gradients back to `logits` are still nondeterministic (for the original GPU implementation) when `logits` is of type `tf.float16`.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent).

cc @reedwm @sanjoy @nluehr ",tensorflow
50098,freedomtan,pr,2021-06-05T01:03:09Z,[tflite] make tflite related stuff build again on non-NNAPI platform,"The NNAPI SL patch 4b949de introduced using of `CreateNnApiFromSupportLibrary(NnApiSLDriverImplFL5 const*)` which caused a problem.

When running `bazel build --config opt tensorflow/lite/tools/benchmark:benchmark_model` on macOS,  I got:
```
Undefined symbols for architecture x86_64:
  ""CreateNnApiFromSupportLibrary(NnApiSLDriverImplFL5 const*)"", referenced from:
      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)
      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

To fix this, I created a simple dummy function

",tensorflow
50135,duncanriach,pr,2021-06-08T02:30:49Z,[determinism] Factor core/kernels RequireDeterminism() into library,"This PR performs a few housekeeping tasks related to GPU-determinism, the most significant of which is factoring various instances of the `RequireDeterminism` function in `core/kernels` into (the previously-implemented) `core/kernels/util/determinism.cc/.h` (as `OpDeterminismRequired`), a follow-up to [this conversation](https://github.com/tensorflow/tensorflow/pull/47772#discussion_r594847461) with @sanjoy on PR [47772](https://github.com/tensorflow/tensorflow/pull/47772).

cc @reedwm @nluehr",tensorflow
50355,duncanriach,pr,2021-06-19T00:09:40Z,[determinism] Add d9m-unimplemented exceptions to sparse/sparse matmul,"This current PR is a follow-on to closed PR [47749](https://github.com/tensorflow/tensorflow/pull/47749) (Add GPU determinism for fp types in GPU SparseTensorDenseMatMul). This current PR adds and tests determinism-unimplemented exception-throwing for `tf.sparse.sparse_dense_matmul` when is running on a GPU. This current PR also adds tests for the bit-exact reproducibility (determinism) of `tf.sparse.sparse_dense_matmul` (forward direction only, not gradients) when running a CPU.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#sparse-dense-matmul).

CC @reedwm, @sanjoy, @nluehr",tensorflow
50505,duncanriach,pr,2021-06-29T03:03:06Z,[determinism] Add d9m-unimplemented exception-throwing to fused batch-norm,"This PR adds determinism-unimplemented exception-throwing to `tf.compat.v1.nn.fused_batch_norm`. If an attempt is made to run the GPU kernel that implements backprop to `x`, `scale`, or `offset` when `is_training=False` (when fine-tuning) and when determinism is expected (i.e. when `TF_DETERMINISTIC_OPS` is set to `""true""` or `""1""`), then a `tf.errors.UnimplementedError` will be thrown.

This PR tests that the exception is thrown as appropriate and also tests that the other paths through the op (on both CPU and GPU) operate deterministically.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#fused-batch-norm).

CC @reedwm, @sanjoy, @nluehr",tensorflow
50640,duncanriach,pr,2021-07-07T02:49:00Z,[determinism] Enhance r2.6 release notes,"This PR enhances `RELEASE.md` in the r2.6 branch by including info about new op-determinism features.

@reedwm @sanjoy @nluehr ",tensorflow
50759,duncanriach,pr,2021-07-14T01:24:39Z,[determinism] Fix line-wrap in r2.6 release notes,This PR is a follow-up to PR [50640](https://github.com/tensorflow/tensorflow/pull/50640). It removes the erroneously included 80-character line-breaks in the content added to `RELEASE.md`. This was prompted by noticing that the formatting was wrong in the [v2.6.0-rc1](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0-rc1) release notes web page.,tensorflow
50874,elfringham,pr,2021-07-21T13:34:55Z,Allow use of non-system compiler to build mlir_generated tests,"Allow setting of LD_LIBRARY_PATH to reach execution environment of tf_to_kernel so it can load correct version of libstdc++ and so avoid build failure when built with non-system gcc
Fixes #50873 ",tensorflow
50976,guillaumekln,pr,2021-07-27T14:08:59Z,Add missing closing backtick in parse_sequence_example docstring,,tensorflow
51023,duncanriach,pr,2021-07-29T02:49:24Z,[determinism] Add unimplemented exception to nearest-neighbor resizing,"This current PR adds and tests determinism-unimplemented exception-throwing for `tf.image.resize` when `method=ResizeMethod.NEAREST` and when its GPU-implemented backprop code-path is executed.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#nearest-neighbor-image-resizing).

CC @reedwm, @sanjoy, @nluehr",tensorflow
51028,qingyunqu,pr,2021-07-29T09:16:04Z,Add hlo to mhlo logistic translation,"Add `hlo-to-mhlo` logistic op translation.
cc @byronyi ",tensorflow
51140,duncanriach,pr,2021-08-04T03:41:44Z,[determinism] Add unimplemented exception to tf.image.adjust_contrast,"This current PR adds and tests determinism-unimplemented exception-throwing for `tf.image.adjust_contrast`.

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#adjust-contrast).

CC @reedwm, @sanjoy, @nluehr",tensorflow
51373,freedomtan,pr,2021-08-08T03:34:23Z,[tflite] add same scale constraint to tflite's mean op,"Add SameOperandsAndResultsScal to tflite's mean. Without this constraint, post-training quantization ([PTQ](https://www.tensorflow.org/lite/performance/post_training_quantization)) may result in MEAN with different quantization scales for inputs and outputs. Because NNAPI only supports MEAN with same quantization parameters for inputs and output, this kind of mean ops could not be delegated to NNAPI via the NNAPI delegate.

E.g., if we do PTQ on the [ResNet 50 from tfhub](https://tfhub.dev/tensorflow/resnet_50/classification/1),
```python
import itertools
import os
import pathlib
import requests
import tarfile

import tensorflow as tf
import tensorflow_datasets as tfds

saved_model_dir = ""./resnet_saved_model/""
resnet_saved_model_file = ""https://tfhub.dev/tensorflow/resnet_50/classification/1?tf-hub-format=compressed""
response = requests.get(resnet_saved_model_file, stream=True)
file = tarfile.open(fileobj=response.raw, mode=""r|gz"")
file.extractall(path=saved_model_dir)

imagenet_validation = tfds.load(name=""imagenet2012"", split=""validation"")

def representative_data_gen():
  for imagenet_example in imagenet_validation.take(100):
    image, label = imagenet_example[""image""], imagenet_example[""label""]
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.central_crop(image, central_fraction=0.875)
    image = tf.expand_dims(image, 0)
    image = tf.image.resize(image, (224,224))
    yield [image]

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8 
converter.inference_output_type = tf.int8 
tflite_quant_model = converter.convert()

tflite_quant_model_file = pathlib.Path('/tmp')/""resnet50_quant.tflite""
tflite_quant_model_file.write_bytes(tflite_quant_model)
```
then `adb push /tmp/resnet50_quant.tflite /data/local/tmp/`, and run `benchmark_model` on device (here I ran it on a Pixel 4)
```
$ ./benchmark_model_validation --graph=resnet50_qnant.tflite   --use_nnapi=1 --enable_op_profiling=1

```
It shows something like:
```
STARTING!
Log parameter values verbosely: [0]
Graph: [resnet_v15_quant_original_mean.tflite]
Enable op profiling: [1]
Use NNAPI: [1]
NNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,google-edgetpu,nnapi-reference]
Loaded model resnet50_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
WARNING: Operator MEAN (v2) refused by NNAPI delegate: NNAPI requires that the input and output have the same quantization parameters.
INFO: Replacing 75 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 3 partitions.
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 2 delegate kernels.
.......
Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	            0.009	   14.431	   14.449	 48.545%	 48.545%	     0.000	        1	[resnet50/activation_48/Relu;resnet50/add_15/add]:76
	                    MEAN	           14.460	   13.399	   13.501	 45.359%	 93.904%	     0.000	        1	[resnet50/reduce_mean/Mean]:73
	     TfLiteNnapiDelegate	           27.962	    1.754	    1.814	  6.096%	100.000%	     0.000	        1	[StatefulPartitionedCall:0]:77
.......
Number of nodes executed: 3
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteNnapiDelegate	        2	    16.263	    54.642%	    54.642%	     0.000	        2
	                    MEAN	        1	    13.500	    45.358%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=29584 curr=22585 min=22585 max=33233 avg=29764 std=1779
Memory (bytes): count=0
3 nodes observe
```
With this constraint applied, for the same ResNet 50 model, I got something like the following:
```
....
Number of nodes executed: 1
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteNnapiDelegate	        1	    14.762	   100.000%	   100.000%	     0.000	        1

Timings (microseconds): count=67 first=14774 curr=15087 min=13959 max=15226 avg=14762.4 std=248
Memory (bytes): count=0
1 nodes observed
```",tensorflow
51470,micmelesse,pr,2021-08-12T19:05:32Z,[ROCM] enable some multigpu tests,This PR enables some of the multi-gpu tests that were disabled in a previous pr https://github.com/tensorflow/tensorflow/pull/50331,tensorflow
51481,freedomtan,pr,2021-08-13T08:04:36Z,[tflite] add same scale constraint for mirror pad,"add `SameOperandsAndResultsScale` to MirrorPad so that when the new MLIR quantizer is used for post-training quantization, all
quantized mirror_pad will have the same quantization scaled for inputs and outputs (as what the old non-MLIR one does).

cf. the constrain in old [quantizer](https://github.com/tensorflow/tensorflow/blob/r2.6/tensorflow/lite/tools/optimize/operator_property.cc#L1020-L1026)",tensorflow
51706,freedomtan,pr,2021-08-27T01:24:05Z,[tflite] fix default of xnnpack_delegate_provider,"xnnpack delegate was enabled by default on Android ARM platforms in 0b78d40a54a991e0f2b67e2a9aa5224609536552. But the corresponding default setting of xnnpack delegate provider is not changed, so users of xnnpack delegate provider, e.g., benchmark_model doesn't get right value. E.g., when `benchmark_model --help`, I got

```
...
	--use_xnnpack=true                       	bool	optional	use XNNPack

```

But actually, it uses tflite with xnnpack enabled already. That's a bit confusing.


tag @multiverse-tf who is the author of 0b78d40",tensorflow
51735,powderluv,pr,2021-08-28T22:59:50Z,Add missing deps for Legalize to Linalg,"When compiling IREE baremetal I ran into mhlo_passes.h.inc file not found
[ 69%] Built target MhloTypeConversion
[ 69%] Building CXX object third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/legalize_to_linalg.cc.o
In file included from /home/foo/github/iree-bare-metal-arm/third_party/iree/third_party/mlir-hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc:24:
/home/foo/github/iree-bare-metal-arm/third_party/iree/third_party/mlir-hlo/include/mlir-hlo/Dialect/mhlo/transforms/PassDetail.h:32:10: fatal error: 'mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc' file not found
#include ""mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
make[2]: *** [third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/build.make:63: third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/legalize_to_linalg.cc.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:52089: third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/all] Error 2
make: *** [Makefile:152: all] Error 2



Fix is similar to https://github.com/tensorflow/tensorflow/pull/51085/

TEST: Builds after the fix",tensorflow
51739,SamuelMarks,pr,2021-08-30T00:03:04Z,Add calls to `reserve()` before populating vectors,"â€¦and am at capacity

PS: WiP. Will finish going through you codebase adding capacity hints to all vectors with obvious opportunity for this optimisation.",tensorflow
51806,cbalint13,pr,2021-09-02T20:45:20Z,Fix abseil compile error using proper HWAES flag on aarch64.,"This small PR fixes a compile error on aarch64.

   - It backports a future [absl upstream fix](https://github.com/abseil/abseil-cpp/commit/2e94e5b6e152df9fa9c2fe8c1b96e1393973d32c) of the issue
   -  Fixes compilation [reported in the issue #51750](https://github.com/tensorflow/tensorflow/issues/51750#issuecomment-910717534) 
   
Other way to address the issue is to rise up [abseil requirement](https://github.com/tensorflow/tensorflow/blob/master/third_party/absl/workspace.bzl#L10) to the minimum of this [commit hash](https://github.com/abseil/abseil-cpp/commit/2e94e5b6e152df9fa9c2fe8c1b96e1393973d32c).

Thank you !",tensorflow
51844,qingyunqu,pr,2021-09-06T03:15:40Z,Add mlir-hlo unfuse-batch-norm-training pattern,"Add mlir-hlo unfuse-batch-norm-training pattern.  
Should I add dynamic shape support?",tensorflow
51920,duncanriach,pr,2021-09-10T03:29:25Z,[determinism] Add d9m-unimplemented exception for tf.nn.depthwise_conv2d,"This current PR adds and tests determinism-unimplemented exception-throwing for `tf.nn.depthwise_conv2d` when determinism is expected but not available. It also tests that the op functions deterministically when determinism is expected and is available (forward/backward CPU and forward/backward GPU using cuDNN).

This PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#depthwise-convolution).

I believe that when this op uses cuDNN convolution functionality, it benefits from all the determinism solutions associated with that, including deterministic algorithm selection (assuming that still works).

CC @reedwm, @sanjoy, @nluehr",tensorflow
52099,micmelesse,pr,2021-09-22T16:52:47Z,[ROCM] enable lu_op (#1433),This prs the lu_op on rocm. ,tensorflow
52100,micmelesse,pr,2021-09-22T16:59:37Z,enable cholesky_op (#1449),This enables the cholesky op on ROCM,tensorflow
52171,elfringham,pr,2021-09-28T16:31:46Z,Provide pbroadcast functions for Neon,Fixes https://github.com/tensorflow/tensorflow/issues/52164,tensorflow
52227,duncanriach,pr,2021-10-02T05:19:46Z,[determinism] Add tests for tf.nn.ctc_loss,"Adds deterministic testing for `tf.nn.ctc_loss`, forwards and backwards on CPU and GPU.

This pull request resolves issue #[38151](https://github.com/tensorflow/tensorflow/issues/38151).

Note that in TensorFlow version 2.3 there was a nondeterminism issue that seems to have now been resolved, in TensorFlow version 2.6, if not earlier.

This pull request is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#ctc-loss).

CC @reedwm, @sanjoy, @nluehr",tensorflow
52241,qingyunqu,pr,2021-10-03T19:21:58Z,Add mlir-hlo unfuse-batch-norm-training pattern,"Add mlir-hlo unfuse-batch-norm-training pattern.
Question when adding dynamic shape: how can I convert `index` to `tensor<f32>`? I need to convert the dynamic reduction size to a `tensor` value. @joker-eph",tensorflow
52334,nluehr,pr,2021-10-11T20:20:26Z,Fixes for various tests on Ampere GPUs,"Fixes various tests mostly related to TF32 execution on Ampere.

CC: @reedwm ",tensorflow
52367,micmelesse,pr,2021-10-13T15:48:21Z,[ROCM] enable QR_op on ROCM,This PR enables the QR op on the rocm platform.,tensorflow
52393,SamuelMarks,pr,2021-10-15T03:23:53Z,[tensorflow/lite/toco/graph_transformations/unroll_batch_matmul.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52394,SamuelMarks,pr,2021-10-15T03:24:19Z,[tensorflow/core/kernels/matmul_op_impl.h] Use correct size type for `batch_size`,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52395,SamuelMarks,pr,2021-10-15T03:24:32Z,[tensorflow/compiler/xrt/tests/raw_api_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52396,SamuelMarks,pr,2021-10-15T03:24:46Z,[tensorflow/compiler/xrt/kernels/xrt_execute_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52397,SamuelMarks,pr,2021-10-15T03:25:03Z,[tensorflow/compiler/xla/tests/vector_ops_simple_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52398,SamuelMarks,pr,2021-10-15T03:25:09Z,[tensorflow/compiler/xla/tests/transfer_manager_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52399,SamuelMarks,pr,2021-10-15T03:25:16Z,[tensorflow/compiler/xla/tests/test_utils.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52400,SamuelMarks,pr,2021-10-15T03:25:21Z,[tensorflow/compiler/xla/tests/select_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52401,SamuelMarks,pr,2021-10-15T03:25:33Z,[tensorflow/compiler/xla/tests/reduce_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52402,SamuelMarks,pr,2021-10-15T03:25:41Z,[tensorflow/compiler/xla/tests/params_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52403,SamuelMarks,pr,2021-10-15T03:25:51Z,[tensorflow/compiler/xla/tests/local_client_execute_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52404,SamuelMarks,pr,2021-10-15T03:26:01Z,[tensorflow/compiler/xla/tests/half_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52405,SamuelMarks,pr,2021-10-15T03:26:10Z,[tensorflow/compiler/xla/tests/dot_operation_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52406,SamuelMarks,pr,2021-10-15T03:26:25Z,[tensorflow/compiler/xla/tests/collective_ops_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52407,SamuelMarks,pr,2021-10-15T03:26:46Z,[tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52408,SamuelMarks,pr,2021-10-15T03:27:10Z,[tensorflow/compiler/xla/shape_util.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52409,SamuelMarks,pr,2021-10-15T03:27:27Z,[tensorflow/compiler/xla/shape_tree_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52410,SamuelMarks,pr,2021-10-15T03:27:34Z,[tensorflow/compiler/xla/service/while_loop_concat_code_motion.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52411,SamuelMarks,pr,2021-10-15T03:27:42Z,[tensorflow/compiler/xla/service/while_loop_all_reduce_code_motion.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52412,SamuelMarks,pr,2021-10-15T03:27:50Z,[tensorflow/compiler/xla/service/tuple_points_to_analysis_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52413,SamuelMarks,pr,2021-10-15T03:28:03Z,[tensorflow/compiler/xla/service/spmd/spmd_partitioner_util.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52414,SamuelMarks,pr,2021-10-15T03:28:19Z,[tensorflow/compiler/xla/service/spmd/spmd_partitioner.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52415,SamuelMarks,pr,2021-10-15T03:28:30Z,[tensorflow/compiler/xla/service/spmd/dot_handler.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52416,SamuelMarks,pr,2021-10-15T03:28:39Z,[tensorflow/compiler/xla/service/space_to_batch_converter.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52417,SamuelMarks,pr,2021-10-15T03:28:49Z,[tensorflow/compiler/xla/service/sharding_propagation.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52418,SamuelMarks,pr,2021-10-15T03:29:02Z,[tensorflow/compiler/xla/service/shape_inference.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52419,SamuelMarks,pr,2021-10-15T03:29:11Z,[tensorflow/compiler/xla/service/service.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52420,SamuelMarks,pr,2021-10-15T03:29:23Z,[tensorflow/compiler/xla/service/layout_assignment.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52421,SamuelMarks,pr,2021-10-15T03:29:32Z,[tensorflow/compiler/xla/service/interpreter/executable_base.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52422,SamuelMarks,pr,2021-10-15T03:29:40Z,[tensorflow/compiler/xla/service/hlo_verifier_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52423,SamuelMarks,pr,2021-10-15T03:29:51Z,[tensorflow/compiler/xla/service/hlo_sharding_util.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52424,SamuelMarks,pr,2021-10-15T03:30:05Z,[tensorflow/compiler/xla/service/hlo_sharding.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52425,SamuelMarks,pr,2021-10-15T03:30:25Z,[tensorflow/compiler/xla/service/hlo_runner.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52426,SamuelMarks,pr,2021-10-15T03:30:35Z,[tensorflow/compiler/xla/service/hlo_module_group_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52427,SamuelMarks,pr,2021-10-15T03:30:45Z,[tensorflow/compiler/xla/service/hlo_instructions.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52428,SamuelMarks,pr,2021-10-15T03:31:05Z,[tensorflow/compiler/xla/service/hlo_instruction.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52429,SamuelMarks,pr,2021-10-15T03:31:14Z,[tensorflow/compiler/xla/service/hlo_element_type_converter.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52430,SamuelMarks,pr,2021-10-15T03:31:31Z,[tensorflow/compiler/xla/service/hlo_dataflow_analysis_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52431,SamuelMarks,pr,2021-10-15T03:31:42Z,[tensorflow/compiler/xla/service/hlo_buffer.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52432,SamuelMarks,pr,2021-10-15T03:31:57Z,[tensorflow/compiler/xla/service/hlo_alias_analysis_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52433,SamuelMarks,pr,2021-10-15T03:32:08Z,[tensorflow/compiler/xla/service/hlo_alias_analysis.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52434,SamuelMarks,pr,2021-10-15T03:32:22Z,[tensorflow/compiler/xla/service/gpu/tests/parallel_reduction_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52435,SamuelMarks,pr,2021-10-15T03:32:34Z,[tensorflow/compiler/xla/service/gpu/tests/mlir_gpu_test_base.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52436,SamuelMarks,pr,2021-10-15T03:32:47Z,[tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52437,SamuelMarks,pr,2021-10-15T03:33:02Z,[tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52438,SamuelMarks,pr,2021-10-15T03:33:08Z,[tensorflow/compiler/xla/service/gpu/ir_emitter_nested.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52439,SamuelMarks,pr,2021-10-15T03:33:14Z,[tensorflow/compiler/xla/service/gpu/ir_emission_utils.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52440,SamuelMarks,pr,2021-10-15T03:33:19Z,[tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52441,SamuelMarks,pr,2021-10-15T03:33:24Z,[tensorflow/compiler/xla/service/elemental_ir_emitter.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52442,SamuelMarks,pr,2021-10-15T03:33:30Z,[tensorflow/compiler/xla/service/dynamic_padder.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52443,SamuelMarks,pr,2021-10-15T03:33:39Z,[tensorflow/compiler/xla/service/dynamic_index_splitter.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52444,SamuelMarks,pr,2021-10-15T03:33:50Z,[tensorflow/compiler/xla/service/dynamic_dimension_inference.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52445,SamuelMarks,pr,2021-10-15T03:33:59Z,[tensorflow/compiler/xla/service/cpu/vector_support_library.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52446,SamuelMarks,pr,2021-10-15T03:34:06Z,[tensorflow/compiler/xla/service/cpu/tiled_dot_emitter.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52447,SamuelMarks,pr,2021-10-15T03:34:24Z,[tensorflow/compiler/xla/service/cpu/ir_emitter.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52448,SamuelMarks,pr,2021-10-15T03:34:31Z,[tensorflow/compiler/xla/service/cpu/cpu_xfeed.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52449,SamuelMarks,pr,2021-10-15T03:34:50Z,[tensorflow/c/c_api.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52450,SamuelMarks,pr,2021-10-15T03:34:57Z,[tensorflow/c/c_api_function_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52451,SamuelMarks,pr,2021-10-15T03:35:05Z,[tensorflow/c/c_test_util.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52452,SamuelMarks,pr,2021-10-15T03:35:10Z,[tensorflow/c/eager/gradients.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52453,SamuelMarks,pr,2021-10-15T03:35:19Z,[tensorflow/c/experimental/filesystem/plugins/gcs/ram_file_block_cache_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52454,SamuelMarks,pr,2021-10-15T03:35:38Z,[tensorflow/c/tf_tensor.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52455,SamuelMarks,pr,2021-10-15T03:35:45Z,[tensorflow/cc/gradients/array_grad.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52456,SamuelMarks,pr,2021-10-15T03:35:53Z,[tensorflow/cc/gradients/functional_grad.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52457,SamuelMarks,pr,2021-10-15T03:36:00Z,[tensorflow/cc/ops/while_loop.cc] Use matching type for loop iterators,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52458,SamuelMarks,pr,2021-10-15T03:36:09Z,[tensorflow/cc/saved_model/saved_model_bundle_lite_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52459,SamuelMarks,pr,2021-10-15T03:36:14Z,[tensorflow/cc/saved_model/saved_model_bundle_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52460,SamuelMarks,pr,2021-10-15T03:36:22Z,[tensorflow/compiler/jit/compilability_check_util.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52461,SamuelMarks,pr,2021-10-15T03:36:29Z,[tensorflow/compiler/jit/deadness_analysis.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52462,SamuelMarks,pr,2021-10-15T03:36:36Z,[tensorflow/compiler/jit/encapsulate_subgraphs_pass_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52463,SamuelMarks,pr,2021-10-15T03:36:59Z,[tensorflow/compiler/jit/extract_outside_compilation_pass.cc] Fold two loops into one; add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52464,SamuelMarks,pr,2021-10-15T03:37:37Z,[tensorflow/compiler/jit/increase_dynamism_for_auto_jit_pass.cc] Use correct size type for `NumElements` iterator,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52465,SamuelMarks,pr,2021-10-15T03:37:46Z,[tensorflow/compiler/jit/test_util.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52466,SamuelMarks,pr,2021-10-15T03:37:52Z,[tensorflow/compiler/mlir/hlo/lib/Analysis/userange_analysis.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52467,SamuelMarks,pr,2021-10-15T03:38:15Z,[tensorflow/compiler/mlir/python/mlir.cc] Use correct size type for `node_names.size()` iterator,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52468,SamuelMarks,pr,2021-10-15T03:38:29Z,[tensorflow/compiler/mlir/tensorflow/transforms/lower_tf.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52469,SamuelMarks,pr,2021-10-15T03:38:38Z,[tensorflow/compiler/mlir/tensorflow/utils/convert_tensor.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52470,SamuelMarks,pr,2021-10-15T03:38:48Z,[tensorflow/compiler/mlir/tfr/integration/tfr_decompose_ctx_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52471,SamuelMarks,pr,2021-10-15T03:38:57Z,[tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52472,SamuelMarks,pr,2021-10-15T03:39:03Z,[tensorflow/compiler/mlir/xla/experimental/conv_emitter/conv_emitter.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52473,SamuelMarks,pr,2021-10-15T03:39:15Z,[tensorflow/compiler/mlir/xla/ir/mlir_hlo_builder.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52474,SamuelMarks,pr,2021-10-15T03:39:29Z,[tensorflow/compiler/mlir/xla/transforms/adjust_layout.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52475,SamuelMarks,pr,2021-10-15T03:39:37Z,[tensorflow/compiler/mlir/xla/transforms/legalize_tf.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52476,SamuelMarks,pr,2021-10-15T03:40:07Z,[tensorflow/compiler/tests/randomized_tests.cc] Reduce function calls; add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52477,SamuelMarks,pr,2021-10-15T03:40:14Z,[tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc] Add calls to `reserve()` before populating vector,"Add calls to reserve vector capacity before populating vector.

This is split from PR https://github.com/tensorflow/tensorflow/pull/51739.",tensorflow
52478,SamuelMarks,pr,2021-10-15T03:40:22Z,[tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc] Add calls to `reserve()` before populating vectors,"Add calls to reserve vector capacity before populating vector.

This is split from PR https://github.com/tensorflow/tensorflow/pull/51739.",tensorflow
52479,SamuelMarks,pr,2021-10-15T03:40:32Z,[tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc] Add calls to `reserve()` before populating vector,"Add calls to reserve vector capacity before populating vector.

This is split from PR https://github.com/tensorflow/tensorflow/pull/51739.",tensorflow
52480,SamuelMarks,pr,2021-10-15T03:40:40Z,[tensorflow/compiler/tf2tensorrt/kernels/trt_engine_resource_ops.cc] Add calls to `reserve()` before populating vector,"Add calls to reserve vector capacity before populating vector.

This is split from PR https://github.com/tensorflow/tensorflow/pull/51739.",tensorflow
52481,SamuelMarks,pr,2021-10-15T03:40:48Z,[tensorflow/compiler/tf2xla/functionalize_while.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52482,SamuelMarks,pr,2021-10-15T03:40:54Z,[tensorflow/compiler/tf2xla/kernels/case_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52483,SamuelMarks,pr,2021-10-15T03:41:02Z,[tensorflow/compiler/tf2xla/kernels/cross_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52484,SamuelMarks,pr,2021-10-15T03:41:12Z,[tensorflow/compiler/tf2xla/kernels/dynamic_partition_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52485,SamuelMarks,pr,2021-10-15T03:41:18Z,[tensorflow/compiler/tf2xla/kernels/dynamic_stitch_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52486,SamuelMarks,pr,2021-10-15T03:41:31Z,[tensorflow/compiler/tf2xla/kernels/reduction_ops_common.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52487,SamuelMarks,pr,2021-10-15T03:42:23Z,[tensorflow/compiler/tf2xla/kernels/resampler_ops.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52488,SamuelMarks,pr,2021-10-15T03:42:33Z,[tensorflow/compiler/tf2xla/kernels/reshape_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52489,SamuelMarks,pr,2021-10-15T03:42:43Z,[tensorflow/compiler/tf2xla/kernels/strided_slice_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52490,SamuelMarks,pr,2021-10-15T03:42:50Z,[tensorflow/compiler/tf2xla/kernels/tensor_list_ops.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52491,SamuelMarks,pr,2021-10-15T03:42:57Z,[tensorflow/compiler/tf2xla/kernels/while_op.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52492,SamuelMarks,pr,2021-10-15T03:43:08Z,[tensorflow/compiler/tf2xla/lib/data_format.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52493,SamuelMarks,pr,2021-10-15T03:43:20Z,[tensorflow/compiler/tf2xla/tf2xla_supported_ops.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52494,SamuelMarks,pr,2021-10-15T03:43:32Z,[tensorflow/compiler/tf2xla/xla_compiler.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52495,SamuelMarks,pr,2021-10-15T03:43:38Z,[tensorflow/compiler/xla/client/client.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52496,SamuelMarks,pr,2021-10-15T03:43:46Z,[tensorflow/compiler/xla/client/lib/approx_topk.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52497,SamuelMarks,pr,2021-10-15T03:44:03Z,[tensorflow/compiler/xla/client/lib/prng.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52498,SamuelMarks,pr,2021-10-15T03:44:19Z,[tensorflow/compiler/xla/client/lib/quantize_test.cc] Reduce function calls; add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52499,SamuelMarks,pr,2021-10-15T03:44:29Z,[tensorflow/compiler/xla/client/lib/slicing.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52500,SamuelMarks,pr,2021-10-15T03:44:40Z,[tensorflow/compiler/xla/client/lib/testing.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52501,SamuelMarks,pr,2021-10-15T03:44:48Z,[tensorflow/compiler/xla/client/value_inference.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52502,SamuelMarks,pr,2021-10-15T03:45:03Z,[tensorflow/compiler/xla/client/xla_builder.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52503,SamuelMarks,pr,2021-10-15T03:45:15Z,[tensorflow/compiler/xla/client/xla_builder_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52504,SamuelMarks,pr,2021-10-15T03:45:22Z,[tensorflow/compiler/xla/layout.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52505,SamuelMarks,pr,2021-10-15T03:45:32Z,[tensorflow/compiler/xla/literal.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52506,SamuelMarks,pr,2021-10-15T03:45:38Z,[tensorflow/compiler/xla/literal_util.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52507,SamuelMarks,pr,2021-10-15T03:45:46Z,[tensorflow/compiler/xla/pjrt/utils.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52508,SamuelMarks,pr,2021-10-15T03:45:52Z,[tensorflow/compiler/xla/python/ops.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52509,SamuelMarks,pr,2021-10-15T03:46:37Z,[tensorflow/compiler/xla/python/tpu_driver/pod_tpu_driver.cc] Use correct size type for `children_ids_size` iterator; Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52510,SamuelMarks,pr,2021-10-15T03:46:49Z,[tensorflow/compiler/xla/python/tpu_driver/recording_tpu_driver.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52511,SamuelMarks,pr,2021-10-15T03:46:57Z,[tensorflow/compiler/xla/service/algebraic_simplifier.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52512,SamuelMarks,pr,2021-10-15T03:47:10Z,[tensorflow/compiler/xla/service/algebraic_simplifier_test.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52513,SamuelMarks,pr,2021-10-15T03:47:20Z,[tensorflow/compiler/xla/service/allocation_tracker.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52514,SamuelMarks,pr,2021-10-15T03:47:27Z,[tensorflow/compiler/xla/service/ar_crs_combiner.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52515,SamuelMarks,pr,2021-10-15T03:47:36Z,[tensorflow/compiler/xla/service/batchnorm_expander.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52516,SamuelMarks,pr,2021-10-15T03:47:42Z,[tensorflow/compiler/xla/service/computation_layout.cc] Add calls to `reserve()` before populating vector,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52518,nSircombe,pr,2021-10-15T10:26:21Z,Updates config=mkl_aarch64 build to oneDNN 2.4 and Compute Library 21.08,"Updates oneDNN version used for `--config=mkl_aarch64` to 2.4
in line with x86 build.

Updates Compute Library (used as optimised oneDNN backend
on AArch64) to the latest release (21.08).

This enables support for SVE and BF16 kernels where
hardware supports.
This provides a performance uplift of between 1.4x and 3x
depending on the hardware support.

Removes some patches to oneDNN which are not required for
the 2.4 release.

NOTE: AArch64 build with --config=mkl_aarch64 requires GCC 10.

Change-Id: Iaf7562da729fc928dce922d33c02b8bb0ce74767",tensorflow
52522,SamuelMarks,pr,2021-10-15T20:58:45Z,"[tensorflow/compiler/{mlir,tf2xla,xla}/**/*.cc] Make tuple_shapes_size calls and comparators to match actual return type","The correct type is `int`, as per: https://github.com/tensorflow/tensorflow/blob/0b6b491/tensorflow/compiler/xla/shape.h#L136",tensorflow
52523,SamuelMarks,pr,2021-10-15T21:28:06Z,[tensorflow/compiler/xla/client/lib/comparators_test.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52524,SamuelMarks,pr,2021-10-15T21:29:27Z,[tensorflow/compiler/xla/service/convolution_group_converter.cc] Add calls to `reserve()` before populating vectors,"https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-940389562 told me to split the larger PR into one PR per file; thus this (thanks `bash`, `git` and `gh`!)",tensorflow
52532,SamuelMarks,pr,2021-10-17T01:33:48Z,[tensorflow/compiler/xla/**/*.cc] Add calls to `reserve()` before populating vectors,https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-945027209 told me to merge into one PR per 'large module/namespace',tensorflow
52614,SamuelMarks,pr,2021-10-22T02:55:20Z,"[tensorflow/{compiler,core}/**/*.cc] Use `int` for `tuple_shapes_size` to match implementation in ""shape.h""","Related: #52522
> The correct type is `int`, as per: https://github.com/tensorflow/tensorflow/blob/0b6b491/tensorflow/compiler/xla/shape.h#L136
",tensorflow
52643,freedomtan,pr,2021-10-24T09:22:31Z,[tflite] make op profiling work when CoreML Delegate is used,"- issue:
When running CoreML delegate + op profiling, I met bus error most of the time.
- how to reproduce: on MacOS, 
```
benchmark_mode --graph=.. --use_coreml=1 --enable_op_profiling=1 ...
```
on iOS devices, add

```
use_coreml : ""true"", 
enable_op_profiling : ""true"", 
``` 

to  `benchmark_params.json`  of  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios.

The model I used to test this problem is [MobilenetEdgeTPU](https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_1.0.tgz).

- when it went wrong:
```
% bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite --enable_op_profiling=1 --use_coreml=1
STARTING!
Log parameter values verbosely: [0]
Graph: [/tmp/mobilenet_edgetpu_224_1.0_float.tflite]
Enable op profiling: [1]
Use CoreML: [1]
Loaded model /tmp/mobilenet_edgetpu_224_1.0_float.tflite
2021-10-24 20:34:08.503 benchmark_model[42608:216982] coreml_version must be 2 or 3. Setting to 3.
COREML delegate created.
INFO: CoreML delegate: 75 nodes delegated out of 77 nodes, with 2 partitions.

Explicitly applied COREML delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
The input model file size (MB): 16.3221
Initialized session in 568.672ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=262 first=3849 curr=1887 min=956 max=11424 avg=1888.11 std=1682

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
zsh: bus error  bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model   --use_coreml=1

```
After adding `profiling_string` initialization
```
% bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite --enable_op_profiling=1 --use_coreml=1
STARTING!
Log parameter values verbosely: [0]
Graph: [/tmp/mobilenet_edgetpu_224_1.0_float.tflite]
Enable op profiling: [1]
Use CoreML: [1]
Loaded model /tmp/mobilenet_edgetpu_224_1.0_float.tflite
2021-10-24 20:38:10.823 benchmark_model[45038:226402] coreml_version must be 2 or 3. Setting to 3.
COREML delegate created.
INFO: CoreML delegate: 75 nodes delegated out of 77 nodes, with 2 partitions.

Explicitly applied COREML delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
The input model file size (MB): 16.3221
Initialized session in 590.552ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=261 first=2610 curr=8604 min=973 max=10389 avg=1908.45 std=1752

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=407 first=2088 curr=5167 min=955 max=11235 avg=2388.13 std=2227

Inference timings in us: Init: 590552, First inference: 2610, Warmup (avg): 1908.45, Inference (avg): 2388.13
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	  576.714	  576.714	 99.988%	 99.988%	     0.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	          576.697	    0.066	    0.034	  0.012%	100.000%	     0.000	        2	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	  576.714	  576.714	 99.988%	 99.988%	     0.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	          576.697	    0.066	    0.034	  0.012%	100.000%	     0.000	        2	AllocateTensors/0

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	 ModifyGraphWithDelegate	        1	   576.714	    99.988%	    99.988%	     0.000	        1
	         AllocateTensors	        1	     0.067	     0.012%	   100.000%	     0.000	        2

Timings (microseconds): count=1 curr=576781
Memory (bytes): count=0
2 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	    TfLiteCoreMlDelegate	            0.000	    2.074	    2.379	 99.710%	 99.710%	     0.000	        1	[MobilenetEdgeTPU/Logits/Conv2d_1c_1x1/BiasAdd]:77
	                 RESHAPE	            2.380	    0.002	    0.001	  0.053%	 99.763%	     0.000	        1	[MobilenetEdgeTPU/Logits/Squeeze]:75
	                 SOFTMAX	            2.381	    0.008	    0.006	  0.237%	100.000%	     0.000	        1	[Softmax]:76

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	    TfLiteCoreMlDelegate	            0.000	    2.074	    2.379	 99.710%	 99.710%	     0.000	        1	[MobilenetEdgeTPU/Logits/Conv2d_1c_1x1/BiasAdd]:77
	                 SOFTMAX	            2.381	    0.008	    0.006	  0.237%	 99.947%	     0.000	        1	[Softmax]:76
	                 RESHAPE	            2.380	    0.002	    0.001	  0.053%	100.000%	     0.000	        1	[MobilenetEdgeTPU/Logits/Squeeze]:75

Number of nodes executed: 3
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	    TfLiteCoreMlDelegate	        1	     2.379	    99.748%	    99.748%	     0.000	        1
	                 SOFTMAX	        1	     0.005	     0.210%	    99.958%	     0.000	        1
	                 RESHAPE	        1	     0.001	     0.042%	   100.000%	     0.000	        1

Timings (microseconds): count=407 first=2084 curr=5164 min=955 max=11231 avg=2386.31 std=2227
Memory (bytes): count=0
3 nodes observed
```

- why it went wrong: in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter.h#L637-L642
The interpreter wanted to access non-null string, it profiling_string is not properly set :-(",tensorflow
52707,elfringham,pr,2021-10-27T15:58:42Z,Test for case of undefined behaviour and throw error if found,"Casting to a lower precision variable type is undefined behaviour if the value being cast overflows the new type.
Relying on the outcome of such a cast lead to differing outcomes on x86 and AARCH64 in the RangeTest.testLargeStarts unit test. Detect this case before it happens and throw an exception then correct the test case to accept the new exception. In the non-eager case an InvalidArgument exception is re-thrown as a ValueError so both exceptions should be allowed in the test.

Fixes https://github.com/tensorflow/tensorflow/issues/52676",tensorflow
52815,nluehr,pr,2021-10-28T20:14:04Z,Do not pass adjoint to real-valued CSR Sparse MatMul Op.,"Starting in CUDA 11.5, cusparseSpMV no longer supports performing
conjugate transposes on non-complex types. Previously, cublas would
implicitly interpret this as a simple tranpose.

This commit explicitly converts adjoint flags on real valued SpMV ops to
transposes, recovering the previous behavior.

CC: @sanjoy ",tensorflow
52911,elfringham,pr,2021-11-02T10:54:43Z,Restore missing comma to fix build,Fixes https://github.com/tensorflow/tensorflow/issues/52910,tensorflow
52966,elfringham,pr,2021-11-05T17:34:19Z,Allow doctest compare to work by removing unneeded white space,Fixes https://github.com/tensorflow/tensorflow/issues/52069,tensorflow
52971,duncanriach,pr,2021-11-06T04:17:09Z,Add op-determinism info to version 2.7 release notes,"This is a PR for the r2.7 branch.

This PR represents release notes that should have been included with version 2.7. I failed to get this completed before the release. I'm hoping that this information can be added to the release notes on GitHub and included in the master branch version of `RELEASE.md`.

Please will someone with experience in this area, such as @goldiegadde, direct a procedure for making sure this information lands in the correct place(s).

Before this information is propagated, please will @reedwm and @pkanwar23 review for correctness and completeness. Note that there is a ""TODO: confirm exception added"" that needs to be removed with respect to `tf.math.bincount`.

",tensorflow
53022,elfringham,pr,2021-11-10T14:06:32Z,Fix build of XLA unit tests on AARCH64,Fixes https://github.com/tensorflow/tensorflow/issues/53021,tensorflow
53035,elfringham,pr,2021-11-11T17:11:51Z,Fix definition of triplet for AARCH64,Fixes https://github.com/tensorflow/tensorflow/issues/53034,tensorflow
53068,elfringham,pr,2021-11-15T14:27:19Z,Tag vectorized_reduce_with_no_vector_registers to allow exclusion,"Fixes https://github.com/tensorflow/tensorflow/issues/53067
The tag can be used on the command line when building on or for AARCH64 platforms to exclude this test that is not applicable.",tensorflow
53131,elfringham,pr,2021-11-19T10:38:30Z,Fix fail to build on AARCH64 for mkl_matmul_op_benchmark,,tensorflow
53167,elfringham,pr,2021-11-23T12:20:56Z,Correct use of StringRef and ArrayRef,"Current code takes a reference to transient objects which go out of scope and get overwritten. This renders the references taken invalid and leads to incorrect operation and possible SIGSEGV.
The objects referred to by StringRef and ArrayRef need to have their own independent lifetime.
Fixes https://github.com/tensorflow/tensorflow/issues/53166",tensorflow
53177,yaochengji,pr,2021-11-24T00:09:28Z,Should use CMAKE_CURRENT_*_DIR,,tensorflow
53274,elfringham,pr,2021-12-01T14:42:11Z,Add tag to tests to allow exclusion on AARCH64,Fixes #53189 ,tensorflow
53323,elfringham,pr,2021-12-06T17:10:30Z,Add tolerance to ragged square root test so it passes on AARCH64,Fixes #53322 ,tensorflow
53329,SamuelMarks,pr,2021-12-07T04:03:21Z,[.clang-format] Init @ root,"Matches https://github.com/tensorflow/tflite-micro/blob/d1a6c79/.clang-format https://github.com/tensorflow/ngraph-bridge/blob/bad873a/.clang-format:
```yml
# Run manually to reformat a file:
# clang-format -i --style=file <file>
BasedOnStyle: Google
DerivePointerAlignment: false
```

Alternatively can match the [4 year old] one in the xla directory https://github.com/tensorflow/tensorflow/blob/1e67c90/tensorflow/compiler/xla/.clang-format:
```yml
BasedOnStyle: Google
Language: Cpp
PointerBindsToType: true

```",tensorflow
53330,SamuelMarks,pr,2021-12-07T04:17:15Z,[tensorflow/compiler/xla/python/types.h] add call to `reserve()` before populating vector,Exciting stuff,tensorflow
53369,nSircombe,pr,2021-12-09T10:21:48Z,Update Compute Library to 21.11 release,"Updates the version of Compute Library for the Arm architecture to 21.11
This release adds support for bf16 fast-maths-mode to additional
oneDNN primitives - inner product and matmul.

oneDNN 2.4 build is patched to expose this functionality to TensorFlow

`TF_ENABLE_ONEDNN_OPTS` flag (runtime selection of oneDNN backend) is
enabled on AArch64 for builds with `--config=mkl_aarch64` set.",tensorflow
53374,SamuelMarks,pr,2021-12-09T17:11:39Z,"[*.py,*.ipynb] blacken","Using version 21.12b0 from Python 3.10:

```sh
$ python -m black .
[â€¦]
All done! âœ¨ ðŸ° âœ¨
2767 files reformatted, 77 files left unchanged.
```

---

**Related**:

   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting
   - My recently merged PR: #53329
   - My #53360

**Disadvantages**:

  - Impact just about every file in the codebase
  - Require open PRs to be modified and forks to be updated

**Mitigation of disadvantages**:

  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs

**Advantages**:

  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)
  - More consistent codebase (which is somewhat of a quality metric)",tensorflow
53375,SamuelMarks,pr,2021-12-09T17:42:43Z,"[*.h,*.hpp,*.cpp,*.proto] clang-format","`clang-format` 13.0.0 at 9ebfeab

```
$ fd -eh -ec -ehpp -ecpp -eproto -x clang-format -i --style=Google {} \;
```

---

**Related**:

   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting
   - My recently merged PR: #53329
   - My #53360
   - My #53374

**Disadvantages**:

  - Impact just about every file in the codebase
  - Require open PRs to be modified and forks to be updated

**Mitigation of disadvantages**:

  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs

**Advantages**:

  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)
  - More consistent codebase (which is somewhat of a quality metric)",tensorflow
53409,nluehr,pr,2021-12-13T22:52:55Z,Patch absl for cuda 11.6 compatibility.,"Works around the following compiler errors.
```
error: expression must have a constant value
note #2721-D: expression cannot be interpreted
```
Attn: @sanjoy ",tensorflow
53465,duncanriach,pr,2021-12-16T20:46:40Z,[determinism] Add v2.8 release notes,"Add op-determinism changes to the version 2.8 release notes in the master branch (prior to the r2.8 branch cut).

@reedwm, please will you review. This is not much different than the notes for version 2.7. I'm wondering if there any additional enhancements in version 2.8 that I'm not yet aware of.",tensorflow
53468,freedomtan,pr,2021-12-17T02:26:55Z,[tflite] fix coreml delegate tmp peak memory issue,"fix https://github.com/tensorflow/tensorflow/issues/53461

It seems some memory management behaviors changed in Xcode 13 or later so that we need a `@autorelease` block to reduce peak memory usage.",tensorflow
53473,nSircombe,pr,2021-12-17T16:05:16Z,Update oneDNN version for AArch64 builds,"Builds on AArch64 with `--config=mkl_aarch64` will now pickup
oneDNN version 2.5 rather than 2.4.",tensorflow
53477,micmelesse,pr,2021-12-17T22:16:14Z,[ROCM] enable self_adjoint_eig op (#1511),This pr enables self_adjoint_eig_v2_op on rocm. ,tensorflow
53490,qingyunqu,pr,2021-12-20T06:36:25Z,[MHLO] add BroadcastOp constant folding and identity folding,* add BroadcastOp constant folding and identity folding,tensorflow
53528,nSircombe,pr,2021-12-22T22:55:03Z,Disable caching for ACL softmax primitives,Co-authored-by: Luke Ireland <luke.ireland@arm.com>,tensorflow
53532,qingyunqu,pr,2021-12-23T07:26:31Z,eliminate broadcast_in_dim + transpose to broadcast_in_dim,transpose(broadcast_in_dim(X)) => broadcast_in_dim(X),tensorflow
53553,qingyunqu,pr,2021-12-27T16:35:10Z,Add folder pattern for AddOp and MulOp,"Add folder pattern:
* mhlo.add(%0, 0) -> %0
* mhlo.mul(%0, 1) -> %0",tensorflow
53576,SamuelMarks,pr,2021-12-29T15:53:18Z,[tensorflow/lite/schema/upgrade_schema.py] Move `(see :schema.fbs).` up to `operator_type` arg descriptor,"Closes #53566

I have custom docstring parsers that is reading in your codebase to make changes down the track (like automatically inferring types and adding them as annotations)â€¦ but this line of your codebase it's hiccuping on because it's on a new line and has a colon in it and lower indentation then the line above (indicating same scope as that arg); but doesn't actually refer to a new argument. This PR moves it to the same line as above; so it refers to that arg.",tensorflow
53607,AdityaKane2001,pr,2022-01-01T15:56:07Z,Nit in scatter_nd_update,"`""Scatter` -> `Scatter`",tensorflow
53624,peterjc123,pr,2022-01-04T06:36:12Z,Fix time_major=False for Hybrid Bidirectional LSTM,"We converted some models to TFLite through our own model converter and found out that the `EvalHybrid` function of the Bidirectional LSTM leads to a segfault. With some investigation, we figured out the cause is that the variable `time_major` is somehow pinned to `true`. This PR changes that by passing the real value of `time_major`.",tensorflow
53714,qingyunqu,pr,2022-01-10T17:04:16Z,Add transpose => reshape simplifier,* Simplify transpose to reshape when they are equivalent,tensorflow
53720,duncanriach,pr,2022-01-11T03:41:36Z,[determinism] Fix indentation in release notes,"NB: this is a PR against the r2.8 branch.

This PR fixes indentation that was lost in or after the cherry pick into the r2.8 branch of PR [53465](https://github.com/tensorflow/tensorflow/pull/53465), which was merged into the master branch.

I wonder if there is a script that auto-formats the release notes, a script which is replacing 2-space indents with 0-space intents, rather than 4-space intents. There may be other indentation losses in the r2.8 release notes file.",tensorflow
53809,joker-eph,pr,2022-01-18T19:05:08Z,Remove DISC from MHLO,"At the moment DISC is developed in https://github.com/alibaba/BladeDISC and no
longer actively in integration in this repository. For now we'll delete the early
contributions that aren't used. They can be brought back here any time in the future as
needed.",tensorflow
53832,qingyunqu,pr,2022-01-20T08:38:12Z,"Remove FunctionPass, use OperationPass<FuncOp> instead",,tensorflow
53841,micmelesse,pr,2022-01-20T19:23:06Z,[ROCM] pass `HIP_PLATFORM` to bazel,We need to pass HIP_PLATFORM to bazel. Authored by @deven-amd ,tensorflow
53847,duncanriach,pr,2022-01-21T05:49:35Z,Add release note about deterministic selection of conv algos,This PR is for the r2.8 branch. It updates the v2.8 release notes based on the #53826 cherry-pick from @reedwm.,tensorflow
53905,qingyunqu,pr,2022-01-24T05:36:35Z,[MHLO] add DynamicBroadcastInDimOp canonicalize pattern,* replace constant shape's `dynamic_broadcast_in_dim` with `broadcast_in_dim`,tensorflow
54119,duncanriach,pr,2022-01-27T02:09:48Z,Add disable for depthwise-conv d9m-unimplemented exception,"This pull request adds the environment variable `TF_DISABLE_DEPTHWISE_CONV_DETERMINISM_EXCEPTIONS`, which, when set to `'true'` or `'1'` will disable the determinism-unimplemented exception-throwing functionality of `tf.nn.depthwise_conv2d`. The intention of this is to allow a program to run further, to discover if other determinism-unimplemented exceptions would be thrown in the absence of this exception. When this disable is activated, nondeterministic operation is, of course, extremely likely.

This environment variable is intended for debug purposes, and is related to issue [47174](https://github.com/tensorflow/tensorflow/issues/47174) and specifically to [this comment](https://github.com/tensorflow/tensorflow/issues/47174#issuecomment-976179644) in that issue.",tensorflow
54141,elfringham,pr,2022-01-27T09:18:42Z,Add some tolerance to linear_operator_circulant_test so it will pass â€¦,"â€¦on AARCH4
This test failure was being masked by disabling FMA instructions but as they will be enabled again according to https://github.com/tensorflow/tensorflow/issues/52544 this test will need to be fixed as well.",tensorflow
54491,yaochengji,pr,2022-02-23T02:03:04Z,Fix mhlo.concatenate pattern when operand has unknown rank,,tensorflow
55134,elfringham,pr,2022-03-08T11:25:08Z,Parser.h has moved inside llvm_project,Fixes https://github.com/tensorflow/tensorflow/issues/55133,tensorflow
55175,duncanriach,pr,2022-03-09T02:23:22Z,Add link to padding notes to depthwise_conv2d doc,,tensorflow
55176,duncanriach,pr,2022-03-09T04:03:36Z,Fix out size eqn in atrous_conv2d doc,"The following code can be found in [this colab notebook](https://colab.research.google.com/drive/1g9FNR-3g4KIxWdUw8q5hGJATFMAhKElU?usp=sharing). It shows that the equation for the height and width of the output of `tf.nn.atrous_conv2d` reported in the documentation is incorrect when `padding` is `'VALID'`, and demonstrates the correct equation, which this PR provides.

```python
import tensorflow as tf

def out_size(in_size, filter_size, rate, padding):
  value = tf.ones((1, in_size, in_size, 1))
  filters = tf.ones((filter_size, filter_size, 1, 1))
  actual_out = tf.nn.atrous_conv2d(value, filters, rate, padding)
  actual_out_size = tf.shape(actual_out)[1]
  if padding == 'VALID':
    current_doc = in_size - 2 * (filter_size - 1)
    proposed_doc = in_size - rate * (filter_size - 1)
  else: # padding == 'SAME'
    current_doc = in_size
    proposed_doc = in_size
  return actual_out_size, current_doc, proposed_doc
  
for padding in ('VALID', 'SAME'):
  for in_size in (8, 9, 10, 11):
    for filter_size in (1, 2, 3):
      for rate in (1, 2, 3):
        actual, current, proposed = out_size(in_size, filter_size, rate,
                                             padding)
        print(""pad: %-5s, in: %2d, f: %d, r: %d / ""
              ""actual: %2d, current doc: %2d, proposed doc: %2d"" %
              (padding, in_size, filter_size, rate, actual, current, proposed))
```

Output:
```
pad: VALID, in:  8, f: 1, r: 1 / actual:  8, current doc:  8, proposed doc:  8
pad: VALID, in:  8, f: 1, r: 2 / actual:  8, current doc:  8, proposed doc:  8
pad: VALID, in:  8, f: 1, r: 3 / actual:  8, current doc:  8, proposed doc:  8
pad: VALID, in:  8, f: 2, r: 1 / actual:  7, current doc:  6, proposed doc:  7
pad: VALID, in:  8, f: 2, r: 2 / actual:  6, current doc:  6, proposed doc:  6
pad: VALID, in:  8, f: 2, r: 3 / actual:  5, current doc:  6, proposed doc:  5
pad: VALID, in:  8, f: 3, r: 1 / actual:  6, current doc:  4, proposed doc:  6
pad: VALID, in:  8, f: 3, r: 2 / actual:  4, current doc:  4, proposed doc:  4
pad: VALID, in:  8, f: 3, r: 3 / actual:  2, current doc:  4, proposed doc:  2
pad: VALID, in:  9, f: 1, r: 1 / actual:  9, current doc:  9, proposed doc:  9
pad: VALID, in:  9, f: 1, r: 2 / actual:  9, current doc:  9, proposed doc:  9
pad: VALID, in:  9, f: 1, r: 3 / actual:  9, current doc:  9, proposed doc:  9
pad: VALID, in:  9, f: 2, r: 1 / actual:  8, current doc:  7, proposed doc:  8
pad: VALID, in:  9, f: 2, r: 2 / actual:  7, current doc:  7, proposed doc:  7
pad: VALID, in:  9, f: 2, r: 3 / actual:  6, current doc:  7, proposed doc:  6
pad: VALID, in:  9, f: 3, r: 1 / actual:  7, current doc:  5, proposed doc:  7
pad: VALID, in:  9, f: 3, r: 2 / actual:  5, current doc:  5, proposed doc:  5
pad: VALID, in:  9, f: 3, r: 3 / actual:  3, current doc:  5, proposed doc:  3
pad: VALID, in: 10, f: 1, r: 1 / actual: 10, current doc: 10, proposed doc: 10
pad: VALID, in: 10, f: 1, r: 2 / actual: 10, current doc: 10, proposed doc: 10
pad: VALID, in: 10, f: 1, r: 3 / actual: 10, current doc: 10, proposed doc: 10
pad: VALID, in: 10, f: 2, r: 1 / actual:  9, current doc:  8, proposed doc:  9
pad: VALID, in: 10, f: 2, r: 2 / actual:  8, current doc:  8, proposed doc:  8
pad: VALID, in: 10, f: 2, r: 3 / actual:  7, current doc:  8, proposed doc:  7
pad: VALID, in: 10, f: 3, r: 1 / actual:  8, current doc:  6, proposed doc:  8
pad: VALID, in: 10, f: 3, r: 2 / actual:  6, current doc:  6, proposed doc:  6
pad: VALID, in: 10, f: 3, r: 3 / actual:  4, current doc:  6, proposed doc:  4
pad: VALID, in: 11, f: 1, r: 1 / actual: 11, current doc: 11, proposed doc: 11
pad: VALID, in: 11, f: 1, r: 2 / actual: 11, current doc: 11, proposed doc: 11
pad: VALID, in: 11, f: 1, r: 3 / actual: 11, current doc: 11, proposed doc: 11
pad: VALID, in: 11, f: 2, r: 1 / actual: 10, current doc:  9, proposed doc: 10
pad: VALID, in: 11, f: 2, r: 2 / actual:  9, current doc:  9, proposed doc:  9
pad: VALID, in: 11, f: 2, r: 3 / actual:  8, current doc:  9, proposed doc:  8
pad: VALID, in: 11, f: 3, r: 1 / actual:  9, current doc:  7, proposed doc:  9
pad: VALID, in: 11, f: 3, r: 2 / actual:  7, current doc:  7, proposed doc:  7
pad: VALID, in: 11, f: 3, r: 3 / actual:  5, current doc:  7, proposed doc:  5
pad: SAME , in:  8, f: 1, r: 1 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 1, r: 2 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 1, r: 3 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 2, r: 1 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 2, r: 2 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 2, r: 3 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 3, r: 1 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 3, r: 2 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  8, f: 3, r: 3 / actual:  8, current doc:  8, proposed doc:  8
pad: SAME , in:  9, f: 1, r: 1 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 1, r: 2 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 1, r: 3 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 2, r: 1 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 2, r: 2 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 2, r: 3 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 3, r: 1 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 3, r: 2 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in:  9, f: 3, r: 3 / actual:  9, current doc:  9, proposed doc:  9
pad: SAME , in: 10, f: 1, r: 1 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 1, r: 2 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 1, r: 3 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 2, r: 1 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 2, r: 2 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 2, r: 3 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 3, r: 1 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 3, r: 2 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 10, f: 3, r: 3 / actual: 10, current doc: 10, proposed doc: 10
pad: SAME , in: 11, f: 1, r: 1 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 1, r: 2 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 1, r: 3 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 2, r: 1 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 2, r: 2 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 2, r: 3 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 3, r: 1 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 3, r: 2 / actual: 11, current doc: 11, proposed doc: 11
pad: SAME , in: 11, f: 3, r: 3 / actual: 11, current doc: 11, proposed doc: 11
```",tensorflow
55248,peterjc123,pr,2022-03-16T01:00:14Z,[conv3d_transpose] Fix dim check for bias,"Per discussion with @thaink in https://github.com/tensorflow/tensorflow/commit/38a77f4bc1c897c75560a0beb53426f20b883f6e#r68775543 , the previous way to do the dim check for bias is not correct. So, we need this change.",tensorflow
55249,qingyunqu,pr,2022-03-16T08:15:40Z,Add mhlo.transpose splat constant folding,"This PR is for splat constant folding.  
Should we fold a nonsplat constant? (need to manipulate constant's value)",tensorflow
55253,elfringham,pr,2022-03-16T11:25:26Z,Allow some tolerance in equality test by using EXPECT_FLOAT_EQ,Float values on AARCH64 fail the plain EXPECT_EQ check with a difference of 1.42109e-14 which is negligible so use the more tolerant EXPECT_FLOAT_EQ form instead.,tensorflow
55258,nSircombe,pr,2022-03-16T22:19:37Z,Update Compute Library to version 22.02,This PR updates the Compute Library verion from 21.11 to 22.02 and updates the build to supports Arm-v8 in addition to Arm-v8.2a and above.,tensorflow
55264,freedomtan,pr,2022-03-17T09:18:25Z,[tflite] replace non-ASCII single quote,"Non-ASCII single quotes prevent the code from successful
compilation on windows 10 + vs 2019 + msys2",tensorflow
55273,nluehr,pr,2022-03-17T17:33:01Z,EIGEN_DEVICE_FUNC annotation cleanup,"Fix some compiler warnings, and reduce chance of future bugs being introduced where device functions call host-only functions.",tensorflow
55310,elfringham,pr,2022-03-21T14:49:33Z,Avoid loss of precision from using reciprocal,"Taking the reciprocal of the calculated value results in a loss of precision. This causes the unit test prepare-tf.mlir.test to fail on AARCH64. So instead of taking the reciprocal of the calculated nudged_scale to get the inv_nudged_scale, calculate this value from the input values.",tensorflow
55352,nSircombe,pr,2022-03-23T23:06:39Z,Update oneDNN version for mkl_dnn_acl_compatible.,Updates the mkl_dnn_acl_compatible for oneDNN 2.6.,tensorflow
55401,nSircombe,pr,2022-03-28T11:21:34Z,Removes third_party/mkl_dnn/onednn-acl-bf16.patch,"Since the release of oneDNN 2.5, the onednn-acl-bf16.patch
patch is no longer required to enable bf16 for matmul and ip.",tensorflow
55409,elfringham,pr,2022-03-28T17:02:40Z,Increase timeout on join for preemption test,The use of less powerful machines for testing can result in //tensorflow/python/distribute/failure_handling:failure_handler_test failing with an internal timeout while waiting for threads to join(). Add a timeout of 300s to override the default 200s in this case.,tensorflow
55434,nSircombe,pr,2022-03-30T07:31:24Z,Updates mkl_aarch64 build to use oneDNN v2.6.,,tensorflow
55460,duncanriach,pr,2022-04-01T03:02:41Z,Fix tf.scatter_nd documentation,"I experienced quite a lot of confusion while trying to understand this documentation. The part that confused me the most was ""`indices` is an integer of shape `shape`"", which is definitely not true. I ended up scrubbing through and fixing or improving the documentation in several other ways.",tensorflow
55504,duncanriach,pr,2022-04-06T00:53:44Z,Enable XLA test of tf.tensor_scatter_nd_update,"See [this conversation](https://github.com/tensorflow/tensorflow/pull/55460#discussion_r842286085) with @reedwm. XLA JIT compiled functions that utilize the `Scatter` HLO are deterministic since 2.8.0, which I have confirmed for `array_ops.tensor_scatter_update`, the entry point used for this test.",tensorflow
55554,nluehr,pr,2022-04-08T15:22:07Z,Fix crash in GPU SparseToDense when validate_indices is false.,"This essentially extends the fix from commit 23c6926c4e4 to the
validate_indices=false code path.

CC: @reedwm ",tensorflow
55561,Yulv-git,pr,2022-04-09T13:29:41Z,Fix some typos.,,tensorflow
55634,freedomtan,pr,2022-04-15T09:02:01Z,add tensorrt 8.4 support,"nvidia Jetson JetPack 5.0 includes TensorRT 8.4 [1], which is not
supported now. Use TensorRT 8.2 interface for now.

[1] https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-4-0-EA",tensorflow
55657,duncanriach,pr,2022-04-18T22:50:07Z,Add GPU-determinism to tf.nn.depthwise_conv2d,"This PR adds GPU-determinism (with cuDNN version 7.6.3 or newer) to `tf.nn.depthwise_conv2d` backprop to `filter` (and therefore also `tf.keras.layers.DepthwiseConv2D`; and `tf.errors.UnimplementedError` is no longer thrown) when op-determinism is enabled via `tf.config.experimental.enable_op_determinism`.

Included in the update to the release notes is the statement, ""This closes issue [47174](https://github.com/tensorflow/tensorflow/issues/47174)"", which is to be confirmed, but is almost certainly true.

cc @reedwm ",tensorflow
55700,nSircombe,pr,2022-04-21T10:34:06Z,Removes unnecessary MKL contraction kernel flag from mkl_aarch64 build,,tensorflow
55712,qingyunqu,pr,2022-04-22T11:01:47Z,[compiler/mhlo] fix lib/CAPI cmake when using python binding,"There are some problems which are import to discuss:
1. There is a spelling mistake `AllMhLoPasses ` => `AllMhloPasses`
2. `AllMhloPasses` is a library `interface` which not defined by `add_mlir_library`. This causes that `AllMhloPasses` doesn't has property `MLIR_AGGREGATE_DEPS`. Without  `MLIR_AGGREGATE_DEPS` property, the python binding compiling process will failed with many `undefined reference`.
3. I think this PR may not be the best fixing method. If you have more suitable method, tell me please.",tensorflow
55739,duncanriach,pr,2022-04-25T19:28:16Z,Improve cuDNN (and XLA) deterministic convolution testing,"This PR improves the cuDNN (and XLA) deterministic convolution testing in the following ways:
* Because the existing tests evolved from TF1 API code but are now running on the TF2 eager API, they are operating such that they test nothing. They always pass, regardless of the state of the functionality that they intend to test. Specifically, code that would previously run a graph-op twice using the TF1 API and compare the results is instead running the op once using the TF2 eager API and then comparing the numerical result to itself, which always matches. This PR changes the test infrastructure to compare the result of a python function that is run twice in eager mode.
* The non-dilated convolution backprop tests are not exercising nondeterminism (when op-determinism is disabled) when using XLA JIT compilation. This PR adjusts the configurations such that nondeterminism is exercised (when op-determinism is disabled) both with and without XLA.
* This PR modifies some of the existing test cases to make them smaller, and therefore possibly makes them run faster, whilst robustly exercising nondeterminism (when op-determinism is disabled).
* This PR adds tests for `tf.nn.conv2d_transpose`.
* This PR adds a test that exercises forward nondeterminism (when op-determinism is disabled) for 3D convolution when using XLA JIT compilation.",tensorflow
55766,Yulv-git,pr,2022-04-27T14:55:01Z,Fix Succesful to Successful.,,tensorflow
55767,Yulv-git,pr,2022-04-27T15:04:52Z,"Fix some typos for do, the, for, to.",,tensorflow
55768,Yulv-git,pr,2022-04-27T15:16:36Z,"Fix typos for occured, appearence, this, is, a, for, agressiveness, tâ€¦","Fix typos for occured, appearence, this, is, a, for, agressiveness, to, instrution, on.",tensorflow
55769,Yulv-git,pr,2022-04-27T15:28:58Z,"Fix typos for atleast, ouput, Retrive, indicies, is, seperate, to, reâ€¦","Fix typos for atleast, ouput, Retrive, indicies, is, seperate, to, recieve, attrbute, the, in, succesful, begining.",tensorflow
55770,Yulv-git,pr,2022-04-27T15:37:57Z,"Fix some typos for occurence, the, in, environment, recieve.","Fix some typos for occurence, the, in, environment, recieve.",tensorflow
55771,Yulv-git,pr,2022-04-27T15:45:58Z,"Fix some typos for modle, accross, recieve, seperate, the, occured, sâ€¦","Fix some typos for modle, accross, recieve, seperate, the, occured, seperate, atleast, probabilites, one, to, begining, retrive.",tensorflow
55773,Yulv-git,pr,2022-04-27T15:51:13Z,"Fix some typos for the, for, recieve, retrive.","Fix some typos for the, for, recieve, retrive.",tensorflow
55774,Yulv-git,pr,2022-04-27T15:55:28Z,"Fix some typos for is, for, have, one, from, to, ouput.","Fix some typos for is, for, have, one, from, to, ouput.",tensorflow
55775,Yulv-git,pr,2022-04-27T15:59:56Z,"Fix some typos for the, to, futher, indicies, ouput, retrive, in.","Fix some typos for the, to, futher, indicies, ouput, retrive, in.",tensorflow
55790,elfringham,pr,2022-04-28T17:15:38Z,Ensure there are more elements in the dataset than twice CPU core count,"Fixes //tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test on machines with more than 50 CPU cores
This test reads half of the elements and assumes that the amount left
is more than will be prefectched which can be equal to the number
of CPU cores. For high CPU core count machines (>50) this is not
true when there are only 100 elements in the dataset.
So instead set the size of the dataset based on the CPU core count
to ensure that the remaining elements in the dataset are always
more than will be prefetched.",tensorflow
55807,duncanriach,pr,2022-04-30T00:58:32Z,Remove no_oss tag for depthwise conv tests,This PR is a follow-up to [this conversation](https://github.com/tensorflow/tensorflow/pull/55657#issuecomment-1105605034) with @reedwm in PR [55657](https://github.com/tensorflow/tensorflow/pull/55657).,tensorflow
55838,duncanriach,pr,2022-05-02T22:29:41Z,Rename IsCudnnSupportedFilterSize for grouped convolution,"This PR is a follow-up to [this discussion](https://github.com/tensorflow/tensorflow/pull/55657#discussion_r853426849) with @reedwm on PR [55657](https://github.com/tensorflow/tensorflow/pull/55657). With hindsight, I realize that these changes ideally would have been included in that PR.",tensorflow
55875,duncanriach,pr,2022-05-04T00:54:51Z,Fix swapped cuda and cuDNN versions,It seems clear to me that the cuda and cuDNN versions are reversed in this config. I don't know how to verify this change.,tensorflow
56047,qingyunqu,pr,2022-05-10T13:40:46Z,[MHLO] add EliminateRedundantConvert canonicalize pattern,add EliminateRedundantConvert canonicalize pattern: `convert(convert(x)) => convert(x)`,tensorflow
56091,freedomtan,pr,2022-05-13T04:00:32Z,fixed llvm-raw build issue,"newer llvm source included `third_party/llvm/fix_ppc64le.patch` so there
is patch conflict which prevents build tensorflow related, e.g.,
`bazel build --config opt //tensorflow/tools/pip_package:build_pip_package` failed",tensorflow
56154,cloudhan,pr,2022-05-18T11:34:46Z,Unbreak windows build,"Address two build problem when building jaxlib for windows.

@hawkinsp ",tensorflow
56246,nluehr,pr,2022-05-24T16:14:01Z,Make cuda header version check flexible w.r.t. whitespace.,"Version macros in CUDA headers may include whitespace, which currently can cause cuda_configure to fail.
This PR makes the version regex in find_cuda_config more flexible.",tensorflow
56364,elfringham,pr,2022-06-06T10:21:01Z,Prevent ARM CD/CI actions from being run in forks,The new ARM workflows should be prevented from running in forks.,tensorflow
56483,qingyunqu,pr,2022-06-16T17:25:28Z,"[MHLO] unfuse batch norm ops with shape dialect, and split inference â€¦","â€¦and training pattern

1. lowering with `shape` dialect. The original lowering pattern is not a Progressive Lowering. As the `shape` dialect is more mature in MLIR, I think lowering dynamic shape pattern with `shape` dialect is more suitable.
2. split inference and training pattern for flexible integration. The original patterns have both `mhlo.batch_norm_inference` and `mhlo.batch_norm_training` lowering pattern. Sometimes we want to only lower one of them, so I split these pattern.",tensorflow
56570,qingyunqu,pr,2022-06-24T16:21:34Z,"[MLIR/XLA] not lowering tf.Reshape with non-HLOTensorType, like tf_tyâ€¦","â€¦pe.string

* not lowering `tf.Reshape` with non-HLOTensorType, like `!tf_type.string`",tensorflow
56620,elfringham,pr,2022-06-29T10:25:18Z,Remove references to no_oss_py2 tag,"As Python2 is not supported, this tag is obsolete and should be removed.",tensorflow
56714,elfringham,pr,2022-07-08T11:31:21Z,Fix unit tests that rely on unsupported behaviour,"numpy 1.23.0 has removed some behaviours that have been
FutureWarn for some time now. Fix those unit tests that
still relied on that old unsupported behaviour

Fixes: https://github.com/tensorflow/tensorflow/issues/56713",tensorflow
56730,elfringham,pr,2022-07-11T16:06:42Z,Fix PyLint error where line is too long,"tensorflow/python/ops/array_ops.py:5456:0: C0301: Line too long (83/80) (line-too-long)

Just move 'the' onto the next line to resolve",tensorflow
56790,nSircombe,pr,2022-07-15T15:55:40Z,Updates Compute Library version to 22.05 and add mkl_aarch64_threadpool option,"- Changes to workspace2.bzl to download the v22.05 sources from GitHub
- third_party/compute_library/BUILD updated to account for changes since
  22.02
- Version number and Git hash in compute_library.patch updated
  accordingly
- Build config for mkl_aarch64 with support for Eigen threadpool added (`--config=mkl_aarch64_threadpool`)",tensorflow
56811,elfringham,pr,2022-07-18T16:24:57Z,Remove incorrect test tags so that all expected pip tests are run,These tags are not needed for pip tests on an installed wheel as is happening here.,tensorflow
56821,nluehr,pr,2022-07-19T16:38:31Z,Use FP32 in DepthwiseConv2D tests on TF32 capable devices,"Recent versions of CUDNN enable TF32 kernels for depthwise convolutions. This tweak configures tests to use true FP32 for these tests since that is what their numerical tolerances have been calibrated for.

Attn: @reedwm ",tensorflow
56823,nluehr,pr,2022-07-19T20:52:06Z,[TF-TRT] Disable combined_nms_test with no_oss tag.,"The combined_nms_test has long failed under TensorRT 8.x. I'm not sure the test was every reliably passing.

I'm disabling it in OSS the tests in anticipation of updating OSS builds from TensorRT 7.2 to TensorRT 8.4 (c.f., https://github.com/tensorflow/build/pull/124)

Attn: @bixia1 ",tensorflow
56841,elfringham,pr,2022-07-20T14:48:40Z,Cache size is too small for machines with more than 49 CPU cores,"The element size in this case is 363 bytes which only allows for
49 entries in the cache with a size of 18000. So instead make
the size of cache dependent on the number of CPU cores.
The count of CPU cores is relevant as the elements of the dataset
are pre-fetched, once per CPU core. This means that if the cache
is too small for the number of CPU cores then the sliding window
will have moved away from the original element read thus making
the cache pointless.

Fixes #56840 ",tensorflow
56900,elfringham,pr,2022-07-26T10:08:21Z,Disable running of quantization_ops_test on AARCH64 MKL build,This is a workaround until the issue #56861 can be resolved.,tensorflow
56924,nSircombe,pr,2022-07-27T15:20:01Z,Updates cpu_arm64_pip.sh to build with Eigen Threadpool support.,"https://github.com/tensorflow/tensorflow/pull/56790 adds support for
Eigen threadpool with oneDNN and Compute Library on AArch64.

This patch sets the --config=mkl_aarch64_threadpool option in place
of mkl_aarch64 in the CI build script.

The OpenMP flags are also removed accordingly (note, these are set in
tensorflow/third_party/compute_library/BUILD when required for an
OpenMP (i.e. --config=mkl_aarch64) build.",tensorflow
56933,elfringham,pr,2022-07-28T11:14:46Z,Some unit tests are fixed and should be removed from exclude list,"Remove those unit tests that have been fixed, from the exclude list
used by the AARCH64 CI/CD actions",tensorflow
56935,nluehr,pr,2022-07-28T14:27:23Z,Allow re-init on same stream in cudamallocasync allocator.,"This fixes a bug in which destroying and re-creating a session causes the cuda_malloc_async allocator to fail.

For example, when `TF_GPU_ALLOCATOR` is set to `cuda_malloc_async` without this patch
```python
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
for i in [0, 1]:
    print(""i={}"".format(i))
    with tf.compat.v1.Session() as sess:
        pass
```
results in the error
```
F tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:390] Trying to set the stream twice. This isn't supported.
```",tensorflow
