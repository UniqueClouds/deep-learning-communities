issue_id,author,issue_type,timestamp,title,body,community
244,Maratyszcza,pr,2016-11-22T17:09:48Z,Make torch.backends.cudnn work on OSX,Tested on MBP with GeForce GT 750M,pytorch
1074,zuoxingdong,pr,2017-03-23T14:22:34Z,DataLoader: Fix batch data type for numpy array,"The original code is reasonable under purpose without importing numpy. However, in some cases, the user shall generate data set themselves by using Numpy and often the labels are of type `np.float32` which gives True for `type(label).__module__ == 'numpy'`, but it is not the purpose for this line and will raise runtime error, since label is not array, it can not be used for `torch.from_numpy()`.",pytorch
1133,zuoxingdong,pr,2017-03-28T22:34:04Z,Add a flag to fix when dataset size is not divisible by batch size.,,pytorch
1194,zuoxingdong,pr,2017-04-05T15:49:51Z,Fix AttributeError: module 'torch.utils' has no attribute 'data',"As @soumith replied in [Discuss Forum](https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666), it can be solved by adding `import torch.utils.data`. However, many tutorials and examples already directly call `torch.utils.data.DataLoader` only with `import torch`. This PR is to fix this need. ",pytorch
2795,zuoxingdong,pr,2017-09-20T01:26:11Z,New features:  parameters to vector and vector to parameters,,pytorch
2814,zuoxingdong,pr,2017-09-20T23:32:00Z,Multinomial backward: Batched gradients filling,,pytorch
4548,ppwwyyxx,pr,2018-01-09T01:00:40Z,Fix type check in `checked_cast`.,"`type.toBackend(toSparse(t.type().backend()))` can crash when `type` does not have a matching sparse type. For now, Half type does not have sparse support.",pytorch
6282,Maratyszcza,pr,2018-04-04T22:29:22Z,[caffe2] Do not print version and build info unless explicitly requested,"All Caffe2-enabled binaries print very verbose version information on initialization. This fix disables it, unless explicitly requested via command-line flag.",pytorch
6365,Maratyszcza,pr,2018-04-06T21:19:58Z,[caffe2] Always build NNPACK together with Caffe2,"Caffe2 started with an option to use NNPACK pre-installed in the system.
Now this option is mostly legacy, as Caffe2 can include NNPACK in its own build on all platforms.
Due to problems when pre-installed NNPACK is built with different dependencies or compiler options, we decided to remove this option and alwyas build NNPACK with Caffe2.
This change makes Caffe2 always build NNPACK as part of its own build, and updates NNPACK and cpuinfo submodules.

",pytorch
6375,Maratyszcza,pr,2018-04-07T02:08:02Z,[caffe2] Support fused Conv+Relu with NNPACK,"Enable the use of fused Convolution+ReLU functionality from NNPACK

",pytorch
6427,zuoxingdong,pr,2018-04-09T18:12:45Z,Add method to calculate perplexity of distribution,,pytorch
6508,zuoxingdong,pr,2018-04-11T16:22:38Z,Add docs for `item()`,,pytorch
6532,Maratyszcza,pr,2018-04-11T23:21:55Z,[caffe2] Minor changes in NNPACK CMake scripts,"- Tell NNPACK to not link pthreadpool, but only its headers
- Remove FindNNPACK.cmake as it is no longer used

",pytorch
6537,Maratyszcza,pr,2018-04-12T02:55:42Z,[caffe2] Unit test for the thread pool,Adapted from pthreadpool unit test,pytorch
6555,Maratyszcza,pr,2018-04-12T18:06:57Z,[caffe2] Fix bug in NNPACK bindings for convolution in precomputed transform,"Caffe2-NNPACK integration creates blobs for precomputed kernel transorms based on the name of Conv operator. When Conv operators have the same name (e.g. empty string), or the blobs for precomputed transforms get the same name and overwrite each other.
This patch ensures that blobs for all precomputed transforms in the network get a unique name.

",pytorch
6557,Maratyszcza,pr,2018-04-12T18:37:01Z,[caffe2] Auto-tuner for mobile engines,"Try different engine options for Conv operators:

- default engine with shared buffer
- NNPACK engine with auto algorithm and shared buffer (only for unit dilation)
- NNPACK engine with WINOGRAD algorithm, precomputed transforms and shared buffer (only for unit dilation, unit or 2x2 stride, and 3x3 kernel)
- NNPACK engine with WINOGRAD_FP16 algorithm, precomputed transforms and shared buffer (only for unit dilation, unit stride, and 3x3 kernel)
- NNPACK engine with FT8x8 algorithm, precomputed transforms and shared buffer (only for unit dilation, unit stride, and kernel <= 8x8)
- NNPACK engine with FT16x16 algorithm, precomputed transforms and shared buffer (only for unit dilation, unit stride, and kernel <= 16x16)
- NNPACK engine with DIRECT algorithm (only for unit dilation, unit stride, and 1x1 kernel)
- NNPACK engine with implicit GEMM algorithm, precomputed transforms, and shared buffer (only for unit dilation)

",pytorch
6601,Maratyszcza,pr,2018-04-14T01:46:34Z,[caffe2] Open-source DEPTHWISE_3x3 engine,"DEPTHWISE_3x3 engine provides an optimized implementation of depthwise 3x3 convolution, e.g. for ShuffleNet, MobileNets
Implementations exist for CPU (generic), ARM CPU, and CUDA GPU.

Originally developed by @ajtulloch

",pytorch
6697,Maratyszcza,pr,2018-04-18T07:18:38Z,[caffe2] Use both __ARM_NEON__ and __ARM_NEON macros,"ARM64 clang from Android NDK doesn't define `__ARM_NEON__`, which results is perf regression on some models.
I figured that some compilers define `__ARM_NEON__` while others define `__ARM_NEON`.
This patch changes all NEON-specific parts in Caffe2 to check both macros.

",pytorch
6753,zuoxingdong,pr,2018-04-19T09:46:38Z,Update docstring in rnn.py: Multiplication notation,,pytorch
7048,Maratyszcza,pr,2018-04-27T19:57:16Z,[caffe2] Fix build of depthwise_3x3 for CUDA compute capability < 3.5,"PR #6601 broke build on older CUDA targets due to `__ldg` intrinsics. This patch adds a work-around.

",pytorch
7177,zuoxingdong,pr,2018-05-02T12:32:49Z,[Update distribution.py] Add probability evaluation,Useful for e.g. computing loss for Mixture Density Networks. ,pytorch
7443,Maratyszcza,pr,2018-05-09T23:26:04Z,Update NNPACK and cpuinfo submodules to latest master,"In Maratyszcza/NNPACK#140 @daquexian reported an error on Faster-RCNN model with MobileNet V2, when running with NNPACK engine. The error disappears when using the latest NNPACK and cpuinfo. Updating submodules upstream to ensure others don't hit this issue.

",pytorch
7540,zasdfgbnm,pr,2018-05-13T23:22:31Z,Add trigonometry functions for ONNX export,"Trigonometry functions are newly added to ONNX in a recent PR https://github.com/onnx/onnx/pull/869

This PR makes pytorch support exporting graphs with trigonometry functions.

This PR might need to wait until it is ready to change
```python
_onnx_opset_version = 6
```
to
```python
_onnx_opset_version = 7
```",pytorch
7561,Maratyszcza,pr,2018-05-15T02:11:37Z,Replace std::to_string with caffe2::to_string in nomnigraph,"`std::to_string` is not available on Android with GNU STL. We conventionally use `caffe2::to_string` as a portable alternative.

",pytorch
7576,zuoxingdong,pr,2018-05-15T13:09:14Z,[Update rnn.py]: change `_all_weights` to `all_weights_names` for easier usability,Users could easily noticed this method supported with Tab-completion. ,pytorch
7591,Maratyszcza,pr,2018-05-15T21:03:55Z,[caffe2] Use caffe2::stod in lexer,"`std::stod` causes build errors on Android

",pytorch
7592,Maratyszcza,pr,2018-05-15T21:07:24Z,[caffe2] Include <array> in fatal_signal_asan_no_sig_test,"fatal_signal_asan_no_sig_test.cc uses `std::array`, but doesn't include the header. It caused build error on Android.

",pytorch
7593,Maratyszcza,pr,2018-05-15T21:12:27Z,[caffe2] Build Android tests and binaries in CI,"Android tests are now broken, and we didn't know about it. Enabling build of tests & binaries for Android to avoid this situation in the future.",pytorch
7607,Maratyszcza,pr,2018-05-16T02:50:10Z,[caffe2] Fix linking of Android unit tests,"Android unit tests failed to link due because libnnpack and libcpuinfo appeared in the linker command line before libcaffe2. This patch somehow fixes it.

",pytorch
7633,zasdfgbnm,pr,2018-05-17T01:16:04Z,Support exporting <= and >= for ONNX,"This PR add support for exporting <= and >= into ONNX, as discussed in https://github.com/onnx/onnx/issues/870#issuecomment-388528619",pytorch
7646,Maratyszcza,pr,2018-05-17T18:01:51Z,[caffe2] Fix warning in net_async_tracing.cc,"Compilers used to report a warning:
```
caffe2/core/net_async_tracing.cc: In member function 'void caffe2::tracing::Tracer::renameThreads()':
caffe2/core/net_async_tracing.cc:210:32: warning: overflow in implicit constant conversion [-Woverflow]
   const long numa_multiplier = 10e9;
```
This patch fixes it.

",pytorch
7666,Maratyszcza,pr,2018-05-18T07:41:51Z,Simplify pthreadpool implementation on top of Caffe2 thread pool,"Remove one layer of pointer dereference when calling the thread pool.

",pytorch
7691,Maratyszcza,pr,2018-05-18T21:24:58Z,Update NNPACK and cpuinfo submodules,"Updated NNPACK to 42d9355
Updated cpuinfo to 1e6c8c9

",pytorch
7707,zasdfgbnm,pr,2018-05-19T18:17:59Z,"make BatchSampler subclass of Sampler, and expose docs","In the docstring of DataLoader, we have `batch_sampler (Sampler, optional)` at [dataloader.py#L415](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L415), which indicate that `BatchSampler` is a subclass of `Sampler`. But currently it is not.

This PR makes BatchSampler a subclass of Sampler, and expose its docs.",pytorch
7752,Maratyszcza,pr,2018-05-22T05:26:07Z,Mark stack as non-executable in NNPACK,"Pull new revision of NNPACK which specifies non-executable stack in assembly files. Previous revision didn't do that, and depending on toolchain could cause linker to mark stack as executable for the linked binaries.

",pytorch
7816,zasdfgbnm,pr,2018-05-24T18:41:46Z,"move Subset, random_split to data, use sequence at some places.","This PR propose the following changes:
1. Move `torch.utils.data.dataset.random_split` to `torch.utils.data.random_split`.
2. Move `torch.utils.data.dataset.Subset` to `torch.utils.data.Subset`.
3. Add docstring for Subset and expose it.
4. Change some wording from 'iterable' and 'list' to 'sequence'.

Note:
The relationship between iterable and sequence is stated at [python#term-sequence](https://docs.python.org/3/glossary.html#term-sequence):

> (A sequence is) an iterable which supports efficient element access using integer indices via the `__getitem__()` special method and defines a `__len__()` method that returns the length of the sequence.",pytorch
7898,zasdfgbnm,pr,2018-05-28T15:39:02Z,docstring support for @script and @script_method,https://github.com/pytorch/pytorch/issues/7527,pytorch
8029,zuoxingdong,pr,2018-06-01T11:46:25Z,[Update nn.rst] Add documentation of convert parameters,,pytorch
8031,zuoxingdong,pr,2018-06-01T15:47:39Z,[Update utils.py] Allow arbitrary dimension for logsumexp,,pytorch
8045,zuoxingdong,pr,2018-06-01T20:38:53Z,[Update ReduceOps.cpp] logsumexp: consistent notation between dim and dim_,,pytorch
8367,zuoxingdong,pr,2018-06-12T09:00:45Z,[Update lr_scheduler.py] Allow lower bound for all schedulers. ,,pytorch
8551,zasdfgbnm,pr,2018-06-15T14:18:47Z,Add CELU activation to pytorch,"Also fuse input scale multiplication into ELU

Paper:
https://arxiv.org/pdf/1704.07483.pdf",pytorch
8564,Maratyszcza,pr,2018-06-15T18:52:31Z,Update NNPACK and cpuinfo submodules,"Bring in extra optimizations in Winograd-based convolution on NEON

",pytorch
8581,zasdfgbnm,pr,2018-06-16T15:06:34Z,Add meshgrid to PyTorch,Part of this issue https://github.com/pytorch/pytorch/issues/7580,pytorch
8587,zasdfgbnm,pr,2018-06-17T05:46:47Z,move unbind to ATen,,pytorch
9305,zuoxingdong,pr,2018-07-10T14:39:15Z,[Update README.md] Add command to install lapack library,It can be helpful for users to build from source who does not have lapack library on the machine. ,pytorch
9346,zuoxingdong,pr,2018-07-11T14:28:31Z,[Update convert_parameters.py] Add dimension validity check,,pytorch
9393,zasdfgbnm,pr,2018-07-12T17:46:41Z,"Add `itertools.{prod, combinations, combinations_with_replacement}` like op to pytorch",closes https://github.com/pytorch/pytorch/issues/7580,pytorch
9422,zasdfgbnm,pr,2018-07-13T15:40:49Z,[JIT] allow reading numeric types and tensor attribute objects from all module,"Currently JIT can only resolve from `torch` and `torch.nn.functional`. This allow JIT to resolve references from other modules. Such as, to allow `@script` functions like:
```python
import torch
import math
import numpy

a = torch.tensor(1.0)

@torch.jit.script
def f(x):
    return math.pi * x

@torch.jit.script
def g(x):
    return numpy.pi * x

print(f(a))
print(g(a))
```",pytorch
9532,zasdfgbnm,pr,2018-07-18T13:14:26Z,Allow scatter and gather to broadcast,"Closes: https://github.com/pytorch/pytorch/issues/9407

- [x] gather broadcast
- [x] unit test for gather with broadcast
- [x] scatter broadcast
- [x] unit test for scatter with broadcast
- [x] gather gradients
- [x] unit tests for gradients of gather with broadcast
- [x] scatter gradients
- [x] unit tests for gradients of scatter with broadcast
- [x] explain how broadcasting of scatter and gather works in https://pytorch.org/docs/master/notes/broadcasting.html
- [x] change the docstring of scatter and gather to explain the broadcast

cc: @gchanan ",pytorch
9571,Maratyszcza,pr,2018-07-18T22:27:34Z,Adapt OnnxifiOp to removed suffix handling in ONNXIFI loader,"Adapt to changes in onnx/onnx#1203

",pytorch
9844,Maratyszcza,pr,2018-07-25T21:26:37Z,Update OnnxifiOp according to onnx/onnx#1224,,pytorch
9968,zuoxingdong,pr,2018-07-28T02:05:07Z,[Update convert_parameters.py] Use new PyTorch API to make code simpler,,pytorch
9974,zuoxingdong,pr,2018-07-28T08:27:48Z,[Update init.py]  Add `init_as` to initialize the weight from a given Tensor. ,,pytorch
9979,zasdfgbnm,pr,2018-07-29T01:59:19Z,allow nn.Sequential to take multiple inputs,The number of inputs would be the same as the first module,pytorch
10230,Maratyszcza,pr,2018-08-04T03:43:56Z,Update OnnxifiOp to reflect onnx/onnx#1256,,pytorch
10374,zasdfgbnm,pr,2018-08-09T13:17:21Z,"[DO NOT MERGE, DO NOT REVIEW] make _wrap_index_once a method to allow profiling",,pytorch
10378,zasdfgbnm,pr,2018-08-09T14:53:37Z,Advanced Indexing: Allow skipping wrapping indices if returned from nonzero,"In https://github.com/pytorch/pytorch/pull/10374, I made the helper function that wrap indices a method of tensor to allow profiling how much overhead this feature adds to advanced indexing. But unfortunately, the indices wrapping is very expensive.

A simple demo is:
```python
import torch
d = torch.device('cuda')

a = torch.rand(1000, 1000, device=d)
i = torch.ones(1000, 1000, dtype=torch.uint8, device=d).triu()

with torch.autograd.profiler.profile(use_cuda=True) as prof:
	a[i]
	
prof.export_chrome_trace('triu')
print(prof)
```

This gives:
```
-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                            CPU time        CUDA time            Calls        CPU total       CUDA total
-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------
_th_get_device                   5.958us          5.216us                1          5.958us          5.216us
_th_get_device                   2.896us          2.272us                1          2.896us          2.272us
_th_get_device                   3.140us          3.072us                1          3.140us          3.072us
_cast_Byte                      15.384us         15.360us                1         15.384us         15.360us
_th_get_device                   2.801us          2.848us                1          2.801us          2.848us
_th_get_device                   2.805us          2.784us                1          2.805us          2.784us
index                         1832.829us       1862.496us                1       1832.829us       1862.496us
_th_get_device                   2.832us          3.040us                1          2.832us          3.040us
nonzero                        328.179us        605.728us                1        328.179us        605.728us
select                          30.519us          1.024us                1         30.519us          1.024us
select                           5.983us          1.024us                1          5.983us          1.024us
_wrap_index_once               628.163us        442.368us                1        628.163us        442.368us
max                            295.205us         78.048us                1        295.205us         78.048us
_local_scalar                   41.098us         41.088us                1         41.098us         41.088us
_th_get_device                   2.954us          3.072us                1          2.954us          3.072us
_local_scalar_dense             19.456us         19.456us                1         19.456us         19.456us
min                             81.825us         82.240us                1         81.825us         82.240us
_local_scalar                   26.363us         26.240us                1         26.363us         26.240us
_th_get_device                   4.620us          4.864us                1          4.620us          4.864us
_local_scalar_dense             12.135us         12.192us                1         12.135us         12.192us
remainder                      154.303us        195.424us                1        154.303us        195.424us
mul                            234.862us        229.376us                1        234.862us        229.376us
_wrap_index_once               255.512us        277.504us                1        255.512us        277.504us
max                             94.118us         76.192us                1         94.118us         76.192us
_local_scalar                   25.571us         26.016us                1         25.571us         26.016us
_th_get_device                   2.874us          2.432us                1          2.874us          2.432us
_local_scalar_dense             12.734us         12.768us                1         12.734us         12.768us
min                             75.452us         76.576us                1         75.452us         76.576us
_local_scalar                   26.015us         25.952us                1         26.015us         25.952us
_th_get_device                   2.980us          3.008us                1          2.980us          3.008us
_local_scalar_dense             11.987us         12.032us                1         11.987us         12.032us
remainder                       12.788us         55.776us                1         12.788us         55.776us
mul                            151.378us        140.288us                1        151.378us        140.288us
add_                            20.204us         53.248us                1         20.204us         53.248us
view                             9.550us          1.024us                1          9.550us          1.024us
take                            82.686us         80.896us                1         82.686us         80.896us
```
We can see that `_wrap_index_once` is a major cost of computation.

However, wrapping indices is not always necessary. For example, if an index tensor is constructed by `expandByteTensors`, then there is no need to wrap indices because `nonzero` will not return indices out of range.

This PR adds a `bool` to store if wrap index can be skipped. And if yes, then skip this expensive operation. As a result of this change, we get the following profiling:

```
------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                       CPU time        CUDA time            Calls        CPU total       CUDA total
------------------  ---------------  ---------------  ---------------  ---------------  ---------------
_th_get_device              7.707us          7.328us                1          7.707us          7.328us
_th_get_device              3.011us          2.784us                1          3.011us          2.784us
_th_get_device              3.460us          3.072us                1          3.460us          3.072us
_cast_Byte                 17.001us         17.408us                1         17.001us         17.408us
_th_get_device              4.109us          4.064us                1          4.109us          4.064us
_th_get_device              2.808us          2.752us                1          2.808us          2.752us
index                     975.918us       1005.408us                1        975.918us       1005.408us
_th_get_device              3.208us          3.072us                1          3.208us          3.072us
nonzero                   339.928us        615.424us                1        339.928us        615.424us
select                     30.331us          1.024us                1         30.331us          1.024us
select                      5.766us          1.024us                1          5.766us          1.024us
mul                       257.175us         81.920us                1        257.175us         81.920us
mul                       146.566us        147.456us                1        146.566us        147.456us
add_                       19.227us         53.248us                1         19.227us         53.248us
view                        9.385us          2.048us                1          9.385us          2.048us
take                       86.470us         69.632us                1         86.470us         69.632us
```

We can see that advanced indexing is now much faster.

One more point is, this pull request is based on https://github.com/pytorch/pytorch/pull/10374, so it include changes there. I suggest not reverting these changes. Since wrap indices is expensive, it worth to add a custom cuda kernel to do that job to reduce cost. Adding it to `native_functions.yaml` allows automatic dispatching CPU and GPU. I'm planning this in a future PR to see how much a custom kernel can improve.",pytorch
10548,Maratyszcza,pr,2018-08-15T18:43:14Z,Update FP16 submodule. Close #10523,"Pull a fix in FP16 for compilation bug when using Intel Compiler

",pytorch
10599,Maratyszcza,pr,2018-08-16T23:28:46Z,Tune minimal work size,"Summary:
Reduce minimal work size to 1 and see what happens to performance.
I think that not spawing threads with spin-lock synchronization may be bad because they will switch to condvar wait, which increases wake-up latency next time they are needed.

Reviewed By: ajtulloch

Differential Revision: D9366664
",pytorch
10668,zasdfgbnm,pr,2018-08-19T19:08:32Z,Advanced Indexing: Allow skipping wrapping indices if all index tensors are returned from nonzero,"This is an improved version of https://github.com/pytorch/pytorch/pull/10378 that complicate the code much less, but can still handle special cases like using advanced indexing as `masked_select`.

This PR reduce the time for `tensor.index` in the example at https://github.com/pytorch/pytorch/pull/10378 from ~1800us to ~1000us.",pytorch
10721,Maratyszcza,pr,2018-08-21T06:11:35Z,"Avoid shadowing i, j vars in GeneralProposals test","Summary: - Fix compilation warning ""declaration of 'i' shadows a previous local [-Werror=shadow-compatible-local]""

Differential Revision: D9419688
",pytorch
10944,zuoxingdong,pr,2018-08-28T14:18:00Z,Support direct access of `nn.RNNCellBase`,,pytorch
10992,zuoxingdong,pr,2018-08-29T08:12:59Z,Support import of `nn.RNNCellBase` in `__all__`,,pytorch
11376,zuoxingdong,pr,2018-09-07T11:00:33Z,[Update serialization.py]: Allow user to choose highest protocol for pickle without importing it,This could be convenient if one wants to use highest protocol for pickling. It does not need to separately import pickle and call `pickle.HIGHEST_PROTOCOL` by themselves. ,pytorch
11581,zasdfgbnm,pr,2018-09-12T17:23:15Z,[ONNX] Add trigonometry functions to docs/source/onnx.rst,,pytorch
11623,zasdfgbnm,pr,2018-09-13T03:50:09Z,Improve doc of `torch.nn.functional.pad`,I'm reading the doc of `torch.nn.functional.pad` and it looks a bit confusing to me. Hopefully this PR makes it clearer.,pytorch
11664,zuoxingdong,pr,2018-09-13T20:11:51Z,[Update container.py]: add `ModuleList.insert`,fixes #11652,pytorch
11749,zuoxingdong,pr,2018-09-17T08:18:57Z,[Update categorical.py] Creating `Categorical` distribution much faster,Fix #11747,pytorch
11921,zasdfgbnm,pr,2018-09-21T01:31:52Z,Fix docstring of `torch.jit.createResolutionCallback`,"The sample code in the docstring of `torch.jit.createResolutionCallback` is not working:

`createResolutionCallback()` gets the frame of `bar`. In order to get the frame of  `baz`, one need to use `createResolutionCallback(1)`",pytorch
12064,zasdfgbnm,pr,2018-09-25T19:41:07Z,Unhide at::unique from C++,Currently we only have `at::_unique` and `at::_unique_dim`. It would be convenient to have a overloaded `at::unique` that handle both cases.,pytorch
12199,zasdfgbnm,pr,2018-09-29T13:51:02Z, Move cosine_similarity to ATen,I'm now traveling and don't have access to a good computer to compile test by myself. Will see the outcome of CI.,pytorch
12203,zasdfgbnm,pr,2018-09-30T14:09:57Z,[JIT] Allow gathering tuple,"Closes: https://github.com/pytorch/pytorch/issues/12365

Current, the following code gives an error:
```python
@torch.jit.script
def f(x):
    return torch.sort(x)[0]
```
With error message:
```
Traceback (most recent call last):
  File ""quicktest.py"", line 3, in <module>
    @torch.jit.script
  File ""/Users/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py"", line 589, in script
    graph = _jit_script_compile(ast, rcb)
RuntimeError: gatherable->type()->isSubtypeOf(DynamicType::get()) ASSERT FAILED at /Users/administrator/nightlies/2018_09_28/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly_1538131714608/work/torch/csrc/jit/script/compiler.cpp:1819, please report a bug to PyTorch. (emitBasicGather at /Users/administrator/nightlies/2018_09_28/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly_1538131714608/work/torch/csrc/jit/script/compiler.cpp:1819)
frame #0: torch::jit::script::to_ir::emitSimpleExpr(std::__1::shared_ptr<torch::jit::script::Tree> const&) + 2371 (0x10c3e5e53 in libtorch.dylib)
frame #1: torch::jit::script::to_ir::emitSugaredExpr(torch::jit::script::Expr, unsigned long) + 1446 (0x10c3dd326 in libtorch.dylib)
frame #2: torch::jit::script::to_ir::emitExpr(torch::jit::script::Expr) + 69 (0x10c3dddd5 in libtorch.dylib)
frame #3: torch::jit::script::to_ir::getNamedValues(std::__1::vector<std::__1::shared_ptr<torch::jit::script::Tree>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Tree> > >, bool) + 1216 (0x10c3e6a40 in libtorch.dylib)
frame #4: torch::jit::script::to_ir::getValues(std::__1::vector<std::__1::shared_ptr<torch::jit::script::Tree>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Tree> > >, bool) + 64 (0x10c3eec30 in libtorch.dylib)
frame #5: torch::jit::script::to_ir::getValues(torch::jit::script::List<torch::jit::script::Expr>, bool) + 86 (0x10c3d8156 in libtorch.dylib)
frame #6: torch::jit::script::to_ir::to_ir(torch::jit::script::Def, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, torch::jit::script::Method&, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, torch::jit::script::Method&> > >&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>, torch::jit::script::Method&) + 2754 (0x10c3d5082 in libtorch.dylib)
frame #7: std::__1::__function::__func<torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>)::$_5, std::__1::allocator<torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>)::$_5>, void (torch::jit::script::Method&)>::operator()(torch::jit::script::Method&) + 111 (0x10c3d452f in libtorch.dylib)
frame #8: torch::jit::script::Method::ensure_defined() + 175 (0x10c3f7fbf in libtorch.dylib)
frame #9: torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>) + 2520 (0x10c3bfa28 in libtorch.dylib)
frame #10: torch::jit::script::compileFunction(torch::jit::script::Def, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> const&) + 620 (0x10c3c405c in libtorch.dylib)
frame #11: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_21, std::__1::shared_ptr<torch::jit::Graph>, torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>, pybind11::name, pybind11::scope, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_21&&, std::__1::shared_ptr<torch::jit::Graph> (*)(torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 408 (0x10937fe58 in _C.cpython-36m-darwin.so)
frame #12: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3308 (0x108fc570c in _C.cpython-36m-darwin.so)
<omitting python frames>
frame #25: start + 1 (0x7fff67c8f085 in libdyld.dylib)
```
This PR fix this issue by allowing JIT to emit code for gathering tuples.",pytorch
12363,zasdfgbnm,pr,2018-10-05T06:52:13Z,"[JIT] Allow tensor.device, tensor.dtype, and tensor.shape in JIT",Closes https://github.com/pytorch/pytorch/issues/12364,pytorch
12413,zasdfgbnm,pr,2018-10-06T06:23:54Z,Move outplace ops to ATen,"So that things like below can be JITable, and available in C++ API:

```python
import torch

@torch.jit.script
def f(x, y, z):
    x.index_add(0, y, z)
```",pytorch
12455,Maratyszcza,pr,2018-10-08T17:12:59Z,Implement 3D and 4D parallelization in Caffe2 thread pool,"Summary: - Mirror changes in pthreadpool

Differential Revision: D10240470
",pytorch
12474,jjsjann123,pr,2018-10-09T01:34:49Z,[distributed sampler update],"  Modifies the DistributedSampler logic. Now each process samples elements with
a given interval, instead of a consecutive section.

  This eliminates the possibility where the DataLoader uses padded data while
dropping the real data. It happens when:
  1. DistributedSampler padded data; and
  2. DataLoader drops_last is effectively true, and drops less then the number
of padded data.
  from the example down, we see that data (10, 11, 12) are padded through
duplicating data sample (1, 2, 3)
  The old sampler drops legit original data (3, 6, 9) and introduces duplication
(10, 11) into the training set; while the new sampler logic samples correct data
points from the data set.
  This example has been added to dataloader unit test

example:
```
  data after shuffle: 1, 2, 3, 4, 5, 6, 7, 8, 9
  padded data : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12

  old sampler:       ->  DataLoader with (batch_size=2 and drop_last=True)
   p 1: 1, 2, 3          1, 2
   p 2: 4, 5, 6          4, 5
   p 3: 7, 8, 9          7, 8
   p 4:10,11,12         10,11

  new sampler:       ->
   p 1: 1, 5, 9          1, 5
   p 2: 2, 6,10          2, 6
   p 3: 3, 7,11          3, 7
   p 4: 4, 8,12          4, 8
```

",pytorch
12505,Maratyszcza,pr,2018-10-09T21:52:53Z,Update NNPACK-related submodules,"Update submodules below:
- NNPACK
- FP16
- pthreadpool
- cpuinfo
- psimd

",pytorch
12519,zasdfgbnm,pr,2018-10-10T05:47:36Z,typo: Aten.h -> ATen.h in cppdocs,,pytorch
12539,Maratyszcza,pr,2018-10-10T18:00:11Z,Update FP16 submodule,"Pull a patch that makes FP16 compatible with Microsoft compiler on Windows

",pytorch
12554,Maratyszcza,pr,2018-10-10T23:33:53Z,Update FP16 submodule,"Pull a patch that fixes remaining incompatibility with Microsoft compiler on Windows

",pytorch
12639,zuoxingdong,pr,2018-10-14T23:35:56Z,"[Update functional.py]: remove unused Iterable, also avoid Python 3.7 deprecation warning",,pytorch
12736,d4l3k,pr,2018-10-16T23:18:34Z,caffe2: UpsampleBilinear support for scales,"Summary: This updates UpsampleBilinearOp and UpsampleBilinearGradientOp to support scales to bring it inline with ResizeNearestOp https://github.com/pytorch/pytorch/pull/12720.

Differential Revision: D10416228
",pytorch
12818,zuoxingdong,pr,2018-10-18T11:53:16Z,[Update independent.py]: add explicit string representation,,pytorch
12843,d4l3k,pr,2018-10-18T22:36:42Z,caffe2: UpsampleBilinear CUDA implementation,"Summary:
This adds a cuda implementation for the UpsampleBilinearOp and UpsampleBilinearGradientOp.

The CUDA code is based off of the corresponding ResizeNearest operators but with bilinear interpolation logic taken from the CPU implementation.

Differential Revision: D10453776
",pytorch
13065,Maratyszcza,pr,2018-10-24T19:31:05Z,Open-source Caffe2 Int8 ops,"Summary:
- Open-source Caffe2 Int8 (quantized) operators
- Update CMakeLists to have a dependency on QNNPACK, gemmlowp, and neon2sse

Differential Revision: D10524381
",pytorch
13145,Maratyszcza,pr,2018-10-25T22:22:30Z,CMake integration for Int8 ops,,pytorch
13199,Maratyszcza,pr,2018-10-27T00:15:42Z,Fix performance regresion introduced in D10524381,"Summary: D10524381 removed inclusion of int8_simd.h in Caffe2 Int8 operators, and although the resuling code still compiles and works, it is up to 50% end-to-end slower (no SIMD!) on some models

Reviewed By: bertmaher

Differential Revision: D12813095
",pytorch
13435,jjsjann123,pr,2018-10-31T23:14:49Z,[merging var/std to use reduction kernel],"Moved torch.var torch.std to use THC reduction kernel, this greatly improves performance for computing variance over non-contiguous dimensions.

Resolving #13192 ",pytorch
13443,Maratyszcza,pr,2018-11-01T06:40:01Z,[caffe2] Remove C2GEMMContext,"C2GEMMContext is a remnant of old times when Int8 ops used gemmlowp.
It is no longer needed: formerly gemmlowp-based ops use QNNPACK with pthreadpool interface, and other ops (Int8Add, Int8ChannelShuffle) use Caffe2 thread pool interface directly.

",pytorch
13485,jjsjann123,pr,2018-11-01T22:41:10Z,Supporting bernoulli with all types and half,,pytorch
13612,jjsjann123,pr,2018-11-06T04:09:13Z,Updating heuristics for cudnn persistent RNN,"modifying rnn heuristics to exclude GPU with sm == 7.5 from using perssistent RNN

",pytorch
13765,jjsjann123,pr,2018-11-09T06:34:34Z,[ATen batch norm],"fix cuda native batch norm for small feature planes.
  1. fixed warp reduction divergent call of WARP_SHFL_XOR, causes hang with CUDA_ARCH > 7.0
  2. split Normalization.cu into two files for code reuse, preparation for sync BN

",pytorch
13884,zasdfgbnm,pr,2018-11-13T04:26:17Z,Skip all builtin functions when importing names from _C._VariableFunctions to torch,We don't want builtin functions of `_C._VariableFunctions` to replace those of `torch`.,pytorch
13967,jjsjann123,pr,2018-11-14T12:59:57Z,remove dynamic initialization warning (#13913),"removed assignment in default constructor.
removed static shared memory and used dynamic shared memory.

",pytorch
14084,zasdfgbnm,pr,2018-11-16T06:23:36Z,[ImgBot] Optimize images,"This is a PR that [ImgBot](https://imgbot.net/) opened on my fork https://github.com/zasdfgbnm/pytorch/pull/1, I forward it here.  ImgBot does lossless compression on images to reduce file size.",pytorch
14089,Maratyszcza,pr,2018-11-16T07:28:19Z,Switch Int8Add operator to QNNPACK,"- Improved single-threaded performance due to optimized low-level micro-kernels
- Improved parallelization (previously was parallelized across images in a batch and pixels only, now within channels as well)
- Slightly different result due to different implementation of fixed-point arithmetics (no accuracy loss expected)",pytorch
14125,Maratyszcza,pr,2018-11-16T22:59:40Z,Disable QNNPACK for multi-architecture iOS builds,"QNNPACK contains assembly files, and CMake tries to build them for wrong architectures in multi-arch builds. This patch has two effects:
- Disables QNNPACK in multi-arch iOS builds
- Specifies a single `IOS_ARCH=arm64` by default (covers most iPhones/iPads on the market)

",pytorch
14128,Maratyszcza,pr,2018-11-16T23:35:35Z,Update FXdiv submodule,"Use the most recent version that disables inline assembly.
I suspect inline assembly causes miscompilation on some versions of gcc7.

",pytorch
14267,jjsjann123,pr,2018-11-21T08:23:53Z,[sync BN],"- Summary:

Added synchronized batch normalization, allows synchronization of stats across mini-batches between processes within a process group.
Current implementation uses a mixture of extended ATen native functions (cpp cuda extension) + torch.nn.modules (c10d python API)

- User-facing api:

1. torch.nn.utils.convert_sync_batchnorm(modules, process_group=None)

2. torch.nn.SyncBatchNorm(num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True, ***process_group=None***)
                           

- supported use case:
DistributedDataParallel with ***single-gpu multi-process***

a. User creates model containing `torch.nn.SyncBatchNorm` layers through one of the ways listed below:

  1. use layers directly:

     torch.nn.SyncBatchNorm(...)

     similar API as with torch.nn.BatchNormXd(...)
     with added argument `process_group` which is used to limit the scope of                                            
     synchronization within each process group. Default value is None, which                                            
     implies synchronization across all GPUs                                                                            

  2. use torch.nn.utils.convert_sync_batchnorm(modules, process_group)                                                  

     recursively convert all `torch.nn.BatchNormXd` into `torch.nn.SyncBatchNorm`                                       
     preserving values of parameters/buffers.                                                                           
     the utility function also allows user to specify process_group value to all                                        
     converted layers.                                                                                                  

b. user wraps their model with                                                                                          
   `torch.distributed.parallel.DataParallelDistributed`, from this point, user                                          
   should follow the general guidelines for DDP use guide                                                               

- Error checking

For use cases not supported, we error out:                                                                              

1. Application launched without ddp:                                                                                    
   > import torch                                                                                                     
   > sbn = torch.nn.SyncBatchNorm(10).cuda()                                                                          
   > inp = torch.randn(5, 10, 3, 3).cuda()                                                                            
   > sbn(inp) --> Error!                                                                                              
   > AttributeError: SyncBatchNorm is only supported within torch.nn.parallel.DistributedDataParallel                     

2. Application launched using DDP with multi-GPU per-process:                                                           
   > ddp_module = nn.parallel.DistributedDataParallel(module, device_ids=device_ids, output_device=args.local_rank)   
   > ValueError: SyncBatchNorm is only supported for DDP with single GPU per process",pytorch
14362,Maratyszcza,pr,2018-11-26T08:01:17Z,Switch Int8ChannelShuffle operator to QNNPACK,1.8-2.2X better performance on ARM devices,pytorch
14520,Maratyszcza,pr,2018-11-29T03:30:45Z,Validate matching input shapes in Int8Add operator,"Summary: Default engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.

Differential Revision: D13250922
",pytorch
14580,jjsjann123,pr,2018-11-29T23:11:58Z,[TensorIterator reduction improvement for mixed-precision sum on GPU],"Removes cast of half to float in torch.sum, with float16 input tensor and
float32 output tensor, instead we cast data when loading input in kernel.

This supposingly would save a kernel launch as well as a full global memory load
on promoted data type (float).

",pytorch
14783,Maratyszcza,pr,2018-12-05T00:21:10Z,Switch Int8AveragePool operator to QNNPACK,"2.2-2.9X better performance on ARM when compiled with gcc (same bad perf when compiled with Clang)

",pytorch
14806,ppwwyyxx,pr,2018-12-05T17:54:52Z,Improve the docs of interpolate(align_corners=),@ailzhang ,pytorch
14832,Maratyszcza,pr,2018-12-06T01:50:34Z,Switch Int8MaxPool operator to QNNPACK,1.6-2.4X speedup on ARM when compiled with gcc,pytorch
14878,jjsjann123,pr,2018-12-07T04:42:37Z,[TensorIterator fixing mean to output correct result for half precisi…,"…on](#12115)

mean is calculated in two step sum()/numel(). For half precision, data gets
casted back to half after sum().
We fused the division into the reduction kernel by adding pre_op/post_op.

This allows us to do torch.ones(65536).cuda().half().mean() to return correct
result.

",pytorch
14883,Maratyszcza,pr,2018-12-07T07:12:39Z,Switch Int8Sigmoid to QNNPACK,"50x-100x speedup compared to current version.
Also, fixes a bug in the current version when batch size exceeds 1 (current version processes only the first image in this case).

",pytorch
14933,Maratyszcza,pr,2018-12-08T13:08:50Z,"Switch Int8Softmax, Int8Relu, and Int8LeakyRelu to QNNPACK",Int8Softmax: 4x-5x speedup compared to previous implementation,pytorch
15197,zasdfgbnm,pr,2018-12-14T02:05:25Z,Mention Jacobian-vector product in the doc of torch.autograd,"A friend of me is learning deep learning and pytorch, and he is confused by the following piece of code from the tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients :

```python
x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() < 1000:
    y = y * 2

print(y)

gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)

print(x.grad)
```



He don't know where the following line comes from:
```python
gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
```

What are we computing? Why don't we compute ""the gradient of `y` w.r.t `x`""?

In the tutorial, it only says
> You can do many crazy things with autograd!

Which does not explain anything. It seems to be hard for some beginners of deep learning to understand why do we ever do backwards with external gradient fed in and what is the meaning of doing so. So I modified the tutorial in https://github.com/pytorch/tutorials/pull/385
and the docstring correspondingly in this PR, explaining the Jacobian vector product. Please review this PR and https://github.com/pytorch/tutorials/pull/385 together.",pytorch
15208,zasdfgbnm,pr,2018-12-14T08:35:09Z,Add at::one_hot,"Closes: https://github.com/pytorch/pytorch/issues/15060

cc: @zou3519 

I happen to need this feature, so I just implement it...

This PR contains removal of some unnecessary lambda in `test_torch.py`. These unnecessary lambdas are discovered by automated tools and I just go ahead and fix some of them them. If you don't like this change, I can revert it.",pytorch
15235,zasdfgbnm,pr,2018-12-14T20:57:38Z,Onehot and optional<int64_t> for JIT,"cc: @zou3519 
Follow up of: https://github.com/pytorch/pytorch/pull/15208",pytorch
15250,d4l3k,pr,2018-12-15T01:54:59Z,caffe2/python/task: added __repr__ methods to all task definitions,"Summary:
This adds `__repr__` methods to all of the classes under task.py. This makes the objects much easier to interact with when using them in an interactive manner, such as in a Jupyter notebook.

The default `__repr__` method just returns the object ID which is very unhelpful.

Differential Revision: D13475758
",pytorch
15256,zasdfgbnm,pr,2018-12-15T06:04:45Z,"Unhide unique from C++, make unique partially scriptable","This PR does three things:

## Null-able integer argument in schema

~~Allow `int64_t?` in function schema,  which provide an elegant way of implementing null-able int arguments, as discussed in https://github.com/pytorch/pytorch/pull/15208#pullrequestreview-185230081~~ 

~~Originally implemented in https://github.com/pytorch/pytorch/pull/15235~~

~~Example:~~

```yaml
- func: myop(Tensor self, int64_t? dim=None) -> Tensor
  variants: function
```

~~cc: @zou3519~~

Edit: implemented in https://github.com/pytorch/pytorch/pull/15234

## Unhide unique from C++

Previously tried in https://github.com/pytorch/pytorch/pull/12064. There was a problem that C++ does not have kwarg support, which makes it confusing to know whether `unique(t, 1)` actually means `unique(t, dim=1)` or `unique(t, sorted=1)`.

Now I think I have a better idea on how to implement this: there are two ATen operators: `unique` and `unique_dim`. `unique` has the same signature as in python, and exported to both python and C++. `unique_dim` has signature `unique_dim(tensor, dim, sorted=False, return_inverse=False)`, and only exported to C++, which could be used more naturally for a C++ user.

cc: @ezyang @t-vi How does it look like this time?

## Unique is now partially scriptable

As a result of the above two, `torch.unique` can now be partially scripted. See test below:

```python
import torch

def f(a):
    b, c = torch.unique(a, return_inverse=True)
    return b.sum()

def g(a):
    b = torch.unique(a)
    return b.sum()

a = torch.rand((5, 6, 7))

traced_f = torch.jit.trace(f, a)
traced_g = torch.jit.trace(g, a)
print('traced f:', traced_f.graph)
print('traced g:', traced_g.graph)

scripted_f = torch.jit.script(f)
print('scripted f:', scripted_f.graph)

scripted_g = torch.jit.script(g)
print('scripted g:', scripted_g.graph)
```

gives

```
traced f: graph(%input : Float(5, 6, 7)) {
  %1 : bool = prim::Constant[value=0]()
  %2 : bool = prim::Constant[value=1]()
  %3 : int? = prim::None()
  %b : Float(210), %5 : Long(5, 6, 7) = aten::unique(%input, %1, %2, %3)
  %6 : Float() = aten::sum(%b)
  return (%6);
}

traced g: graph(%input : Float(5, 6, 7)) {
  %1 : bool = prim::Constant[value=0]()
  %2 : bool = prim::Constant[value=0]()
  %3 : int? = prim::None()
  %b : Float(210), %5 : Long(0) = aten::unique(%input, %1, %2, %3)
  %6 : Float() = aten::sum(%b)
  return (%6);
}

scripted f: graph(%a : Tensor) {
  %2 : bool = prim::Constant[value=0]()
  %1 : bool = prim::Constant[value=1]()
  %5 : int? = prim::None()
  %b : Tensor, %c : Tensor = aten::unique(%a, %2, %1, %5)
  %9 : Tensor = aten::sum(%b)
  return (%9);
}

Traceback (most recent call last):
  File ""test.py"", line 21, in <module>
    scripted_g = torch.jit.script(g)
  File ""/home/gaoxiang/.virtualenvs/pt/lib/python3.7/site-packages/torch/jit/__init__.py"", line 692, in script
    _jit_script_compile(mod, ast, _rcb, get_default_args(fn))
RuntimeError: 
arguments for call are not valid:
  
  for operator aten::sum(Tensor self, int[] dim, bool keepdim=<default>, *, Tensor out) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, int[] dim, *, int dtype, Tensor out) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, int[] dim, bool keepdim, *, int dtype, Tensor out) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, *, int dtype) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, int[] dim, bool keepdim=<default>) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, int[] dim, *, int dtype) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
  
  for operator aten::sum(Tensor self, int[] dim, bool keepdim, *, int dtype) -> Tensor:
  expected a value of type Tensor for argument 'self' but found (Tensor, Tensor)
  def g(a):
      b = torch.unique(a)
      return b.sum()
             ~~~~~ <--- HERE
for call at:
def g(a):
    b = torch.unique(a)
    return b.sum()
           ~~~~~ <--- HERE
```",pytorch
15385,Maratyszcza,pr,2018-12-19T05:09:26Z,Update cpuinfo submodule,"Pull cpuinfo changes that should make it work on AWS Lambda servers (which don't have `/sys/devices/system/cpu/{possible,present}` files, and probably don't mount sysfs at all).

I'm not 100% sure it will fix the issue, but getting this update in would make it easier for users to test using a nightly build.

",pytorch
15402,Maratyszcza,pr,2018-12-19T19:15:25Z,Make cpuinfo logging less verbose. Fix #15401,"Log only errors in cpuinfo

",pytorch
15405,Maratyszcza,pr,2018-12-19T19:28:45Z,Make cpuinfo logging less verbose,"Log only errors in cpuinfo.

Fix to #15401 and #15398

",pytorch
15414,jjsjann123,pr,2018-12-20T00:17:49Z,Moving torch.norm to ATen using TensorIterator,"Adding supports for torch.nomr:
i. multi dimensions for dim
ii. dtype that specifies math/output tensor type",pytorch
15429,zasdfgbnm,pr,2018-12-20T08:22:18Z,Return namedtuples from torch.* function with multiple return arguments for C++ operators,"Partially fixes: https://github.com/pytorch/pytorch/issues/394

Implementation detail:

Codegen is modified to generate codes that looks like below:
```C++
static PyObject * THPVariable_svd(PyObject* self_, PyObject* args, PyObject* kwargs)
{
  HANDLE_TH_ERRORS
  static PythonArgParser parser({
    ""svd(Tensor input, bool some=True, bool compute_uv=True, *, TensorList[3] out=None)"",
  }, /*traceable=*/true);

  ParsedArgs<6> parsed_args;
  auto r = parser.parse(args, kwargs, parsed_args);
  static PyStructSequence_Field fields0[] = {
    {""U"", """"}, {""S"", """"}, {""V"", """"}, {nullptr}
  };
  static PyStructSequence_Desc desc0 = {
    ""torch.return_types.svd_out"", nullptr,
    fields0, 3
  };
  static PyTypeObject type0;
  static bool namedtuple_type_initialized0 = false;
  if (!namedtuple_type_initialized0) {
    PyStructSequence_InitType(&type0, &desc0);
    namedtuple_type_initialized0 = true;
  }
  static PyStructSequence_Field fields1[] = {
    {""U"", """"}, {""S"", """"}, {""V"", """"}, {nullptr}
  };
  static PyStructSequence_Desc desc1 = {
    ""torch.return_types.svd"", nullptr,
    fields1, 3
  };
  static PyTypeObject type1;
  static bool namedtuple_type_initialized1 = false;
  if (!namedtuple_type_initialized1) {
    PyStructSequence_InitType(&type1, &desc1);
    namedtuple_type_initialized1 = true;
  }
  if (r.idx == 0) {
    if (r.isNone(3)) {
      return wrap(&type1, dispatch_svd(r.tensor(0), r.toBool(1), r.toBool(2)));
    } else {
      auto results = r.tensorlist_n<3>(3);
      return wrap(&type0, dispatch_svd(r.tensor(0), r.toBool(1), r.toBool(2), results[0], results[1], results[2]));
    }
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
```
Types are defined as static member of `THPVariable_${op_name}` functions, and initialized at the first time the function is called.

When parsing function prototypes in `native_functions.yaml`, the parser will set the specified name as `field_name` when see things like `-> (Tensor t1, ...)`. These field names will be the field names of namedtuple. The class of namedtuples will be named `torch.return_types.${op_name}`.

In some python 2, `PyStructSequence` is not a subtype of tuple, so we have to create some functions to check if an object is a tuple or namedtuple for compatibility issue.

Operators in `native_functions.yaml` are changed such that only `max` and `svd` are generated as namedtuple. Tests are added for these two operators to see if the return value works as expected. Docs for these two ops are also updated to explicitly mention the return value is a namedtuple. More ops will be added in later PRs.

There is some issue with Windows build of linker unable to resolve `PyStructSequence_UnnamedField`, and some workaround is added to deal with this case.",pytorch
15466,zasdfgbnm,pr,2018-12-21T03:59:15Z,Fix typo: szie -> size,,pytorch
15503,zasdfgbnm,pr,2018-12-22T15:50:54Z,[Do not land][Do not review][Testing only] Namedtuple experiments,,pytorch
15512,zasdfgbnm,pr,2018-12-23T20:50:56Z,Add torch.rot90 to torch.rst,,pytorch
15561,Maratyszcza,pr,2018-12-27T08:29:33Z,Update QNNPACK,"Summary:
- Update QNNPACK submodule to master (API-incompatible)
- Do matching changes in Caffe2 Int8 operators

Differential Revision: D13551322
",pytorch
15889,Maratyszcza,pr,2019-01-09T21:23:26Z,AABB versions of Mask R-CNN operators,"Summary:
Refactored AABB (Axis-Aligned Bounding-Box) versions of Mask R-CNN operators
- Removed legacy arguments and shape options
- Removed dependency on Eigen, re-implemented all code in simple C++
- Removed +1 adjustment for height & width in bounding boxes. Per rbgirshick, this adjustment is a 10+ years old artifact needed for old datasets. It is not needed for CoCo, and I verified that accuracy of object detection and keypoints detection doesn't decrease on CoCo for our FBNet-based Mask R-CNN model, even though the model was trained with +1 adjustment.
- Removed support for weights in AABBTransform (must be fused into FC layer which produced the deltas)
- Standardized all operators to use batch splits representation for RoIs in each input image. Removed the first component of bounding boxes that used to specify image index. Now all AABB boxes use (x1, y1, x2, y2) representation.
- Removed clamping of deltas after exp in AABBTransform (not needed for inference per rbgirshick, and evaluation on CoCo confirms it)
- Removed SoftNMS options in AABBNMS (should be implemented as a separate operator as almost no code is shared with ""hard"" NMS)
- Standardized AABBRoINMS to always use 3 inputs (batch splits, scores, boxes) and 4 outputs (batch splits, scores, boxes, classes)
- Standardized AABBRoIProposals to always use 4 inputs (scores, deltas, anchors, image info) and 2 required (batch splits, boxes) + 1 options (scores) outputs
- Standardized AABBRoITransform to always use 4 inputs (batch splits, boxes, deltas, image info) and 1 output (boxes)

Differential Revision: D13603371
",pytorch
15980,zasdfgbnm,pr,2019-01-12T00:13:31Z,"[WIP, but please review] Enforce doc coverage, clean torch.* namespace","People (PR authors, reviewers) keep forgetting to add docs for new ops to `torch.rst`, and sometimes they add things that should be in `torch.nn.functional` mistakenly to `torch.*` namespace.

So I write a `test_docs_coverage.py`, that assert everything in `torch` should also be in `torch.rst` vice versa, unless explicitly whitelisted in `test_docs_coverage.py`.

This work is currently in progress, but since there are big API changes, before moving on, I'd like to hear comments from PyTorch people to know if this is a good idea.",pytorch
16039,zasdfgbnm,pr,2019-01-15T18:54:13Z,"Add some missing docs to torch.rst, new unittest to enforce torch.rst no longer miss anything","This prevent people (reviewer, PR author) from forgetting adding things to `torch.rst`.

When something new is added to `_torch_doc.py` or `functional.py` but intentionally not in `torch.rst`, people should manually whitelist it in `test_docs_coverage.py`.",pytorch
16057,zasdfgbnm,pr,2019-01-16T01:07:24Z,"Add some missing docs for tensor methods and attributes, new unittest to enforce tensors.rst no longer miss anything","This depend on https://github.com/pytorch/pytorch/pull/16039

This prevent people (reviewer, PR author) from forgetting adding things to `tensors.rst`.

When something new is added to `_tensor_doc.py` or `tensor.py` but intentionally not in `tensors.rst`, people should manually whitelist it in `test_docs_coverage.py`.",pytorch
16107,Maratyszcza,pr,2019-01-17T07:48:05Z,Update cpuinfo to avoid reporting error when sysfs is not accessible,"On some cloud-based x86 systems /sys/ is not mounted.
cpuinfo has a work-around for these systems, but it reports an error if sysfs files fail to read, and this error was confusing to some users (e.g. pytorch/cpuinfo#20). This update downgrades the error to a warning, so it is not reported with default configuration options.

",pytorch
16143,zasdfgbnm,pr,2019-01-18T07:34:46Z,[DO NOT MERGE] Testing for #15429,,pytorch
16186,zasdfgbnm,pr,2019-01-19T05:43:02Z,"Add namedtuple return for min, median, mode, kthvalue, add test for namedtuple return API","This partially fixes https://github.com/pytorch/pytorch/issues/394 and depend on https://github.com/pytorch/pytorch/pull/15429. I suggest to review this only after https://github.com/pytorch/pytorch/pull/15429 get landed, otherwise the diff might be large to review.

The test only allows explicitly whitelisted operators to have named return.

cc: @ezyang  ",pytorch
16224,jjsjann123,pr,2019-01-22T05:48:50Z,TensorIterator cuda launch configs update,"Summary:
  Update launch configs for TensorIterator gpu_reduce_kernel. Enable flexible
block dimension to improve efficiency for reduction cases with small fast
dimension.

Previously TensorIterator launches blocks with fixed 32x16 threads.
For cases like:

  import torch
  torch.randn(2**20, 4, device='cuda').sum(0)

The fixed launch config does handle coalesced memory access efficiently.

Updated launch configure enables flexible block dimension. Combining with
improved reduction scheme (using flexible vertical / horizontal reduction
instead of limited warp / block reduction in the old code), it ensures optimal
memory access pattern even with reduction on dimension with small stride.

Possible future improvements:
1. Precise dynamic shared memory allocation.
2. Using warp shuffle for vertical (block_y) reduction.

",pytorch
16253,zasdfgbnm,pr,2019-01-23T06:09:51Z,Support named tuple return from operators on JIT,"Fixes: https://github.com/pytorch/pytorch/issues/16233

The following changes are made:
- Modify `TupleType` to store optional field names
- Modify schema matching to return fill in those field names when creating  `TupleType` as return type.
- Modify codegen of JIT to copy field names to schema string
- Modify `SchemaParser` to set field names of returned schema.
- Modify `SimpleValue::attr` to emit tuple indexing for named tuple.",pytorch
16505,zasdfgbnm,pr,2019-01-29T16:51:34Z,Allow namedtuple to be input of a scripted function,"This PR allows to use namedtuple as input of scripted function, such as:
```python
import torch
from collections import namedtuple

@torch.jit.script
def f(x):
    # type: (NamedTuple('T1', [('a', int), ('b', int)]))
    return x.a, x.b

T = namedtuple('T', ['a', 'b'])
input_ = T(a=1, b=2)
print(f(input_))
```
~~This PR depends on https://github.com/pytorch/pytorch/pull/16253, for review, please wait until https://github.com/pytorch/pytorch/pull/16253 is merged or read the diff https://github.com/pytorch/pytorch/commit/0da3fe062c8a882fbada2c68ca2d42028005f6c9 and after.~~",pytorch
16659,zasdfgbnm,pr,2019-02-01T16:34:12Z,[DO NOT LAND][TESTING ONLY] Test argparser namedtuple as tuple,,pytorch
16754,jjsjann123,pr,2019-02-05T07:11:48Z,[cudnn persistent rnn],"Updating cudnn math type for persistent RNN. Putting correct requirement  as
fp32 for math type.
Adding a WAR for cudnn 7.1.3.

",pytorch
16788,zasdfgbnm,pr,2019-02-06T02:54:50Z,Move outplace ops to ATen,"Based on https://github.com/pytorch/pytorch/pull/12413, with the following additional changes:

-  Inside `native_functions.yml` move those outplace operators right next to everyone's corresponding inplace operators for convenience of checking if they match when reviewing
- `matches_jit_signature: True` for them
- Add missing `scatter` with Scalar source
- Add missing `masked_fill` and `index_fill` with Tensor source.
- Add missing test for `scatter` with Scalar source
- Add missing test for `masked_fill` and `index_fill` with Tensor source by checking the gradient w.r.t source
- Add missing docs to `tensor.rst`

cc: @ezyang  This version should work.",pytorch
16790,zasdfgbnm,pr,2019-02-06T03:17:39Z,Clean up autograd method tests,,pytorch
16825,jjsjann123,pr,2019-02-06T22:15:40Z,[cudnn rnn fp32 math accumulation],"Summary: setting the correct math type for cudnn rnn, which is enforced starting from cudnn 7.5+

1. Updating persistent rnn check with input data type instead of rnn math type;
2. Updating rnn type promotion to set correct math type for accumulation;
3. Replace datatype check for filter descriptor from rnn.datatype to input.datatype;",pytorch
16950,zasdfgbnm,pr,2019-02-11T06:37:50Z,"Namedtuple return for symeig, eig, pstrf, qr, geqrf","More ops for https://github.com/pytorch/pytorch/issues/394

cc: @ezyang ",pytorch
17040,jjsjann123,pr,2019-02-13T01:12:55Z,Second PR to restore reverted commit (#16224),"update:
  1. global_reduce check for should_block_y_reduce first.
     This avoids the enabling global_reduce without block_y_reduce. Leading to
     accessing shared memory during global reduce without allocation.
  2. updating block_y_reduce heuristics. Improves perf on tiny tensors
  3. adding test case covering old cases where illegal memory access might occur

  TensorIterator cuda launch configs update (#16224)

    Summary:
    Update launch configs for TensorIterator gpu_reduce_kernel. Enable flexible
    block dimension to improve efficiency for reduction cases with small fast
    dimension.

    Previously TensorIterator launches blocks with fixed 32x16 threads.
    For cases like:

      import torch
      torch.randn(2**20, 4, device='cuda').sum(0)

    The fixed launch config does handle coalesced memory access efficiently.

    Updated launch configure enables flexible block dimension. Combining with
    improved reduction scheme (using flexible vertical / horizontal reduction
    instead of limited warp / block reduction in the old code), it ensures optimal
    memory access pattern even with reduction on dimension with small stride.

    Possible future improvements:
    1. Precise dynamic shared memory allocation.
    2. Using warp shuffle for vertical (block_y) reduction.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/16224

",pytorch
17047,jjsjann123,pr,2019-02-13T08:14:37Z,at::native batch norm kernel launch config update,"limit block dimension to avoid configuration error on batch norm kernel launch

This should resolve #16998 
",pytorch
17093,zasdfgbnm,pr,2019-02-14T04:43:35Z,"Namedtuple return for solve, slogdet, sort, topk","More ops for https://github.com/pytorch/pytorch/issues/394. ~~Also need to rebase after landing #16186, because we need to update the whitelist of the new unit test added in #16186.~~

cc: @ezyang ",pytorch
17097,zasdfgbnm,pr,2019-02-14T05:03:31Z,"Unhide unique from C++, make unique partially scriptable, support namedtuple return","Reopen of https://github.com/pytorch/pytorch/pull/15256 with the new namedtuple return (https://github.com/pytorch/pytorch/issues/394) added. Also need to rebase after landing #16186, because we need to update the whitelist of the new unit test added in #16186.

cc: @wanchaol @ezyang ",pytorch
17099,zasdfgbnm,pr,2019-02-14T05:59:47Z,Move argsort to C++,,pytorch
17103,zasdfgbnm,pr,2019-02-14T06:52:54Z,"Cleanup arg{min, max}","Why do we need this workaround? `PythonArgParser` handles these two cases well.

The discussion started at https://github.com/pytorch/pytorch/pull/6201#issuecomment-378724406. The conclusion at that time by @goldsborough was:

> Because we wanted to allow `dim=None` in Python and route to a different function. Essentially the problem was wanting to wrap the C++ function in Python. AFAIK there is no way of translating `dim=None` behavior into C++? So Richard and I came up with this strategy

Maybe at that time `PythonArgParser` was not powerful enough to handle the routing of two function with same name but different C++ signature.

Will keep an eye on the CI.",pytorch
17136,zasdfgbnm,pr,2019-02-14T23:57:57Z,Customize the printing of namedtuple return,"Fixes https://github.com/pytorch/pytorch/issues/17112
```python
print(""good"", torch.randn(5,5,5).max(1))
print(""terrible"", torch.randn(5,5,10).max(1))
print(""not as good"", torch.randn(5,5,500).max(1))
print (""old behaviour = gold standard"")
print(tuple(torch.randn(5,5,5).max(1)))
print(tuple(torch.randn(5,5,10).max(1)))
print(tuple(torch.randn(5,5,500).max(1)))
```
now gives
```
>>> import torch
>>> print(""good"", torch.randn(5,5,5).max(1))
good torch.return_types.max(
values=tensor([[ 1.2821,  1.8063,  1.8075,  1.3082, -0.1267],
        [ 0.3437,  0.7353,  1.2619,  0.7557,  1.6662],
        [ 0.8583,  1.8906,  1.0246,  1.7598,  1.1184],
        [ 1.7821,  0.0230,  0.9452,  1.0318,  1.0823],
        [ 0.4116, -0.0379, -0.1843,  1.4129,  1.8796]]),
indices=tensor([[4, 4, 3, 2, 1],
        [1, 2, 4, 1, 1],
        [2, 4, 0, 2, 1],
        [0, 2, 0, 3, 1],
        [0, 4, 4, 4, 4]]))
>>> print(""terrible"", torch.randn(5,5,10).max(1))
terrible torch.return_types.max(
values=tensor([[ 2.1272,  1.3664,  2.2067,  1.3974, -0.0883,  1.2505,  1.0074,  1.1217,
          0.3849,  0.6936],
        [ 0.6288, -0.4560,  1.2748,  1.5482,  1.2777,  1.6874,  0.7151,  0.6041,
          1.3572,  1.6232],
        [ 1.6703,  1.0075,  1.6480,  2.2839,  1.3390,  0.4938,  1.6449,  1.7628,
          0.8141,  2.5714],
        [ 0.7079,  1.8677,  3.2478,  1.5591,  2.4870,  0.8635, -0.1450,  1.6923,
          1.4924,  1.6298],
        [ 2.4056,  0.8002,  0.9317,  0.7455,  0.7866,  2.1191,  0.3492,  1.2095,
          1.8637,  1.7470]]),
indices=tensor([[1, 1, 0, 0, 0, 0, 3, 4, 4, 4],
        [4, 2, 2, 1, 2, 2, 3, 1, 1, 3],
        [0, 3, 3, 0, 2, 1, 4, 1, 0, 1],
        [4, 1, 3, 0, 3, 2, 0, 1, 4, 3],
        [1, 0, 3, 2, 1, 0, 0, 1, 0, 1]]))
>>> print(""not as good"", torch.randn(5,5,500).max(1))
not as good torch.return_types.max(
values=tensor([[ 0.3877,  0.7873,  1.8701,  ...,  0.5971,  1.6103, -0.3435],
        [ 1.1300,  2.2418,  1.4239,  ...,  1.3943,  0.3872,  1.6475],
        [ 2.0656,  1.3136,  0.9896,  ...,  2.3918,  0.8226,  1.0517],
        [ 1.1054,  0.9945,  1.0561,  ...,  2.1039,  1.1524,  3.0304],
        [ 1.5041,  2.2809,  1.0883,  ...,  0.8504,  2.4774,  1.1041]]),
indices=tensor([[4, 3, 1,  ..., 1, 4, 0],
        [4, 4, 4,  ..., 3, 0, 3],
        [3, 0, 1,  ..., 2, 2, 4],
        [0, 1, 1,  ..., 4, 2, 2],
        [1, 0, 4,  ..., 2, 0, 2]]))
>>> print (""old behaviour = gold standard"")
old behaviour = gold standard
>>> print(tuple(torch.randn(5,5,5).max(1)))
(tensor([[ 1.1908,  1.1807,  1.3151,  1.7184,  0.3556],
        [ 0.3798,  0.9213,  0.3001,  1.3087,  2.2419],
        [ 1.4233,  1.4814,  1.9900,  1.7744,  1.3059],
        [ 1.0026, -0.0330,  1.3061,  1.8730,  2.0685],
        [ 1.3041,  1.6458,  1.3449,  1.8948,  3.6206]]), tensor([[0, 4, 3, 4, 0],
        [1, 1, 4, 0, 4],
        [4, 1, 0, 3, 3],
        [1, 2, 1, 4, 0],
        [3, 3, 0, 3, 3]]))
>>> print(tuple(torch.randn(5,5,10).max(1)))
(tensor([[-0.1232,  0.8275,  0.6732,  1.1223,  0.8247,  1.2851,  1.6009,  1.9979,
          1.9109,  0.7313],
        [ 0.2260,  0.5922,  1.6928,  0.6024,  2.1158,  3.0619,  0.5653,  0.7426,
          0.8316,  0.6346],
        [ 0.4319,  0.2231,  0.5255,  1.7620,  1.1657,  0.8875,  0.5782,  0.6506,
          0.5032,  1.7097],
        [ 0.4137,  1.7265,  1.4260,  2.0301,  1.2244,  0.7128,  2.6345,  0.7230,
          1.3553,  1.6508],
        [ 1.0684,  1.7195,  1.4068,  0.7076, -0.0242,  0.8474,  0.8754,  1.7108,
          0.2188,  1.1584]]), tensor([[0, 1, 3, 4, 2, 3, 4, 2, 1, 0],
        [1, 4, 0, 0, 3, 2, 0, 0, 3, 3],
        [2, 3, 1, 1, 4, 0, 1, 4, 4, 4],
        [0, 4, 1, 3, 2, 0, 2, 0, 3, 1],
        [1, 0, 0, 0, 0, 3, 3, 3, 2, 0]]))
>>> print(tuple(torch.randn(5,5,500).max(1)))
(tensor([[0.9395, 1.5572, 1.8797,  ..., 2.0494, 0.8202, 0.9623],
        [1.7937, 0.7225, 1.8836,  ..., 0.7927, 1.4976, 1.1813],
        [0.8558, 1.6943, 1.4192,  ..., 0.8327, 1.9661, 0.4197],
        [1.2993, 1.4995, 0.9357,  ..., 0.7810, 1.3030, 2.6216],
        [1.4206, 1.8315, 1.0338,  ..., 1.4312, 1.3198, 1.5233]]), tensor([[0, 4, 3,  ..., 3, 0, 2],
        [0, 1, 0,  ..., 0, 4, 3],
        [3, 4, 3,  ..., 3, 0, 0],
        [3, 2, 3,  ..., 1, 2, 1],
        [1, 2, 4,  ..., 3, 1, 3]]))
```",pytorch
17195,zasdfgbnm,pr,2019-02-16T06:51:53Z,"Namedtuple return for gels, triangular_solve, and test refactor","Partial fix of: https://github.com/pytorch/pytorch/issues/394
- `gels` and `triangular_solve` now returns namedtuple
- refactor test for namedtuple API for better coverage and maintainability",pytorch
17208,zasdfgbnm,pr,2019-02-16T22:17:31Z,Allow structseq to be input of operators where tuple is expected,"Currently the following code gives an error on python 2 because `ret` is a structseq which is not a tuple
```python
ret = a.max(dim=0)
ret1 = torch.max(a, dim=0, out=ret)
```

This PR modify tuple check in python arg parser to allow structseq to be input of operators where tuple is expected, which would make the above code work.

Depend on: https://github.com/pytorch/pytorch/pull/17136
Partially fixes: https://github.com/pytorch/pytorch/issues/16813",pytorch
17306,zasdfgbnm,pr,2019-02-20T18:06:08Z,"[WIP] Cleanup arg{min, max}, with dim=None support",See also: https://github.com/pytorch/pytorch/pull/17103,pytorch
17428,jjsjann123,pr,2019-02-23T02:23:48Z,int32 indexing for Tensor Iterator Reduction,"Summary:
1. Enabling int32 indexing for cases where TI cannot accumulate in output due to
incompatible data types (e.g. Welford).
2. Updating Welford kernel to use int32 instead of int64 indexing on GPU.

This change improves performance for torch.var / torch.std

Implementation:
1. Allocated extra buffer to handle accumulation between sub Tensor Iterators.
2. Removed int64 indexing in gpu_reduce_kernel
3. WelfordOps now supports index type / combination typeas a template parameter.
While GPU uses int32_t and float, CPU implementation uses int64_t and double.

",pytorch
17667,jjsjann123,pr,2019-03-04T22:32:50Z,Tensor Iterator loop unrolling,"Summary:

Modified Tensor Iterator gpu reduction kernel.
Creating multiple accumulator during thread reduce, this removes data dependency
between unrolled loops, expose instruction level parallelism that benefits
latency bounded kernels (e.g. welford used by `torch.std`)

This approach increases register usage, such that we need to tune unrolling
factors to prevent register spilling.
Current implementation tune down the unrolling factor to 2 for welford (register
heavy kernel), while keeping it unchanged (4) for the rest of reduction kernels.

",pytorch
18262,zasdfgbnm,pr,2019-03-21T05:00:26Z,Change deprecated IntList to IntArrayRef,,pytorch
18264,zasdfgbnm,pr,2019-03-21T05:43:30Z,[ONNX] Support dim=None for argmax and argmin,"Fixes: https://github.com/pytorch/pytorch/issues/18263
cc: @houseroad ",pytorch
18265,zasdfgbnm,pr,2019-03-21T06:24:28Z,Fix deprecated scalar type in ATen/native/Distributions.cpp,,pytorch
18299,zasdfgbnm,pr,2019-03-21T23:21:24Z,Add torch.version.git_version,"Fixes: https://github.com/pytorch/pytorch/issues/18293
cc: @colesbury ",pytorch
18391,zasdfgbnm,pr,2019-03-23T23:59:57Z,Add return_counts to torch.unique,"Fixes: https://github.com/pytorch/pytorch/issues/12598

This PR was originally authorized by @ptrblck at https://github.com/pytorch/pytorch/pull/15495, but since there was no update for months after the request change, I clone that branch and resolve the code reviews here. Hope everything is good now. Especially, the implementation of count is changed from @ptrblck's original algorithm to the one @ngimel suggest, i.e. using `unique_by_key` and `adjacent_difference`.

The currently implementation of `_unique_dim` is VERY slow for computing inverse index and counts, see https://github.com/pytorch/pytorch/issues/18405. I will refactor `_unique_dim` in a later PR. For this PR, please allow me to keep the implementation as is.

cc: @ptrblck @ezyang @ngimel @colesbury ",pytorch
18394,zasdfgbnm,pr,2019-03-24T03:39:49Z,Fix deprecated: type() -> scalar_type(),,pytorch
18395,zasdfgbnm,pr,2019-03-24T04:44:30Z,Add numpy like repeat as torch.repeat_interleave,"Fixes: https://github.com/pytorch/pytorch/issues/14093
cc: @SsnL ",pytorch
18406,zasdfgbnm,pr,2019-03-24T18:14:45Z,Fix deprecated: type() -> scalar_type(),"Sorry for not sending these fixes in a single PR. I found this compiler warning when I was working on something else, and I just go to GitHub and modify the file directly for convenience...",pytorch
18459,zasdfgbnm,pr,2019-03-26T00:01:55Z,Refactor CUDA implementation of unique and unique_dim to improve performance of unique_dim,"Fixes: https://github.com/pytorch/pytorch/issues/18405

This is based on https://github.com/pytorch/pytorch/pull/18391, I suggest first wait until #18391 get merged and then I will rebase.

cc: @ngimel 

# Benchmark

Benchmark is on a tensor of shape `torch.Size([15320, 2])`

## Before:

```python
%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=False)
%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=False)
%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=True)
%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=True)
```
```
200 µs ± 3.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.04 s ± 21.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.03 s ± 15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.07 s ± 27.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
```python
%timeit a.unique(sorted=True, return_inverse=False, return_counts=False)
%timeit a.unique(sorted=True, return_inverse=True, return_counts=False)
%timeit a.unique(sorted=True, return_inverse=False, return_counts=True)
%timeit a.unique(sorted=True, return_inverse=True, return_counts=True)
```
```
227 µs ± 2.38 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
291 µs ± 2.34 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
249 µs ± 1.56 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
316 µs ± 2.64 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

## After

```python
%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=False)
%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=False)
%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=True)
%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=True)
```
```
187 µs ± 4.67 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
248 µs ± 3.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
212 µs ± 906 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
274 µs ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
```python
%timeit a.unique(sorted=True, return_inverse=False, return_counts=False)
%timeit a.unique(sorted=True, return_inverse=True, return_counts=False)
%timeit a.unique(sorted=True, return_inverse=False, return_counts=True)
%timeit a.unique(sorted=True, return_inverse=True, return_counts=True)
```
```
226 µs ± 763 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
294 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
253 µs ± 3.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
323 µs ± 1.52 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```",pytorch
18584,jeffdaily,pr,2019-03-28T20:40:31Z,"in caching allocator, ignore and clear the error if not ready",,pytorch
18642,zasdfgbnm,pr,2019-03-30T00:43:44Z,Deprecated type() -> scalar_type(),,pytorch
18644,zasdfgbnm,pr,2019-03-30T01:12:03Z,"TEST ONLY, NOT FOR MERGE",,pytorch
18648,zasdfgbnm,pr,2019-03-30T07:24:52Z,"Step 1: Secretly add return_counts to unique, and refactor unique_dim for performance","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* #18655 Step 6: Rename _unique2 to unique and add int? dim
* #18654 Step 5: remove _unque_dim in favor of unique_dim
* #18651 Step 4: add support for unique with dim=None
* #18650 Step 3: Add support for return_counts to torch.unique for dim not None
* #18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim
* **#18648 Step 1: Secretly add return_counts to unique, and refactor unique_dim for performance**

`unique` is fragile, previously I tried to change it in #18391 and #17097, they all pass OSS tests but finally get reverted due to internal failure. My previous work of refactoring unique #18459 is based on #18391, and after #18391 get reverted, I could not work on #18459. To continue working on #18459, #18391, and #17097 without worrying about internal failures, I am suggesting the following steps for the improvements of `unique` and `unique_dim`. @soumith Please take this and there is no need to put #18391 back.

The motivation is basically to move forward as much as possible without causing any internal failures. So I will try to divide it into steps and sort from low probability of internal failure to high probability. (I don't know what the internal failure is, so I have to guess). Let's merge these PR stack one by one until we enounter internal failure.

Step 1: Create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and keep `_unique` and `_unique_dim` unchanged. The backend of these two functions and `_unique` and `_unique_dim` are all the same, the only difference is the temporary ones support `return_counts` but not the `_unique` and `_unique_dim`. Step one is mostly #18391 + #18459. The cuda8 errors has been fixed. At this point, there is no user visible API change, so no docs are updated. `torch.unique` does not support `return_counts` yet, and `return_counts` is tested through the newly added temporary operators. This step just added two new ATen operators, so there shouldn't be any internal failure.

Step 2: Rename `_unique_dim2_temporary_will_remove_soon` to `unique_dim`. This should cause no internal failure either, because no change to existing operators. The only thing to worry about is to delete `unique_dim` from python side because we don't want users to use it. At this point, C++ users now have `return_counts` support for `unique_dim`.

Step 3: Update the docs of `torch.unique` and use `unique_dim` inside `torch.unique` to support `return_counts` In the docs, we should say `torch.unique` with None dim support does not support `return_counts` yet. This might cause internal failure.

Step 4: Rename `_unique2_temporary_will_remove_soon` to `_unique2` and use `_unique2` inside `torch.unique` to support `return_counts`. Update the docs saying that `torch.unique` with None dim now support `return_counts`. This might cause internal failure.

Step 5: Remove `_unique_dim`. This might cause internal failure.

Step 6: Rename `_unique2` to `unique`, add optional `dim` argument to make it looks like the signature of Python's `torch.unique`. Inside `torch.unique`, use `unique` and get rid of `unique_dim`. Unbind `unique_dim` totally from Python at codegen. This is likely to cause internal fail.

Step 7: Remove `_unique`. This is very likely to cause internal failure.

This PR
======

This PR is for step 1. This create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and implement `return_counts` inside them and do refactor for performance improvements.

Please review @ngimel @VitalyFedyunin. They are mostly copied from #18391 and #18459, so the review should be easy.

Below is a benchmark on a tensor of shape `torch.Size([15320, 2])`:

Before
---------

```python
print(torch.__version__)
%timeit a.unique(dim=0, sorted=True, return_inverse=False); torch.cuda.synchronize()
%timeit a.unique(dim=0, sorted=True, return_inverse=True); torch.cuda.synchronize()
```

```
1.0.1
192 µs ± 1.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
548 ms ± 3.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

```python
print(torch.__version__)
%timeit a.unique(sorted=True, return_inverse=False); torch.cuda.synchronize()
%timeit a.unique(sorted=True, return_inverse=True); torch.cuda.synchronize()
```

```
1.0.1
226 µs ± 929 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
302 µs ± 7.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

After
-------

```python
print(torch.__version__)
%timeit a.unique(dim=0, sorted=True, return_inverse=False); torch.cuda.synchronize()
%timeit a.unique(dim=0, sorted=True, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=False, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()
```

```
1.1.0a0+83ab8ac
190 µs ± 2.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
237 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
219 µs ± 2.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
263 µs ± 1.15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

```python
print(torch.__version__)
%timeit a.unique(sorted=True, return_inverse=False); torch.cuda.synchronize()
%timeit a.unique(sorted=True, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=False, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()
```

```
1.1.0a0+83ab8ac
232 µs ± 2.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
301 µs ± 1.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
264 µs ± 7.67 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
339 µs ± 9.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

Differential Revision: [D14730905](https://our.internmc.facebook.com/intern/diff/D14730905)",pytorch
18649,zasdfgbnm,pr,2019-03-30T07:24:57Z,Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* #18655 Step 6: Rename _unique2 to unique and add int? dim
* #18654 Step 5: remove _unique_dim in favor of unique_dim
* #18651 Step 4: add support for unique with dim=None
* #18650 Step 3: Add support for return_counts to torch.unique for dim not None
* **#18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim**

See step 1 for the description of the full stack

Differential Revision: [D14888292](https://our.internmc.facebook.com/intern/diff/D14888292)",pytorch
18650,zasdfgbnm,pr,2019-03-30T07:25:01Z,Step 3: Add support for return_counts to torch.unique for dim not None,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* #18655 Step 6: Rename _unique2 to unique and add int? dim
* #18654 Step 5: remove _unique_dim in favor of unique_dim
* #18651 Step 4: add support for unique with dim=None
* **#18650 Step 3: Add support for return_counts to torch.unique for dim not None**
* #18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim

See step 1 for the description of the full stack

Differential Revision: [D14892319](https://our.internmc.facebook.com/intern/diff/D14892319)",pytorch
18651,zasdfgbnm,pr,2019-03-30T07:25:08Z,Step 4: add support for unique with dim=None,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* #18655 Step 6: Rename _unique2 to unique and add int? dim
* #18654 Step 5: remove _unique_dim in favor of unique_dim
* **#18651 Step 4: add support for unique with dim=None**

See step 1 for the description of the full stack

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)

Differential Revision: [D15000463](https://our.internmc.facebook.com/intern/diff/D15000463)",pytorch
18654,zasdfgbnm,pr,2019-03-30T13:12:11Z,Step 5: remove _unique_dim in favor of unique_dim,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* #18655 Step 6: Rename _unique2 to unique and add int? dim
* **#18654 Step 5: remove _unique_dim in favor of unique_dim**

See step 1 for the description of the full stack

Differential Revision: [D15000635](https://our.internmc.facebook.com/intern/diff/D15000635)",pytorch
18655,zasdfgbnm,pr,2019-03-30T14:17:35Z,Step 6: Rename _unique2 to unique and add int? dim,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18661 Step 7: remove _unique
* **#18655 Step 6: Rename _unique2 to unique and add int? dim**
* #18654 Step 5: remove _unque_dim in favor of unique_dim
* #18651 Step 4: add support for unique with dim=None
* #18650 Step 3: Add support for return_counts to torch.unique for dim not None
* #18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim
* #18648 Step 1: Secretly add return_counts to unique, and refactor unique_dim for performance

- Rename `_unique2` to `unique`
- Add optional `dim` argument to make it looks like the signature of Python's `torch.unique`.
- Inside `torch.unique`, use `unique` and get rid of `unique_dim`.
- Unbind `unique_dim` totally from Python at codegen.
- Add OSS ONNX test for unique
- Add jit test for unique


Previously tried in #17097 and cause internal error, not sure about this
time.

cc: @wanchaol

Differential Revision: [D15034051](https://our.internmc.facebook.com/intern/diff/D15034051)",pytorch
18661,zasdfgbnm,pr,2019-03-31T00:42:56Z,Step 7: remove _unique,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18661 Step 7: remove _unique**
* #18655 Step 6: Rename _unique2 to unique and add int? dim

See step 1 for the description of the full stack",pytorch
18980,zasdfgbnm,pr,2019-04-06T02:40:38Z,Remove Trainer from README.md,"Trainer has been removed long time ago

",pytorch
19060,zasdfgbnm,pr,2019-04-09T06:31:12Z,Add torch.unique_consecutive,"Fixes: https://github.com/pytorch/pytorch/issues/19045

Please review: @VitalyFedyunin @ngimel 

This is independent on the #18649 series. This will cause merge conflicts in #18649 series, but please merge this first, and I will resolve the merge conflicts there.

The new feature is exposed in `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon`. But not at `torch.unique` yet. I will take care of the API after #18649 series get merged completely.

Benchmark on a tensor of shape `torch.Size([15320, 2])`:

```python
print(torch.__version__)
print()
a = tensor.sort().values.to('cpu')
print('cpu, sorted_input=False:')
%timeit torch._unique2_temporary_will_remove_soon(a)
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True)
%timeit torch._unique2_temporary_will_remove_soon(a, return_counts=True)
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True, return_counts=True)
print()
print('cpu, sorted_input=True:')
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_inverse=True)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_counts=True)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_inverse=True, return_counts=True)
print()
a = a.to('cuda')
print('cuda, sorted_input=False:')
%timeit torch._unique2_temporary_will_remove_soon(a); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True, return_counts=True); torch.cuda.synchronize()
print()
print('cuda, sorted_input=True:')
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique2_temporary_will_remove_soon(a, sorted_input=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()
``` 

```
1.1.0a0+2addccc

cpu, sorted_input=False:
340 µs ± 5.88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
717 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
52.3 ms ± 2.75 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
52.3 ms ± 1.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

cpu, sorted_input=True:
32.8 µs ± 285 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
49.9 µs ± 557 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
51.6 µs ± 1.08 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
78 µs ± 782 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

cuda, sorted_input=False:
213 µs ± 1.52 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
291 µs ± 3.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
250 µs ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
321 µs ± 1.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

cuda, sorted_input=True:
45.6 µs ± 2.13 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
110 µs ± 2.47 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
82 µs ± 857 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
143 µs ± 409 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

```python
print(torch.__version__)
print()
a1, a2 = tensor.unbind(1)
indices = (a1 * tensor.max() + a2).sort().indices 
a = tensor.index_select(0, indices).to('cpu')
print('cpu, sorted_input=False:')
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_inverse=True)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_counts=True)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_inverse=True, return_counts=True)
print()
print('cpu, sorted_input=True:')
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_inverse=True)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_counts=True)
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_inverse=True, return_counts=True)
print()
a = a.to('cuda')
print('cuda, sorted_input=False:')
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, return_inverse=True, return_counts=True); torch.cuda.synchronize()
print()
print('cuda, sorted_input=True:')
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_inverse=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_counts=True); torch.cuda.synchronize()
%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted_input=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()
```

```
cpu, sorted_input=False:
55.4 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
55.8 ms ± 616 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
55.2 ms ± 402 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
55.1 ms ± 725 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

cpu, sorted_input=True:
54.7 ms ± 585 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
55.2 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
54.5 ms ± 865 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
54.9 ms ± 577 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

cuda, sorted_input=False:
171 µs ± 783 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
220 µs ± 1.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
203 µs ± 2.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
251 µs ± 2.83 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

cuda, sorted_input=True:
59.6 µs ± 757 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
113 µs ± 431 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
93.2 µs ± 2.13 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
147 µs ± 2.81 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```
The CPU implementation of `unique_dim` is super slow, see https://github.com/pytorch/pytorch/issues/18987, but this PR will not worry about this issue.",pytorch
19071,z-a-f,pr,2019-04-09T17:29:17Z,Basic implementation of QRelu in C10,"Summary: Implements a basic quantized ReLU (uint8). This is a temporary solution before using the `QTensor` type instead of the tuple.

Differential Revision: D14565413
",pytorch
19091,z-a-f,pr,2019-04-10T00:39:56Z,Basic implementation of QRelu in C10,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19091 Basic implementation of QRelu in C10**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D14565413/)

Implements a basic quantized ReLU (uint8). This is a temporary solution before using the `QTensor` type instead of the tuple.

Differential Revision: [D14565413](https://our.internmc.facebook.com/intern/diff/D14565413/)",pytorch
19191,zasdfgbnm,pr,2019-04-12T07:32:18Z,Add return_index for torch.unique_consecutive,"Partially fixes: https://github.com/pytorch/pytorch/issues/16330
For the `unique_dim_consecutive_cpu` it is not implemented, but will be done after rewriting the CPU implementation of `unique_dim` to resolve performance issue https://github.com/pytorch/pytorch/issues/18987. ",pytorch
19273,z-a-f,pr,2019-04-15T20:22:11Z,Decorator to make sure we can import `core` from caffe2,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19273 Decorator to make sure we can import `core` from caffe2**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D14936387/)

Some of the CIs are failing if the protobuf is not installed. Protobuf is imported as part of the `caffe2.python.core`, and this adds a skip decorator to avoid running tests that depend on `caffe2.python.core`

Differential Revision: [D14936387](https://our.internmc.facebook.com/intern/diff/D14936387/)",pytorch
19312,z-a-f,pr,2019-04-16T18:04:06Z,Use the QTensor with QReLU,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19312 Use the QTensor with QReLU**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D14819460/)

Replaces the tuple hack with the QTensor. Please, note this can be landed ONLY after #18960 (D14810261) is landed.

Differential Revision: [D14819460](https://our.internmc.facebook.com/intern/diff/D14819460/)",pytorch
19319,z-a-f,pr,2019-04-16T20:28:03Z,Quantized SumRelu,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19312 Use the QTensor with QReLU&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D14819460/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19319 [RFC] Quantized SumRelu**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D14866442/)

Quantized SUM + ReLU (Fused). The implementation is the same as the one in the DNNLOWP.

Differential Revision: [D14866442](https://our.internmc.facebook.com/intern/diff/D14866442/)",pytorch
19352,zasdfgbnm,pr,2019-04-17T12:39:30Z,Improve unique CPU performance for returning counts,"Benchmark on a tensor of shape `torch.Size([15320, 2])`. Benchmark code:

```python
print(torch.__version__)
print()
a = tensor.flatten()
print('cpu, sorted=False:')
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=False)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=False, return_inverse=True)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=False, return_counts=True)
%timeit torch._unique2_temporary_will_remove_soon(a, sorted=False, return_inverse=True, return_counts=True)
print()
print('cpu, sorted=True:')
%timeit torch._unique2_temporary_will_remove_soon(a)
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True)
%timeit torch._unique2_temporary_will_remove_soon(a, return_counts=True)
%timeit torch._unique2_temporary_will_remove_soon(a, return_inverse=True, return_counts=True)
print()
```

Before
```
1.1.0a0+36854fe

cpu, sorted=False:
340 µs ± 4.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
724 µs ± 6.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
54.3 ms ± 469 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
54.6 ms ± 659 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

cpu, sorted=True:
341 µs ± 7.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
727 µs ± 7.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
54.7 ms ± 795 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
54.3 ms ± 647 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

After
```
1.1.0a0+261d9e8

cpu, sorted=False:
350 µs ± 865 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
771 µs ± 598 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.09 ms ± 6.86 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.09 ms ± 4.74 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

cpu, sorted=True:
324 µs ± 4.99 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
705 µs ± 3.18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.09 ms ± 5.22 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.09 ms ± 5.63 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```",pytorch
19387,z-a-f,pr,2019-04-18T01:25:27Z,Adds `fakeQuantizePerTensorAffineOp` to pytorch,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19387 Adds `fakeQuantizePerTensorAffineOp` to pytorch**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D13739657/)

Adding fakequant op so that we can use it in pytorch models, the exact implementation might change.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/19387

Differential Revision: [D13739657](https://our.internmc.facebook.com/intern/diff/D13739657/)",pytorch
19500,z-a-f,pr,2019-04-19T17:47:51Z,Use `fbgemm` for quantize/dequantize ops,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19500 Use `fbgemm` for quantize/dequantize ops**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15014561/)

Changes the `quantize_linear` and `dequantize` to `fbgemm`-based implementation.

Differential Revision: [D15014561](https://our.internmc.facebook.com/intern/diff/D15014561/)",pytorch
19610,zasdfgbnm,pr,2019-04-23T07:29:51Z,[WIP] Refactor unique implementation,,pytorch
19714,z-a-f,pr,2019-04-25T02:33:38Z,Changing the rounding in the QTensor,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#19714 Changing the rounding in the QTensor**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15077095/)

We had rounding in the quantizer set as `round(x/scale) + zp`. To make it consistent, converting it to `round(x/scale + zp)`.

Differential Revision: [D15077095](https://our.internmc.facebook.com/intern/diff/D15077095/)",pytorch
19785,zuoxingdong,pr,2019-04-25T23:22:42Z,[Update transforms.py]: Add `TanhTransform`,Resolves #33195,pytorch
19802,zuoxingdong,pr,2019-04-26T14:07:17Z,[Update transforms.py] Fix numerically instability of `SigmoidTransform`,fix #18254 for numerically instability of `SigmoidTransform`,pytorch
20212,zasdfgbnm,pr,2019-05-07T15:03:53Z,Fix test_namedtuple_return,"Fixes: https://github.com/pytorch/pytorch/issues/20198

Input matrix created randomly might be rank deficient or not positive semidefinite

Not tested yet, will look at CI.",pytorch
20233,z-a-f,pr,2019-05-07T19:24:26Z,Adds quantized addition and renames sum to add,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20233 Adds quantized addition and renames sum to add**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15245791/)

Adding a quantized addition (without relu)

Differential Revision: [D15245791](https://our.internmc.facebook.com/intern/diff/D15245791/)",pytorch
20473,z-a-f,pr,2019-05-14T05:07:32Z,Fixes the tests after introduction of `qint8`,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20474 [RFC] Quantized Max Pool op&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20473 Fixes the tests after introduction of `qint8`**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15332106/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19984 [pt1][quant] Add qint8 type (int8_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15150715/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19932 [pt1][quant] Rename qint8 data type&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15137838/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19816 [pt1][quant] Add QInt32 ScalarType and qint32 data type&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15094174/)

The initial assumption was that `qint8` would be unsigned. After introduction of `quint8` and `qint8`, some tests break.

Differential Revision: [D15332106](https://our.internmc.facebook.com/intern/diff/D15332106/)",pytorch
20474,z-a-f,pr,2019-05-14T05:09:12Z,Quantized Max Pool op,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20474 Quantized Max Pool op**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)

parallel implementaiton of the MaxPool (no ReLU).

Differential Revision: [D15327923](https://our.internmc.facebook.com/intern/diff/D15327923/)",pytorch
20519,zasdfgbnm,pr,2019-05-15T03:56:23Z,Import things on torch.utils,So that users can use it directly,pytorch
20647,z-a-f,pr,2019-05-17T16:51:21Z,Renaming the relu kernel and adding hypothesis tests,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20474 Quantized Max Pool op&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20647 Renaming the relu kernel and adding hypothesis tests**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15332106/)

The initial assumption was that `qint8` would be unsigned. After introduction of `quint8` and `qint8`, some tests break.

Differential Revision: [D15332106](https://our.internmc.facebook.com/intern/diff/D15332106/)",pytorch
20780,z-a-f,pr,2019-05-21T22:12:16Z,Fixes a bug when incorrect number of bytes are copied.,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20780 Fixes a bug when incorrect number of bytes are copied.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15440923/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20656 [pt1][quant] int_repr for different quantized types&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15398134/)

`memcpy`'s third argument is a count of bytes to copy. When using non-byte length data, it might cause an error.

Differential Revision: [D15440923](https://our.internmc.facebook.com/intern/diff/D15440923/)",pytorch
20892,z-a-f,pr,2019-05-24T00:32:56Z,Change the quantizer to match the behavior of the FBGEMM implementation,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20474 Quantized Max Pool op&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20647 Renaming the relu kernel and adding hypothesis tests&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15332106/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20892 Change the quantizer to match the behavior of the FBGEMM implementation**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15487664/)

FBGEMM uses 64 bit values. Need to change our implementation to match

Differential Revision: [D15487664](https://our.internmc.facebook.com/intern/diff/D15487664/)",pytorch
21146,z-a-f,pr,2019-05-30T16:25:53Z,Fixes a bug in the test,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21146 Fixes a bug in the test**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15557418/)

The error was reported by https://our.intern.facebook.com/intern/test/562949965807317?ref_report_id=1837062

The API changed from `a.quantize_linear(...)` to `torch.quantize_linear(a, ...)`

Differential Revision: [D15557418](https://our.internmc.facebook.com/intern/diff/D15557418/)",pytorch
21225,z-a-f,pr,2019-05-31T21:41:00Z,[pytorch][PR] Functional conv2d,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21225 [pytorch][PR] Functional conv2d**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15544061/)

Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; https://github.com/pytorch/pytorch/issues/21323 Quantized Conv2d Module&nbsp;&nbsp;[:yellow_heart:](https://our.intern.facebook.com/intern/diff/D15551835/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **https://github.com/pytorch/pytorch/issues/21225 Functional conv2d**&nbsp;&nbsp;[:yellow_heart:](https://our.intern.facebook.com/intern/diff/D15544061/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/21225

Differential Revision: [D15544061](https://our.internmc.facebook.com/intern/diff/D15544061/)",pytorch
21323,z-a-f,pr,2019-06-04T01:25:36Z,Quantized Conv2d Module,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #21808 Quantized conv avoid functional usage&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15835572/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21323 Quantized Conv2d Module**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15551835/)

Quantized Conv2d Module

Differential Revision: [D15551835](https://our.internmc.facebook.com/intern/diff/D15551835/)",pytorch
21600,z-a-f,pr,2019-06-10T19:29:57Z,Adding the quantized namespace to torch.nn and importing it from torch,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21600 Adding the quantized namespace to torch**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15742149/)

Add nn.quantized name space to torch

Differential Revision: [D15742149](https://our.internmc.facebook.com/intern/diff/D15742149/)",pytorch
21694,jjsjann123,pr,2019-06-12T18:45:28Z,UpSample-nearest cuda kernel update,"updating upsampling kernel:
1. avoids atomicAdd for better fp16 performance.
2. better launch configures for 2D input.",pytorch
21749,z-a-f,pr,2019-06-13T18:13:14Z,Quantized concatenation (+fused relu).,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #22408 [WIP] Concat with out (1/x)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16061526/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21749 Quantized concatenation (+fused relu).**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15807940/)

This is the first version without ""requantization""

Differential Revision: [D15807940](https://our.internmc.facebook.com/intern/diff/D15807940/)",pytorch
21808,z-a-f,pr,2019-06-14T22:52:24Z,Quantized conv avoid functional usage,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#21808 Quantized conv avoid functional usage**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15835572/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #21323 Quantized Conv2d Module&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15551835/)

This refactor changes the conv module to avoid the usage of the functional ops.

Differential Revision: [D15835572](https://our.internmc.facebook.com/intern/diff/D15835572/)",pytorch
21879,jjsjann123,pr,2019-06-17T22:34:02Z,updating upsampling bilinear2d kernel:,"1. faster atomicAdd trick for fp16 backward kernel
2. better launch configs for backward kernel
3. removed unnecessary buffer initialization for forward kernel",pytorch
22165,z-a-f,pr,2019-06-24T22:35:27Z,README for the quantized op creation,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22165 README for the quantized op creation**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15975977/)

Workflow description for the quantized ops design.

Differential Revision: [D15975977](https://our.internmc.facebook.com/intern/diff/D15975977/)",pytorch
22168,d4l3k,pr,2019-06-24T23:17:10Z,add custom StatValue support into StatRegistry,"Summary: In distributed training we're using a thread to periodically poll stats and log them after doing some custom aggregations on them. This will allow us to add custom stat handling logic that will log directly to ODS and let ODS do all the aggregations. We lose most of the functionality of ODS if we do ourselves first.

Differential Revision: D15975737

",pytorch
22173,jeffdaily,pr,2019-06-25T00:11:47Z,"Fix race condition, bad lock hierarchy. Move getFreeMutex() into AutoNcclGroup.","There are two mutexes within CUDACachingAllocator that cause a deadlock.  One of the mutexes was added in order to work around the issue of NCCL interacting poorly with cudaFree.  See

- https://github.com/ROCmSoftwarePlatform/pytorch/commit/68ff58d77188fa9c9a491f858b3ec11912770c37
- https://github.com/pytorch/pytorch/pull/880

As of NCCL version 2 and its new group start/end APIs, the protection surrounding cudaFree() is no longer needed.  The PyTorch code was updated to use the NCCL2 group start/end API, but the corresponding cuda_free_mutex and its getter getFreeMutex() were not revised.  This PR removes the use of the getFreeMutex() when NCCL2 is used by moving calls to getFreeMutex() into the AutoNcclGroup.  That way, depending on the NCCL version used, we either use the mutex or we use the new group APIs.

The race condition is as follows, thanks to @skeelyamd:

The deadlock occurs between hip_free_mutex (aka cuda_free_mutex in github) (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L165) and mutex (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L162).

hip_free_mutex is exported from THCCachingAllocator in getFreeMutex (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L660) and is acquired in ProcessGroupNCCL::collective (https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp#L397), which then calls back into THCCachingAllocator via c10::cuda::CUDACachingAllocator::recordStream (https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupNCCL.cpp#L416 to https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L655 to https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L379).  At this point it acquires mutex (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L384).

This requires hip_free_mutex to be locked before mutex.

However, in free_blocks (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L505) THCCachingAllocator locks hip_free_mutex.  Free_blocks is called from emptyCache (https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp#L328) which locks mutex.

That requires mutex to be locked before hip_free_mutex.


emptyCache and ProcessGroupNCCL::collective may not be executed concurrently but this is occurring and deadlocking the CPU.


free_blocks is also called by malloc (via cuda_malloc_retry -> free_cached_blocks -> free_blocks) which also locks mutex first and so malloc must not execute concurrent with ProcessGroupNCCL::collective.

",pytorch
22316,z-a-f,pr,2019-06-27T21:21:07Z,Quantized relu to native_functions,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22316 Quantized relu to native_functions**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D16038441/)

Adding the quantized ReLU to the native_functions.yamp, as it has the same signature as non-quantized relu

Differential Revision: [D16038441](https://our.internmc.facebook.com/intern/diff/D16038441/)",pytorch
22408,z-a-f,pr,2019-07-01T20:58:04Z,Concat with out,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22408 Concat with out**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16061526/)

Quantized Concatenation with out argument

Differential Revision: [D16061526](https://our.internmc.facebook.com/intern/diff/D16061526/)",pytorch
22602,jjsjann123,pr,2019-07-08T23:32:17Z,[UpSample nearest],"1. update on restricting block.z <= 64, compliant to CUDA maximum z-dimension of
a block;
2. clang-format",pytorch
22717,z-a-f,pr,2019-07-11T00:07:12Z,V9: (no description),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Differential Revision: D15544061
Differential Version: 84141756",pytorch
22718,z-a-f,pr,2019-07-11T00:07:22Z,V10: Added prepacked test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Differential Revision: D15544061
Differential Version: 84148224",pytorch
22719,z-a-f,pr,2019-07-11T00:07:31Z,V2: Initial commit,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Differential Revision: D15551835
Differential Version: 84162108",pytorch
22733,z-a-f,pr,2019-07-11T05:50:48Z,Quantized conv avoid functional usage,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22733 Quantized conv avoid functional usage**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15835572/)

This refactor changes the conv module to avoid the usage of the functional ops.

Differential Revision: [D15835572](https://our.internmc.facebook.com/intern/diff/D15835572/)",pytorch
22765,z-a-f,pr,2019-07-11T21:07:25Z,MaxPool2d in the torch,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22765 MaxPool2d in the torch**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16102608/)

the pooling signature is the same as the non-quantized one. Adding it to the native_functions.yaml

Differential Revision: [D16102608](https://our.internmc.facebook.com/intern/diff/D16102608/)",pytorch
22768,z-a-f,pr,2019-07-11T21:20:04Z,V1: Initial commit,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #22770 Added test for both native and quantized relu
* #22769 const tensor
* **#22768 V1: Initial commit**

Differential Revision: D16038441
Differential Version: 85747760",pytorch
22769,z-a-f,pr,2019-07-11T21:20:16Z,const tensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #22770 Added test for both native and quantized relu
* **#22769 const tensor**
* #22768 V1: Initial commit

",pytorch
22770,z-a-f,pr,2019-07-11T21:20:25Z,Added test for both native and quantized relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#22770 Added test for both native and quantized relu**
* #22769 const tensor
* #22768 V1: Initial commit

",pytorch
22775,jjsjann123,pr,2019-07-11T22:41:19Z,[Bug fix for JIT pass - GraphFuser],"passing FusionCallback and Symbol to recursive GraphFuser calls. It ensures
consistent fusion in nested Blocks.",pytorch
22814,ppwwyyxx,pr,2019-07-12T19:06:15Z,Provide an easier way to run collect_env,,pytorch
22827,jjsjann123,pr,2019-07-12T23:47:29Z,[fixing reduction kernel launch],"1. Fix out of range memory access for reduction on all dimensions for non-packed
tensor.

2. Enabling launch config that maps block width to reduction on fastest striding
dimension. This mapping was previously only active when reducing on fastest
striding dimension of packed tensor, which is not necessary.

",pytorch
22830,z-a-f,pr,2019-07-13T00:54:17Z,Rewriting hypothesis_utils,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #22408 [WIP] Concat with out (1/x)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16061526/)
&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #21749 Quantized concatenation (+fused relu).&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15807940/)
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#22830 Rewriting hypothesis_utils**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D16234314/)

Separating the tensor generation and the generation of the quantization parameters

- Introducing hypothesis filter `assume_not_overflowing`, which makes sure that the generated tensor and qparams play well with each other. **Note: This is an expensive filter!**
- `qtensor` -> Renameed to `tensor`
- `qtensor_conv` -> Renamed to `tensor_conv2d`
- The tensors don't return the quantization parameters anymore, use `qparams` for it
- The `dtypes` argument is just a quantized dtype now.
- The enforcement for zero_point is predefined as before. As before, if set to `None` the zero_point will be sampled. However, if `None`, you can override sampling with `zero_point_min` and `zero_point_max`
- Scale sampling can also be overriden using `scale_min` and `scale_max`

Differential Revision: [D16234314](https://our.internmc.facebook.com/intern/diff/D16234314/)",pytorch
23003,z-a-f,pr,2019-07-18T01:08:40Z,[WIP] Fusion and _intrinsic modules,"Stack:
&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#23003 [WIP] Fusion and _intrinsic modules**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D16199720/)

torch.quantization.fuse_module and torch.nn._intrinsic convRelu and LinearRelu

Fusion function to combine specific modules: (conv,bn) and  (conv,bn,relu).
In all cases, replace modules in place. The first module is replaced with the _intrinsic fused module and the remaining modules are replaced by nn.Identity.
Support both training and eval. For training, the modules are ""fused"" with a sequential container. This is to allow for further module swaps for quantization aware training.
Also add: torch.nn._intrinsic for convRelu and LinearRelu.

TODO: Add tests for _intrinsic modules.

Conv BN fusion code is based on DsKhudia's implementation

Differential Revision: [D16199720](https://our.internmc.facebook.com/intern/diff/D16199720/)",pytorch
23040,jeffdaily,pr,2019-07-18T21:29:28Z,Remove cuda free mutex,Revision of #22173 to address CI failure after merging.,pytorch
23115,z-a-f,pr,2019-07-19T23:15:59Z,"[WIP, ignore] Cleanup in quantized","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23115 Cleanup in quantized**

",pytorch
23118,z-a-f,pr,2019-07-19T23:59:10Z,Moving np function to test area,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23118 Moving np function to test area**

Differential Revision: [D16400634](https://our.internmc.facebook.com/intern/diff/D16400634)",pytorch
23127,zasdfgbnm,pr,2019-07-20T07:32:02Z, Speedup bilinear by implementing using einsum,"And `_trilinear` is no longer used and is therefore removed.

Benchmark:

After:
![image](https://user-images.githubusercontent.com/1032377/61575817-bb926900-aa9e-11e9-8d45-da64cf63232b.png)

Before:
![image](https://user-images.githubusercontent.com/1032377/61575828-d533b080-aa9e-11e9-92f2-ecd96e2f7c3f.png)
",pytorch
23135,z-a-f,pr,2019-07-20T21:11:32Z,Lint fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23135 Lint fix**

Lint error introduced in #23123

Differential Revision: [D16403272](https://our.internmc.facebook.com/intern/diff/D16403272)",pytorch
23137,z-a-f,pr,2019-07-21T01:10:51Z,Adding check for a single batch in adaptive_avg_pool,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23137 Adding check for a single batch in adaptive_avg_pool**

Differential Revision: [D16403804](https://our.internmc.facebook.com/intern/diff/D16403804)",pytorch
23143,z-a-f,pr,2019-07-21T08:46:48Z,[WIP] Quantized Average Pool kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23143 [WIP] Quantized Average Pool kernel**

Differential Revision: [D16406281](https://our.internmc.facebook.com/intern/diff/D16406281)",pytorch
23146,z-a-f,pr,2019-07-21T23:10:47Z,Minor refactor: propagating messages in TestCase,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23146 Minor refactor: propagating messages in TestCase**

Differential Revision: [D16413801](https://our.internmc.facebook.com/intern/diff/D16413801)",pytorch
23148,z-a-f,pr,2019-07-22T00:19:49Z,Documentation cleanup,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23148 Documentation cleanup**

Differential Revision: [D16414202](https://our.internmc.facebook.com/intern/diff/D16414202)",pytorch
23275,z-a-f,pr,2019-07-23T22:30:23Z,Optimizing out the division in the fusion,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23275 Optimizing out the division in the fusion**

Differential Revision: [D16450294](https://our.internmc.facebook.com/intern/diff/D16450294)",pytorch
23288,z-a-f,pr,2019-07-24T03:17:35Z,make_module: First version,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23288 make_module: First version**

Differential Revision: [D16455390](https://our.internmc.facebook.com/intern/diff/D16455390)",pytorch
23289,ppwwyyxx,pr,2019-07-24T03:17:43Z,Update distributed.rst,Different backend is supported since https://github.com/pytorch/pytorch/pull/18595,pytorch
23351,z-a-f,pr,2019-07-24T22:55:57Z,Factoring out the quantized mappings,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23351 Factoring out the quantized mappings**

Differential Revision: [D16470907](https://our.internmc.facebook.com/intern/diff/D16470907)",pytorch
23479,zasdfgbnm,pr,2019-07-27T16:53:53Z,Allow scatter and gather to broadcast,"# Changes:

## 1-3. merged seperately in https://github.com/pytorch/pytorch/pull/23510

## 4. `gather`, `scatter`, `scatter_`, `scatter_add`, `scatter_add_` now support broadcasting.

The semantics is explained in `docs/source/notes/broadcasting.rst`. The docs of these operators are updated to reflect the change and point to `docs/source/notes/broadcasting.rst`.

Note:
`self.scatter(dim, index, src)` requires `self` and `src` to have the same number of dimensions. Further, both `self.scatter(dim, index, src)` and `self.gather(dim, index)` requires the number of dimensions of `index` <= number of dimensions of  `self`. When the number of dims in `index` is smaller than `self`, then `index` will be aligned to the left (instead of to the right as in broadcasting of other operators). My personal experience tells me that align to the left is more useful than to the right, and this behavior makes `torch.gather` get a very similar behavior to [`tensorflow.batch_gather`](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/batch_gather).",pytorch
23510,zasdfgbnm,pr,2019-07-29T17:32:17Z,"Improvements on scatter and gather, prepare for implementing broadcasting","Splitted changes 1-3 out from https://github.com/pytorch/pytorch/pull/23479
cc: @ezyang 

# Changes:
## 1. `scatter_add_` and `scatter_add` now support scalar and scalar-tensor as input

`self.scatter_(dim, index, 1.0)` has already been supported. In this PR, the `scatter_add` version of that support is added.

## 2. ~~the inplace version `scatter_` and `scatter_add_` now reject non-contiguous `self`.~~

~~The implementation of autograd we have been using assumes contiguous, but non-contiguous inputs were mistakenly accepted. To reproduce the bug, see the following code:~~

```python
x = torch.tensor(1.0, dtype=torch.double, requires_grad=True).expand(2, 3)
source = torch.zeros(2, 2, dtype=torch.double)
index = torch.randint_like(source, 3, dtype=torch.long)
gradcheck(lambda x: x.scatter_(1, index, source), x)
```

~~The above gradcheck fails.~~

~~New tests are added to ensure non-contiguous `self` is correctly rejected.~~

## 3. the docs on the requirement of parameters of `scatter`, `scatter_`, `scatter_add`, `scatter_add_` has changed.
Previously the documents says: 
> It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim.

But this statement is actually wrong:

First of all, the implementation of autograd assumes the sizes are equal except for `d==dim`. See the below:

```python
x = torch.zeros(5, 6, dtype=torch.double, requires_grad=True)
source = torch.zeros(10, 10, dtype=torch.double, requires_grad=True)
index = torch.randint(5, (2, 6), dtype=torch.long)
x.scatter(0, index, source)
print('forward success')
x.scatter(0, index, source).sum().backward()
print('backward success')
```
The above code satisfied the explained conditions. It prints `forward success` and then fails with an error on the backward: `RuntimeError: Function ScatterBackward0 returned an invalid gradient at index 1 - got [2, 6] but expected shape compatible with [10, 10]`.

Second, the following code does not even run, even though it satisfies the `index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim` condition described:

```python
x = torch.zeros(5, 6, dtype=torch.double, requires_grad=True)
source = torch.zeros(10, 10, dtype=torch.double, requires_grad=True)
index = torch.randint(5, (2, 2), dtype=torch.long)
x.scatter(0, index, source)
print('forward success')
```

The above code fails at the scatter with an error message `RuntimeError: Invalid index in scatter at /build/python-pytorch/src/pytorch-1.1.0-cuda/aten/src/TH/generic/THTensorEvenMoreMath.cpp:491`.",pytorch
23574,jjsjann123,pr,2019-07-30T22:25:45Z,[Copy update to assert for overlap storage at destination buffer],"Assert that there's no multiple written-to to a single memory location, which
caused corrupted output.
Fixed batched matrix trlu logic, which relies on the previous copy behavior to
support tensors with stride 0 at leading dimension.

This fixes the issue proposed at: https://github.com/pytorch/pytorch/issues/23063",pytorch
23578,zasdfgbnm,pr,2019-07-30T23:39:07Z,Add scalar support for scatter_add_,"Hi @gchanan, I have reverted most of the document change in https://github.com/pytorch/pytorch/pull/23510 and sending `scatter_add_` scalar support as a separate PR. The change of docs in this PR only reflect that it now supports scalar.

Please land this first, and I will check 
https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorScatterGather.cu 
and find other works needed on error check and possible changes to the broadcasting implemented in https://github.com/pytorch/pytorch/pull/23479 

After this get merged, I will first make a sperate PR for the broadcasting of `gather`  only (not `scatter` and `scatter_add`) because it is easy. Then I will work on the modification of `scatter` error checks and documents.

Also cc: @ezyang 
",pytorch
23586,jjsjann123,pr,2019-07-31T02:19:34Z,Merging `maybeOverlappingIndices` and `has_internal_overlap`,"These two functions basically serves the same purpose.
`has_internal_overlap` provides a more flexible API (where
`MemOverlap::TOO_HARD` is provided), while `maybeOverlappingIndices` provides
meaningful flags on more general cases.

We get the best out of each and unifies the API.

Test cases are also provided. One thing to note here is, we should NEVER assert
on return being `MemOverlap::TOO_HARD`, which would block future improvement to
the `has_internal_overlap` function to cover more grounds.
We should instead assert on not returning the incorrect flags for cases not yet
supported by current implementation.

",pytorch
23632,z-a-f,pr,2019-07-31T23:36:26Z,Use protected _FLOAT_MODULE class variable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23632 Use protected _FLOAT_MODULE class variable**

Renaming all the `__FLOAT_MODULE` and `__FLOAT_MODULE__` into `_FLOAT_MODULE`.

Differential Revision: [D16593077](https://our.internmc.facebook.com/intern/diff/D16593077)",pytorch
23635,z-a-f,pr,2019-07-31T23:57:48Z,Removing the make_module script.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24151 Replacing axis with dim in quantized cat
* **#23635 Removing the make_module script.**

n128675

Differential Revision: [D16593364](https://our.internmc.facebook.com/intern/diff/D16593364)",pytorch
23636,z-a-f,pr,2019-08-01T00:26:48Z,Tests for the wrapped/generated modules and their quantized counterparts,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23636 Tests for the wrapped/generated modules and their quantized counterparts**

Differential Revision: [D16593964](https://our.internmc.facebook.com/intern/diff/D16593964)",pytorch
23704,z-a-f,pr,2019-08-02T00:21:08Z,Enabling inplace in quantized relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23704 Enabling inline in quantized relu**

Differential Revision: [D16634539](https://our.internmc.facebook.com/intern/diff/D16634539)",pytorch
23717,zasdfgbnm,pr,2019-08-02T04:18:39Z,Zero sized tensor support for repeat_interleave,Fixes https://github.com/pytorch/pytorch/issues/22753,pytorch
23861,jjsjann123,pr,2019-08-06T08:27:19Z,[cudnn nhwc support],"Added nhwc support for:
1. cudnn_batch_norm & cudnn_batch_norm_backward
2. cudnn_convolution_forward & cudnn_convolution_backward
3. cudnn_convolution_transpose & cudnn_convolution_transpose_backward

patching suggest_memory_format for convolution

suggest_memory_format has ambiguous meaning for two cases:
1. tensor with NCHW where C = 1.
   we could use stride of C as a hint to tell the intended memory format.
2. tensor with NCHW where H == W == 1.
   there's no way to identify the intended memory format from strides.

Currently we fallback to NCHW whenever we see contiguous tensor. Hence avoiding
ambiguity for some of the special cases.",pytorch
23906,z-a-f,pr,2019-08-06T22:12:55Z,Increasing precision for avg pool,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23906 Increasing precision for avg pool**

The 1-off error is expected for the average pool due to double rounding.
Increasing unittest precision tolerance to 1.0 to avoid failing.

Differential Revision: [D16678044](https://our.internmc.facebook.com/intern/diff/D16678044)",pytorch
23909,z-a-f,pr,2019-08-06T22:18:13Z,Adding dequantize_val and requantize_val,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#23909 Adding dequantize_val and requantize_val**

Differential Revision: [D16678276](https://our.internmc.facebook.com/intern/diff/D16678276)

```
$> ./build/bin/quantized_test
Running main() from ../third_party/googletest/googletest/src/gtest_main.cc
[==========] Running 4 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 4 tests from TestQTensor
[ RUN      ] TestQTensor.QuantDequantAPIs
[       OK ] TestQTensor.QuantDequantAPIs (0 ms)
[ RUN      ] TestQTensor.RoundingMode
[       OK ] TestQTensor.RoundingMode (0 ms)
[ RUN      ] TestQTensor.Item
[       OK ] TestQTensor.Item (0 ms)
[ RUN      ] TestQTensor.EmptyQuantized
[       OK ] TestQTensor.EmptyQuantized (0 ms)
[----------] 4 tests from TestQTensor (0 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 1 test case ran. (0 ms total)
[  PASSED  ] 4 tests.
```",pytorch
23956,z-a-f,pr,2019-08-07T18:10:15Z,Quantized addition refactor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24217 AddReLU module
* #23971 Quantized addition with 'out'
* **#23956 Quantized addition refactor**

Differential Revision: [D16692445](https://our.internmc.facebook.com/intern/diff/D16692445)",pytorch
23971,z-a-f,pr,2019-08-07T20:21:19Z,Quantized addition with 'out',"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24217 AddReLU module
* **#23971 Quantized addition with 'out'**

Differential Revision: [D16695592](https://our.internmc.facebook.com/intern/diff/D16695592)",pytorch
24113,jjsjann123,pr,2019-08-09T20:12:12Z,[NHWC support] Revoking mutually exclusive requirement on channels last and contiguous tensor,"The old implementation assumed `is_channels_last_contiguous_` to be mutually
exclusive to `is_contiguous_`, which is not true.
Properly set the flag by checking strides.

",pytorch
24151,z-a-f,pr,2019-08-11T00:30:55Z,Replacing axis with dim in quantized cat,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24151 Replacing axis with dim in quantized cat**

Differential Revision: [D16754347](https://our.internmc.facebook.com/intern/diff/D16754347)",pytorch
24212,z-a-f,pr,2019-08-12T23:47:20Z,Ignoring the test logs in case the tests are ran from the parent directory,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24212 Ignoring the test logs in case the tests are ran from the parent directory**

Differential Revision: [D16775806](https://our.internmc.facebook.com/intern/diff/D16775806)",pytorch
24217,z-a-f,pr,2019-08-13T00:46:05Z,[quant] AddReLU module,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24217 [quant] AddReLU module**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24217

Differential Revision: [D16777880](https://our.internmc.facebook.com/intern/diff/D16777880)",pytorch
24374,z-a-f,pr,2019-08-14T23:07:53Z,Enables `inplace` in the quantized relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24374 Enables `inplace` in the quantized relu**

This is a duplicate to bring back #23704 with diff revision D16634539

Differential Revision: [D16818664](https://our.internmc.facebook.com/intern/diff/D16818664)",pytorch
24387,z-a-f,pr,2019-08-15T03:37:19Z,[quant] Quantized comparators,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24387 [quant] Quantized comparators**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24387

Differential Revision: [D16824421](https://our.internmc.facebook.com/intern/diff/D16824421)",pytorch
24396,jjsjann123,pr,2019-08-15T08:06:42Z,[nhwc support for adaptive_avg_pool2d & adaptive_avg_pool2d_backward],"Initial kernel support added for optimized NHWC tensor.

TODO: currently backwards kernel spits out tensor with NHWC stride.
Unfortunately autograd restores grad to contiguous (in either copy or add). This
makes real perf tuning annoying to do. (since I cannot easily measure end-to-end
time in my python script)

My current kernel is blazing fast comparing to the original NCHW kernel in fp16,
since I avoided atomicAdd. I'll finish perf tuning after we merged some future
PR expanding NHWC support in the core.

",pytorch
24418,z-a-f,pr,2019-08-15T18:12:37Z,Fixes the adding of the observer to the FloatFunctional,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24418 Fixes the adding of the observer to the FloatFunctional**

Fixes #24394

The observer is not added correctlty, because one of the conditions is not met.

Differential Revision: [D16833951](https://our.internmc.facebook.com/intern/diff/D16833951)",pytorch
24421,z-a-f,pr,2019-08-15T18:27:52Z,Adds a placeholder for the 'mul' operator.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24447 Adding Scalar add/mul.
* #24444 Adding quantized mul kernel
* **#24421 Adds a placeholder for the 'mul' operator.**

Differential Revision: [D16833438](https://our.internmc.facebook.com/intern/diff/D16833438)",pytorch
24444,z-a-f,pr,2019-08-15T23:20:29Z,[quant] Adding quantized mul kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24873 [quant] Rewrite of the scalar mul
* #24447 [quant] Adding Scalar add/mul.
* **#24444 [quant] Adding quantized mul kernel**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24444

Differential Revision: [D16844824](https://our.internmc.facebook.com/intern/diff/D16844824)",pytorch
24447,z-a-f,pr,2019-08-15T23:56:49Z,[quant] Adding Scalar add/mul.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #25080 [quant] Rewriting the scalar add
* #24873 [quant] Rewrite of the scalar mul
* **#24447 [quant] Adding Scalar add/mul.**

Note: This should be landed ONLY after #24259

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24447

Differential Revision: [D16846006](https://our.internmc.facebook.com/intern/diff/D16846006)",pytorch
24785,zasdfgbnm,pr,2019-08-16T19:44:55Z,"Fix typo ""takes takes"" -> ""takes""",,pytorch
24792,z-a-f,pr,2019-08-16T21:34:34Z,[quant] Use absolute import of the parent folder without alias.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #24217 [quant] AddReLU module
* **#24792 [quant] Use absolute import of the parent folder without alias.**

This will prevent the circular dependencies in the future

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24792

Differential Revision: [D16868861](https://our.internmc.facebook.com/intern/diff/D16868861)",pytorch
24799,z-a-f,pr,2019-08-17T00:22:20Z,[quant] Added relu6 kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24799 [quant] Added relu6 kernel**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24799

Differential Revision: [D16875493](https://our.internmc.facebook.com/intern/diff/D16875493)",pytorch
24815,ppwwyyxx,pr,2019-08-18T05:57:43Z,Allow SyncBatchNorm without DDP in inference mode,"Summary: Fix https://github.com/pytorch/pytorch/issues/22538

Differential Revision: D16883694

",pytorch
24858,zasdfgbnm,pr,2019-08-19T19:27:04Z,Migrate gather into ATen,"Fixes: https://github.com/pytorch/pytorch/issues/24702 #24567 

This prepares for implementing broadcasting in the future. This diff is mostly copy paste and variable renaming. 

Edit: remove the outdated benchmark",pytorch
24873,z-a-f,pr,2019-08-19T23:22:11Z,[quant] Rewrite of the scalar mul,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#24873 [quant] Rewrite of the scalar mul**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/24873

Differential Revision: [D16906728](https://our.internmc.facebook.com/intern/diff/D16906728)",pytorch
24954,zasdfgbnm,pr,2019-08-21T06:36:25Z,Extend TensorIterator to support dim apply,"Example usage:

```C++
Tensor gather_with_broadcast_cpu(IntArrayRef outsizes, const Tensor &src, int64_t dim, const Tensor &index) {
  Tensor result = at::empty(outsizes, src.options());
  auto iter = TensorIterator::dim_apply_op(result, index, src, dim);
  int64_t size = outsizes[dim];
  cpu_apply_dim_kernel(iter,
    [=](float *result_data, int64_t result_stride, int64_t *index_data, int64_t index_stride, float *src_data, int64_t src_stride) {
      for (int64_t i = 0; i < size; i++) {
        int64_t index = *(index_data + i * index_stride);
        *(result_data + i * result_stride) = *(src_data + index * src_stride);
      }
    });
  return result;
}

Tensor gather_with_broadcast_cuda(IntArrayRef outsizes, const Tensor &src, int64_t dim, const Tensor &index) {
  Tensor result = at::empty(outsizes, src.options());
  auto iter = TensorIterator::dim_apply_op(result, index, src, dim);
  int64_t size = outsizes[dim];
  gpu_apply_dim_kernel(iter,
    [=] GPU_LAMBDA (float *result_data, int64_t result_stride, int64_t *index_data, int64_t index_stride, float *src_data, int64_t src_stride) {
      for (int64_t i = 0; i < size; i++) {
        int64_t index = *(index_data + i * index_stride);
        *(result_data + i * result_stride) = *(src_data + index * src_stride);
      }
    });
  return result;
}
```

Currently, there is no operator implemented using dim apply of tensor iterator. So the test is written in `tensor_iterator_test.cpp` and `cuda_tensor_iterator_test.cu` by manually implementing `gather` using tensor iterator.

Some optimizations like vectorization in CPU and 32bit indexing in GPU is not implemented in this PR. This could be further improved in the future when there is a real operator using this feature.",pytorch
25054,z-a-f,pr,2019-08-22T23:21:50Z,[quant] making quant utilities inplace,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25054 [quant] making quant utilities inplace**

Differential Revision: [D16974198](https://our.internmc.facebook.com/intern/diff/D16974198)",pytorch
25080,z-a-f,pr,2019-08-23T03:24:23Z,[quant] Rewriting the scalar add,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25080 [quant] Rewriting the scalar add**

Differential Revision: [D16979377](https://our.internmc.facebook.com/intern/diff/D16979377)",pytorch
25168,z-a-f,pr,2019-08-26T01:29:25Z,[quant] Adding return for the observer in the functional_modules.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25168 [quant] Adding return for the observer in the functional_modules.py**

Differential Revision: [D17048164](https://our.internmc.facebook.com/intern/diff/D17048164)",pytorch
25193,z-a-f,pr,2019-08-26T20:30:57Z,[quant] Fixing the enforcement of the zero_point,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25193 [quant] Fixing the enforcement of the zero_point**

Differential Revision: [D17058781](https://our.internmc.facebook.com/intern/diff/D17058781)",pytorch
25195,z-a-f,pr,2019-08-26T21:12:30Z,[quant] Reducing the test size for adaptive avg pool,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25195 [quant] Reducing the test size for adaptive avg pool**

The test will fila for large samples adue to deadline constraint in the hypothesis framework.

Differential Revision: [D17059087](https://our.internmc.facebook.com/intern/diff/D17059087)",pytorch
25275,z-a-f,pr,2019-08-27T22:28:38Z,[quant] Fixes test_equal,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25275 [quant] Fixes test_equal**

Differential Revision: [D17083204](https://our.internmc.facebook.com/intern/diff/D17083204)",pytorch
25291,z-a-f,pr,2019-08-28T04:37:49Z,"Back out ""[fix] Specify width for st.floats in hypothesis_utils.tensor""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25291 Back out ""[fix] Specify width for st.floats in hypothesis_utils.tensor""**

Original commit changeset: 0572fb810d8c

Differential Revision: [D17089044](https://our.internmc.facebook.com/intern/diff/D17089044/)",pytorch
25296,z-a-f,pr,2019-08-28T07:18:17Z,Removing future imports from the test fixtures.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#25296 Removing future imports from the test fixtures.**

Differential Revision: [D17090201](https://our.internmc.facebook.com/intern/diff/D17090201)",pytorch
26201,zasdfgbnm,pr,2019-09-13T21:16:48Z,Port fused layer_norm from APEX to ATen,"Originally implemented in:
https://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu

# Changes:
- Copy-paste the implementation from APEX to ATen. Codes are modified a bit to fit into ATen (for example: add support for the case `grad_out` is undefined, and to make it compatible with ROCm).
- API change: `at::layer_norm` no longer requires the `bool cudnn_enabled` as an argument. (do I need to keep this argument for back compatibility? It is no longer used.)
- `layer_norm` is now infinitely differentiable, but if higher order derivatives are asked (`create_graph=True` when computing backward), the performance of backwards is no longer optimized. (described below)
- Some minor maintainability thing:
  - The variables currently are named quite inconsistent, such as `X` vs `input`, `M` vs `n1`, `gamma` vs `weight`, I renamed a lot of them to make them consistent.
  - Also inconsistency in code style: `)\n{` vs `) {`, I changed all of them to `)\n{`

# About differentiability:
**Current implementation:**
- CPU: twice differentiable, optimized forward, backward and double backward code.
- GPU: infinitely differentiable, not optimized for performance

**This PR:**
Both CPU and GPU code now becomes infinitely differentiable and has optimized forward. And for backward, if `create_graph` set to false, then autograd will use the optimized code for computing backwards. But if `create_graph` set to true, then autograd will use a fallback infinitely differentiable implementation of the backward using ATen operators, which is not optimized for performance. This behavior is similar to what is done in WeightNorm (See: #10842).

Due to this change, the optimized CPU code for double backward becomes a dead code and is removed.

# Benchmark
Code (Jupyter Notebook):
```python
import torch
from torch.nn import LayerNorm
import warnings
import gc


LINE_WIDTH = 80
warnings.filterwarnings('ignore')
print('PyTorch version:', torch.__version__)
print()


def benchmark(*sizes):
    print('=' * LINE_WIDTH)
    print(""Benchmarking input shape"", sizes)
    normalized_shape = sizes[1:]
    layer_norm_cuda = LayerNorm(normalized_shape).cuda()
    layer_norm_cpu = LayerNorm(normalized_shape).cpu()
    
    input_cuda = torch.randn(*sizes, device='cuda', requires_grad=True)
    input_cpu = torch.randn(*sizes, device='cpu', requires_grad=True)

    input_bytes = input_cuda.numel() * input_cuda.element_size()
    print(""Element size"", input_cuda.element_size())
    print(""Size of the input tensor is"", input_bytes, ""bytes"")

    print('-' * LINE_WIDTH)

    print(""cuda forward:"")
    %timeit layer_norm_cuda(input_cuda); torch.cuda.synchronize()
    print(""cpu forward:"")
    %timeit layer_norm_cpu(input_cpu)

    print('-' * LINE_WIDTH)
    
    out_cuda = layer_norm_cuda(input_cuda)
    out_cpu = layer_norm_cpu(input_cpu)
    upstream_grad_cuda = torch.randn_like(out_cuda)
    upstream_grad_cpu = torch.randn_like(out_cpu)
    
    print('cuda backward, create_graph=False:')
    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True); torch.cuda.synchronize()
    gc.collect()
    print('cpu backward, create_graph=False:')
    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True)
    gc.collect()

    print('-' * LINE_WIDTH)
    
    print('cuda backward, create_graph=True:')
    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True, create_graph=True); torch.cuda.synchronize()
    gc.collect()
    print('cpu backward, create_graph=True:')
    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True, create_graph=True)
    gc.collect()

    print('=' * LINE_WIDTH)
    print()


benchmark(100, 100)
benchmark(1000, 100)
benchmark(100, 500)
```
Result on torch-nightly installed by pip:
```
PyTorch version: 1.3.0.dev20190920

================================================================================
Benchmarking input shape (100, 100)
Element size 4
Size of the input tensor is 40000 bytes
--------------------------------------------------------------------------------
cuda forward:
146 µs ± 16 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
55.1 µs ± 5.51 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
392 µs ± 66.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=False:
104 µs ± 15.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
1.01 ms ± 135 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=True:
146 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
================================================================================

================================================================================
Benchmarking input shape (1000, 100)
Element size 4
Size of the input tensor is 400000 bytes
--------------------------------------------------------------------------------
cuda forward:
116 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
191 µs ± 383 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
210 µs ± 7.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=False:
298 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
735 µs ± 53.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=True:
341 µs ± 1.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
================================================================================

================================================================================
Benchmarking input shape (100, 500)
Element size 4
Size of the input tensor is 200000 bytes
--------------------------------------------------------------------------------
cuda forward:
82 µs ± 958 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
101 µs ± 200 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
209 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=False:
168 µs ± 2.45 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
980 µs ± 337 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=True:
7.18 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 1000 loops each)
================================================================================
```
Result for this PR:
```
PyTorch version: 1.3.0a0+77e6902

================================================================================
Benchmarking input shape (100, 100)
Element size 4
Size of the input tensor is 40000 bytes
--------------------------------------------------------------------------------
cuda forward:
33.7 µs ± 97.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
26.3 µs ± 66.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
112 µs ± 3.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu backward, create_graph=False:
57.7 µs ± 1.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
1.15 ms ± 68 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=True:
369 µs ± 7.04 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
================================================================================

================================================================================
Benchmarking input shape (1000, 100)
Element size 4
Size of the input tensor is 400000 bytes
--------------------------------------------------------------------------------
cuda forward:
37.4 µs ± 95.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
136 µs ± 49.3 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
116 µs ± 1.83 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu backward, create_graph=False:
201 µs ± 7.78 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
The slowest run took 4.15 times longer than the fastest. This could mean that an intermediate result is being cached.
1.84 ms ± 1.16 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
cpu backward, create_graph=True:
592 µs ± 27.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
================================================================================

================================================================================
Benchmarking input shape (100, 500)
Element size 4
Size of the input tensor is 200000 bytes
--------------------------------------------------------------------------------
cuda forward:
34.5 µs ± 91.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu forward:
70.4 µs ± 25.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=False:
118 µs ± 5.52 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
cpu backward, create_graph=False:
131 µs ± 7.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
--------------------------------------------------------------------------------
cuda backward, create_graph=True:
1.72 ms ± 125 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
cpu backward, create_graph=True:
471 µs ± 45.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
================================================================================
```",pytorch
26220,zasdfgbnm,pr,2019-09-13T23:35:52Z,[DO NOT MERGE] Testing Coverage of layer_norm_double_backward_cpu,,pytorch
26550,jjsjann123,pr,2019-09-20T17:36:19Z,[BatchNorm num_batches_tracked update],"1. remove in-place operator on num_batches_tracked during forward calls:
     a. in-place operator on an input param puts heavy contraint on data
        dependency, as AliasDb assumes all input to alias each other.
     b. moreover, currently we incrementing on a scalar instead of tensor, which
        in-place operator is not supported in scripting.
2. move computation onto a scalar instead of a tensor, this saves a kernel
launch when we have the layer on GPU.

",pytorch
27057,zuoxingdong,pr,2019-09-30T09:59:31Z,[Update init.py]: Support independent random generator without resetting global seed,,pytorch
27164,z-a-f,pr,2019-10-01T20:32:11Z,[quantization] Factored out the default mappings,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27183 [quantization] Replacing the skip_list with white_list in the qconfig propagation
* **#27164 [quantization] Factored out the default mappings**

Differential Revision: [D17694475](https://our.internmc.facebook.com/intern/diff/D17694475)",pytorch
27183,z-a-f,pr,2019-10-01T23:24:18Z,[quantization] Replacing the skip_list with white_list in the qconfig propagation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27183 [quantization] Replacing the skip_list with white_list in the qconfig propagation**

Differential Revision: [D17700548](https://our.internmc.facebook.com/intern/diff/D17700548)",pytorch
27193,z-a-f,pr,2019-10-02T01:31:40Z,[quantization] Suppressing hypothesis health check for qnnpack_add,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27194 [quantization] Rename _intrinsic to intrinsic
* **#27193 [quantization] Suppressing hypothesis health check for qnnpack_add**

Differential Revision: [D17704958](https://our.internmc.facebook.com/intern/diff/D17704958)",pytorch
27194,z-a-f,pr,2019-10-02T01:31:50Z,[quantization] Rename _intrinsic to intrinsic,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27194 [quantization] Rename _intrinsic to intrinsic**

Differential Revision: [D17704957](https://our.internmc.facebook.com/intern/diff/D17704957)",pytorch
27299,jjsjann123,pr,2019-10-03T18:19:50Z,[BatchNorm update],"Removing in-place operator for num_batches_tracked increment. The in-place
operator used here turns out to block many optimization opportunities due to
alias assumption for inputs.

",pytorch
27339,z-a-f,pr,2019-10-03T22:54:41Z,[quantization] Show a warning that not all dir members of quantized work.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27339 [quantization] Show a warning that not all dir members of quantized work.**

This PR just shows a warning message.
Eventually we will show a correct __dir__

Differential Revision: [D17751333](https://our.internmc.facebook.com/intern/diff/D17751333)",pytorch
27351,z-a-f,pr,2019-10-04T00:54:13Z,[quantization] Adding aliased modules from nn to nn.quantized,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27351 [quantization] Adding aliased modules from nn to nn.quantized**

Differential Revision: [D17755356](https://our.internmc.facebook.com/intern/diff/D17755356)",pytorch
27363,z-a-f,pr,2019-10-04T05:27:06Z,[quantization] Adding docstrings for nnq.functional,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27363 [quantization] Adding docstrings for nnq.functional**

Differential Revision: [D17758907](https://our.internmc.facebook.com/intern/diff/D17758907)",pytorch
27473,z-a-f,pr,2019-10-07T18:01:04Z,Adding docstrings for nnq.functional,"This one didn't make it to the cut: https://github.com/pytorch/pytorch/pull/27363

Trying to catch the departing train :)",pytorch
27491,z-a-f,pr,2019-10-07T20:37:19Z,.gitignore for the docs folder,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27491 .gitignore for the docs folder**

Differential Revision: [D17796152](https://our.internmc.facebook.com/intern/diff/D17796152)",pytorch
27499,z-a-f,pr,2019-10-07T21:34:53Z,[DO NOT COMMIT] Test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27500 [DO NOT COMMIT] Test 1
* **#27499 [DO NOT COMMIT] Test**

Differential Revision: [D17797707](https://our.internmc.facebook.com/intern/diff/D17797707)",pytorch
27500,z-a-f,pr,2019-10-07T21:35:04Z,[DO NOT COMMIT] Test 1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27500 [DO NOT COMMIT] Test 1**
* #27499 [DO NOT COMMIT] Test

Differential Revision: [D17797706](https://our.internmc.facebook.com/intern/diff/D17797706)",pytorch
27657,z-a-f,pr,2019-10-09T23:46:28Z,Fix test_overwrite_module_params_on_conversion_cpu_cuda after type promotion introduced for comparison ops (#27066),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

",pytorch
27670,z-a-f,pr,2019-10-10T00:07:02Z,[quantization] Refactoring names for consistency,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #27779 [quantization] Changing observer name
* **#27670 [quantization] Refactoring names for consistency**

Differential Revision: [D17846269](https://our.internmc.facebook.com/intern/diff/D17846269)",pytorch
27779,z-a-f,pr,2019-10-11T21:12:54Z,[quantization] Changing observer name,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27779 [quantization] Changing observer name**
* #27670 [quantization] Refactoring names for consistency

Differential Revision: [D17886605](https://our.internmc.facebook.com/intern/diff/D17886605)",pytorch
27781,z-a-f,pr,2019-10-11T21:25:55Z,Changing the hypothesis dev verbosity to 'normal',"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27781 Changing the hypothesis dev verbosity to 'normal'**

Differential Revision: [D17887043](https://our.internmc.facebook.com/intern/diff/D17887043)",pytorch
27791,z-a-f,pr,2019-10-11T23:09:55Z,[quantization] Adding docstring to the observers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27791 [quantization] Adding docstring to the observers**

This is the first part of the change. The next ones will amend more :)
Fixes #27849 

Differential Revision: [D17889913](https://our.internmc.facebook.com/intern/diff/D17889913)",pytorch
27935,zasdfgbnm,pr,2019-10-14T21:22:05Z,[DO NOT MERGE][TESTING ONLY] Add cuda memcheck,,pytorch
27939,z-a-f,pr,2019-10-14T21:39:46Z,[quantization] Adding 'no_grad' to observer forward methods,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#27939 [quantization] Adding 'no_grad' to observer forward methods**

Differential Revision: [D17916804](https://our.internmc.facebook.com/intern/diff/D17916804)",pytorch
27967,zasdfgbnm,pr,2019-10-15T08:22:26Z,Install CUDA for clang-tidy,"fixes: https://github.com/pytorch/pytorch/issues/28009

clang-tidy is reporting `'cuda_runtime_api.h' file not found` when a PR modifying some file including this header.

Installation script take from official site:
https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=debnetwork",pytorch
28034,zasdfgbnm,pr,2019-10-15T20:52:10Z,Adding cuda-memcheck to CI,"This adds cuda-memcheck to our CI. This new test is ""important"" (triggered on PR). Currently it is only enabled on `test_torch.py` and we might need to disable some tests depending on the outcome of the test.",pytorch
28042,zasdfgbnm,pr,2019-10-15T21:18:42Z,Add scripts to run cuda-memcheck on unittests manually,"With this script we could run tests one by one using something like:
```
python test/run_test.py -i torch --cuda-memcheck --verbose TestTorch.test_empty_like
```
to see errors/hangs",pytorch
28127,zasdfgbnm,pr,2019-10-16T19:53:48Z,Add scripts to run cuda-memcheck,"This PR adds scripts that could be used for https://github.com/pytorch/pytorch/issues/26052

Example output:

```
Success: TestTorchDeviceTypeCPU.test_advancedindex_big_cpu
Success: TestTorchDeviceTypeCPU.test_addcmul_cpu
Success: TestTorchDeviceTypeCPU.test_addbmm_cpu_float32
Success: TestTorchDeviceTypeCPU.test_advancedindex_cpu_float16
Success: TestTorchDeviceTypeCPU.test_addmv_cpu
Success: TestTorchDeviceTypeCPU.test_addcdiv_cpu
Success: TestTorchDeviceTypeCPU.test_all_any_empty_cpu
Success: TestTorchDeviceTypeCPU.test_atan2_cpu
Success: TestTorchDeviceTypeCPU.test_advancedindex_cpu_float64
Success: TestTorchDeviceTypeCPU.test_baddbmm_cpu_float32
Success: TestTorchDeviceTypeCPU.test_atan2_edgecases_cpu
Success: TestTorchDeviceTypeCPU.test_add_cpu
Success: TestTorchDeviceTypeCPU.test_addr_cpu_bfloat16
Success: TestTorchDeviceTypeCPU.test_addr_cpu_float32
```",pytorch
28149,zasdfgbnm,pr,2019-10-16T21:37:31Z,[DO NOT MERGE] CudaNaiveAllocator,Naively use cudaMalloc to allocate memory. For debugging purposes only.,pytorch
28193,z-a-f,pr,2019-10-17T01:46:38Z,[quantization] Add default arg to `prepare_qat` mapping.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28193 [quantization] Add default arg to `prepare_qat` mapping.**

Fixes #28015

Differential Revision: [D17973121](https://our.internmc.facebook.com/intern/diff/D17973121)",pytorch
28241,zasdfgbnm,pr,2019-10-17T19:11:29Z,[WIP] Stop promoting by copying for operator add,"This is a partial fix of https://github.com/pytorch/pytorch/issues/26401.

I decide to keep the promote-by-copy support of `TensorIterator` because it allows us to easily implement type promotion with much less work, and allows us to intentionally implement type promotion of some operators by copy to keep the binary size small. It also allows us to improve the performance of type promotion for different operators incrementally.

In this PR, only GPU is fixed in this PR because the vectorized CPU code does not have support for operations of different types, which would require much more work to do.

This PR only changes add, other operators will be supported in future PR.

### nvprof result:
```python
import torch

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
torch.cuda.profiler.start()
r.add_(d)
torch.cuda.profiler.stop()
torch.cuda.synchronize()
```

### benchmark:
```python
import torch
print(torch.__version__)
print(torch.version.git_version)

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
%timeit r.add_(d); torch.cuda.synchronize()
```
original
```
1.4.0a0+7d277b0
7d277b0670eb1f9098a7e098e93b20453e8b5c9f
6.83 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
new
```
```",pytorch
28261,zasdfgbnm,pr,2019-10-17T23:33:24Z,[WIP] Make TensorIterator stop promoting types by copying,"Fixes: https://github.com/pytorch/pytorch/issues/26401

This PR fixes the issue by using the newly added dynamic cast inside `TensorIterator` so that instead of converting the type at the beginning (which generates extra kernel launches), the `TensorIterator` do a load-cast-compute-store for each element while looping. So there is only one read and one write of memory.

# Tests

**nvprof:**
```python
import torch

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
torch.cuda.profiler.start()
r.add_(d)
torch.cuda.profiler.stop()
torch.cuda.synchronize()
```

```
==11407== NVPROF is profiling process 11407, command: /home/xgao/anaconda3/bin/python simple.py
==11407== Profiling application: /home/xgao/anaconda3/bin/python simple.py
==11407== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:  100.00%  2.0611ms         1  2.0611ms  2.0611ms  2.0611ms  _ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE1_clEvEUlddE_EEvS4_RKT_EUliE_EEviT1_
      API calls:  100.00%  1.05006s         1  1.05006s  1.05006s  1.05006s  cudaLaunchKernel
                    0.00%  2.7740us         2  1.3870us     673ns  2.1010us  cudaGetDevice
                    0.00%  2.3730us         1  2.3730us  2.3730us  2.3730us  cudaSetDevice
                    0.00%     830ns         1     830ns     830ns     830ns  cudaGetLastError
```

**benchmark**
```python
import torch
print(torch.__version__)
print(torch.version.git_version)

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
%timeit r.add_(d); torch.cuda.synchronize()
```

original
```
1.4.0a0+7d277b0
7d277b0670eb1f9098a7e098e93b20453e8b5c9f
6.83 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

after
```
1.4.0a0+f0f2f65
f0f2f654cba9b8c569f0bcd583732bbc891f80b2
2.08 ms ± 139 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
28264,d4l3k,pr,2019-10-18T00:14:13Z,autograd/profiler: make python record_function use JIT methods,"This makes the python autograd.profiler.record_function use two new jit methods. This allows for using record_function in TorchScript modules.

See #28249 for discussion.

Differential Revision: D17997612

",pytorch
28325,z-a-f,pr,2019-10-18T23:37:19Z,Changing the hypothesis strategies to generate different values in the tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28325 Changing the hypothesis strategies to generate different values in the tensors**

Differential Revision: [D18021796](https://our.internmc.facebook.com/intern/diff/D18021796)",pytorch
28335,zasdfgbnm,pr,2019-10-19T16:49:30Z,"assertEquals is deprecated, use assertEqual instead",,pytorch
28342,zasdfgbnm,pr,2019-10-20T01:21:31Z,Fix some issue discovered by clang-tidy,,pytorch
28343,zasdfgbnm,pr,2019-10-20T03:15:19Z,Move type casting to c10/util/TypeCast.h,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28343 Move type casting to c10/util/TypeCast.h**
* #28352 Simplify copy kernel
* #28344 Make TensorIterator stop promoting types by copying
* #28343 Move type casting to c10/util/TypeCast.h

Type casting is used in copy, and will be used also in tensor iterator
in the next stacked diff. I move it to c10 to make it serve as an common
util for different things.

I also add two dynamic casting functions
- fetch_and_cast
- cast_and_store

fetch_and_cast fetch a value with dynamic type specified by a ScalarType
from a void pointer and cast it to a static type.

cast_and_store casts a static typed value into dynamic type specified
by a ScalarType, and store it into a void pointer.",pytorch
28344,zasdfgbnm,pr,2019-10-20T03:15:25Z,Make TensorIterator stop promoting types by copying,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28352 Simplify copy kernel
* **#28344 Make TensorIterator stop promoting types by copying**
* #28343 Move type casting to c10/util/TypeCast.h

Fixes: https://github.com/pytorch/pytorch/issues/26401

This PR fixes the issue by using the newly added dynamic cast inside
`TensorIterator` so that instead of converting the type at the beginning
(which generates extra kernel launches), the `TensorIterator` do a
load-cast-compute-store for each element while looping. So there is only
one read and one write of memory.

**nvprof:**
```python
import torch

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
torch.cuda.profiler.start()
r.add_(d)
torch.cuda.profiler.stop()
torch.cuda.synchronize()
```

```
==11407== NVPROF is profiling process 11407, command:
/home/xgao/anaconda3/bin/python simple.py
==11407== Profiling application: /home/xgao/anaconda3/bin/python
simple.py
==11407== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min
Max  Name
 GPU activities:  100.00%  2.0611ms         1  2.0611ms  2.0611ms
2.0611ms
_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE1_clEvEUlddE_EEvS4_RKT_EUliE_EEviT1_
      API calls:  100.00%  1.05006s         1  1.05006s  1.05006s
1.05006s  cudaLaunchKernel
                    0.00%  2.7740us         2  1.3870us     673ns
2.1010us  cudaGetDevice
                    0.00%  2.3730us         1  2.3730us  2.3730us
2.3730us  cudaSetDevice
                    0.00%     830ns         1     830ns     830ns
830ns  cudaGetLastError
```

**benchmark**
```python
import torch
print(torch.__version__)
print(torch.version.git_version)

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
%timeit r.add_(d); torch.cuda.synchronize()
```

original
```
1.4.0a0+7d277b0
7d277b0670eb1f9098a7e098e93b20453e8b5c9f
6.83 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

after
```
1.4.0a0+f0f2f65
f0f2f654cba9b8c569f0bcd583732bbc891f80b2
2.08 ms ± 139 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
28348,zasdfgbnm,pr,2019-10-20T22:04:45Z,[DO NOT MERGE] Simplify copy kernel,"Using the new type promotion and dynamic casting added to `TensorIterator`, the copy kernels could be greatly simplified.

# Benchmark

**Script:**
```python
import torch
import timeit
import pandas
import itertools
from tqdm import tqdm
import math
print(torch.__version__)
print()

_10M = 10 * 1024 ** 2

d = {}

for from_, to in tqdm(itertools.product(torch.testing.get_all_dtypes(), repeat=2)):
    if from_ not in d:
        d[from_] = {}
    a = torch.zeros(_10M, dtype=from_)
    min_ = math.inf
    for i in range(100):
        start = timeit.default_timer()
        a.to(to)
        end = timeit.default_timer()
        elapsed = end - start
        if elapsed < min_:
            min_ = elapsed
    d[from_][to] = int(elapsed * 1000 * 1000)
    
pandas.DataFrame(d)
```

**Before:**
![image](https://user-images.githubusercontent.com/1032377/67171274-2e93d000-f36b-11e9-8fa0-91edd7dbc8ec.png)

**After:**
![image](https://user-images.githubusercontent.com/1032377/67171200-d361dd80-f36a-11e9-9b22-66292e395a09.png)
",pytorch
28352,zasdfgbnm,pr,2019-10-21T03:46:39Z,Simplify copy kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28352 Simplify copy kernel**
* #28344 Make TensorIterator stop promoting types by copying
* #28343 Move type casting to c10/util/TypeCast.h

Using the new type promotion and dynamic casting added to
`TensorIterator`, the copy kernels could be greatly simplified.

**Script:**
```python
import torch
import timeit
import pandas
import itertools
from tqdm import tqdm
import math
print(torch.__version__)
print()

_10M = 10 * 1024 ** 2

d = {}

for from_, to in tqdm(itertools.product(torch.testing.get_all_dtypes(),
repeat=2)):
    if from_ not in d:
        d[from_] = {}
    a = torch.zeros(_10M, dtype=from_)
    min_ = math.inf
    for i in range(100):
        start = timeit.default_timer()
        a.to(to)
        end = timeit.default_timer()
        elapsed = end - start
        if elapsed < min_:
            min_ = elapsed
    d[from_][to] = int(elapsed * 1000 * 1000)

pandas.DataFrame(d)
```

**Before:**
![image](https://user-images.githubusercontent.com/1032377/67171274-2e93d000-f36b-11e9-8fa0-91edd7dbc8ec.png)

**After:**
![image](https://user-images.githubusercontent.com/1032377/67171200-d361dd80-f36a-11e9-9b22-66292e395a09.png)",pytorch
28426,zasdfgbnm,pr,2019-10-22T17:37:39Z,Move type casting to c10/util/TypeCast.h,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28428 Simplify copy kernel
* #28427 Make TensorIterator stop promoting types by copying
* **#28426 Move type casting to c10/util/TypeCast.h**

Type casting is used in copy, and will be used also in tensor iterator
in the next stacked diff. I move it to c10 to make it serve as an common
util for different things.

I also add two dynamic casting functions
- fetch_and_cast
- cast_and_store

fetch_and_cast fetch a value with dynamic type specified by a ScalarType
from a void pointer and cast it to a static type.

cast_and_store casts a static typed value into dynamic type specified
by a ScalarType, and store it into a void pointer.

Differential Revision: [D18170996](https://our.internmc.facebook.com/intern/diff/D18170996)",pytorch
28427,zasdfgbnm,pr,2019-10-22T17:37:45Z,Make TensorIterator stop promoting types by copying,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28428 Simplify copy kernel
* **#28427 Make TensorIterator stop promoting types by copying**
* #28426 Move type casting to c10/util/TypeCast.h

Fixes: https://github.com/pytorch/pytorch/issues/26401

This PR fixes the issue by using the newly added dynamic cast inside
`TensorIterator` so that instead of converting the type at the beginning
(which generates extra kernel launches), the `TensorIterator` do a
load-cast-compute-store for each element while looping. So there is only
one read and one write of memory.

**nvprof:**
```python
import torch

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
torch.cuda.profiler.start()
r.add_(d)
torch.cuda.profiler.stop()
torch.cuda.synchronize()
```

```
==11407== NVPROF is profiling process 11407, command:
/home/xgao/anaconda3/bin/python simple.py
==11407== Profiling application: /home/xgao/anaconda3/bin/python
simple.py
==11407== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min
Max  Name
 GPU activities:  100.00%  2.0611ms         1  2.0611ms  2.0611ms
2.0611ms
_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_15gpu_kernel_implIZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE1_clEvEUlddE_EEvS4_RKT_EUliE_EEviT1_
      API calls:  100.00%  1.05006s         1  1.05006s  1.05006s
1.05006s  cudaLaunchKernel
                    0.00%  2.7740us         2  1.3870us     673ns
2.1010us  cudaGetDevice
                    0.00%  2.3730us         1  2.3730us  2.3730us
2.3730us  cudaSetDevice
                    0.00%     830ns         1     830ns     830ns
830ns  cudaGetLastError
```

**benchmark**
```python
import torch
print(torch.__version__)
print(torch.version.git_version)

_100M = 100 * 1024 ** 2
r = torch.randn(_100M, dtype=torch.float32, device='cuda')
d = torch.randn(_100M, dtype=torch.float64, device='cuda')
torch.cuda.synchronize()
%timeit r.add_(d); torch.cuda.synchronize()
```

original
```
1.4.0a0+7d277b0
7d277b0670eb1f9098a7e098e93b20453e8b5c9f
6.83 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

after
```
1.4.0a0+f0f2f65
f0f2f654cba9b8c569f0bcd583732bbc891f80b2
2.08 ms ± 139 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

For more benchmark, see: https://github.com/pytorch/pytorch/pull/28344

Differential Revision: [D18170997](https://our.internmc.facebook.com/intern/diff/D18170997)",pytorch
28428,zasdfgbnm,pr,2019-10-22T17:37:52Z,Simplify copy kernel,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28428 Simplify copy kernel**
* #28427 Make TensorIterator stop promoting types by copying
* #28426 Move type casting to c10/util/TypeCast.h

Using the new type promotion and dynamic casting added to
`TensorIterator`, the copy kernels could be greatly simplified.

Benchmark on CUDA:
```python
import torch
import timeit
import pandas
import itertools
from tqdm.notebook import tqdm
import math
print(torch.__version__)
print()

_10M = 10 * 1024 ** 2

d = {}

for from_, to in tqdm(itertools.product(torch.testing.get_all_dtypes(), repeat=2)):
    if from_ not in d:
        d[from_] = {}
    a = torch.empty(_10M, dtype=from_, device='cuda')
    min_ = math.inf
    for i in range(100):
        torch.cuda.synchronize()
        start = timeit.default_timer()
        a.to(to)
        torch.cuda.synchronize()
        end = timeit.default_timer()
        elapsed = end - start
        if elapsed < min_:
            min_ = elapsed
    d[from_][to] = int(min_ * 1000 * 1000)
    
pandas.DataFrame(d)
```

original:
![image](https://user-images.githubusercontent.com/1032377/67623519-e3e6dd80-f7da-11e9-86ea-9cc9f237123b.png)

new:
![image](https://user-images.githubusercontent.com/1032377/67623527-fc56f800-f7da-11e9-82bd-dc1ff9821b68.png)

Differential Revision: [D18170995](https://our.internmc.facebook.com/intern/diff/D18170995)",pytorch
28563,zasdfgbnm,pr,2019-10-24T00:15:59Z,Split DifferentiableGraphOp out of graph_executor.cpp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28856 Allow printing DifferentiableGraphBackward objects
* #28626 Expose DifferentiableGraphBackward
* **#28563 Split DifferentiableGraphOp out of graph_executor.cpp**

DifferentiableGraphOp and the graph executor are two independent things.
Spliting them out will make future development more convenient.

This PR is purely copy and paste, and could be easily reviewed.

In the future I will further refactor things in backward.cpp,
in order to provide Python API to visualize backward graph created by
autodiff (#28549).",pytorch
28626,zasdfgbnm,pr,2019-10-24T23:42:56Z,Expose DifferentiableGraphBackward,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28856 Allow printing DifferentiableGraphBackward objects
* **#28626 Expose DifferentiableGraphBackward**
* #28563 Split DifferentiableGraphOp out of graph_executor.cpp

This is another step towards #28549

The original DifferentiableGraphBackward is refactored into
DifferentiableGraphBackward and DifferentiableGraphBackwardInternal

The DifferentiableGraphBackward is the base class that means to be
exposed a bit more (at the torch::jit::detail level vs previously
inside an anonymous namespace) and provide API that would be used
outside (outside of the graph executor, but still very internal
among torch) to access limited information.

The DifferentiableGraphBackwardInternal class contains all the details
of the actual implementation that is used inside JIT's graph executor.
This class is kept inside an anonymous namespace to avoid implementation
detail unexpectly exposed towards outside.",pytorch
28635,z-a-f,pr,2019-10-25T03:26:35Z,Better error message for quantized dispatch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28635 Better error message for quantized dispatch**

Fixes #28518

Differential Revision: [D18132566](https://our.internmc.facebook.com/intern/diff/D18132566)",pytorch
28710,z-a-f,pr,2019-10-25T21:23:46Z,[quantization] Enabling inplace relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28710 [quantization] Enabling inplace relu**

Differential Revision: [D18146120](https://our.internmc.facebook.com/intern/diff/D18146120)",pytorch
28843,zasdfgbnm,pr,2019-10-29T19:04:03Z,Bring back the stack #28426 with Windows build fixed,@ezyang This brings back the stack https://github.com/pytorch/pytorch/pull/28426 with hopefully windows build fixed. Let's wait for the CI to see what happens.,pytorch
28853,z-a-f,pr,2019-10-29T21:22:59Z,Fixing the shape calculation for pool tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #28325 Changing the hypothesis strategies to generate different values in the tensors
* **#28853 Fixing the shape calculation for pool tests**

Differential Revision: [D18212290](https://our.internmc.facebook.com/intern/diff/D18212290)",pytorch
28856,zasdfgbnm,pr,2019-10-29T22:20:33Z,Allow printing DifferentiableGraphBackward objects,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#28856 Allow printing DifferentiableGraphBackward objects**
* #28626 Expose DifferentiableGraphBackward
* #28563 Split DifferentiableGraphOp out of graph_executor.cpp

__repr__ is added for printing.

Sample code:

```python
import torch

@torch.jit.script
def f(a):
    return (a + 100) * 5

a = torch.tensor(0., requires_grad=True)

result = f(a)
print(result.grad_fn)
```

result:

```
graph(%0 : Tensor):
  %1 : int = prim::Constant[value=5]()
  %2 : Tensor = prim::GradOf[name=""aten::mul""](%0)
    block0():
      %3 : Tensor = aten::mul(%0, %1) # <string>:18:23
      -> (%3)
  %4 : Tensor = prim::GradOf[name=""aten::add""](%2)
    block0():
      -> (%2)
  %5 : (Tensor) = prim::TupleConstruct(%4)
  return (%5)

```

Question: Do I need to add unit test asserting the printted string?
I don't think so because DifferentiableGraphBackward is internal
and asserting the output adds overhead for the JIT development.",pytorch
28927,jjsjann123,pr,2019-10-30T21:56:27Z,[fixing cuda launch config failure on UpSampleNearest],"This is to fix #22526 

Adding limitation on launch config for grid sizes as well, previous code is asking to launch blocks more than what's supported by the hardware;
Test added in test_cuda;",pytorch
28989,zasdfgbnm,pr,2019-10-31T20:28:49Z,Fix CUDA shared memory out of bound access in findPattern,"This fixes https://github.com/pytorch/pytorch/issues/28789

Only the first two elements of `smem` are used in this function but at the beginning, it resets all the `C10_WARP_SIZE` to 0. When the `scalar_t` is 64bit, it goes out of the total shared memory size which is `sizeof(int) * C10_WARP_SIZE`, although this does not lead to any failure in CI.",pytorch
28995,zasdfgbnm,pr,2019-10-31T21:26:50Z,Remove copy-pasted code in THCTensorTopK.cuh,"This is independent from https://github.com/pytorch/pytorch/pull/28989, but when #28989 get landed, this fixes https://github.com/pytorch/pytorch/issues/28792 .",pytorch
29016,jjsjann123,pr,2019-11-01T03:22:20Z,[fixing cuda launch config failure on UpSampleNearest],"Adding limitation on launch config for grid size 
Test added in test_cuda;",pytorch
29054,z-a-f,pr,2019-11-01T19:16:02Z,Removing quantization from the dispatcher. Changing the message.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29054 Removing quantization from the dispatcher. Changing the message.**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/29054

Differential Revision: [D18276881](https://our.internmc.facebook.com/intern/diff/D18276881)",pytorch
29165,zasdfgbnm,pr,2019-11-05T01:00:39Z,Add note to docs of torch.unique,Fixes https://github.com/pytorch/pytorch/issues/19151,pytorch
29173,z-a-f,pr,2019-11-05T02:14:22Z,[quantization] Extracted quantize/dequantize out of linear.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29173 [quantization] Extracted quantize/dequantize out of linear.**

Differential Revision: [D18318561](https://our.internmc.facebook.com/intern/diff/D18318561)",pytorch
29174,z-a-f,pr,2019-11-05T02:36:03Z,qrelu benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29258 Removed the observer from the benchmarks
* #29257 Adding short tests
* #29250 qpool benchmarking
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.
* **#29174 qrelu benchmarking**

Differential Revision: [D18319345](https://our.internmc.facebook.com/intern/diff/D18319345)",pytorch
29182,z-a-f,pr,2019-11-05T03:43:54Z,Creating benchmarking for activations.,"This is a squash of the following PRs:

- #29427
- #29259
- #29320
- #29183


Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29182 Creating benchmarking for activations.**

Differential Revision: [D18319456](https://our.internmc.facebook.com/intern/diff/D18319456)",pytorch
29183,z-a-f,pr,2019-11-05T03:49:25Z,qrelu6 benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29427 Combining the ops in the qactivation benchmarks
* #29259 Adding longer tests for relu
* #29320 Adding short tests for qactivations
* **#29183 qrelu6 benchmarking**
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18319499](https://our.internmc.facebook.com/intern/diff/D18319499)",pytorch
29233,zasdfgbnm,pr,2019-11-05T19:53:23Z,Use cuDNN's handle pool mechanism to manage cublas handles,"Fixes https://github.com/pytorch/pytorch/issues/6962

The PR implements the handle pool mechanism for cublas as suggested by @mcarilli  in https://github.com/pytorch/pytorch/issues/6962#issuecomment-530563872.

~~I didn't add any unit test here yet because as @mcarilli mentioned:~~
> ~~On my local machine, out of curiosity I also rewrote that test to use gemms instead of convolutions. The race condition seemed rarer, but the test did show that cublas use is not thread safe. I can share the script if you want.~~

~~Please share your script with me @mcarilli. And if the race condition is rare, would it still be possible for the CI to detect it?~~

cc: @colesbury ",pytorch
29243,zasdfgbnm,pr,2019-11-05T21:04:30Z,Make test_torch.py pass cuda-memcheck,"Make the following changes:
- When there are more than 10k errors, cuda-memcheck only shows 10k errors, in this case we shouldn't raise an Exception
- Add UNDER_CUDA_MEMCHECK environment to allow disabling `pin_memory` tests when running cuda-memcheck. 
- Add a `--ci` command option, when turned on, then this script would run output to stdout instead of writing a file, and exit with an error if cuda-memcheck fails
- Add a `--nohang` command option. When turned on, then hang would be treated as pass instead of error
- Do simple filtering on the test to run: if `'cpu'` in the test name but not `'cuda'` is not in the test name
- Add `--split` and `--rank` to allowing splitting the work (NVIDIA CI has a limitation of 3 hours, we have to split the work to satisfy this limitation)
- The error summary could be `ERROR SUMMARY: 1 error`, or `ERROR SUMMARY: 2 errors`, the tail could be `error` or `errors`, it is not of the same length. The script is fixed to handle this case.
- Ignore errors from `cufft`",pytorch
29245,z-a-f,pr,2019-11-05T21:28:15Z,Adding inplace quantized relu6,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29245 Adding inplace quantized relu6**

Differential Revision: [D18334541](https://our.internmc.facebook.com/intern/diff/D18334541)",pytorch
29250,z-a-f,pr,2019-11-05T23:57:04Z,qpool benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29274 qadaptive_avgpool2d benchmarking
* #29268 qavgpool benchmarking
* #29257 Adding short tests to qmaxpool benchmarking
* **#29250 qpool benchmarking**

Differential Revision: [D18339142](https://our.internmc.facebook.com/intern/diff/D18339142)",pytorch
29257,z-a-f,pr,2019-11-06T00:48:10Z,Adding short tests to qmaxpool benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29274 qadaptive_avgpool2d benchmarking
* #29268 qavgpool benchmarking
* **#29257 Adding short tests to qmaxpool benchmarking**

Differential Revision: [D18340536](https://our.internmc.facebook.com/intern/diff/D18340536)",pytorch
29258,z-a-f,pr,2019-11-06T01:14:49Z,Removed the observer from the benchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29274 qadaptive_avgpool2d benchmarking
* #29268 qavgpool benchmarking
* #29259 Adding longer tests for relu
* **#29258 Removed the observer from the benchmarks**
* #29257 Adding short tests
* #29250 qpool benchmarking
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18341544](https://our.internmc.facebook.com/intern/diff/D18341544)",pytorch
29259,z-a-f,pr,2019-11-06T01:23:10Z,Adding longer tests for relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29427 Combining the ops in the qactivation benchmarks
* **#29259 Adding longer tests for relu**
* #29320 Adding short tests for qactivations
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18341545](https://our.internmc.facebook.com/intern/diff/D18341545)",pytorch
29268,z-a-f,pr,2019-11-06T02:02:56Z,qavgpool benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29274 qadaptive_avgpool2d benchmarking
* **#29268 qavgpool benchmarking**
* #29257 Adding short tests to qmaxpool benchmarking

Differential Revision: [D18342589](https://our.internmc.facebook.com/intern/diff/D18342589)",pytorch
29270,z-a-f,pr,2019-11-06T02:27:32Z,qadaptive avg pooling benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29270 qadaptive avg pooling benchmarking**
* #29268 qavgpool benchmarking
* #29259 Adding longer tests for relu
* #29258 Removed the observer from the benchmarks
* #29257 Adding short tests
* #29250 qpool benchmarking
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18342898](https://our.internmc.facebook.com/intern/diff/D18342898)",pytorch
29274,z-a-f,pr,2019-11-06T03:38:05Z,qadaptive_avgpool2d benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29274 qadaptive_avgpool2d benchmarking**
* #29268 qavgpool benchmarking
* #29257 Adding short tests to qmaxpool benchmarking

Differential Revision: [D18343569](https://our.internmc.facebook.com/intern/diff/D18343569)",pytorch
29320,z-a-f,pr,2019-11-06T20:32:26Z,Adding short tests for qactivations,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29427 Combining the ops in the qactivation benchmarks
* #29259 Adding longer tests for relu
* **#29320 Adding short tests for qactivations**
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18355887](https://our.internmc.facebook.com/intern/diff/D18355887)",pytorch
29361,jjsjann123,pr,2019-11-07T05:20:34Z,NHWC support in cuDNN BatchNorm & Conv2d,"This reverts the 9a9bb448ee49a1493f22bbbeed4af92b1364fce9

Fixing the broken case which reverts the previous commit.
details about fix:
	modified:   aten/src/ATen/native/Convolution.cpp

called contiguous on 3D input tensor. This avoids the code path to accidentally
recognize the input as channel_last stride, due to unsqueezing of permuted 3d
tensor.

",pytorch
29415,z-a-f,pr,2019-11-07T22:55:27Z,Factoring out quantized benchmark imports,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29415 Factoring out quantized benchmark imports**

Differential Revision: [D18382284](https://our.internmc.facebook.com/intern/diff/D18382284)",pytorch
29420,z-a-f,pr,2019-11-07T23:30:44Z,qadd benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29424 Added all binary arithmetic tests in QFunctional
* **#29420 qadd benchmarking**

Differential Revision: [D18383402](https://our.internmc.facebook.com/intern/diff/D18383402)",pytorch
29424,z-a-f,pr,2019-11-08T00:34:26Z,Added all binary arithmetic tests in QFunctional,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29424 Added all binary arithmetic tests in QFunctional**
* #29420 qadd benchmarking

Differential Revision: [D18385689](https://our.internmc.facebook.com/intern/diff/D18385689)",pytorch
29426,zasdfgbnm,pr,2019-11-08T00:42:09Z,Use handle pool to manage cusparse handles,"Fixes https://github.com/pytorch/pytorch/issues/29352

The newly added test fails consistently with illegal memory access without this PR, and now it succeeds consistently.",pytorch
29427,z-a-f,pr,2019-11-08T00:47:40Z,Combining the ops in the qactivation benchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29427 Combining the ops in the qactivation benchmarks**
* #29259 Adding longer tests for relu
* #29320 Adding short tests for qactivations
* #29183 qrelu6 benchmarking
* #29182 Creating a base benchmarking class for activations.

Differential Revision: [D18386286](https://our.internmc.facebook.com/intern/diff/D18386286)",pytorch
29428,zasdfgbnm,pr,2019-11-08T00:51:26Z,Improving BinaryOpsKernel.cu,"- Building `BinaryOpsKernel.cu` takes extremely long. Split the original file into 3 pieces, and copy-paste code into these files.
- Remove some useless logic
- change some wrong ops name `*_cpu` -> `*_cuda`",pytorch
29431,z-a-f,pr,2019-11-08T01:41:38Z,Quantized concat benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29431 Quantized concat benchmarking**

Differential Revision: [D18387765](https://our.internmc.facebook.com/intern/diff/D18387765)",pytorch
29437,z-a-f,pr,2019-11-08T04:02:30Z,quantized comparators benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29437 quantized comparators benchmarking**

Differential Revision: [D18389909](https://our.internmc.facebook.com/intern/diff/D18389909)",pytorch
29503,z-a-f,pr,2019-11-09T05:03:56Z,Quantized unary ops benchmarking (mostly template),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29505 quantized topk benchmarking
* **#29503 Quantized unary ops benchmarking (mostly template)**

Differential Revision: [D18414589](https://our.internmc.facebook.com/intern/diff/D18414589)",pytorch
29505,z-a-f,pr,2019-11-09T06:05:10Z,quantized topk benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29505 quantized topk benchmarking**
* #29503 Quantized unary ops benchmarking (mostly template)

Differential Revision: [D18414851](https://our.internmc.facebook.com/intern/diff/D18414851)",pytorch
29506,z-a-f,pr,2019-11-09T06:42:57Z,per-tensor quantize/dequantize benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #29507 FakeQuantize benchmarking
* **#29506 per-tensor quantize/dequantize benchmarking**

Differential Revision: [D18415017](https://our.internmc.facebook.com/intern/diff/D18415017)",pytorch
29507,z-a-f,pr,2019-11-09T07:01:07Z,FakeQuantize benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29507 FakeQuantize benchmarking**
* #29506 per-tensor quantize/dequantize benchmarking

Differential Revision: [D18415084](https://our.internmc.facebook.com/intern/diff/D18415084)",pytorch
29508,z-a-f,pr,2019-11-09T07:36:29Z,observer benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29508 observer benchmarking**

Differential Revision: [D18415171](https://our.internmc.facebook.com/intern/diff/D18415171)",pytorch
29509,z-a-f,pr,2019-11-09T08:07:53Z,Quantized interpolation benchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29509 Quantized interpolation benchmarks**

Differential Revision: [D18415367](https://our.internmc.facebook.com/intern/diff/D18415367)",pytorch
29576,z-a-f,pr,2019-11-11T19:58:28Z,Disabling 'contig' in quantized arithmetic test,"Temporarily disabling the non-contiguous test. Should be reenabled once #29435 is resolved.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29576 Disabling 'contig' in quantized arithmetic test**

Differential Revision: [D18433052](https://our.internmc.facebook.com/intern/diff/D18433052)",pytorch
29612,zasdfgbnm,pr,2019-11-11T23:49:12Z,Complex support on GPU for dynamic casting,"Currently, the dynamic casting mechanism is implemented assuming no support of complex on GPU. This will no longer be true in the soon future.

https://github.com/pytorch/pytorch/pull/29547 could clear some clang warning but the complex support on GPU is still not complete:
- fetch is not supported
- casting between complex64 and complex128 is not supported
- complex scalar types are not tested

This PR is what should be done for type promotion in order to add support to complex dtype on GPU, as suggested in https://github.com/pytorch/pytorch/issues/755#issuecomment-552631381

Note that what is newly added here in this PR is not tested due to the lack of basic support of complex dtypes (I can not construct a complex tensor). But his PR shouldn't break any existing part of PyTorch. 

For the merge this PR, consider two options:
- We could merge this PR now so that @dylanbespalko could conveniently work based on master, if there is something wrong here not found by code review, @dylanbespalko would find when adding complex integration.
- Or, we could just leave this PR open, don't merge it. But then @dylanbespalko might need to manually apply this to his branch in order to support type promotion of complex.
",pytorch
29625,z-a-f,pr,2019-11-12T01:42:32Z,Benchmarking quantized methods,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29625 Benchmarking quantized methods**

This PR also adds a template for benchmarking methods that require no input.

Differential Revision: [D18443485](https://our.internmc.facebook.com/intern/diff/D18443485)",pytorch
29626,jjsjann123,pr,2019-11-12T02:00:47Z,SyncBatchNorm Update on input dimension checks,"update the requirements on input dimensions for `torch.nn.SyncBatchNorm`:
1. 2D inputs is now permissible, #20204 ;
2. requires at least two element along normalization plane (BatchNorm behavior);

",pytorch
29627,z-a-f,pr,2019-11-12T02:00:55Z,Benchmarking per channel quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29627 Benchmarking per channel quantization**

Differential Revision: [D18443929](https://our.internmc.facebook.com/intern/diff/D18443929)",pytorch
29631,zasdfgbnm,pr,2019-11-12T04:48:54Z,Simplify copy kernel with static_cast_with_inter_type,"After https://github.com/pytorch/pytorch/pull/29612 get merged, `static_cast_with_inter_type` can now automatically convert complex types to its real values, therefore there is no need to do it inside copy kernel.

This should wait until https://github.com/pytorch/pytorch/pull/29612 get merged, otherwise it won't pass CI.",pytorch
29640,z-a-f,pr,2019-11-12T08:01:17Z,Changed default args in quantization observers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29640 Changed default args in quantization observers**

Differential Revision: [D18447297](https://our.internmc.facebook.com/intern/diff/D18447297)",pytorch
29663,z-a-f,pr,2019-11-12T19:56:43Z,Fixing data type in quantized pool benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29663 Fixing data type in quantized pool benchmarking**

Differential Revision: [D18456671](https://our.internmc.facebook.com/intern/diff/D18456671)",pytorch
29685,z-a-f,pr,2019-11-12T23:23:25Z,Fix a missing comma in quantized benchmark,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#29685 Fix a missing comma in quantized benchmark**

Differential Revision: [D18463246](https://our.internmc.facebook.com/intern/diff/D18463246)",pytorch
29700,jjsjann123,pr,2019-11-13T01:47:34Z,AdaptiveAvgPooling nhwc cuda update,"1. Add clip on grid launch configs (Tests added in test_nn.py)
2. Assert on shared memory requirement, gives better hint when error out;",pytorch
29742,zasdfgbnm,pr,2019-11-13T19:30:10Z,Update ATen/native/README.md about broadcasting,Is this description still true? I have never seen any `s_` ops.,pytorch
29743,zasdfgbnm,pr,2019-11-13T20:00:04Z,Improve compare kernel,"Currently, the way the compare kernels handle dtypes is very funny (this behavior is introduced in https://github.com/pytorch/pytorch/pull/28427 and I just realize it today):

Let's say `a, b` are two float tensors on CUDA.

If you do `a < b`, this is what would happen inside the loop:
- Step 1: Fetch `a` and `b`, dynamically cast them from `float` to `float`. (i.e. check the scalar type to figure out if it needs cast. it doesn't. so do nothing then.)
- Step 2: compute `a < b`, get a `bool` result
- Step 3: statically cast the result into `float`
- Step 3: do a dynamic cast of the result from `float` to `bool` and store the value

And if you do `a.lt_(b)`, this is what would happen:
- Step 1: Fetch `a` and `b`, no casting
- Step 2: compute `a < b`, get a `bool` result
- Step 3: statically cast the result into `float`
- Step 4: store the result to memory, no casting

Although dynamic casting happens on registers, it still hurt the performance a bit (~8%).

This PR fixes this issue. Now for compare kernels, if the output is bool and inputs have the same dtype, then there is no dynamic casting. Otherwise, there will be dynamic casting for each input and output. That is, the dynamic casting behavior of the two cases described above are swapped.

Benchmark on `a < b` for tensor of 1000000000 fp32 elements:
Before #28427 6.35 ms
Current master: 6.88 ms
With this PR: 6.36 ms
Benchmark on `a.lt_(b)` does not show any difference across versions.

Besides this, what worries me most is, with type promotion, the logic for tensor iterator is becoming super complicated, and it is hard to see if one change causes the performance regression of others. I suggest we create scripts that could benchmark tensor iterator entirely, review that code and put it somewhere inside the repository (maybe under `/tools` or `/test/scripts`?), and whenever we are not certain about the performance we could run it to check. (I guess not on this PR but on PRs after the script is done. If there are worries about performance, the author of PRs should run the script manually, and the reviewer should remind PR author to do so if necessary) If this is a good idea, I will send a PR for the script.",pytorch
29974,jjsjann123,pr,2019-11-16T22:17:07Z,[NHWC SUPPORT] cuDNN conv channels_last behavior update.,"1. Updated channels_last propagation rule on cuDNN convolution:
If input or weight is in channels_last memory_format, we'll compute the layer
using channels_last memory_format and propagate the format to the output as well

This is to allow user to easily specify the desired memory_format for
convolution layers, which could potentially offer considerable speed up when
using proper memory_format.

2. Provided util function to recursively convert memory format of
``nn.Conv2d.weight`` to specified memory_format:

    >>> convert_conv2d_weight_memory_layout(module, torch.channels_last)

",pytorch
30031,jjsjann123,pr,2019-11-18T20:29:40Z,disabling persistent mode for cuDNN BN on NCHW,"This is to help the bisecting for unstable convergence that #29997 targets, comparing to the other PR, this one is a smaller hammer (few lines of code change) and would facilitate our future repro/fix.",pytorch
30148,z-a-f,pr,2019-11-20T08:04:14Z,dynamicly quantized linear benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30148 dynamicly quantized linear benchmarking**

Differential Revision: [D18613006](https://our.internmc.facebook.com/intern/diff/D18613006)",pytorch
30149,z-a-f,pr,2019-11-20T08:38:55Z,dynamicly quantized lstm benchmarking,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#30149 dynamicly quantized lstm benchmarking**

Differential Revision: [D18613005](https://our.internmc.facebook.com/intern/diff/D18613005)",pytorch
30174,jjsjann123,pr,2019-11-20T19:59:19Z,[NHWC SUPPORT] Fix suggest_memory_format() for ambiguous cases,"This PR is to fix the ambiguity in NC11 and N1HW tensor when specifying
memory_format.
Previous implementation unnecessarily associate memory contiguity with memory
format, which undermines the expressiveness of memory_format for the ambiguous
cases.

e.g. Previously, if we have NC11 tensor in channels_last format. Calling
contiguous on that tensor doesn't change the memory_format to NCHW.

",pytorch
30203,zasdfgbnm,pr,2019-11-21T00:34:11Z,[JIT] Fix bug of not supporting `retain_graph` in autograd,Fixes https://github.com/pytorch/pytorch/issues/30204,pytorch
30234,zuoxingdong,pr,2019-11-21T14:36:16Z,[Update init.py]: Support independent random generator without resetting global seed,@soumith Cleaner way for #27057,pytorch
30677,d4l3k,pr,2019-12-03T19:38:38Z,autograd/profiler: support merging FunctionEventAvg,"Summary: Currently you can only add FunctionEvents to FunctionEventAvg. This makes it so you can add multiple FunctionEventAvg objects together. This is useful for merging multiple profiles together such as when dealing with distributed training.

Test Plan:
added unit test

  buck test //caffe2/test:autograd -- test_profiler

Differential Revision: D18785578

",pytorch
30697,zasdfgbnm,pr,2019-12-03T22:41:39Z,Create benchmarking tool for TensorIterator,"Fixes https://github.com/pytorch/pytorch/issues/30248

This adds a benchmarking tool for TensorIterator, currently supporting unary and binary ops. You can see a live demo here: http://atl.ipv6.ai/

The live demo could be run by the following command
```
python main.py compare --port 80 1.3.1.json 1.4.0.dev20191203.json
```
where `1.3.1` is the baseline and `1.4.0.dev20191203` is the version we want to compare against the baseline.

If you select a configuration and no plot is shown, this means there is no data for that configuration (it is not benchmarked).

An interesting catch is below:
![image](https://user-images.githubusercontent.com/1032377/70095484-bb23e600-15d9-11ea-82d1-21fc3fd52656.png)

The way type promotion is handled is changed, removing a few copies.

Please explore the demo to see if it make sense, and look carefully at
```
benchmarks/tensor_iterator_benchmark/benchmark/timing.py
```
to check if I am measuring the timing correctly.

cc: @csarofeen for visibility

",pytorch
30838,zasdfgbnm,pr,2019-12-05T19:23:20Z,Fix half->float case of softmax backward when inner_size is not 1,"Fixes https://github.com/pytorch/pytorch/issues/30572

That unit test is tested to fail with master and success with this PR.",pytorch
30849,zasdfgbnm,pr,2019-12-05T22:51:56Z,[WIP] Remove unused dgemv_ and sgemv_,"These functions are declared and used but never defined anywhere. So the guard
```c++
#if defined(USE_BLAS) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
```
must always be false",pytorch
30898,zasdfgbnm,pr,2019-12-06T19:45:29Z,Migrate addmv and mv from legacy to ATen native (CUDA & CPU),"# Summary
Fixes: https://github.com/pytorch/pytorch/issues/24605 https://github.com/pytorch/pytorch/issues/24535 https://github.com/pytorch/pytorch/issues/24739 https://github.com/pytorch/pytorch/issues/24680 https://github.com/pytorch/pytorch/issues/30986

This does not fix https://github.com/pytorch/pytorch/issues/29984, it will be fixed in later PR.

Most of this PR is just following the same logic inside TH and THC except the handle of n-dimensional zero-sized tensor, in specific the case:
```
(m,).addmv((m, 0), (0,), beta, alpha)
```

#  Legacy code bugs and how this PR deal with it

The above case is a case where BLAS often have a mismatch of semantics with PyTorch: For BLAS and cuBLAS, the above is a noop, but for PyTorch, it is a scalar-vector multiplication `output = beta * input`. The handle of this case is already very poor in legacy code and it is poorly tested:

For the CPU implementation, there are two code paths:
- Path 1: when dtype is float or double and `USE_BLAS`, then use BLAS
- Path 2: when other dtypes or not `USE_BLAS`, use a fallback kernel in PyTorch

For the CUDA implementation, there are also two code paths:
- Path 1: when float or double, then use `cublasSgemv` or `cublasDgemv` in cuBlas
- Path 2: when half, dispatch to `addmm`

`test_blas_alpha_beta_empty` is supposed to cover all cases, but unfortunately, it only tests the Path 1 of CUDA and Path 1 of CPU, and both uncovered paths (path 2 for CPU and path 2 for CUDA) are buggy in legacy code. In this PR, I expanded the coverage of `test_blas_alpha_beta_empty`, but unfortunately, I have to skip the `half` dtype on CUDA 9. See the description below for detail:

## Bug on CPU implementation

For the CPU implementation, the fallback kernel in path 2 already has the same semantics as PyTorch, not BLAS. But the code that tries to correct BLAS semantics to match PyTorch also runs on this case, leading to double correction, that is, `output = beta * input` now becomes `output = beta * beta * input`. 

This leads to the issue https://github.com/pytorch/pytorch/issues/30986 I just opened, and it is fixed in this PR.

## Bug on CUDA implementation

For the CUDA implementation, path 2 dispatches to
```
(m, 1).addmm((m, 0), (0, 1), beta, alpha)
```
But unfortunately, for some old CUDA version when on old GPU on half dtype, the above is also noop, which is definitely not correct.

But from what I see, on newer CUDA version or newer GPU, this is not a problem. This is a bug of PyTorch in `addmm`, so I opened a new issue https://github.com/pytorch/pytorch/issues/31006 to track this problem. But this is highly likely a dependency bug for PyTorch originating from cuBLAS, and it is only on a rarely used edge case on old hardware and software, so this issue would be a `won't_fix` unless some real requirements strongly indicate that this should be fixed.

This issue is already with legacy code, and this PR does not make it worse. To prevent this issue from bothering us, I disable the test of `half` dtype for CUDA 9 when expanding the coverage of `test_blas_alpha_beta_empty`.

I promote a CircleCI CUDA 10.1 test to `XImportant` so that it runs on PRs, because the path 2 of CUDA implementation is only covered by this configuration. Let me know if I should revert this change.

## An additional problem

In legacy code for `addmv`, dtype `bfloat16` is enabled and dispatch to `addmm`, but `addmm` does not support `bfloat16` from what I test. I do the same thing in the new code. Let me know if I should do it differently.

# Benchmark

Code:
```python
import torch
print(torch.__version__)

for i in range(1000):
    torch.arange(i, device='cuda')

print('cpu')
for i in 10, 100, 1000, 10000:
    a = torch.randn((i,))
    b = torch.randn((i, i))
    c = torch.randn((i,))
    %timeit a.addmv(b, c, alpha=1, beta=2)
    
print('cuda')
for i in 10, 100, 1000, 10000:
    a = torch.randn((i,)).cuda()
    b = torch.randn((i, i)).cuda()
    c = torch.randn((i,)).cuda()
    torch.cuda.synchronize()
    %timeit a.addmv(b, c, alpha=1, beta=2); torch.cuda.synchronize()
```

Before:
```
1.5.0a0+2b45368
cpu
2.74 µs ± 30.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
8.5 µs ± 85.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
686 µs ± 2.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
74 ms ± 410 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
cuda
The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached.
27.6 µs ± 23 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
17.3 µs ± 151 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
20.5 µs ± 369 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
756 µs ± 6.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

After:
```
1.5.0a0+66b4034
cpu
3.29 µs ± 20 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
9.09 µs ± 7.41 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
687 µs ± 7.01 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
73.8 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
cuda
18.2 µs ± 478 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
17.7 µs ± 299 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
21.5 µs ± 2.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
751 µs ± 35.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```",pytorch
30991,zasdfgbnm,pr,2019-12-09T20:42:04Z,[DO_NOT_MERGE] test_blas_alpha_beta_empty,,pytorch
31013,z-a-f,pr,2019-12-09T23:50:38Z,qnnpack TanH,"
```console
./build/local/tanh-test
Running main() from gtest_main.cc
[==========] Running 18 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 18 tests from TANH_OP
[ RUN      ] TANH_OP.zero_batch
[       OK ] TANH_OP.zero_batch (0 ms)
[ RUN      ] TANH_OP.unit_batch
[       OK ] TANH_OP.unit_batch (0 ms)
[ RUN      ] TANH_OP.unit_batch_with_qmin
[       OK ] TANH_OP.unit_batch_with_qmin (1 ms)
[ RUN      ] TANH_OP.unit_batch_with_qmax
[       OK ] TANH_OP.unit_batch_with_qmax (0 ms)
[ RUN      ] TANH_OP.unit_batch_with_input_scale
[       OK ] TANH_OP.unit_batch_with_input_scale (1 ms)
[ RUN      ] TANH_OP.unit_batch_with_input_zero_point
[       OK ] TANH_OP.unit_batch_with_input_zero_point (1 ms)
[ RUN      ] TANH_OP.small_batch
[       OK ] TANH_OP.small_batch (0 ms)
[ RUN      ] TANH_OP.small_batch_with_input_stride
[       OK ] TANH_OP.small_batch_with_input_stride (1 ms)
[ RUN      ] TANH_OP.small_batch_with_output_stride
[       OK ] TANH_OP.small_batch_with_output_stride (0 ms)
[ RUN      ] TANH_OP.small_batch_with_qmin
[       OK ] TANH_OP.small_batch_with_qmin (0 ms)
[ RUN      ] TANH_OP.small_batch_with_qmax
[       OK ] TANH_OP.small_batch_with_qmax (1 ms)
[ RUN      ] TANH_OP.small_batch_with_input_scale
[       OK ] TANH_OP.small_batch_with_input_scale (1 ms)
[ RUN      ] TANH_OP.small_batch_with_input_zero_point
[       OK ] TANH_OP.small_batch_with_input_zero_point (1 ms)
[ RUN      ] TANH_OP.strided_batch
[       OK ] TANH_OP.strided_batch (0 ms)
[ RUN      ] TANH_OP.strided_batch_with_qmin
[       OK ] TANH_OP.strided_batch_with_qmin (1 ms)
[ RUN      ] TANH_OP.strided_batch_with_qmax
[       OK ] TANH_OP.strided_batch_with_qmax (0 ms)
[ RUN      ] TANH_OP.strided_batch_with_input_scale
[       OK ] TANH_OP.strided_batch_with_input_scale (1 ms)
[ RUN      ] TANH_OP.strided_batch_with_input_zero_point
[       OK ] TANH_OP.strided_batch_with_input_zero_point (2 ms)
[----------] 18 tests from TANH_OP (11 ms total)

[----------] Global test environment tear-down
[==========] 18 tests from 1 test case ran. (11 ms total)
[  PASSED  ] 18 tests.
```

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #31031 [Draft] Quantized H Tangent function
* **#31013 qnnpack TanH**

Differential Revision: [D18898903](https://our.internmc.facebook.com/intern/diff/D18898903)",pytorch
31031,z-a-f,pr,2019-12-10T03:48:35Z,Quantized H Tangent function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31031 Quantized H Tangent function**

This activation will be needed for the LSTM implementation.
Also includes the QNNPack implementation.

Differential Revision: [D19334280](https://our.internmc.facebook.com/intern/diff/D19334280)",pytorch
31144,zasdfgbnm,pr,2019-12-12T00:23:18Z,Replace some thrust usage with cub in torch.unique to gain 2-3x speedup on GPU,"Some of the thrust usages are replaced with cub for `torch.unique` and `torch.unique_consecutive`.

Benchmark code:

```python
import torch
print(torch.__version__)
print()

a = torch.randint(1000, (100000,)).cuda()
torch.cuda.synchronize()

for i in range(100):
    a.unique()

print('single runs:')
torch.cuda.synchronize()
%timeit a.unique(); torch.cuda.synchronize()
%timeit a.unique(return_counts=True); torch.cuda.synchronize()
%timeit a.unique(return_counts=True, return_inverse=True); torch.cuda.synchronize()
%timeit a.unique_consecutive(); torch.cuda.synchronize()
%timeit a.unique_consecutive(return_counts=True); torch.cuda.synchronize()
%timeit a.unique_consecutive(return_counts=True, return_inverse=True); torch.cuda.synchronize()

def run100(f):
    for i in range(100):
        f()

print()
print('100 runs:')
torch.cuda.synchronize()
%timeit run100(lambda: a.unique()); torch.cuda.synchronize()
%timeit run100(lambda: a.unique(return_counts=True)); torch.cuda.synchronize()
%timeit run100(lambda: a.unique(return_counts=True, return_inverse=True)); torch.cuda.synchronize()
%timeit run100(lambda: a.unique_consecutive()); torch.cuda.synchronize()
%timeit run100(lambda: a.unique_consecutive(return_counts=True)); torch.cuda.synchronize()
%timeit run100(lambda: a.unique_consecutive(return_counts=True, return_inverse=True)); torch.cuda.synchronize()
```

Result on master:
```
1.4.0a0+ed20937

single runs:
771 µs ± 60.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
879 µs ± 110 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
744 µs ± 17.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
126 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
237 µs ± 14.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
352 µs ± 71.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

100 runs:
71.8 ms ± 3.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
95.6 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
96.6 ms ± 17.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
10 ms ± 523 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.5 ms ± 2.04 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
34.1 ms ± 5.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

With this PR:
```
1.4.0a0+7723340

single runs:
289 µs ± 1.48 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
291 µs ± 1.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
393 µs ± 2.41 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
49.5 µs ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
51.2 µs ± 481 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
102 µs ± 539 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

100 runs:
28.1 ms ± 90.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
28 ms ± 73.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
38.4 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
4.01 ms ± 45.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.01 ms ± 41.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9 ms ± 24.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
31211,zasdfgbnm,pr,2019-12-12T21:48:32Z,Refactor test for unique and unique_consecutive and fix some bugs,Tests for unique_dim will be refactored in a separate PR.,pytorch
31225,zasdfgbnm,pr,2019-12-13T01:18:06Z,Why flake8-py2 is still there?,,pytorch
31262,z-a-f,pr,2019-12-13T19:25:53Z,Adding version check for hypothesis deadline,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31262 Adding version check for hypothesis deadline**

Differential Revision: [D19036700](https://our.internmc.facebook.com/intern/diff/D19036700)",pytorch
31263,zasdfgbnm,pr,2019-12-13T19:27:56Z,thrust is included in SortingKthValue.cu but never used,,pytorch
31279,zasdfgbnm,pr,2019-12-14T02:22:23Z,Fix copy kernel speed regression introduced in #29631,"Fixes https://github.com/pytorch/pytorch/issues/31271

This fixes copy kernel speed regression introduced in #29631.

The previous implementation forces the compiler to instantiate `static_cast_with_inter_type` because it is passed as an argument of a function. This behavior makes it impossible for compilers to do optimizations like automatic vectorization, and, function call itself is expensive compared to a single casting instruction.

To check the change, run 
```
readelf -Ws /home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so | grep static_cast_with_inter_type
```

On nightly build, we have output
```
168217: 0000000001852bf0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsdE5applyEd
168816: 0000000001852d30    33 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEaE5applyEa
168843: 00000000018531f0     7 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIblE5applyEl
168930: 0000000001852c20     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIslE5applyEl
168935: 00000000018528d0   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_4HalfEE5applyES1_
169023: 0000000001852f30    17 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEhE5applyEh
169713: 00000000018525c0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIahE5applyEh
170033: 0000000001852c10     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsiE5applyEi
170105: 0000000001852bd0     5 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIshE5applyEh
170980: 0000000001852fc0    27 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdES1_IfEE5applyES3_
171398: 0000000001852810    13 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdbE5applyEb
171574: 00000000018532e0    35 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbNS_8BFloat16EE5applyES1_
171734: 0000000001852b20     6 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlSt7complexIdEE5applyES2_
172422: 0000000001853350    54 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EaE5applyEa
172704: 00000000018533c0    38 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_8BFloat16EfE5applyEf
172976: 0000000001852890    10 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIflE5applyEl
173038: 0000000001852f80     9 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIdEfE5applyEf
173329: 00000000018531c0    20 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIbfE5applyEf
173779: 00000000018524d0     3 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIhiE5applyEi
174032: 0000000001852960    14 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIfNS_8BFloat16EE5applyES1_
174334: 0000000001852d60    29 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeISt7complexIfEdE5applyEd
174470: 0000000001852c60   124 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIsNS_4HalfEE5applyES1_
174770: 0000000001852bc0    15 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIlNS_8BFloat16EE5applyES1_
176408: 0000000001853980   144 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeINS_4HalfEbE5applyEb
176475: 0000000001852790   128 FUNC    LOCAL  DEFAULT    9 _ZN3c1027static_cast_with_inter_typeIdNS_4HalfEE5applyES1_
....
```

And after this PR, we get empty output
```
```",pytorch
31322,zasdfgbnm,pr,2019-12-16T19:26:02Z,Fix copy kernel regression for v1.4.0,Are we still accepting PRs for 1.4.0? I see the issue unpinned.,pytorch
31335,d4l3k,pr,2019-12-16T22:10:39Z,caffe2/event: allow multiple errors such as when cancelled,"Summary:
When an error occurs in a net we end up cancelling all the async ops. If one error occurs it's highly likely other errors will occur as well.

Typically we see:
1. SendOp failed due to a network error
2. async scheduling cancels all other ops via `SetFinished(""Cancelled"");`
3. Another SendOp fails due to a network error and crashes the process when the exception is thrown.

This changes caffe2 ops to allow failing twice.

Test Plan: buck test //caffe2/caffe2:caffe2_test_cpu

Differential Revision: D19106548

",pytorch
31346,d4l3k,pr,2019-12-17T00:52:38Z,autograd/profiler: make record_function more threadsafe,"Summary:
This makes it so that if profiling is enabled/disabled from a different thread while a RecordFunction span is active via an op it doesn't crash the process.

We currently see when using torch.distributed.rpc to enable/disable profiling on other nodes while other things are running.

Test Plan: buck test //caffe2/test:autograd -- test_record_function

Differential Revision: D19133258

",pytorch
31379,zasdfgbnm,pr,2019-12-17T20:02:56Z,Split on batch dimension when 32bit indexing not enough for convolution forward,"Partially fixes https://github.com/pytorch/pytorch/issues/22496

This is just a first step towards the support of 64bit convolution on CUDA. In the forward of convolution, if the total tensor size is larger than 2^31, then we split it on the batch dimension. I want to get some review feedback before moving forward for the same splitting approach for backward.

There are real-world use cases that even when N=1 the input is still larger than 2^31. For this case, the splitting would be complicated, so I am planning to modify `use_cudnn` to just dispatch to the slow fallback kernel in PyTorch in a later PR.

Update: `later PR` is https://github.com/pytorch/pytorch/pull/31383",pytorch
31383,zasdfgbnm,pr,2019-12-17T21:21:35Z,Don't dispatch to cudnn if it is not possible to make it 32bit by splitting batch dim,"Also a step towards supporting 64bit indexing in convolution.

See also: https://github.com/pytorch/pytorch/pull/31379",pytorch
31444,jjsjann123,pr,2019-12-18T22:00:35Z,[NHWC] cudnn grouped convolution nhwc patch,"Earlier cudnn version doesn't support grouped convolution in NHWC well. Legit
configuration in later cudnn version might return CUDNN_STATUS_NOT_SUPPORTED.
We are falling back to NCHW when runtime check of cudnn version is < 7.6.0 to
keep the logic simple.

Note:
We might update the heuristics, 7.6.0 is very conservative.

",pytorch
31510,zasdfgbnm,pr,2019-12-20T07:43:43Z,Conv transpose/backward split 32bit,"Basically the same as https://github.com/pytorch/pytorch/pull/31379 except for that I write a separate function `split_batch_dim_to_32bit_out` for the logic. This function could also be used for convolution forward, and I will rebase this PR after #31379 get merged and then change `raw_cudnn_convolution_forward_out` to use `split_batch_dim_to_32bit_out` here.",pytorch
31512,zasdfgbnm,pr,2019-12-20T08:30:20Z,cuDNN conv add bias 32bit split,"Changes in `Conv.cpp` is independent, but the test requires https://github.com/pytorch/pytorch/pull/31379 to pass. The test for large tensor is never run on our CI, so we could land this independently with #31379 ... (I have tested to merge them locally and test passes)",pytorch
31524,zasdfgbnm,pr,2019-12-20T20:51:53Z,Don't handle bias inside cudnn_convolution*,"Compared to cuDNN bias, PyTorch add has the following advantage:
- faster, especially for backward (see: https://github.com/zasdfgbnm/things/blob/master/2019/conv-backward-profile.md)
- handles 64bit indexing automatically
- has less code, less maintenance effort

@ngimel I submit this PR early so the CI could start building it. But I have not tested it locally yet (still waiting for compiling).",pytorch
31532,zasdfgbnm,pr,2019-12-20T23:06:00Z,setCuDNNStreamToCurrent in cudnn_convolution_transpose,"Quote from the comments in the same file:

```C++
// Where does argument checking happen?  Here's the division of
// responsibility:
//  - Things that happen in at::Tensor
//    - TensorArg allocation
//    - setCuDNNStreamToCurrent
//  - Things that happen in TensorArg
//    - Check arguments (type, GPU, shape)
```

So `cudnn_convolution_transpose` is responsible to set the stream.",pytorch
31537,zasdfgbnm,pr,2019-12-21T00:08:26Z,set stream everytime when we get a cuBlas handle,"I don't see any reason for not doing so, because it is a common error that people forget to set the stream. And I don't think there is a reason for not running on the current stream.

This is just for cublas, cusparse and cudnn should be modified also.",pytorch
31538,zasdfgbnm,pr,2019-12-21T00:21:09Z,set stream everytime when we get a cuSparse handle,cuSparse version of https://github.com/pytorch/pytorch/pull/31537,pytorch
31540,z-a-f,pr,2019-12-21T01:03:34Z,repr and _*state_dict for qRNN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31540 repr and _*state_dict for qRNN**

Fixes #31468

Differential Revision: [D19205894](https://our.internmc.facebook.com/intern/diff/D19205894)",pytorch
31541,zasdfgbnm,pr,2019-12-21T01:31:57Z,set stream everytime when we get a cuDNN handle,"cudnn version of https://github.com/pytorch/pytorch/pull/31537

https://github.com/pytorch/pytorch/pull/31532 is a quick fix and this is a bigger change. This would deprecate https://github.com/pytorch/pytorch/pull/31532, but we could also merge https://github.com/pytorch/pytorch/pull/31532 first for a quick fix and then work on this later.",pytorch
31587,z-a-f,pr,2019-12-24T00:11:06Z,Fixed concatenation benchmark + added it to the microbenchmarking runs,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#31587 Fixed concatenation benchmark + added it to the microbenchmarking runs**

Differential Revision: [D19221813](https://our.internmc.facebook.com/intern/diff/D19221813)",pytorch
31716,ppwwyyxx,pr,2019-12-30T21:22:26Z,Include two caffe2 ops in v1.4.0,"These ops will allow detectron2 models to run under caffe2.

This PR is a cherry-pick of landed PR https://github.com/pytorch/pytorch/pull/31350 and https://github.com/pytorch/pytorch/pull/31281.",pytorch
31851,z-a-f,pr,2020-01-04T01:15:08Z,Quantized sigmoid function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32335 Benchmarks for quantized LSTM (static)
* #32074 Static quantization for LSTM (front)
* #31934 Static quantization for LSTM (back)
* **#31851 Quantized sigmoid function**

Differential Revision: [D19280716](https://our.internmc.facebook.com/intern/diff/D19280716)",pytorch
31862,zasdfgbnm,pr,2020-01-04T22:52:08Z,[WIP] Reenable test_conv_transposed_large on windows,,pytorch
31887,zasdfgbnm,pr,2020-01-06T18:24:06Z,Reenable test_conv_transposed_large,"Fixes https://github.com/pytorch/pytorch/issues/31650

Result:
```
test_conv_transposed_large_cuda (__main__.TestNNDeviceTypeCUDA) ... skipped 'This test fails with OOM on Windows CI, but succeeds on Linux with same memory.See https://github.com/pytorch/pytorch/issues/31650'
```",pytorch
31889,zasdfgbnm,pr,2020-01-06T18:54:32Z,Fix weight backward for cudnn conv of large tensor,This is the last PR for https://github.com/pytorch/pytorch/issues/22496,pytorch
31903,jjsjann123,pr,2020-01-07T00:41:26Z,TensorIterator norm update,"special case for norm out where p == 2. Instead of calling `pow`,
we use multiplication as a faster code path.",pytorch
31929,zasdfgbnm,pr,2020-01-08T00:51:00Z,Add device debug info to CUDA build,Also print NVCC flags in the summary,pytorch
31934,z-a-f,pr,2020-01-08T02:03:34Z,Static quantization for LSTM (back),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32335 Benchmarks for quantized LSTM (static)
* #32074 Static quantization for LSTM (front)
* **#31934 Static quantization for LSTM (back)**

Differential Revision: [D19339398](https://our.internmc.facebook.com/intern/diff/D19339398)",pytorch
31951,zasdfgbnm,pr,2020-01-08T18:34:51Z,Switch important CI from CUDA 9 to 10.1,Fixes https://github.com/pytorch/pytorch/issues/31427,pytorch
31952,jjsjann123,pr,2020-01-08T18:35:12Z,Fix cudnn channels_last descriptors problem,This is to append fixes to #31783 so we can pull the fixes in without breaking tests.,pytorch
31964,zasdfgbnm,pr,2020-01-08T21:44:33Z,DO NOT MERGE,,pytorch
31966,d4l3k,pr,2020-01-08T22:13:41Z,caffe2/core/plan_executor: add cancellation of async nets on error + propagate exceptions via std::exception_ptr for stack traces,"Summary:
This has two parts:
* When `--caffe2_handle_executor_threads_exceptions` is set when a parallel execution step throws an exception it can hang waiting for async nets to finish. This adds cancellation code to cancel any async nets.
* This makes the exceptions returned from parallel workers pass a std::exception_ptr so the stack trace can be recorded with folly::SmartExceptionTracer. This does use newer C++ features but per from recent discussion all C++14 features can be used now.

Test Plan: buck test //caffe2/caffe2:caffe2_test_cpu

Differential Revision: D19320177

",pytorch
31974,zasdfgbnm,pr,2020-01-09T01:25:27Z,"TensorIterator unrolling and vectorized load - step 0, 1","This is step 0 and 1 for  https://github.com/pytorch/pytorch/issues/31975:

- Old code is moved to namespace `legacy`
- New `elementwise_kernel` and `launch_kernel` added to namespace `modern`, they only support 1d contiguous case for now
- In `gpu_kernel_impl`, dispatch to the new code if the problem is trivial 1d contiguous.

In terms of performance, this PR affect elementwise operators on contiguous tensors. The performance is improved slightly (up to 8%) for medium size tensors on Volta.

## compiled code
See https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise.ipynb

We can see that, previously, the add kernel compiles to
```
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 71
        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;
        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;
        /*0020*/                   S2R R0, SR_TID.X ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 73
        /*0030*/                   S2R R3, SR_CTAID.X ;
        /*0040*/                   IMAD R0, R3, 0x200, R0 ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 76
        /*0050*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x160], PT ;
        /*0060*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 110
        /*0070*/                   IMAD R3, R0.reuse, c[0x0][0x194], RZ ;
        /*0080*/                   IMAD R6, R0, c[0x0][0x198], RZ ;
        /*0090*/                   IADD3 R4, P0, R3.reuse, c[0x0][0x178], RZ ;
        /*00a0*/                   IADD3 R2, P1, R6.reuse, c[0x0][0x180], RZ ;
        /*00b0*/                   LEA.HI.X.SX32 R5, R3, c[0x0][0x17c], 0x1, P0 ;
        /*00c0*/                   LEA.HI.X.SX32 R3, R6, c[0x0][0x184], 0x1, P1 ;
        /*00d0*/                   LDG.E.SYS R5, [R4] ;
        /*00e0*/                   LDG.E.SYS R2, [R2] ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 77
        /*00f0*/                   IMAD R0, R0, c[0x0][0x190], RZ ;
        /*0100*/                   IADD3 R6, P0, R0, c[0x0][0x170], RZ ;
        /*0110*/                   LEA.HI.X.SX32 R7, R0, c[0x0][0x174], 0x1, P0 ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 110
        /*0120*/                   FFMA R9, R2, c[0x0][0x1a0], R5 ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 170
        /*0130*/                   STG.E.SYS [R6], R9 ;
	//## File ""/home/xgao/pytorch-master/aten/src/ATen/native/cuda/Loops.cuh"", line 81
        /*0140*/                   EXIT ;
.L_16826:
        /*0150*/                   BRA `(.L_16826);
        /*0160*/                   NOP;
        /*0170*/                   NOP;
.L_29063:
```
Now it compiles to
```
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 210
        /*0000*/                   MOV R1, c[0x0][0x28] ;
        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;
        /*0020*/                   S2R R6, SR_CTAID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 217
        /*0030*/                   MOV R7, 0x4 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 208
        /*0040*/                   S2R R3, SR_TID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 210
        /*0050*/                   LEA R6, R6, R3, 0x8 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 225
        /*0060*/                   IADD3 R2, R6.reuse, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 217
        /*0070*/                   IMAD.WIDE R4, R6.reuse, R7.reuse, c[0x0][0x190] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 225
        /*0080*/                   IADD3 R3, R6, 0x80, RZ ;
        /*0090*/                   ISETP.GE.AND P1, PT, R2, c[0x0][0x160], PT ;
        /*00a0*/                   ISETP.GE.AND P0, PT, R6.reuse, c[0x0][0x160], PT ;
        /*00b0*/                   ISETP.GE.AND P2, PT, R3, c[0x0][0x160], PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 217
        /*00c0*/                   IMAD.WIDE R2, R6.reuse, R7, c[0x0][0x188] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 225
        /*00d0*/                   IADD3 R14, R6, 0xc0, RZ ;
        /*00e0*/                   ISETP.GE.AND P3, PT, R14, c[0x0][0x160], PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 228
        /*00f0*/              @!P1 LDG.E.SYS R11, [R4+0x100] ;
        /*0100*/              @!P0 LDG.E.SYS R0, [R2] ;
        /*0110*/              @!P0 LDG.E.SYS R9, [R4] ;
        /*0120*/              @!P1 LDG.E.SYS R8, [R2+0x100] ;
        /*0130*/              @!P2 LDG.E.SYS R10, [R2+0x200] ;
        /*0140*/              @!P2 LDG.E.SYS R13, [R4+0x200] ;
        /*0150*/              @!P3 LDG.E.SYS R12, [R2+0x300] ;
        /*0160*/              @!P3 LDG.E.SYS R15, [R4+0x300] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 245
        /*0170*/                   IMAD.WIDE R6, R6, R7, c[0x0][0x180] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 191
        /*0180*/                   FFMA R9, R9, c[0x0][0x168], R0 ;
        /*0190*/                   FFMA R11, R11, c[0x0][0x168], R8 ;
        /*01a0*/                   FFMA R13, R13, c[0x0][0x168], R10 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 245
        /*01b0*/              @!P0 STG.E.SYS [R6], R9 ;
        /*01c0*/              @!P1 STG.E.SYS [R6+0x100], R11 ;
        /*01d0*/              @!P2 STG.E.SYS [R6+0x200], R13 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 191
        /*01e0*/                   FFMA R15, R15, c[0x0][0x168], R12 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 244
        /*01f0*/               @P3 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 245
        /*0200*/                   STG.E.SYS [R6+0x300], R15 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 248
        /*0210*/                   EXIT ;
.L_727:
        /*0220*/                   BRA `(.L_727);
        /*0230*/                   NOP;
        /*0240*/                   NOP;
        /*0250*/                   NOP;
        /*0260*/                   NOP;
        /*0270*/                   NOP;
.L_32233:
```

## benchmark

The benchmark is for add kernel on Volta.

See https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-unroll.ipynb

For tensors of size from 2^20 to 2^30, previously we had
```
1.5.0a0+dedd16b
dedd16b4181cae81e37e978cd3bf24c1ba35ca05
33 µs ± 31.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
48.7 µs ± 75 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
78.9 µs ± 122 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
140 µs ± 51.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
261 µs ± 71.4 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
506 µs ± 159 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
993 µs ± 189 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.96 ms ± 139 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.9 ms ± 955 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.79 ms ± 187 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
Now we have
```
1.5.0a0+b1a239b
b1a239be8d529e89875fe47cd09964ef3a9516ac
30.4 µs ± 18 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
45.2 µs ± 46.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
75 µs ± 476 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
134 µs ± 192 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
253 µs ± 354 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
489 µs ± 138 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
961 µs ± 431 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.91 ms ± 578 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.8 ms ± 88.8 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.57 ms ± 763 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
It is slightly better.
",pytorch
32013,zasdfgbnm,pr,2020-01-09T23:44:29Z,CUDA 8 cleanup,CUDA 8 is no longer supported,pytorch
32057,zasdfgbnm,pr,2020-01-10T18:53:48Z,DO NOT MERGE: show nvcc version in CI,,pytorch
32069,zasdfgbnm,pr,2020-01-10T21:05:03Z,Display NVCC version in CI for convenience to look at,,pytorch
32071,zasdfgbnm,pr,2020-01-10T21:17:41Z,"DO NOT MERGE: Experiments for #31974, try disabling sccache",,pytorch
32074,z-a-f,pr,2020-01-10T21:55:40Z,Static quantization for LSTM (front),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32335 Benchmarks for quantized LSTM (static)
* **#32074 Static quantization for LSTM (front)**
* #31934 Static quantization for LSTM (back)

Differential Revision: [D19359209](https://our.internmc.facebook.com/intern/diff/D19359209)",pytorch
32075,z-a-f,pr,2020-01-10T22:23:05Z,Docs entry for the `is_quantized`,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32075 Docs entry for the `is_quantized`**

Differential Revision: [D19353861](https://our.internmc.facebook.com/intern/diff/D19353861)",pytorch
32107,zasdfgbnm,pr,2020-01-13T18:12:53Z,DO NOT MERGE: trigger docker rebuild for #31951,,pytorch
32143,z-a-f,pr,2020-01-14T00:13:23Z,Defining the deadline in hypothesis,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32143 Defining the deadline in hypothesis**

Differential Revision: [D19385221](https://our.internmc.facebook.com/intern/diff/D19385221)",pytorch
32145,zasdfgbnm,pr,2020-01-14T00:30:26Z,Improve documents on how to use new docker images,"The original list

```yaml
# * cimodel/data/pytorch_build_definitions.py
# * cimodel/data/caffe2_build_definitions.py
```

is not complete. We should suggest using grep to find where to change because the CI files are changing.",pytorch
32252,z-a-f,pr,2020-01-16T00:21:28Z,Adding native qconcat,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32335 Benchmarks for quantized LSTM (static)
* #32074 Static quantization for LSTM (front)
* #31934 Static quantization for LSTM (back)
* #31851 Quantized sigmoid function
* **#32252 Adding native qconcat**

Differential Revision: [D19422889](https://our.internmc.facebook.com/intern/diff/D19422889)",pytorch
32308,zasdfgbnm,pr,2020-01-16T18:37:17Z,DO NOT MERGE: recache cuda9 build for #31974,,pytorch
32333,zasdfgbnm,pr,2020-01-17T01:03:01Z,Don't dispatch to integral types in smooth_l1_kernel,,pytorch
32334,z-a-f,pr,2020-01-17T01:40:58Z,Unittests for quantized LSTM (static),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #32335 Benchmarks for quantized LSTM (static)
* **#32334 Unittests for quantized LSTM (static)**
* #32074 Static quantization for LSTM (front)
* #31934 Static quantization for LSTM (back)
* #31851 Quantized sigmoid function
* #32252 Adding native qconcat

Differential Revision: [D19442266](https://our.internmc.facebook.com/intern/diff/D19442266)",pytorch
32335,z-a-f,pr,2020-01-17T02:02:40Z,Benchmarks for quantized LSTM (static),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32335 Benchmarks for quantized LSTM (static)**
* #32074 Static quantization for LSTM (front)
* #31934 Static quantization for LSTM (back)

Differential Revision: [D19443067](https://our.internmc.facebook.com/intern/diff/D19443067)",pytorch
32383,zasdfgbnm,pr,2020-01-18T04:56:02Z,Vectorized memory access in TensorIterator GPU loop for 1d contiguous case,"Step 2 of https://github.com/pytorch/pytorch/issues/31975

Vectorized memory access is enabled. Generated code: https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise-vec.ipynb

```
void at::native::modern::elementwise_kernel<4, 64, 4, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)

**ASM:**

	.section	.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,""ax"",@progbits
	.sectioninfo	@""SHI_REGISTERS=20""
	.align	128
        .global         _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_
        .type           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@function
        .size           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,(.L_40898 - _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_)
        .other          _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@""STO_CUDA_ENTRY STV_DEFAULT""
_ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:
.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 294
        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;
        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;
        /*0020*/                   S2R R9, SR_CTAID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 177
        /*0030*/                   S2R R0, SR_TID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 294
        /*0040*/                   IMAD.SHL.U32 R9, R9, 0x100, RZ ;
        /*0050*/                   IADD3 R5, -R9, c[0x0][0x160], RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 256
        /*0060*/                   SHF.R.S32.HI R17, RZ, 0x1f, R9 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 296
        /*0070*/                   ISETP.GE.AND P0, PT, R5, 0x100, PT ;
        /*0080*/              @!P0 BRA `(.L_3173) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 256
        /*0090*/                   IMAD.SHL.U32 R12, R9.reuse, 0x4, RZ ;
        /*00a0*/                   SHF.L.U64.HI R17, R9, 0x2, R17 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 260
        /*00b0*/                   IADD3 R8, P0, R12.reuse, c[0x0][0x188], RZ ;
        /*00c0*/                   IADD3 R2, P1, R12, c[0x0][0x190], RZ ;
        /*00d0*/                   IADD3.X R9, R17.reuse, c[0x0][0x18c], RZ, P0, !PT ;
        /*00e0*/                   IADD3.X R3, R17, c[0x0][0x194], RZ, P1, !PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 218
        /*00f0*/                   IMAD.WIDE R8, R0, 0x10, R8 ;
        /*0100*/                   IMAD.WIDE R2, R0, 0x10, R2 ;
        /*0110*/                   LDG.E.128.SYS R8, [R8] ;
        /*0120*/                   LDG.E.128.SYS R4, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 256
        /*0130*/                   IADD3 R12, P0, R12, c[0x0][0x180], RZ ;
        /*0140*/                   IADD3.X R13, R17, c[0x0][0x184], RZ, P0, !PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 238
        /*0150*/                   IMAD.WIDE R12, R0, 0x10, R12 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 196
        /*0160*/                   FFMA R7, R7, c[0x0][0x168], R11 ;
        /*0170*/                   FFMA R6, R6, c[0x0][0x168], R10 ;
        /*0180*/                   FFMA R5, R5, c[0x0][0x168], R9 ;
        /*0190*/                   FFMA R4, R4, c[0x0][0x168], R8 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 238
        /*01a0*/                   STG.E.128.SYS [R12], R4 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 301
        /*01b0*/                   EXIT ;
.L_3173:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*01c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;
        /*01d0*/                   BMOV.32.CLEAR RZ, B0 ;
        /*01e0*/                   BSSY B0, `(.L_3174) ;
        /*01f0*/               @P0 BRA `(.L_3175) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*0200*/                   IADD3 R3, P1, R9, R0, RZ ;
        /*0210*/                   LEA.HI.X.SX32 R4, R0, R17, 0x1, P1 ;
        /*0220*/                   LEA R2, P1, R3, c[0x0][0x188], 0x2 ;
        /*0230*/                   LEA.HI.X R3, R3, c[0x0][0x18c], R4, 0x2, P1 ;
        /*0240*/                   LDG.E.SYS R8, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*0250*/                   IADD3 R4, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*0260*/                   ISETP.GE.AND P1, PT, R4, R5, PT ;
        /*0270*/               @P1 BRA `(.L_3175) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*0280*/                   LDG.E.SYS R4, [R2+0x100] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*0290*/                   IADD3 R6, R0, 0x80, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*02a0*/                   ISETP.GE.AND P1, PT, R6, R5, PT ;
        /*02b0*/               @P1 BRA `(.L_3175) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*02c0*/                   IADD3 R10, R0, 0xc0, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*02d0*/                   LDG.E.SYS R7, [R2+0x200] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*02e0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*02f0*/              @!P1 LDG.E.SYS R6, [R2+0x300] ;
.L_3175:
        /*0300*/                   BSYNC B0 ;
.L_3174:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*0310*/                   BMOV.32.CLEAR RZ, B0 ;
        /*0320*/                   BSSY B0, `(.L_3176) ;
        /*0330*/               @P0 BRA `(.L_3177) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*0340*/                   IADD3 R3, P1, R9, R0, RZ ;
        /*0350*/                   LEA.HI.X.SX32 R10, R0, R17, 0x1, P1 ;
        /*0360*/                   LEA R2, P1, R3, c[0x0][0x190], 0x2 ;
        /*0370*/                   LEA.HI.X R3, R3, c[0x0][0x194], R10, 0x2, P1 ;
        /*0380*/                   LDG.E.SYS R11, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*0390*/                   IADD3 R10, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*03a0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;
        /*03b0*/               @P1 BRA `(.L_3177) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*03c0*/                   LDG.E.SYS R13, [R2+0x100] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*03d0*/                   IADD3 R10, R0, 0x80, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*03e0*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;
        /*03f0*/               @P1 BRA `(.L_3177) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 184
        /*0400*/                   IADD3 R10, R0, 0xc0, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 180
        /*0410*/                   ISETP.GE.AND P1, PT, R10, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 183
        /*0420*/                   LDG.E.SYS R10, [R2+0x200] ;
        /*0430*/              @!P1 LDG.E.SYS R15, [R2+0x300] ;
.L_3177:
        /*0440*/                   BSYNC B0 ;
.L_3176:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*0450*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*0460*/                   IADD3 R9, P0, R9, R0, RZ ;
        /*0470*/                   FFMA R11, R11, c[0x0][0x168], R8 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 197
        /*0480*/                   IADD3 R14, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*0490*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P0 ;
        /*04a0*/                   LEA R2, P0, R9.reuse, c[0x0][0x180], 0x2 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*04b0*/                   ISETP.GE.AND P1, PT, R14, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*04c0*/                   LEA.HI.X R3, R9, c[0x0][0x184], R12, 0x2, P0 ;
        /*04d0*/                   STG.E.SYS [R2], R11 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*04e0*/               @P1 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 197
        /*04f0*/                   IADD3 R8, R0, 0x80, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 196
        /*0500*/                   FFMA R13, R13, c[0x0][0x168], R4 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*0510*/                   ISETP.GE.AND P0, PT, R8, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*0520*/                   STG.E.SYS [R2+0x100], R13 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*0530*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 197
        /*0540*/                   IADD3 R0, R0, 0xc0, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/Loops.cuh"", line 196
        /*0550*/                   FFMA R7, R10, c[0x0][0x168], R7 ;
        /*0560*/                   FFMA R15, R15, c[0x0][0x168], R6 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*0570*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*0580*/                   STG.E.SYS [R2+0x200], R7 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 193
        /*0590*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 196
        /*05a0*/                   STG.E.SYS [R2+0x300], R15 ;
        /*05b0*/                   EXIT ;
.L_3178:
        /*05c0*/                   BRA `(.L_3178);
        /*05d0*/                   NOP;
        /*05e0*/                   NOP;
        /*05f0*/                   NOP;
.L_40898:
```

We can clearly see the `LDG.E.128` in it, which is a result of vectorization.

Benchmark: https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-vec.ipynb

Benchmark on P100, dtype `uint8`:

before:
```
1.4.0a0+a5b4d78
e1d97025eeeddcf083e9bee0c8f6a53168991a71
22.2 µs ± 89.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
34.7 µs ± 38.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
52 µs ± 312 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
86.9 µs ± 135 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
154 µs ± 204 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
291 µs ± 668 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
566 µs ± 1.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.18 ms ± 1.54 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.29 ms ± 1.48 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.4 ms ± 1.15 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

after:
```
1.4.0a0+a5b4d78
1281cdfd8188fe86241ecaf71d001809d016c3a3
24 µs ± 116 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.5 µs ± 355 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
43.1 µs ± 300 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
67.6 µs ± 113 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
116 µs ± 275 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
215 µs ± 142 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
413 µs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
824 µs ± 891 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.63 ms ± 478 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.19 ms ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

Benchmark on P100, dtype `half`:

Before:
```
1.4.0a0+a5b4d78
1c017f0c14c91bd5125ab387a90441b0c0e2f3ad
30.8 µs ± 226 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
43.4 µs ± 164 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
69.1 µs ± 83 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
119 µs ± 103 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
224 µs ± 99.1 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
418 µs ± 206 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
865 µs ± 237 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.69 ms ± 695 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.3 ms ± 527 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
6.77 ms ± 741 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

After

```
1.4.0a0+a5b4d78
7e50ee27333e7047072d328d03767b4845286356
28.9 µs ± 61.3 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
40.2 µs ± 244 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
63.8 µs ± 350 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
109 µs ± 196 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
199 µs ± 157 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
380 µs ± 446 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
743 µs ± 2.17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.47 ms ± 1.34 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.91 ms ± 9.17 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.8 ms ± 296 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

cc: @csarofeen @ptrblck ",pytorch
32401,ppwwyyxx,pr,2020-01-19T08:05:17Z,support empty batch in group normalization,"Summary: https://github.com/pytorch/pytorch/issues/12013

Differential Revision: D19463720

",pytorch
32473,zasdfgbnm,pr,2020-01-22T00:27:13Z,Only run test_conv_large and test_conv_transposed_large_cuda on 32GB device,"For some reason, these two tests start to fail on 16GB Volta on Linux...

Also fixes https://github.com/pytorch/pytorch/issues/31650",pytorch
32482,jjsjann123,pr,2020-01-22T07:30:56Z,[NHWC CUDNN CONV]Update cudnn convolution memory_format behavior,"1. Allows both the memory_format of weight & input to dictate the output
memory_format.
2. Provides utility function to recursively convert memory_format of Conv2d and
ConvTranspose2d layers. This allows easy model conversion and ensures that lost
memory_format through incompatible layers could be restored at Convolution-like
layer, where significant performance boost is expected on later generation CUDA
devices.

",pytorch
32484,zasdfgbnm,pr,2020-01-22T08:32:36Z,DO NOT MERGE: debug ROCm for #32383,#32383,pytorch
32498,zasdfgbnm,pr,2020-01-22T18:58:54Z,DO NOT MERGE: test for #32383,,pytorch
32499,zasdfgbnm,pr,2020-01-22T19:01:45Z,DO NOT MERGE: test for #32383,,pytorch
32601,zasdfgbnm,pr,2020-01-24T23:24:19Z,DO NOT MERGE,,pytorch
32678,zasdfgbnm,pr,2020-01-28T01:26:53Z,"Vectorized memory access in TensorIterator GPU loop for 1d contiguous case, without ROCm regression","Basically https://github.com/pytorch/pytorch/pull/32383, but vectorization disabled on ROCm to avoid regression.",pytorch
32730,zasdfgbnm,pr,2020-01-28T22:35:09Z,Deduplication of type casting codes,"These codes are implemented twice at different places by different people, we should merge them together.",pytorch
32746,zasdfgbnm,pr,2020-01-29T00:30:40Z,Remove unnecessary dynamic casting in TensorIterator,"Postpone the computation of `needs_dynamic_casting` from build time to dispatch time so that if the operand list is changed after build, we can still get the updated result. This will improve the vectorization coverage of https://github.com/pytorch/pytorch/pull/32678 a lot at `optimizer.step()`.

Previously
```python
torch.zeros(10, device='cuda').mul_(0.9)
```
requires dynamic casting, but after this PR it won't.

New C++ tests are added to test for different cases to make sure dynamic casting is not turned on for unnecessary cases.",pytorch
32755,zasdfgbnm,pr,2020-01-29T01:58:09Z,Remove needs_dynamic_casting from TensorIterator and move it to Loops.cuh,"Remove `needs_dynamic_casting` from TensorIterator and move it to `Loops.cuh`.

The original design of `needs_dynamic_casting` is fundamentally flawed: it injects logics into TensorIterator and uses a bunch of boolean values to test whether the dynamic casting is needed. This makes it very fragile, as the TensorIterator is so complicated and it is easy to introduce unnecessary dynamic casts. It also makes the `gpu_kernel` very unflexible, differently cases needs to manipulate TensorIterator to make it work.

For example, currently
```python
torch.zeros(10, device='cuda').mul_(0.9)
```
needs dynamic cast, but it shouldn't.

Testing whether dynamic casting is needed could be easy: just compare the dtypes of the lambda with the dtypes of operands. If they don't match, then dynamically cast, otherwise don't cast.",pytorch
32777,zasdfgbnm,pr,2020-01-29T19:06:01Z,Loops.cuh legacy code cleanup -- gpu_kernel_with_index,I didn't see any use case where the functor of `gpu_kernel_with_index` needs to have argument other than the index. Merge conflict with https://github.com/pytorch/pytorch/pull/32755.,pytorch
32778,zasdfgbnm,pr,2020-01-29T19:19:43Z,Fix ivalue_inl.h:353:29: warning: comparison of unsigned expression >= 0 is always true,"`slot` is unsigned integer which is `always >= 0`

",pytorch
32886,z-a-f,pr,2020-01-31T23:13:03Z,Adding scalar to the c10 registration type check,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32886 Adding scalar to the c10 registration type check**

Differential Revision: [D19673484](https://our.internmc.facebook.com/intern/diff/D19673484)",pytorch
32890,z-a-f,pr,2020-01-31T23:46:59Z,TorchScript add check if quantized,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#32890 TorchScript add check if quantized**

Differential Revision: [D19673463](https://our.internmc.facebook.com/intern/diff/D19673463)",pytorch
32984,zasdfgbnm,pr,2020-02-04T20:27:01Z,Move where cuda implementation to TensorIterator,"`where` is special because the arguments do not have the same type, which does not satisfy the assumption in modern https://github.com/pytorch/pytorch/pull/32383. I migrate it to TensorIterator so that there is something to test that this case is not broken. Currently, this case fallback to using legacy (not vectorized, not unrolled) code. It should be supported in the future when I cleanup `Loops.cuh`.

I also move some sharing part of `CUDALoops.cuh` and `ROCmLoops.cuh` into `Loops.cuh` so that to logic for checking whether `func_t` has the same arg types could be shared.",pytorch
32996,zasdfgbnm,pr,2020-02-05T00:56:27Z,Allow cudnn convolution to fail,Fixes https://github.com/pytorch/pytorch/issues/31336 https://github.com/pytorch/pytorch/issues/1664,pytorch
33004,z-a-f,pr,2020-02-05T07:50:44Z,Quantized leaky relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33004 Quantized leaky relu**

Differential Revision: [D19740193](https://our.internmc.facebook.com/intern/diff/D19740193)",pytorch
33056,zasdfgbnm,pr,2020-02-06T19:48:46Z,Make CUDA OOM error a type,"There are cases when we want to recover from CUDA OOM, for example, some cuDNN algorithms use huge workspace and we want to recover from OOM to pick a different algorithm, in such cases, there is no reason to catch all errors.",pytorch
33073,zasdfgbnm,pr,2020-02-07T01:11:01Z,cuDNN convolution try multiple algo,"Fixes: https://github.com/pytorch/pytorch/issues/31336 https://github.com/pytorch/pytorch/issues/1664

Sometimes cuDNN heuristics return algorithms that can not be used. Instead of just using the first algorithm returned, we should try these algorithms one by one until one of them succeed.

Benchmark:
https://github.com/zasdfgbnm/things/blob/master/2020Q1/conv-benchmark.ipynb
```python
i = torch.randn(256, 3, 256, 256).cuda()
c = torch.nn.Conv2d(3, 3, 3, 3).cuda()

%timeit c(i); torch.cuda.synchronize()
```
before vs after = 498 vs 490 µs

The performance is improved I guess because, before this PR, we always call the heuristics to get the algorithm, but after this PR, we only do at the first time.",pytorch
33133,ppwwyyxx,pr,2020-02-09T20:12:03Z,Remove `clean_tag` from tensorboard,"The function originally comes from https://github.com/tensorflow/tensorflow/blob/4279f99847e9fcce9410bda61d3b71065e0df65f/tensorflow/python/ops/summary_op_util.py#L45-L68

As its comment says:
```   
    # In the past, the first argument to summary ops was a tag, which allowed
    # arbitrary characters. Now we are changing the first argument to be the node
    # name. This has a number of advantages (users of summary ops now can
    # take advantage of the tf name scope system) but risks breaking existing
    # usage, because a much smaller set of characters are allowed in node names.
    # This function replaces all illegal characters with _s, and logs a warning.
    # It also strips leading slashes from the name.
```

This function is only for compatibility with TF's operator name restrictions, and is therefore no longer valid in pytorch. By removing it, tensorboard summaries can use more characters in the names.

Before:
![0209-12:10:14](https://user-images.githubusercontent.com/1381301/74109072-37382e00-4b35-11ea-8c9f-ab37a8bd5808.png)


After:
![0209-12:10:57](https://user-images.githubusercontent.com/1381301/74109081-4323f000-4b35-11ea-9dab-447f8466a41e.png)
",pytorch
33222,zasdfgbnm,pr,2020-02-12T01:15:00Z,Allow vectorized gpu loop to have different argument types,"Although currently the only user of GPU loops that has args with different dtypes is `where`, it sounds strange to restrict the args to have the same dtype. Allowing args to have different dtypes also makes it possible for me to clean up legacy code by reusing current code to implement unrolled GPU loop for non-contiguous tensors.

The stack storage of `elementwise_kernel_helper` is changed from `arg_t args[nt][arity]` to `traits:: ArgsTuple args[nt]`. Due to this change, we can no longer get element by `operator[]`, but instead we should use `std::get`. As a result, we can no longer unroll the loop wrt arity using pragma, but we have to
create a `static_unroll` to make use of template meta-programming to do the same job.

A good side effect of this change is, `invoke_with_array` is no longer needed and can be replaced with already existing `c10::guts::apply`. And we don't need the `namespace arg_type` workaround either. This makes the code less ugly.

The same approach might also work for ROCm loops, but I didn't change anything on ROCm in this PR, because I don't want potential compilation error or perf regression to delay this PR. But after this gets merged, I will try on ROCm and send a separate PR to make the code less diverge if the same approach trivially applies (trivially apply means a mindless copy-paste doesn't introduce unexpected compilation error or perf regression).

Assembly (https://github.com/zasdfgbnm/things/blob/master/2020Q1/disassembly-elementwise-vec.ipynb#33222):
```
**Symbol:**
void at::native::modern::elementwise_kernel<4, 64, 4, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::add_kernel_cuda(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)

**ASM:**

	.section	.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,""ax"",@progbits
	.sectioninfo	@""SHI_REGISTERS=20""
	.align	128
        .global         _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_
        .type           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@function
        .size           _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,(.L_40520 - _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_)
        .other          _ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_,@""STO_CUDA_ENTRY STV_DEFAULT""
_ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:
.text._ZN2at6native6modern18elementwise_kernelILi4ELi64ELi4EZZZNS0_15add_kernel_cudaERNS_14TensorIteratorEN3c106ScalarEENKUlvE_clEvENKUlvE2_clEvEUlffE_NS_6detail5ArrayIPcLi3EEEEEviT2_T3_:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 253
        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;
        /*0010*/              @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ ;
        /*0020*/                   S2R R9, SR_CTAID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 39
        /*0030*/                   S2R R0, SR_TID.X ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 253
        /*0040*/                   IMAD.SHL.U32 R9, R9, 0x100, RZ ;
        /*0050*/                   IADD3 R5, -R9, c[0x0][0x160], RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 227
        /*0060*/                   SHF.R.S32.HI R17, RZ, 0x1f, R9 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 255
        /*0070*/                   ISETP.GE.AND P0, PT, R5, 0x100, PT ;
        /*0080*/              @!P0 BRA `(.L_2919) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 227
        /*0090*/                   IMAD.SHL.U32 R12, R9.reuse, 0x4, RZ ;
        /*00a0*/                   SHF.L.U64.HI R17, R9, 0x2, R17 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 229
        /*00b0*/                   IADD3 R8, P0, R12.reuse, c[0x0][0x188], RZ ;
        /*00c0*/                   IADD3 R2, P1, R12, c[0x0][0x190], RZ ;
        /*00d0*/                   IADD3.X R9, R17.reuse, c[0x0][0x18c], RZ, P0, !PT ;
        /*00e0*/                   IADD3.X R3, R17, c[0x0][0x194], RZ, P1, !PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 82
        /*00f0*/                   IMAD.WIDE R8, R0, 0x10, R8 ;
        /*0100*/                   IMAD.WIDE R2, R0, 0x10, R2 ;
        /*0110*/                   LDG.E.128.SYS R8, [R8] ;
        /*0120*/                   LDG.E.128.SYS R4, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 227
        /*0130*/                   IADD3 R12, P0, R12, c[0x0][0x180], RZ ;
        /*0140*/                   IADD3.X R13, R17, c[0x0][0x184], RZ, P0, !PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 102
        /*0150*/                   IMAD.WIDE R12, R0, 0x10, R12 ;
	//## File ""/usr/include/c++/8/tuple"", line 1315
        /*0160*/                   FFMA R7, R7, c[0x0][0x168], R11 ;
        /*0170*/                   FFMA R6, R6, c[0x0][0x168], R10 ;
        /*0180*/                   FFMA R5, R5, c[0x0][0x168], R9 ;
        /*0190*/                   FFMA R4, R4, c[0x0][0x168], R8 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 102
        /*01a0*/                   STG.E.128.SYS [R12], R4 ;
        /*01b0*/                   EXIT ;
.L_2919:
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*01c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;
        /*01d0*/                   BMOV.32.CLEAR RZ, B0 ;
        /*01e0*/                   BSSY B0, `(.L_2920) ;
        /*01f0*/                   IMAD.MOV.U32 R4, RZ, RZ, RZ ;
        /*0200*/                   CS2R R6, SRZ ;
        /*0210*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;
        /*0220*/                   IMAD.MOV.U32 R10, RZ, RZ, RZ ;
        /*0230*/               @P0 BRA `(.L_2921) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*0240*/                   IADD3 R3, P1, R9, R0, RZ ;
        /*0250*/                   LEA.HI.X.SX32 R6, R0, R17, 0x1, P1 ;
        /*0260*/                   LEA R2, P1, R3, c[0x0][0x188], 0x2 ;
        /*0270*/                   LEA.HI.X R3, R3, c[0x0][0x18c], R6, 0x2, P1 ;
        /*0280*/                   LDG.E.SYS R10, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*0290*/                   IADD3 R6, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*02a0*/                   ISETP.GE.AND P1, PT, R6, R5, PT ;
        /*02b0*/               @P1 BRA `(.L_2922) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*02c0*/                   LDG.E.SYS R6, [R2+0x100] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*02d0*/                   IADD3 R8, R0, 0x80, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*02e0*/                   ISETP.GE.AND P1, PT, R8, R5, PT ;
        /*02f0*/               @P1 BRA `(.L_2923) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*0300*/                   IADD3 R8, R0, 0xc0, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*0310*/                   ISETP.GE.AND P1, PT, R8, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*0320*/                   LDG.E.SYS R8, [R2+0x200] ;
        /*0330*/              @!P1 LDG.E.SYS R7, [R2+0x300] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 102
        /*0340*/               @P1 IMAD.MOV.U32 R7, RZ, RZ, RZ ;
        /*0350*/                   BRA `(.L_2921) ;
.L_2923:
        /*0360*/                   IMAD.MOV.U32 R7, RZ, RZ, RZ ;
        /*0370*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;
        /*0380*/                   BRA `(.L_2921) ;
.L_2922:
        /*0390*/                   CS2R R6, SRZ ;
        /*03a0*/                   IMAD.MOV.U32 R8, RZ, RZ, RZ ;
.L_2921:
        /*03b0*/                   BSYNC B0 ;
.L_2920:
        /*03c0*/                   BMOV.32.CLEAR RZ, B0 ;
        /*03d0*/                   BSSY B0, `(.L_2924) ;
        /*03e0*/               @P0 BRA `(.L_2925) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*03f0*/                   IADD3 R3, P1, R9, R0, RZ ;
        /*0400*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P1 ;
        /*0410*/                   LEA R2, P1, R3, c[0x0][0x190], 0x2 ;
        /*0420*/                   LEA.HI.X R3, R3, c[0x0][0x194], R12, 0x2, P1 ;
        /*0430*/                   LDG.E.SYS R11, [R2] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*0440*/                   IADD3 R12, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*0450*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;
        /*0460*/               @P1 BRA `(.L_2926) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*0470*/                   LDG.E.SYS R13, [R2+0x100] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*0480*/                   IADD3 R12, R0, 0x80, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*0490*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;
        /*04a0*/               @P1 BRA `(.L_2927) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*04b0*/                   LDG.E.SYS R15, [R2+0x200] ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 46
        /*04c0*/                   IADD3 R12, R0, 0xc0, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 42
        /*04d0*/                   ISETP.GE.AND P1, PT, R12, R5, PT ;
        /*04e0*/               @P1 BRA `(.L_2928) ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 45
        /*04f0*/                   LDG.E.SYS R4, [R2+0x300] ;
        /*0500*/                   BRA `(.L_2928) ;
.L_2927:
        /*0510*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;
        /*0520*/                   BRA `(.L_2928) ;
.L_2926:
        /*0530*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;
        /*0540*/                   IMAD.MOV.U32 R13, RZ, RZ, RZ ;
        /*0550*/                   BRA `(.L_2928) ;
.L_2925:
        /*0560*/                   IMAD.MOV.U32 R15, RZ, RZ, RZ ;
        /*0570*/                   IMAD.MOV.U32 R13, RZ, RZ, RZ ;
        /*0580*/                   IMAD.MOV.U32 R11, RZ, RZ, RZ ;
.L_2928:
        /*0590*/                   BSYNC B0 ;
.L_2924:
	//## File ""/usr/include/c++/8/tuple"", line 1315
        /*05a0*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*05b0*/                   IADD3 R9, P0, R9, R0, RZ ;
	//## File ""/usr/include/c++/8/tuple"", line 1315
        /*05c0*/                   FFMA R11, R11, c[0x0][0x168], R10 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 59
        /*05d0*/                   IADD3 R14, R0, 0x40, RZ ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*05e0*/                   LEA.HI.X.SX32 R12, R0, R17, 0x1, P0 ;
        /*05f0*/                   LEA R2, P0, R9.reuse, c[0x0][0x180], 0x2 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*0600*/                   ISETP.GE.AND P1, PT, R14, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*0610*/                   LEA.HI.X R3, R9, c[0x0][0x184], R12, 0x2, P0 ;
        /*0620*/                   STG.E.SYS [R2], R11 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*0630*/               @P1 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 59
        /*0640*/                   IADD3 R10, R0, 0x80, RZ ;
	//## File ""/usr/include/c++/8/tuple"", line 1315
        /*0650*/                   FFMA R13, R13, c[0x0][0x168], R6 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*0660*/                   ISETP.GE.AND P0, PT, R10, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*0670*/                   STG.E.SYS [R2+0x100], R13 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*0680*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 59
        /*0690*/                   IADD3 R0, R0, 0xc0, RZ ;
	//## File ""/usr/include/c++/8/tuple"", line 1315
        /*06a0*/                   FFMA R15, R15, c[0x0][0x168], R8 ;
        /*06b0*/                   FFMA R7, R4, c[0x0][0x168], R7 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*06c0*/                   ISETP.GE.AND P0, PT, R0, R5, PT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*06d0*/                   STG.E.SYS [R2+0x200], R15 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 55
        /*06e0*/               @P0 EXIT ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/MemoryAccess.cuh"", line 58
        /*06f0*/                   STG.E.SYS [R2+0x300], R7 ;
	//## File ""/home/xgao/pytorch/aten/src/ATen/native/cuda/CUDALoops.cuh"", line 260
        /*0700*/                   EXIT ;
.L_2929:
        /*0710*/                   BRA `(.L_2929);
        /*0720*/                   NOP;
        /*0730*/                   NOP;
        /*0740*/                   NOP;
        /*0750*/                   NOP;
        /*0760*/                   NOP;
        /*0770*/                   NOP;
.L_40520:
```",pytorch
33228,zasdfgbnm,pr,2020-02-12T04:12:58Z,Move where cuda implementation to TensorIterator,Reopen of https://github.com/pytorch/pytorch/pull/32984,pytorch
33244,zasdfgbnm,pr,2020-02-12T18:31:01Z,Make ELU great again,"Due to compiler bug, we have to make some workaround on ELU for CUDA. A necessary condition for this bug to happen is `invoke_with_array` in `Loops.cuh`. Now, #33222 will kill that function, and we need to remove that workaround once #33222 is landed.",pytorch
33260,zasdfgbnm,pr,2020-02-13T00:54:09Z,[WIP] Partially migrate gpu_kernel_with_index_impl to modern,Step 7 of https://github.com/pytorch/pytorch/issues/31975,pytorch
33302,zasdfgbnm,pr,2020-02-13T17:58:04Z,Kill old cuda support,,pytorch
33308,zasdfgbnm,pr,2020-02-13T19:32:17Z,Get rid of some template arguments in GPU loop,"Globally define
```C++
constexpr int num_threads = C10_WARP_SIZE * 2;
constexpr int thread_work_size = 4;
constexpr int block_work_size = thread_work_size * num_threads;
```
and kill all the template arguments passing these values.

These are effectively global, but we are now passing them around by template arguments, causing many inconvenience in coding.",pytorch
33370,zasdfgbnm,pr,2020-02-15T01:59:59Z,Remove gpu_kernel_with_index,"Although `gpu_kernel_with_index` might look like a quite general helper function at first look, it actually isn't.

The problem is not only 32bit indexing, but something more fundamental: `TensorIterator` reorder dims and shapes, so if you have non-contiguous tensor such as `torch.empty(5, 5).t()` , the index won't be correct. Since the whole point of `TensorIterator` is to manipulate shapes/strides to speedup loops, it is fundamentally impossible to get the correct linear index without tons of efforts.

Currently, the range factories are not failing on an `out=non_contiguous_tensor`  is because it is so lucky that  `has_internal_overlap` is stupid enough to return everything not contiguous as `TOO_HARD`.

Since `gpu_kernel_with_index` is not general, we should move it from `Loops.cuh` to `RangeFactories.cu`. And since the kernel is so simple to implement, it makes no sense to use `TensorIterator` which goes through tons of unnecessary checks like `compute_dtypes`.

`torch.range` is not tested for 64bit-indexing, and I will file a new PR to remove it (it was supposed to be removed at 0.5).

Benchmark:
The device is GTX-1650, I don't have a good GPU at home.

Code:
```python
import torch
print(torch.__version__)

for i in range(100):
    torch.randn(1000, device='cuda')
torch.cuda.synchronize()

for i in range(15, 29):
    %timeit torch.arange(2 ** i, device='cuda'); torch.cuda.synchronize()
```

Before:
```
1.5.0a0+c37a9b8
11.9 µs ± 412 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
12.7 µs ± 309 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
19.6 µs ± 209 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
28.9 µs ± 923 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
48.4 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
85.7 µs ± 1.46 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
162 µs ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
312 µs ± 9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
618 µs ± 15.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.22 ms ± 9.91 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.45 ms ± 97.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.9 ms ± 155 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10.1 ms ± 378 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

After:
```
1.5.0a0+7960d19
11 µs ± 29.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
12.4 µs ± 550 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
18.4 µs ± 230 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
27.6 µs ± 10.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
46.2 µs ± 18.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
83.3 µs ± 5.61 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 373 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
307 µs ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
603 µs ± 112 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.2 ms ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.4 ms ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.77 ms ± 25.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9.51 ms ± 933 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
33374,zasdfgbnm,pr,2020-02-15T09:58:04Z,Kill torch.range,"It is deprecated and should be removed on 0.5
```C++
  if (r.idx == 0) {	
    PyErr_WarnEx(PyExc_UserWarning, ""torch.range is deprecated in favor of torch.arange ""	
        ""and will be removed in 0.5. Note that arange generates values in [start; end), ""	
        ""not [start; end]."", 1);
```",pytorch
33375,zasdfgbnm,pr,2020-02-15T10:31:24Z,Kill THCState_getNumDevices,,pytorch
33376,zasdfgbnm,pr,2020-02-15T10:46:25Z,Kill THCState_getCurrentStream,,pytorch
33380,zasdfgbnm,pr,2020-02-15T17:49:09Z,Kill cudaDeviceAllocator in THCState,,pytorch
33451,zasdfgbnm,pr,2020-02-18T18:45:21Z,Merge minor cases of GPU loops,"Most of the use cases of GPU loops are for contiguous tensors and no dynamic casting. For other cases, they are not used much, therefore does not worth specialize by so many if statements. 

End to end resnet50 benchmark shows no difference in performance: https://github.com/zasdfgbnm/things/blob/master/2020Q1/resnet50-end2end-pr33451.ipynb",pytorch
33454,zasdfgbnm,pr,2020-02-18T19:34:19Z,[DO NOT MERGE]C10_HOST_DEVICE,,pytorch
33457,zasdfgbnm,pr,2020-02-18T20:36:34Z,[DO NOT MERGE]C10_HOST_DEVICE,,pytorch
33527,jjsjann123,pr,2020-02-20T01:10:14Z,[CUDA_FUSER] Fork CUDA fuser,"Separating CUDA fuser from CPU fuser.

1. New node in IR - prim::CudaFusionGroup:
   This enables the cuda fuser to co-exist along side the old fuser. Allows us
   to incrementally build and expand cuda fuser.

2. copied FuseGraph optimization passes to CudaFuserGraph:
   We will re-factor & reuse Chunk/Concat in the old fuser logic, which is
   handled in the optimization pass at this moment. Unfortunately many code in
   the pass is tightly binded with the legacy fuser, which makes code sharing
   difficult.
   The CudaFusionGraph will support only a subset of operations comparing to
   legacy fuser (CUDA only). It is registered as a custom pass post fusion via
     ```torch._C._jit_register_cuda_fuser()```
   To have it in effect, you should also turn off fusion on GPU via
     ```torch._C._jit_override_can_fuse_on_gpu(False)```

3. We don't have codegen in this PR yet (WIP). Currently we just fall back to
   the old fuser.",pytorch
33553,zasdfgbnm,pr,2020-02-20T18:00:27Z,"Revert ""Revert D19964089: [pytorch][PR] Allow vectorized gpu loop to …","…have different argument types""

This reverts commit 05fb160048b71c1b8b00d2083a08618318158c1a.

Please go to https://github.com/pytorch/pytorch/pull/33558 and check the CUDA9 on CI",pytorch
33558,zasdfgbnm,pr,2020-02-20T18:37:26Z,[DO NOT MERGE] 33222 CUDA 9 ci,,pytorch
33575,zasdfgbnm,pr,2020-02-20T20:47:06Z,Make TestCuda.test_memory_stats more robust,"IIUC Python does not guarantee when an object is garbage collected. So it is possible that, some other test running before `TestCuda.test_memory_stats` creates object which is only garbage collected during  `TestCuda.test_memory_stats`, causing mem stats to change and causing this test to fail. This kind of failure is very hard to debug (it took me and @mcarilli and @ptrblck quite a while to figure out what is happening), and it is the root cause of @mcarilli's gradient scaling PR https://github.com/pytorch/pytorch/pull/26512 failing on Windows.

cc: @csarofeen ",pytorch
33676,zuoxingdong,pr,2020-02-23T20:28:21Z,[Update independent.py]: add explicit string representation,,pytorch
33720,zasdfgbnm,pr,2020-02-24T20:08:17Z,"CUDA Loops: move address computation into policy, make policy.load load all arguments","So that in the future we can make policy accept an offset calculator in its constructor for the support of non-contiguous tensors.

The `elementwise_kernel_helper` is now very general and it can handle any cases:

```C++
template<typename func_t, typename policy_t>
__device__ inline void elementwise_kernel_helper(func_t f, policy_t policy) {
  using traits = function_traits<func_t>;
  using return_t = typename traits::result_type;
  using args_t = typename traits::ArgsTuple;

  int idx = blockIdx.x;

  return_t results[thread_work_size];
  cuda9::workaround::enable_default_constructor<args_t> args_[thread_work_size];
  args_t *args = reinterpret_cast<args_t *>(&args_);

  // load
  policy.load(args, idx);

  // compute
  #pragma unroll
  for (int i = 0; i < thread_work_size; i++) {
    if (policy.check_inbounds(i)) {
      results[i] = c10::guts::apply(f, args[i]);
    }
  }

  // store
  policy.store(results, idx);
}
```",pytorch
33744,zasdfgbnm,pr,2020-02-25T04:47:20Z,Migrate fake_quant_slice to TensorIterator,"This is a quick improvement for per tensor quantization.

per-channel should remove the loop in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp

# Benchmark:
device = GTX-1650
```python
import torch
print(torch.__version__)

for i in range(1000):
    torch.randn(1024 * 128, device='cuda')

def f(e):
    a = torch.randn(2 ** e, device='cuda')
    torch.cuda.synchronize()
    %timeit torch.fake_quantize_per_tensor_affine(a, 0.5, 0, 0, 1); torch.cuda.synchronize()
    
for i in range(15, 27):
    f(i)
```
Before
```
1.5.0a0+bf00b4d
14.5 µs ± 981 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
18.2 µs ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)
25.6 µs ± 2.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
38.6 µs ± 135 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
70.2 µs ± 5.21 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
125 µs ± 4.98 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
231 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
461 µs ± 22.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
891 µs ± 88.2 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.77 ms ± 8.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.77 ms ± 80.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.16 ms ± 216 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
After
```
1.5.0a0+3f18ac3
12.5 µs ± 738 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
13.7 µs ± 195 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
17.9 µs ± 850 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
29.7 µs ± 285 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
50.4 µs ± 1.94 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
95 µs ± 8.23 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
173 µs ± 7.37 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
348 µs ± 29.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
657 µs ± 22.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.33 ms ± 77.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.71 ms ± 211 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.33 ms ± 439 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
33772,zasdfgbnm,pr,2020-02-25T20:02:07Z,Per channel quantization performance improvement,"Benchmark:
NVIDIA GTX 1650 + AMD Ryzen Threadripper 3970X
```python
import torch
print(torch.__version__)

for i in range(1000):
    torch.randn(1024 * 128, device='cuda')

def cuda(e):
    a = torch.randn(2 ** e, 32, device='cuda')
    s = torch.randn(32, device='cuda')
    z = torch.randn(32, device='cuda')
    torch.cuda.synchronize()
    %timeit torch.fake_quantize_per_channel_affine(a, s, z, 1, -999, 999); torch.cuda.synchronize()
    
def cpu(e):
    a = torch.randn(2 ** e, 32, device='cpu')
    s = torch.randn(32, device='cpu')
    z = torch.randn(32, device='cpu')
    %timeit torch.fake_quantize_per_channel_affine(a, s, z, 1, -999, 999);
    
for i in range(10, 24):
    cuda(i)
print()
for i in range(10, 32):
    cpu(i)
```
Before
```
1.5.0a0+9bc922d
849 µs ± 44.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
817 µs ± 30.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
814 µs ± 2.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.11 ms ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.19 ms ± 4.19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.6 ms ± 5.58 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.44 ms ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.14 ms ± 2.55 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.41 ms ± 2.46 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
13.9 ms ± 2.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
26.9 ms ± 254 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
52.6 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
104 ms ± 176 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
207 ms ± 1.24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

249 µs ± 158 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
420 µs ± 230 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
766 µs ± 391 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.45 ms ± 574 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.84 ms ± 34.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.69 ms ± 83 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.29 ms ± 2.58 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.32 ms ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.4 ms ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
47.5 ms ± 264 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
187 ms ± 1.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
379 ms ± 5.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
652 ms ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.22 s ± 4.58 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
2.34 s ± 8.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
4.56 s ± 7.15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
8.97 s ± 33.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
17.8 s ± 32.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
35.2 s ± 167 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
After
```
1.5.0a0+a7ec8cc
92.5 µs ± 2.03 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
97.7 µs ± 469 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
109 µs ± 4.73 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
119 µs ± 6.17 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
146 µs ± 1.84 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
211 µs ± 2.45 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
347 µs ± 4.18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
624 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.17 ms ± 16.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
2.25 ms ± 48.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.43 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
8.51 ms ± 44.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.9 ms ± 30.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
33.7 ms ± 7.64 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

201 µs ± 234 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
285 µs ± 465 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
287 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
287 µs ± 221 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
287 µs ± 761 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
347 µs ± 399 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
675 µs ± 213 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.34 ms ± 643 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
4.82 ms ± 34.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10.7 ms ± 88.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
20.3 ms ± 25.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
39.4 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
78.8 ms ± 2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
153 ms ± 786 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
285 ms ± 911 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
541 ms ± 1.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.03 s ± 1.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.97 s ± 8.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
3.81 s ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

Fixes #33647 ",pytorch
33784,jjsjann123,pr,2020-02-25T22:29:33Z,[CUDNN NHWC CONVOLUTION] Re-stride input tensors for wgrad in cudnn_convolution,,pytorch
33994,zasdfgbnm,pr,2020-02-29T01:57:53Z,Migrate Lerp from CUDA_tensor_apply4 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33998 Kill CUDA_tensor_apply4
* #33997 Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator
* #33996 Migrate dirichlet_grad from CUDA_tensor_apply4 to TensorIterator
* #33995 Migrate binary_cross_entropy_backward from CUDA_tensor_apply4 to
* **#33994 Migrate Lerp from CUDA_tensor_apply4 to TensorIterator**

Differential Revision: [D20196788](https://our.internmc.facebook.com/intern/diff/D20196788)",pytorch
33995,zasdfgbnm,pr,2020-02-29T01:57:58Z,Migrate binary_cross_entropy_backward from CUDA_tensor_apply4 to,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33998 Kill CUDA_tensor_apply4
* #33997 Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator
* #33996 Migrate dirichlet_grad from CUDA_tensor_apply4 to TensorIterator
* **#33995 Migrate binary_cross_entropy_backward from CUDA_tensor_apply4 to**

TensorIterator

Differential Revision: [D20196790](https://our.internmc.facebook.com/intern/diff/D20196790)",pytorch
33996,zasdfgbnm,pr,2020-02-29T01:58:04Z,Migrate dirichlet_grad from CUDA_tensor_apply4 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33998 Kill CUDA_tensor_apply4
* #33997 Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator
* **#33996 Migrate dirichlet_grad from CUDA_tensor_apply4 to TensorIterator**
* #33995 Migrate binary_cross_entropy_backward from CUDA_tensor_apply4 to

Differential Revision: [D20196789](https://our.internmc.facebook.com/intern/diff/D20196789)",pytorch
33997,zasdfgbnm,pr,2020-02-29T01:58:10Z,Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #33998 Kill CUDA_tensor_apply4
* **#33997 Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator**

Differential Revision: [D20196786](https://our.internmc.facebook.com/intern/diff/D20196786)",pytorch
33998,zasdfgbnm,pr,2020-02-29T01:58:15Z,Kill CUDA_tensor_apply4,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#33998 Kill CUDA_tensor_apply4**
* #33997 Migrate prelu_cuda_backward from CUDA_tensor_apply4 to TensorIterator

Differential Revision: [D20196787](https://our.internmc.facebook.com/intern/diff/D20196787)",pytorch
34003,zasdfgbnm,pr,2020-02-29T07:48:43Z,Migrate prelu from CUDA_tensor_apply2 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34007 Completely kill CUDA_tensor_apply2
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator
* **#34003 Migrate prelu from CUDA_tensor_apply2 to TensorIterator**

Differential Revision: [D20196994](https://our.internmc.facebook.com/intern/diff/D20196994)",pytorch
34004,zasdfgbnm,pr,2020-02-29T07:48:48Z,Migrate poisson from CUDA_tensor_apply2 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36135 Completely kill CUDAApplyUtils.cuh
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* **#34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator**

Differential Revision: [D20196997](https://our.internmc.facebook.com/intern/diff/D20196997)",pytorch
34005,zasdfgbnm,pr,2020-02-29T07:48:54Z,Migrate gamma from CUDA_tensor_apply2 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36135 Completely kill CUDAApplyUtils.cuh
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* **#34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator**
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator

Differential Revision: [D20196996](https://our.internmc.facebook.com/intern/diff/D20196996)",pytorch
34006,zasdfgbnm,pr,2020-02-29T07:49:00Z,Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36135 Completely kill CUDAApplyUtils.cuh
* **#34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator**
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator

The CUDA 9 curand workaround is removed in order to be able to use
TensorIterator. I don't know ""have great perf on old CUDA"" vs ""
be able to completely kill CUDA_tensor_apply2"" which is more valuable.

Differential Revision: [D20196995](https://our.internmc.facebook.com/intern/diff/D20196995)",pytorch
34007,zasdfgbnm,pr,2020-02-29T07:49:06Z,Completely kill CUDA_tensor_apply2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34007 Completely kill CUDA_tensor_apply2**
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator

Differential Revision: [D20196998](https://our.internmc.facebook.com/intern/diff/D20196998)",pytorch
34020,zasdfgbnm,pr,2020-02-29T22:06:36Z,Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* #34024 Migrate dropout inference from CUDA_tensor_apply3 to TensorIterator
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator
* #34022 Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator
* #34021 Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator
* **#34020 Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator**

Differential Revision: [D20196083](https://our.internmc.facebook.com/intern/diff/D20196083)",pytorch
34021,zasdfgbnm,pr,2020-02-29T22:06:41Z,Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* #34024 Migrate dropout inference from CUDA_tensor_apply3 to TensorIterator
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator
* #34022 Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator
* **#34021 Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator**
* #34020 Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator

Differential Revision: [D20196082](https://our.internmc.facebook.com/intern/diff/D20196082)",pytorch
34022,zasdfgbnm,pr,2020-02-29T22:06:47Z,Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* #34024 Migrate dropout inference from CUDA_tensor_apply3 to TensorIterator
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator
* **#34022 Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator**
* #34021 Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator
* #34020 Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator

Differential Revision: [D20196080](https://our.internmc.facebook.com/intern/diff/D20196080)",pytorch
34023,zasdfgbnm,pr,2020-02-29T22:06:52Z,Migrate bce loss from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* **#34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator**

Differential Revision: [D20196084](https://our.internmc.facebook.com/intern/diff/D20196084)",pytorch
34024,zasdfgbnm,pr,2020-02-29T22:06:59Z,Migrate dropout inference from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* **#34024 Migrate dropout inference from CUDA_tensor_apply3 to TensorIterator**
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator
* #34022 Migrate kl_div_backward from CUDA_tensor_apply3 to TensorIterator
* #34021 Migrate dirichlet from CUDA_tensor_apply3 to TensorIterator
* #34020 Migrate gamma grad from CUDA_tensor_apply3 to TensorIterator

Copied from https://github.com/pytorch/pytorch/pull/33879

Differential Revision: [D20196081](https://our.internmc.facebook.com/intern/diff/D20196081)",pytorch
34025,zasdfgbnm,pr,2020-02-29T22:07:05Z,Migrate lerp from CUDA_tensor_apply3 to TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #34026 Completely kill CUDA_tensor_apply3
* **#34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator**
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator

Differential Revision: [D20196079](https://our.internmc.facebook.com/intern/diff/D20196079)",pytorch
34026,zasdfgbnm,pr,2020-02-29T22:07:10Z,Completely kill CUDA_tensor_apply3,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34026 Completely kill CUDA_tensor_apply3**
* #34025 Migrate lerp from CUDA_tensor_apply3 to TensorIterator
* #34023 Migrate bce loss from CUDA_tensor_apply3 to TensorIterator

Differential Revision: [D20196078](https://our.internmc.facebook.com/intern/diff/D20196078)",pytorch
34091,zasdfgbnm,pr,2020-03-02T21:37:04Z,DO NOT MERGE: debugging ROCm failure,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#34091 DO NOT MERGE: debugging ROCm failure**
* #34007 Completely kill CUDA_tensor_apply2
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator
* #34003 Migrate prelu from CUDA_tensor_apply2 to TensorIterator

",pytorch
34153,zasdfgbnm,pr,2020-03-03T21:13:50Z,Make DEBUG == REL_WITH_DEB_INFO on CUDA build,"Related issue: https://github.com/pytorch/pytorch/issues/34079

I don't know how much we care about the difference between `-G` and `-lineinfo` in `DEBUG` vs `REL_WITH_DEB_INFO`, but since `-G` never worked, let's just use `-lineinfo` on both `DEBUG` and `REL_WITH_DEB_INFO`. This would resolve the failure in `DEBUG=1` build. Locally tested to work.",pytorch
34259,zasdfgbnm,pr,2020-03-04T20:46:04Z,Reenable large conv tests,"Please merge after https://github.com/pytorch/pytorch/pull/33073

With that PR, we are now trying different algorithms when OOM, so hopefully there will be some algo working at low memory.",pytorch
34364,jjsjann123,pr,2020-03-06T11:36:26Z,[JIT interpreter fix] Disable interpreter inline for Fusion,Inlining fusion node is NOT an optimization.,pytorch
34379,zasdfgbnm,pr,2020-03-06T18:40:03Z,Add nhwc memory format test for dropout,cc: @ptrblck ,pytorch
34381,zasdfgbnm,pr,2020-03-06T19:15:20Z,"Fix Conv.cpp, &&= is not a C++ operator",,pytorch
34407,zasdfgbnm,pr,2020-03-07T00:49:14Z,Fix cudnn 64bit indexing issue,Fix https://github.com/pytorch/pytorch/issues/33143,pytorch
34471,ppwwyyxx,pr,2020-03-09T06:10:34Z,Update compiler error message,"The header is used for compiling extensions as well. The original message can get a bit confusing

",pytorch
34472,ppwwyyxx,pr,2020-03-09T06:15:33Z,Update compiler warning about ABI compatibility,"3ac42677633a39c588c3fea19d2d4121f114edb3 already forces pytorch to use gcc>=5 everywhere

",pytorch
34526,zasdfgbnm,pr,2020-03-10T03:52:12Z,Preserve memory format for torch.cat on CUDA,"fix https://github.com/pytorch/pytorch/issues/34084

cc: @ptrblck @VitalyFedyunin ",pytorch
34528,zasdfgbnm,pr,2020-03-10T04:56:31Z,Clean up include list of Shape.cu,"The include list seems to be copied from somewhere else, and some totally unrelated files are included.",pytorch
34548,zasdfgbnm,pr,2020-03-10T19:51:02Z,Remove cudaMemcpy on full memory overlap,"TensorIterator is already checking partial overlap, so there is no trivial UB, but TensorITerator allows full overlap, and it is not a bad idea to skip the memcpy in such case.

fixes: https://github.com/pytorch/pytorch/issues/34525",pytorch
34558,zasdfgbnm,pr,2020-03-10T22:40:04Z,DO NOT MERGE,,pytorch
34575,zasdfgbnm,pr,2020-03-11T03:33:47Z,DO NOT MERGE,,pytorch
34695,jjsjann123,pr,2020-03-13T00:19:23Z,Add TORCH_API to getOperatorForLiteral,Exposing getOperatorForLiteral in torch_cpu build. Allows torch_gpu to search for operator via String literal as well.,pytorch
34708,zasdfgbnm,pr,2020-03-13T08:36:35Z,Make discontiguous tensors also benefit from unrolling,"This is based on https://github.com/pytorch/pytorch/pull/33720, I didn't use stacked diff because is not very convenient for cherry-picking. Please review after #33720 merged.

Benchmark shows an up to ~10% improvement on half on RTX 2080Ti:
https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-unroll-with-discontig-input.ipynb

We now have a `TrivialOffsetCalculator`, and the unroll strategy takes input offset calculator and output offset calculator as arguments of its constructor. In case of when we know that it is contiguous (for example when the unroll strategy is used inside vectorized kernel), the trivial offset calculator will be used.",pytorch
34749,zasdfgbnm,pr,2020-03-13T23:39:03Z,Make dynamic casting case also benefit from unrolling,"This is based on #34708, I didn't use stacked diff because is not very convenient for cherry-picking. Please review after #34708 merged.

**Legacy kernels are now completely gone. And the rewrite of GPU loops is done.**

Benchmark shows big improvements in performance on RTX 2080ti:
https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-unroll-with-dyn-casting.ipynb",pytorch
34753,d4l3k,pr,2020-03-14T00:44:08Z,[dt][caffe2] enable using smart exceptions in async nets,"Summary: This improves support for exceptions and capturing stack traces in caffe2 async nets. We generally want to use exceptions everywhere we can in order to preserve stack information. It also makes the exception timestamp more accurate so multiple exceptions at the same time can be correctly ordered.

Test Plan: Updated the tests to use the new error semantics + adds a test to ensure the stack is correctly propagated through deferrable async scheduling.

Differential Revision: D20449887

",pytorch
34764,zasdfgbnm,pr,2020-03-14T09:48:52Z,Increase the prec of test_baddbmm,"This test is flaky on my computer, the error is:
```
AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05
```",pytorch
34853,jjsjann123,pr,2020-03-17T01:26:21Z,[CUDNN] Improve cuDNN error message for Convolution,,pytorch
34926,zasdfgbnm,pr,2020-03-17T23:20:09Z,Add operator [] to c10::impl::ListIterator,"This is causing failures on my Windows build

",pytorch
34946,zasdfgbnm,pr,2020-03-18T07:55:17Z,[WIP][TESTING] Merge CUDALoops and ROCmLoops,,pytorch
34978,jeffdaily,pr,2020-03-18T20:29:49Z,"in test_data_parallel.py, remove skipIfRocm from tests that pass",,pytorch
35015,zasdfgbnm,pr,2020-03-19T01:59:12Z,Make GPU loops support mutable lambda,"I will need it for https://github.com/pytorch/pytorch/pull/34004

The `mutable` qualifier allows a lambda to capture some values, and modify its own copy. This would be useful for random kernels: we capture a `state` of RNG, initialize it when it first run, and the initialized stated will be used later:

```C++
gpu_kernel(iter, [state, initialized](scalar_t arg) mutable -> scalar_t {
  if (!initialized) {
    curand_init(..., state);
    initialized = true;
  }
  return some_math(curand_uniform(state), arg);
}
```

The `operator()` of `mutable` lambda is not `const`, so we can not pass it as constant reference. It can not be called inside a non-`mutable` lambda either. 

Example usage:

```C++
auto t = at::empty({4096}, kCUDA);
float thread_work_index_ = 0;
auto iter = TensorIterator::nullary_op(t);
gpu_kernel(iter, [thread_work_index_]GPU_LAMBDA() mutable -> float {
  return thread_work_index_++;
});
```",pytorch
35031,jjsjann123,pr,2020-03-19T08:54:28Z,[JIT] Shape inference improvement,"Support `aten::div` in `PropagateCompleteShapeOnNode`.

complete shape propagation on `aten::div` is disabled, because shape inference
relies on running node to propagate shape. For `aten::div` we run into
deviding-by-zero problem.

However, shape propagation for pointwise operatoins should be identical. We
would be able to swap the operation for `aten::div` with `aten::mul`.

",pytorch
35051,jjsjann123,pr,2020-03-19T18:43:47Z,[JIT] Shape inference improvement,"Support `aten::div` in `PropagateCompleteShapeOnNode`.

complete shape propagation on `aten::div` is disabled, because shape inference
relies on running node to propagate shape. For `aten::div` we run into
deviding-by-zero problem.

However, shape propagation for pointwise operatoins should be identical. We
would be able to swap the operation for `aten::div` with `aten::mul`.

",pytorch
35063,zasdfgbnm,pr,2020-03-19T21:18:07Z,Update docs about DP and DDP for CUDA,We should recommend DDP instead of DP. Hope we can also cherry-pick this for 1.5,pytorch
35093,z-a-f,pr,2020-03-20T08:08:27Z,quantized Conv1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35134 Minor fix to quantized conv docstring
* **#35093 quantized Conv1d**

Differential Revision: [D20555070](https://our.internmc.facebook.com/intern/diff/D20555070)",pytorch
35124,jeffdaily,pr,2020-03-20T21:40:50Z,"in test_c10d.py, remove skip_if_rocm from tests that pass locally","@iotamudelta  Test passed three iterations on the CI, no flakiness detected.",pytorch
35131,zasdfgbnm,pr,2020-03-20T22:46:51Z,Add TORCH_CUDA_API to FilterDescriptor,"`FilterDescriptor` is missing a `TORCH_CUDA_API`, so this symbol is not exported from `torch_cuda.so`, and users could have trouble building cpp_extension when using cudnn.

cc: @ptrblck ",pytorch
35134,z-a-f,pr,2020-03-20T23:27:47Z,Minor fix to quantized conv docstring,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35134 Minor fix to quantized conv docstring**
* #35093 quantized Conv1d

Differential Revision: [D20571996](https://our.internmc.facebook.com/intern/diff/D20571996)",pytorch
35146,zasdfgbnm,pr,2020-03-21T03:27:43Z,[1.5] Update docs about DP and DDP for CUDA (#35063),Cherry-pick of  https://github.com/pytorch/pytorch/pull/35063,pytorch
35150,z-a-f,pr,2020-03-21T05:03:35Z,Making sure all tensors in `torch.cat` sequence have the same dtype.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35150 Making sure all tensors in `torch.cat` sequence have the same dtype.**

Fixes #35014

Differential Revision: [D20578589](https://our.internmc.facebook.com/intern/diff/D20578589)",pytorch
35243,robieta,pr,2020-03-23T21:27:24Z,Optimize index_select for 1D inputs,"`gather` turns out to be much faster than `index_select` for this function. (Anywhere from 2-10x faster across my testing.) We do have to match the shape for the generated indices, however this does not affect performance since `.expand` does not copy the underlying buffer.

I experimented with a custom kernel, but the improvement over this implementation didn't justify the approach since it would have added significant complexity and reduced the use of shared infrastructure in the PyTorch codebase.",pytorch
35255,zasdfgbnm,pr,2020-03-23T22:26:00Z,DO NOT MERGE: test for 35225,,pytorch
35370,z-a-f,pr,2020-03-25T07:46:04Z,Base class for the quantized ConvTranspose,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35460 Quantized ConvTranspose1d
* #35371 Quantized ConvTranspose2d
* **#35370 Base class for the quantized ConvTranspose**

Differential Revision: [D20641812](https://our.internmc.facebook.com/intern/diff/D20641812)",pytorch
35371,z-a-f,pr,2020-03-25T08:09:50Z,Quantized ConvTranspose2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #35460 Quantized ConvTranspose1d
* **#35371 Quantized ConvTranspose2d**
* #35370 Base class for the quantized ConvTranspose

Differential Revision: [D20641811](https://our.internmc.facebook.com/intern/diff/D20641811)",pytorch
35430,d4l3k,pr,2020-03-25T22:51:25Z,"[dt] [caffe2] add/fix shape inference for StumpFunc, SliceGradient and ResizeLike","Summary:
This fixes and adds tests for several commonly used operators.

There's some formatting differences due to running clang-format on one of the files.

Test Plan: buck test //caffe2/caffe2/fb/operators:hypothesis_test //caffe2/caffe2/python/operator_test:utility_ops_test //caffe2/caffe2/python/operator_test:concat_split_op_test

Differential Revision: D20657405

",pytorch
35432,z-a-f,pr,2020-03-25T23:05:09Z,Default scale for the quantized convolution with Kaiming initialization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35432 Default scale for the quantized convolution with Kaiming initialization**

Differential Revision: [D20658720](https://our.internmc.facebook.com/intern/diff/D20658720)",pytorch
35438,zasdfgbnm,pr,2020-03-26T00:13:50Z,Call uncheckedSetDevice in ~InlineDeviceGuard only when device index are different,"Setting device could be expensive, especially when a debugger is present. We should check the device are different before we set.

cc: @ptrblck ",pytorch
35460,z-a-f,pr,2020-03-26T07:49:15Z,Quantized ConvTranspose1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#35460 Quantized ConvTranspose1d**
* #35371 Quantized ConvTranspose2d
* #35370 Base class for the quantized ConvTranspose

Differential Revision: [D20666445](https://our.internmc.facebook.com/intern/diff/D20666445)",pytorch
35524,zasdfgbnm,pr,2020-03-27T07:37:29Z,Add core of c10::complex,"Step 0 of https://github.com/pytorch/pytorch/issues/35284

Reference: https://en.cppreference.com/w/cpp/numeric/complex
We are targeting C++20. The difference across C++ versions are mostly `constexpr` qualifiers, newer version has more function declared as `constexpr`

This PR adds the core of `c10::complex`, it includes
- standard constructors as in `std::complex`
- explicit conversion constructors converting from `std/thrust::complex` to `c10::complex`
- standard assignment operators as in `std::complex`
- conversion assignment operators converting from `std/thrust::complex` to `c10::complex`
- other standard operators as in `std::complex`
- standard methods as in `std::complex`
- explicit casting operators to std/thrust
- basic non-member functions as in `std::complex`:
  - arithmetic operators
  - `==`, `!=`
  - `<<`, `>>`
  - `std::real`, `std::imag`, `std::abs`, `std::arg`, `std::norm`, `std::conj`, `std::proj`, `std::polar`
    - Some of them are intentionally not completely implemented, these are marked as `TODO` and will be implemented in the future.

This PR does not include:
- overload of math functions

which will come in the next PR",pytorch
35583,zasdfgbnm,pr,2020-03-27T22:54:05Z,Make c10::ComplexHalf a template specialization of c10::complex,"Issue: https://github.com/pytorch/pytorch/issues/35284

~This depends on and contains https://github.com/pytorch/pytorch/pull/35524. Please review after the dependency gets merged and I will rebase to get a clean diff.~

This PR basically makes `c10::ComplexHalf` a template specialization of `c10::complex`. Since `c10::ComplexHalf` is not used much, this does not include much change.

Due to the fact that `c10::Half` does not have much `constexpr` methods, it is impossible to keep the same API. Currently, we are just completely reusing the old implementation. It is just the name getting changed from `c10::ComplexHalf` to `c10::complex<c10::Half>`. We can always change the implementation in the future when needed. But for now, I think this is OK.
",pytorch
35725,zasdfgbnm,pr,2020-03-30T22:47:14Z,Add overloads of std:: math functions for c10::complex,"Issue: https://github.com/pytorch/pytorch/issues/35284

~This depends on and contains https://github.com/pytorch/pytorch/pull/35524. Please review after the dependency gets merged and I will rebase to get a clean diff.~

The implementation of most functions follow the pattern

```C++
template<typename T>
C10_HOST_DEVICE c10::complex<T> some_function(c10::complex<T> x) {
#if defined(__CUDACC__) || defined(__HIPCC__)
  return static_cast<c10::complex<T>>(thrust::some_function(static_cast<thrust::complex<T>>(x)));
#else
  return static_cast<c10::complex<T>>(std::some_function(static_cast<std::complex<T>>(x)));
#endif
}
```",pytorch
35765,robieta,pr,2020-03-31T17:27:58Z,Break circular dependency between ATen.h and TensorIndexing.h,This is mostly just so VS Code will stop yelling at me.,pytorch
35774,zasdfgbnm,pr,2020-03-31T20:38:01Z,Merge two function_traits into one,"Changes:
- `c10::guts::function_traits` + `::function_traits` --> `c10::guts::function_traits`
- `ATen/detail/FunctionTraits.h` + `c10/util/Metaprogramming.h` --> `c10/util/Metaprogramming.h`
- `infer_function_traits_t` --> template specialization of `function_traits`
- `traits::number_of_parameters` --> `traits::arity`
- `traits::return_type` + `traits::result_type` --> `traits::return_type`",pytorch
35857,d4l3k,pr,2020-04-02T01:22:28Z,[caffe2] fix type and shape inference for common gradient ops,"Summary:
This fixes a lot of common ops for InferBlobShapesAndTypes as well as adds support for testing the inferred shapes and types of gradient ops.

Ops:
* Concat
* Split
* LeakyReLU
* Relu
* Prelu
* Gelu
* Elu
* Sinh, Tanh, Cosh
* Abs
* ... and a number of other simple element wise ops

Test Plan:
Added support to hypothesis test to check the shape and type of gradient ops.

Enabled it for all the ops I fixed the shape and type inference for.

  buck test caffe2/caffe2/python/operator_test:

Differential Revision: D20806284

",pytorch
35946,zasdfgbnm,pr,2020-04-03T11:15:24Z,Output more debugging information for reduce kernel,,pytorch
35953,jeffdaily,pr,2020-04-03T16:54:56Z,[ROCm] enable mem leak check for rocm,CC @iotamudelta ,pytorch
35997,zasdfgbnm,pr,2020-04-03T22:47:02Z,Target 8192 blocks instead of split to large grid for large reduction,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36014 Refactor thread_reduce for better unrolling and vectorization in the future
* **#35997 Target 4096 blocks instead of split to large grid for large reduction**

When the number of blocks is large enough, we are already achieving
blalanced SM allocation. But we still should keep the number of inputs
per thread large, because thread reduce is cheap.

Benchmark for Half on V100:
https://github.com/zasdfgbnm/things/blob/master/2020Q2/reduction-benchmark.ipynb

On large tensor, it is: 1.37ms vs 1.25ms

Differential Revision: [D20927533](https://our.internmc.facebook.com/intern/diff/D20927533)",pytorch
36014,zasdfgbnm,pr,2020-04-04T03:20:08Z,Refactor thread_reduce for better unrolling and vectorization in the future,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36014 Refactor thread_reduce for better unrolling and vectorization in the future**
* #35997 Target 4096 blocks instead of split to large grid for large reduction

Benchmark on RTX2080Ti: 2.13ms vs 1.88ms
https://github.com/zasdfgbnm/things/blob/master/2020Q2/reduction-benchmark-refactor.ipynb

Differential Revision: [D20927535](https://our.internmc.facebook.com/intern/diff/D20927535)",pytorch
36097,z-a-f,pr,2020-04-06T20:52:24Z,[quant] Adding dynamic qRNN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36097 [quant] Adding dynamic qRNN**

Differential Revision: [D20878115](https://our.internmc.facebook.com/intern/diff/D20878115)",pytorch
36134,zasdfgbnm,pr,2020-04-07T09:06:33Z,Fix prelu_backward TensorIterator split,"We should have
```C++
    for (auto& sub_iter : iter.with_32bit_indexing()) {
      launch_prelu_cuda_backward_share_weights_kernel(sub_iter, weight_data);
    }
```

But I mistakenly wrote it as 

```C++
    for (auto& sub_iter : iter.with_32bit_indexing()) {
      launch_prelu_cuda_backward_share_weights_kernel(iter, weight_data);
    }
```

in my previous PR. Which leads to infinite recursion on it.

I found this bug when working on https://github.com/pytorch/pytorch/pull/34004

I also add a `TORCH_INTERNAL_ASSERT_DEBUG_ONLY` to test for this.

Besides, the caller is already guaranteed contiguous, so we don't need to handle no-contiguous tensors.",pytorch
36135,zasdfgbnm,pr,2020-04-07T09:31:38Z,Completely kill CUDAApplyUtils.cuh,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36135 Completely kill CUDAApplyUtils.cuh**
* #34006 Migrate bernoulli from CUDA_tensor_apply2 to TensorIterator
* #34005 Migrate gamma from CUDA_tensor_apply2 to TensorIterator
* #34004 Migrate poisson from CUDA_tensor_apply2 to TensorIterator

",pytorch
36166,jeffdaily,pr,2020-04-07T20:59:41Z,"[ROCm] increase timeout, enable test_backend_group",CC @iotamudelta ,pytorch
36172,d4l3k,pr,2020-04-07T21:43:54Z,"Back out ""Revert D20449887: [dt][caffe2] enable using smart exceptions in async nets""","Summary:
Original commit changeset: 3d7801613f86

D20449887 broke some OSS tests as the OSS export sync wasn't working correctly.

Test Plan:
Manually export latest version to OSS to trigger the tests

+ test plan in D20449887

Differential Revision: D20902279

",pytorch
36267,d4l3k,pr,2020-04-08T21:35:23Z,caffe2: preserve python exception type from PythonOp,"Summary: This makes PythonOp throw the original python exception instead of wrapping it in a c10::Error type. This allows throwing exceptions from Python and preserving the type when they're caught again in Python. This is important for structured logging and handling non-retryable error types.

Test Plan: buck test caffe2/caffe2/python:python_op_test

Differential Revision: D20928098

",pytorch
36299,zasdfgbnm,pr,2020-04-09T07:50:20Z,"Canonicalize includes in c10, and add tests for it","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36303 Canonicalize includes in torch, and add tests for it
* #36301 Canonicalize includes in aten, and add tests for it
* **#36299 Canonicalize includes in c10, and add tests for it**

Differential Revision: [D20943005](https://our.internmc.facebook.com/intern/diff/D20943005)",pytorch
36301,zasdfgbnm,pr,2020-04-09T08:16:46Z,"Canonicalize includes in aten, and add tests for it","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36303 Canonicalize includes in torch, and add tests for it
* **#36301 Canonicalize includes in aten, and add tests for it**

Differential Revision: [D20943004](https://our.internmc.facebook.com/intern/diff/D20943004)",pytorch
36303,zasdfgbnm,pr,2020-04-09T09:02:02Z,"Canonicalize includes in torch, and add tests for it","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36303 Canonicalize includes in torch, and add tests for it**

Differential Revision: [D20943003](https://our.internmc.facebook.com/intern/diff/D20943003)",pytorch
36352,z-a-f,pr,2020-04-09T23:56:54Z,Adding Conv1d to quantization default_mappings,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36352 Adding Conv1d to quantization default_mappings**

Differential Revision: [D20955781](https://our.internmc.facebook.com/intern/diff/D20955781)",pytorch
36366,ppwwyyxx,pr,2020-04-10T04:31:24Z,[caffe2/detectron2] fix Mask R-CNN caffe2 conversion on GPU,"Summary: fix issues introduced in D20528758

Differential Revision: D20959367

",pytorch
36382,jjsjann123,pr,2020-04-10T10:37:14Z,Fixing SyncBN dgrad,"Previous PR #22248 which provides support for variadic batch size across processes doesn't account the mean_dy/mean_dy_xmu on backward path, which produces wrong dgrad.",pytorch
36397,z-a-f,pr,2020-04-10T17:32:55Z,[quant] Fix for the conv1d kernel shape,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36405 [quant] fixing torch scripting in conv1d
* **#36397 [quant] Fix for the conv1d kernel shape**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/36397

Differential Revision: [D20966295](https://our.internmc.facebook.com/intern/diff/D20966295)",pytorch
36405,z-a-f,pr,2020-04-10T19:34:40Z,[quant] fixing torch scripting in conv1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36405 [quant] fixing torch scripting in conv1d**

Differential Revision: [D20969058](https://our.internmc.facebook.com/intern/diff/D20969058)",pytorch
36442,z-a-f,pr,2020-04-11T22:58:05Z,[quant] qtensor resize,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36450 [quant] quantized reflection_pad1d
* #36449 [quant] Generalizing _calculate_dynamic_qparams in quantized test
* **#36442 [quant] qtensor resize**

Differential Revision: [D20984080](https://our.internmc.facebook.com/intern/diff/D20984080)",pytorch
36449,z-a-f,pr,2020-04-12T04:16:49Z,[quant] Generalizing _calculate_dynamic_qparams in quantized test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36450 [quant] quantized reflection_pad1d
* **#36449 [quant] Generalizing _calculate_dynamic_qparams in quantized test**

Differential Revision: [D20984966](https://our.internmc.facebook.com/intern/diff/D20984966)",pytorch
36450,z-a-f,pr,2020-04-12T04:16:55Z,[quant] quantized reflection_pad1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36450 [quant] quantized reflection_pad1d**
* #36449 [quant] Generalizing _calculate_dynamic_qparams in quantized test

Differential Revision: [D20984967](https://our.internmc.facebook.com/intern/diff/D20984967)",pytorch
36626,zasdfgbnm,pr,2020-04-14T22:43:40Z,Add core of c10::complex [resubmit],"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36626 Add core of c10::complex [resubmit]**

This reverts commit 9216c67c9eb0dfc58b530740d954997130e05b13.

Differential Revision: [D21140441](https://our.internmc.facebook.com/intern/diff/D21140441)",pytorch
36627,zasdfgbnm,pr,2020-04-14T22:46:42Z,[DO NOT MERGE] Enable CUDA 9 CI on this,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36627 [DO NOT MERGE] Enable CUDA 9 CI on this**
* #36626 Add core of c10::complex [resubmit]

",pytorch
36709,zasdfgbnm,pr,2020-04-16T03:06:54Z,Vectorize reduction when reducing on fastest striding dimension,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36709 Vectorize reduction when reducing on fastest striding dimension**

Differential Revision: [D21083393](https://our.internmc.facebook.com/intern/diff/D21083393)",pytorch
36716,z-a-f,pr,2020-04-16T07:33:20Z,[quant] QNNPACK Add deconvolution parameters,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv
* #37405 [quant] QNNPACK deconvolution packing
* **#36716 [quant] QNNPACK Add deconvolution parameters**

Differential Revision: [D21110112](https://our.internmc.facebook.com/intern/diff/D21110112)",pytorch
36759,zasdfgbnm,pr,2020-04-16T22:41:29Z,[DO NOT MERGE] Enable CUDA 10.1 tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36759 [DO NOT MERGE] Enable CUDA 10.1 tests**
* #36626 Add core of c10::complex [resubmit]

",pytorch
36790,z-a-f,pr,2020-04-17T06:33:42Z,[quant] QNNPACK deconv kernel and tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* **#36790 [quant] QNNPACK deconv kernel and tests**
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21110111](https://our.internmc.facebook.com/intern/diff/D21110111)",pytorch
36821,zasdfgbnm,pr,2020-04-17T19:20:37Z,Remove CUDA9Workarounds.cuh,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36822 [DO NOT MERGE] Enable CUDA 9.2 CI
* **#36821 Remove CUDA9Workarounds.cuh**

",pytorch
36822,zasdfgbnm,pr,2020-04-17T19:20:43Z,[DO NOT MERGE] Enable CUDA 9.2 CI,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36822 [DO NOT MERGE] Enable CUDA 9.2 CI**
* #36821 Remove CUDA9Workarounds.cuh

",pytorch
36840,zasdfgbnm,pr,2020-04-17T22:33:31Z,Remove CUDA9Workarounds.cuh,"
",pytorch
36846,zasdfgbnm,pr,2020-04-17T23:11:00Z,"RIP CUDA <9.2: circleci, aten, and caffe2","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36848 RIP CUDA <9.2: THC
* **#36846 RIP CUDA <9.2: circleci, aten, and caffe2**

Differential Revision: [D21620850](https://our.internmc.facebook.com/intern/diff/D21620850)",pytorch
36848,zasdfgbnm,pr,2020-04-17T23:44:47Z,RIP CUDA <9.2: THC,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36848 RIP CUDA <9.2: THC**
* #36846 RIP CUDA <9.2: circleci, aten, and caffe2

",pytorch
36849,zasdfgbnm,pr,2020-04-18T00:01:03Z,debug ROCm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36849 debug ROCm**
* #36848 RIP CUDA <9.2: THC
* #36846 RIP CUDA <9.2: circleci, aten, and caffe2

",pytorch
36854,ppwwyyxx,pr,2020-04-18T01:25:24Z,"Back out ""Vectorize reduction when reducing on fastest striding dimension""","Summary: Original commit changeset: ea3f7f29709c

Test Plan: n/a

Differential Revision: D21103684

",pytorch
36873,zasdfgbnm,pr,2020-04-18T18:45:49Z,Vectorize reduction when reducing on fastest striding dimension [resubmit],,pytorch
36875,robieta,pr,2020-04-18T19:08:51Z,Reduce overheads on several CPU kernels by avoiding restrides.,"Calling `t.as_strided(..., ...)` must make a new `TensorImpl` to back the new tensor, which takes 300-400 ns. Reduction, scatter/gather, and comparison kernels currently restride inputs and outputs in order to handle `dim` inside the function passed to TensorIterator. Because these Tensors are created solely for consumption by the iterator a full restride and metadata copy is surplus to requirements. Moreover, shapes are already checked by these kernels prior to calling `add_input` and `add_output`, so shape inference and broadcasting are also unnecessary.

This PR adds a `TensorIterator::declare_static_shape(...)` method, which allows certain kernels to use a much more constrained and efficient shape path. This results in a 900-1200 ns speedup for `gather / scatter / scatter_add / cumsum / cumprod` and a 250-500 ns speedup for elementwise `min` and `max`.

Measurements were taken with [this python script](https://gist.github.com/robieta/51ac5db2f9c7e812d5ff264403ce4f92), which is driven by [this bash script](https://gist.github.com/robieta/1420e917cf38885de3093f8c3a7bd437). The general procedure for mitigating environmental skew is to repeatedly switch between an environment which is built with master and one which is built with this branch while running the python script. Within the python measurement script the following was used to reduce variation:
* Set number of threads to 1
* Aggressively and randomly interleave task measurements to limit correlation between tasks and system state based on when they were run or what task preceded the current one.
* Warmup period, dropping the first three passes through all of the tasks.
Two independent end-to-end runs are included since there is some variation even with the above measures. Overall measurement error seems to be about +/- 100 ns.

The benchmark also includes several tasks which are not affected by this PR, both to check for a degradation in TensorIterator performance when static shapes are not set (which did happen for an earlier iteration of this optimization) and to estimate measurement variability and validate that measured improvements are significant.

**First run**:
```
                          Delta (median)     Master     (25%,  75%)          Branch     (25%,  75%)
---------------------------------------------------------------------------------------------------------
gather_1D                |     920      |    4,000     (-170, +230)       |  3,100     (-110, +140)
gather_dim0              |     910      |    4,100     (-170, +230)       |  3,200     (-110, +150)
gather_dim1              |   1,200      |    4,400     (-190, +240)       |  3,200     (-120, +150)
scatter_1D               |   1,100      |    2,800     (-120, +160)       |  1,700     (-64 , +81)
scatter_dim0             |   1,000      |    2,900     (-130, +160)       |  1,900     (-72 , +95)
scatter_dim1             |   1,200      |    3,200     (-130, +170)       |  1,900     (-67 , +87)
scatter_add_1D           |   1,100      |    2,800     (-120, +150)       |  1,700     (-68 , +89)
scatter_add_dim0         |   1,000      |    2,900     (-120, +150)       |  1,900     (-77 , +93)
scatter_add_dim1         |   1,300      |    3,100     (-140, +180)       |  1,900     (-76 , +92)
cumsum_1D                |   1,000      |    4,600     (-200, +260)       |  3,600     (-120, +170)
cumsum_dim0              |     860      |    4,500     (-190, +240)       |  3,700     (-140, +180)
cumsum_dim1              |   1,200      |    4,800     (-210, +260)       |  3,700     (-130, +180)
cumprod_1D               |   1,000      |    4,600     (-200, +270)       |  3,600     (-130, +170)
cumprod_dim0             |     910      |    4,600     (-210, +270)       |  3,700     (-130, +170)
cumprod_dim1             |   1,200      |    4,900     (-220, +290)       |  3,700     (-130, +170)
min_dim0                 |     280      |    5,900     (-220, +270)       |  5,600     (-220, +260)
min_dim1                 |     560      |    6,200     (-230, +310)       |  5,600     (-230, +270)
max_dim0                 |     320      |    5,900     (-220, +280)       |  5,600     (-200, +250)
max_dim1                 |     540      |    6,100     (-250, +310)       |  5,600     (-200, +250)
std       (reference)    |      58      |    4,300     (-180, +280)       |  4,200     (-160, +200)
clamp     (reference)    |      87      |    3,400     (-160, +220)       |  3,400     (-140, +170)
argmin    (reference)    |     -85      |    3,900     (-170, +250)       |  4,000     (-170, +200)
sum       (reference)    |     -11      |    4,200     (-180, +240)       |  4,200     (-160, +190)
x < y     (reference)    |     110      |    3,700     (-170, +290)       |  3,500     (-140, +150)
max(x, y) (reference)    |     170      |    3,600     (-170, +200)       |  3,400     (-140, +180)

* Times in nanoseconds
**Deltas: positive is improvement, negative is regression.
```


**Second run:**
```
                          Delta (median)     Master     (25%,  75%)          Branch     (25%,  75%)
---------------------------------------------------------------------------------------------------------
gather_1D                |     850      |    3,900     (-130, +150)       |  3,000     (-110, +130)
gather_dim0              |     860      |    4,000     (-140, +150)       |  3,200     (-110, +150)
gather_dim1              |   1,200      |    4,300     (-160, +160)       |  3,200     (-110, +150)
scatter_1D               |   1,100      |    2,700     (-98 , +110)       |  1,700     (-64 , +83)
scatter_dim0             |     950      |    2,800     (-100, +110)       |  1,900     (-67 , +88)
scatter_dim1             |   1,200      |    3,100     (-120, +140)       |  1,900     (-69 , +88)
scatter_add_1D           |   1,100      |    2,700     (-92 , +110)       |  1,700     (-65 , +95)
scatter_add_dim0         |     960      |    2,800     (-100, +100)       |  1,900     (-74 , +100)
scatter_add_dim1         |   1,200      |    3,100     (-100, +130)       |  1,900     (-72 , +100)
cumsum_1D                |     960      |    4,500     (-140, +190)       |  3,600     (-130, +170)
cumsum_dim0              |     820      |    4,500     (-140, +180)       |  3,700     (-130, +170)
cumsum_dim1              |   1,100      |    4,800     (-160, +200)       |  3,600     (-120, +170)
cumprod_1D               |     960      |    4,500     (-130, +190)       |  3,600     (-130, +180)
cumprod_dim0             |     820      |    4,500     (-150, +190)       |  3,700     (-130, +180)
cumprod_dim1             |   1,100      |    4,800     (-150, +220)       |  3,700     (-130, +180)
min_dim0                 |     260      |    5,800     (-210, +250)       |  5,500     (-200, +230)
min_dim1                 |     580      |    6,100     (-230, +270)       |  5,500     (-200, +220)
max_dim0                 |     250      |    5,800     (-210, +230)       |  5,600     (-170, +210)
max_dim1                 |     520      |    6,100     (-220, +240)       |  5,600     (-180, +210)
std       (reference)    |     170      |    4,300     (-210, +220)       |  4,100     (-160, +190)
clamp     (reference)    |     140      |    3,400     (-140, +170)       |  3,300     (-120, +170)
argmin    (reference)    |     -51      |    3,800     (-170, +190)       |  3,900     (-140, +160)
sum       (reference)    |     -58      |    4,100     (-160, +170)       |  4,200     (-170, +190)
x < y     (reference)    |      64      |    3,600     (-150, +210)       |  3,500     (-140, +180)
max(x, y) (reference)    |     120      |    3,500     (-130, +150)       |  3,400     (-130, +150)

* Times in nanoseconds
**Deltas: positive is improvement, negative is regression.
```

CC @ilia-cher @VitalyFedyunin @glaringlee @gdankel",pytorch
36880,z-a-f,pr,2020-04-19T01:57:28Z,[quant][graphmode] tanh pattern and test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36889 [quant][graphmode] linear_relu
* #36887 [quant][graphmode] torch.clamp
* #36885 [quant][graphmode] fused conv3d + relu
* #36884 [quant][graphmode] conv3d prepack pattern
* #36883 [quant] Minor refactor in fused conv names
* **#36880 [quant][graphmode] tanh pattern and test**

Differential Revision: [D21110105](https://our.internmc.facebook.com/intern/diff/D21110105)",pytorch
36883,z-a-f,pr,2020-04-19T03:39:04Z,[quant] Minor refactor in fused conv names,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36889 [quant][graphmode] linear_relu
* #36887 [quant][graphmode] torch.clamp
* #36885 [quant][graphmode] fused conv3d + relu
* #36884 [quant][graphmode] conv3d prepack pattern
* **#36883 [quant] Minor refactor in fused conv names**
* #36880 [quant][graphmode] tanh pattern and test

Differential Revision: [D21110360](https://our.internmc.facebook.com/intern/diff/D21110360)",pytorch
36884,z-a-f,pr,2020-04-19T03:39:11Z,[quant][graphmode] conv3d prepack pattern,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36889 [quant][graphmode] linear_relu
* #36887 [quant][graphmode] torch.clamp
* #36885 [quant][graphmode] fused conv3d + relu
* **#36884 [quant][graphmode] conv3d prepack pattern**
* #36883 [quant] Minor refactor in fused conv names
* #36880 [quant][graphmode] tanh pattern and test

Differential Revision: [D21110358](https://our.internmc.facebook.com/intern/diff/D21110358)",pytorch
36885,z-a-f,pr,2020-04-19T03:39:17Z,[quant][graphmode] fused conv3d + relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36994 [quant][graphmode] Test refactor -- extract graph checker
* #36889 [quant][graphmode] linear_relu
* #36887 [quant][graphmode] torch.clamp
* **#36885 [quant][graphmode] fused conv3d + relu**

Differential Revision: [D21110359](https://our.internmc.facebook.com/intern/diff/D21110359)",pytorch
36887,z-a-f,pr,2020-04-19T04:09:33Z,[quant][graphmode] torch.clamp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36994 [quant][graphmode] Test refactor -- extract graph checker
* #36889 [quant][graphmode] linear_relu
* **#36887 [quant][graphmode] torch.clamp**
* #36885 [quant][graphmode] fused conv3d + relu

Differential Revision: [D21110469](https://our.internmc.facebook.com/intern/diff/D21110469)",pytorch
36889,z-a-f,pr,2020-04-19T06:26:07Z,[quant][graphmode] linear_relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #36994 [quant][graphmode] Test refactor -- extract graph checker
* **#36889 [quant][graphmode] linear_relu**

Differential Revision: [D21111540](https://our.internmc.facebook.com/intern/diff/D21111540)",pytorch
36960,zasdfgbnm,pr,2020-04-20T22:46:54Z,[DO NOT MERGE] debug ROCm,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36960 [DO NOT MERGE] debug ROCm**
* #36848 RIP CUDA <9.2: THC
* #36846 RIP CUDA <9.2: circleci, aten, and caffe2

",pytorch
36983,ppwwyyxx,pr,2020-04-21T06:02:53Z,[pytorch] add test for empty tensor support in nn.Linear,"Summary:
fix https://github.com/pytorch/pytorch/issues/34202

it seems to be fixed now but without a test

Test Plan: sandcastle

Differential Revision: D21149623

",pytorch
36994,z-a-f,pr,2020-04-21T09:45:32Z,[quant][graphmode] Test refactor -- extract graph checker,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#36994 [quant][graphmode] Test refactor -- extract graph checker**
* #36889 [quant][graphmode] linear_relu

Differential Revision: [D21225873](https://our.internmc.facebook.com/intern/diff/D21225873)",pytorch
37080,zasdfgbnm,pr,2020-04-22T16:07:21Z,Add missing ${CMAKE_CURRENT_SOURCE_DIR}/complex_test.cpp,"This test is never built in OSS CI

",pytorch
37095,zasdfgbnm,pr,2020-04-22T18:51:00Z,Vectorize output for reduction,"Benchmark on P100: https://github.com/zasdfgbnm/things/blob/master/2020Q2/reduction-benchmark-vectorize-output.ipynb

```python
import torch
print(torch.__version__)
print()

for i in range(1000):
    torch.arange(10000, device='cuda')

def benchmark(dtype, i):
    size0 = 2 ** (i // 2)
    size1 = 2 ** ((i + 1) // 2)
    a = torch.zeros(size0, size1, device='cuda', dtype=dtype)
    torch.cuda.synchronize()
    %timeit a.sum(dtype=dtype, dim=0); torch.cuda.synchronize()

for dtype in [torch.int8, torch.half, torch.float, torch.double]:
    print(dtype)
    for i in range(18, 30):
        benchmark(dtype, i)
    print()
```
Before
```
1.5.0a0+3bbb36e

torch.int8
24.5 µs ± 111 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.1 µs ± 216 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
26.1 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.9 µs ± 132 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
39 µs ± 504 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
59.6 µs ± 244 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
111 µs ± 112 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
186 µs ± 300 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
397 µs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
665 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.45 ms ± 837 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.03 ms ± 2.79 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float16
24.2 µs ± 66.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.6 µs ± 255 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
27.2 µs ± 53.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
32 µs ± 91 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
48.1 µs ± 89.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
66.9 µs ± 66.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
121 µs ± 102 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
218 µs ± 384 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
431 µs ± 554 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
854 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.75 ms ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.63 ms ± 849 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float32
24.2 µs ± 117 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.4 µs ± 237 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.3 µs ± 34.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
40.5 µs ± 36.4 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
57.4 µs ± 44.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
85.5 µs ± 41.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 106 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
288 µs ± 181 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
557 µs ± 904 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1e+03 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.98 ms ± 533 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.8 ms ± 1.98 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float64
25 µs ± 54.4 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
26.9 µs ± 320 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
37.1 µs ± 51.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
54.3 µs ± 45.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
84.9 µs ± 65.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
139 µs ± 68.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
275 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
504 µs ± 702 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
987 µs ± 613 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.84 ms ± 1.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.64 ms ± 2.44 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.19 ms ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
After
```
1.5.0a0+3bbb36e

torch.int8
29.8 µs ± 213 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.7 µs ± 1.41 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
33.4 µs ± 4.48 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
32.5 µs ± 110 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
40.6 µs ± 94.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
53.7 µs ± 66.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
68 µs ± 69.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
98.2 µs ± 88.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 116 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
283 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
522 µs ± 563 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
967 µs ± 495 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

torch.float16
29.4 µs ± 68.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.2 µs ± 45.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.8 µs ± 41 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
35.3 µs ± 20.3 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
50.1 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
70.4 µs ± 67.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
101 µs ± 325 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
157 µs ± 179 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
275 µs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
486 µs ± 122 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
936 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.85 ms ± 124 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

torch.float32
29.9 µs ± 36.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.5 µs ± 108 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
33 µs ± 93.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
46 µs ± 37.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
64 µs ± 73.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
99.4 µs ± 82.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
157 µs ± 74.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
265 µs ± 68.8 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
490 µs ± 319 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
960 µs ± 669 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.84 ms ± 632 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.6 ms ± 1.63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float64
33.1 µs ± 74.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
36.7 µs ± 86.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
46.7 µs ± 39.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
61.6 µs ± 196 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
100 µs ± 23.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 202 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
270 µs ± 332 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
491 µs ± 445 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
939 µs ± 339 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.88 ms ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.65 ms ± 5.18 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.3 ms ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",pytorch
37102,robieta,pr,2020-04-22T20:44:26Z,extend gather shape check to handle incorrectly sized outputs,"Fixes a safety issue (Nonsense values and segfaults) introduced by https://github.com/pytorch/pytorch/pull/36875 when in-place gather tries to use incorrect shapes.

Consider the following block of code:
```
k0 = 8
k1 = 8
m = 100

x = torch.rand((k0, k1))
ind = torch.randint(0, k0, (m, k1))
output = torch.empty((m, k1))

print(torch.gather(x, 0, ind, out=output))
print(torch.gather(x, 1, ind, out=output))
```

The first gather is legal, the second is not. (`ind` and `output` need to be transposed) Previously this was caught when the kernel tried to restride inputs for TensorIterator, but we can no longer rely on those checks and must test explicitly. If `m` is small the second gather returns gibberish; if it is large enough to push the read out of memory block the program segfaults.",pytorch
37158,zasdfgbnm,pr,2020-04-23T17:37:59Z,[WIP] Disable BLAS on MacOS build,"Looks like the BLAS on MacOS is compiled with instructions not available on CircleCI's CPU

",pytorch
37161,zasdfgbnm,pr,2020-04-23T18:19:11Z,[WIP] Reland addmv PR with BLAS disabled on Mac OS,,pytorch
37205,zasdfgbnm,pr,2020-04-24T03:02:44Z,Split reduction compile units,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37206 Vectorize on output for reduction kernels
* **#37205 Split reduction compile units**

Differential Revision: [D21233254](https://our.internmc.facebook.com/intern/diff/D21233254)",pytorch
37206,zasdfgbnm,pr,2020-04-24T03:04:45Z,Vectorize loads for reduction kernels when reducing on slow striding dimesions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37206 Vectorize loads for reduction kernels when reducing on slow striding dimesions**

Benchmark on P100: https://github.com/zasdfgbnm/things/blob/master/2020Q2/reduction-benchmark-vectorize-output.ipynb

```python
import torch
print(torch.__version__)
print()

for i in range(1000):
    torch.arange(10000, device='cuda')

def benchmark(dtype, i):
    size0 = 2 ** (i // 2)
    size1 = 2 ** ((i + 1) // 2)
    a = torch.zeros(size0, size1, device='cuda', dtype=dtype)
    torch.cuda.synchronize()
    %timeit a.sum(dtype=dtype, dim=0); torch.cuda.synchronize()

for dtype in [torch.int8, torch.half, torch.float, torch.double]:
    print(dtype)
    for i in range(18, 30):
        benchmark(dtype, i)
    print()
```
Before
```
1.5.0a0+3bbb36e

torch.int8
24.5 µs ± 111 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.1 µs ± 216 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
26.1 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.9 µs ± 132 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
39 µs ± 504 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
59.6 µs ± 244 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
111 µs ± 112 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
186 µs ± 300 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
397 µs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
665 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.45 ms ± 837 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.03 ms ± 2.79 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float16
24.2 µs ± 66.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.6 µs ± 255 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
27.2 µs ± 53.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
32 µs ± 91 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
48.1 µs ± 89.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
66.9 µs ± 66.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
121 µs ± 102 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
218 µs ± 384 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
431 µs ± 554 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
854 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.75 ms ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.63 ms ± 849 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float32
24.2 µs ± 117 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
24.4 µs ± 237 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.3 µs ± 34.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
40.5 µs ± 36.4 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
57.4 µs ± 44.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
85.5 µs ± 41.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 106 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
288 µs ± 181 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
557 µs ± 904 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1e+03 µs ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.98 ms ± 533 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.8 ms ± 1.98 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float64
25 µs ± 54.4 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
26.9 µs ± 320 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
37.1 µs ± 51.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
54.3 µs ± 45.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
84.9 µs ± 65.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
139 µs ± 68.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
275 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
504 µs ± 702 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
987 µs ± 613 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.84 ms ± 1.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.64 ms ± 2.44 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.19 ms ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
After
```
1.5.0a0+3bbb36e

torch.int8
29.8 µs ± 213 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.7 µs ± 1.41 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
33.4 µs ± 4.48 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
32.5 µs ± 110 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
40.6 µs ± 94.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
53.7 µs ± 66.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
68 µs ± 69.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
98.2 µs ± 88.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 116 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
283 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
522 µs ± 563 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
967 µs ± 495 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

torch.float16
29.4 µs ± 68.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.2 µs ± 45.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
30.8 µs ± 41 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
35.3 µs ± 20.3 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
50.1 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
70.4 µs ± 67.6 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
101 µs ± 325 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
157 µs ± 179 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
275 µs ± 791 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
486 µs ± 122 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
936 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.85 ms ± 124 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

torch.float32
29.9 µs ± 36.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
29.5 µs ± 108 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
33 µs ± 93.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
46 µs ± 37.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
64 µs ± 73.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
99.4 µs ± 82.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
157 µs ± 74.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
265 µs ± 68.8 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
490 µs ± 319 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
960 µs ± 669 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.84 ms ± 632 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.6 ms ± 1.63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

torch.float64
33.1 µs ± 74.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
36.7 µs ± 86.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
46.7 µs ± 39.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
61.6 µs ± 196 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
100 µs ± 23.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
158 µs ± 202 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
270 µs ± 332 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
491 µs ± 445 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
939 µs ± 339 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.88 ms ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
3.65 ms ± 5.18 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.3 ms ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

Differential Revision: [D21233255](https://our.internmc.facebook.com/intern/diff/D21233255)",pytorch
37219,zasdfgbnm,pr,2020-04-24T09:45:01Z,MACOSX_DEPLOYMENT_TARGET should be consistent with CircleCI,"https://github.com/pytorch/pytorch/blob/master/.circleci/config.yml:

```
  pytorch_macos_10_13_py3_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-build
    macos:
      xcode: ""9.4.1""
```

",pytorch
37220,zasdfgbnm,pr,2020-04-24T09:49:28Z,Show cpu info for macos jobs,,pytorch
37222,zasdfgbnm,pr,2020-04-24T11:19:12Z,"Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* **#37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""**

This reverts commit c306f2ed0832e0545228e779d3dce2ded2153566.",pytorch
37223,zasdfgbnm,pr,2020-04-24T11:19:18Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* **#37223 [DO NOT MERGE] debug mac os**
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37224,zasdfgbnm,pr,2020-04-24T11:19:25Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* **#37224 [DO NOT MERGE] debug mac os**
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37225,zasdfgbnm,pr,2020-04-24T11:19:32Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* **#37225 [DO NOT MERGE] debug mac os**
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37226,zasdfgbnm,pr,2020-04-24T11:19:39Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* **#37226 [DO NOT MERGE] debug mac os**
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37227,zasdfgbnm,pr,2020-04-24T11:19:46Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* **#37227 [DO NOT MERGE] debug mac os**
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37228,zasdfgbnm,pr,2020-04-24T11:19:53Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* **#37228 [DO NOT MERGE] debug mac os**
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37229,zasdfgbnm,pr,2020-04-24T11:19:59Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* **#37229 [DO NOT MERGE] debug mac os**
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37230,zasdfgbnm,pr,2020-04-24T11:20:06Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* #37231 [DO NOT MERGE] debug mac os
* **#37230 [DO NOT MERGE] debug mac os**
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37231,zasdfgbnm,pr,2020-04-24T11:20:13Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37232 [DO NOT MERGE] debug mac os
* **#37231 [DO NOT MERGE] debug mac os**
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37232,zasdfgbnm,pr,2020-04-24T11:20:20Z,[DO NOT MERGE] debug mac os,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37232 [DO NOT MERGE] debug mac os**
* #37231 [DO NOT MERGE] debug mac os
* #37230 [DO NOT MERGE] debug mac os
* #37229 [DO NOT MERGE] debug mac os
* #37228 [DO NOT MERGE] debug mac os
* #37227 [DO NOT MERGE] debug mac os
* #37226 [DO NOT MERGE] debug mac os
* #37225 [DO NOT MERGE] debug mac os
* #37224 [DO NOT MERGE] debug mac os
* #37223 [DO NOT MERGE] debug mac os
* #37222 Revert ""Revert D20660338: [pytorch][PR] Migrate addmv and mv from legacy to ATen native (CUDA & CPU)""

",pytorch
37236,zasdfgbnm,pr,2020-04-24T14:32:21Z,addmv migration [resubmit],,pytorch
37247,jeffdaily,pr,2020-04-24T18:37:59Z,[ROCm] Update to ROCm 3.3,"CC @ezyang .

ROCm 3.3 packages went live on 2020-04-01.  Tag 376 was pushed on 2020-04-15, so it should be based on ROCm 3.3.

The upgrade to ROCm 3.3 is required as part of the effort to stabilize ROCm CI.",pytorch
37274,zasdfgbnm,pr,2020-04-25T01:21:12Z,Make c10::complex the C++ type for complex tensors,"# Overview

This PR changes the backing type of complex tensors in `ScalarType` from `std::complex` to `c10::complex`. 

Since `c10::complex` and `std::complex` are reinterpret-castable, we can freely use `std::complex *` to access `c10::complex` data and vice versa. The implementation of `c10::complex` is not complete yet, so we are reinterpret casting all complex data to `std::complex` during dispatch, and do all operations in `std::complex`.

# `std::complex` and `c10::complex` interoperatability

To use `std::complex *` to access  `c10::complex` data, the following specializations are added: 
```C++
template <> inline std::complex<float>* Tensor::data_ptr();
template <> inline std::complex<double>* Tensor::data_ptr();
template <> inline std::complex<float> Tensor::item();
template <> inline std::complex<double> Tensor::item();
```

See [`aten/src/ATen/templates/TensorMethods.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-0e8bf6f5024b32c240a4c1f0b4d8fd71)

And

```C++
template <> inline std::complex<float> Scalar::to();
template <> inline std::complex<double> Scalar::to();
```

is added in [`c10/core/Scalar.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-aabe1c134055c8dcefad830c1c7ae957)

# Dispatch

Macros in [`Dispatch.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-737cfdab7707be924da409a98d46cb98) still using `std::complex` as its type. We will add macros such as `AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND3` as needed during the migration and not in this PR.

Note that `AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3` is only used in copy kernel of CUDA, and this PR is already changing it to use `c10::complex` because CUDA copy kernel has to use its original dtype otherwise there will be funny casting of dtypes causing cuda unspecified launch error.

When all the migration is done, the c10 version of macros will be removed, and the default version will have `std::complex` replaced by `c10::complex` by default. This design allows us to incrementally migrate from `std::complex` to `c10::complex`.

# Note

Note that the `std::complex` is not completely replaced by `c10::complex` in c10 yet, for example `c10::Scalar` is still using `std::complex`. This will be fixed in later PRs.",pytorch
37284,z-a-f,pr,2020-04-25T05:22:00Z,[quant] Refactor to allow running tests individually from python,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37284 [quant] Refactor to allow running tests individually from python**

Differential Revision: [D21253415](https://our.internmc.facebook.com/intern/diff/D21253415)",pytorch
37405,z-a-f,pr,2020-04-28T07:59:00Z,[quant] QNNPACK deconvolution packing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* **#37405 [quant] QNNPACK deconvolution packing**



Differential Revision: [D21301246](https://our.internmc.facebook.com/intern/diff/D21301246)",pytorch
37421,zasdfgbnm,pr,2020-04-28T17:18:18Z,Make c10::complex the C++ type for complex tensors,"# Overview

This PR changes the backing type of complex tensors in `ScalarType` from `std::complex` to `c10::complex`. 

Since `c10::complex` and `std::complex` are reinterpret-castable, we can freely use `std::complex *` to access `c10::complex` data and vice versa. The implementation of `c10::complex` is not complete yet, so we are reinterpret casting all complex data to `std::complex` during dispatch, and do all operations in `std::complex`.

# `std::complex` and `c10::complex` interoperatability

To use `std::complex *` to access  `c10::complex` data, the following specializations are added: 
```C++
template <> inline std::complex<float>* Tensor::data_ptr();
template <> inline std::complex<double>* Tensor::data_ptr();
template <> inline std::complex<float> Tensor::item();
template <> inline std::complex<double> Tensor::item();
```

See [`aten/src/ATen/templates/TensorMethods.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-0e8bf6f5024b32c240a4c1f0b4d8fd71)

And

```C++
template <> inline std::complex<float> Scalar::to();
template <> inline std::complex<double> Scalar::to();
```

is added in [`c10/core/Scalar.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-aabe1c134055c8dcefad830c1c7ae957)

# Dispatch

Macros in [`Dispatch.h`](https://github.com/pytorch/pytorch/pull/37274/files#diff-737cfdab7707be924da409a98d46cb98) still using `std::complex` as its type. We will add macros such as `AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND3` as needed during the migration and not in this PR.

Note that `AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3` is only used in copy kernel of CUDA, and this PR is already changing it to use `c10::complex` because CUDA copy kernel has to use its original dtype otherwise there will be funny casting of dtypes causing cuda unspecified launch error.

When all the migration is done, the c10 version of macros will be removed, and the default version will have `std::complex` replaced by `c10::complex` by default. This design allows us to incrementally migrate from `std::complex` to `c10::complex`.

# Note

Note that the `std::complex` is not completely replaced by `c10::complex` in c10 yet, for example `c10::Scalar` is still using `std::complex`. This will be fixed in later PRs.",pytorch
37426,zasdfgbnm,pr,2020-04-28T17:59:42Z,Make c10::ComplexHalf a template specialization of c10::complex,"This PR basically makes `c10::ComplexHalf` a template specialization of `c10::complex`. Since `c10::ComplexHalf` is not used much, this does not include much change.

Due to the fact that `c10::Half` does not have much `constexpr` methods, it is impossible to keep the same API. Currently, we are just completely reusing the old implementation. It is just the name getting changed from `c10::ComplexHalf` to `c10::complex<c10::Half>`. We can always change the implementation in the future when needed. But for now, I think this is OK.",pytorch
37451,z-a-f,pr,2020-04-28T19:53:46Z,[quant] Generalizing _calculate_dynamic_qparams in quantized test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37452 [quant] quantized reflection_pad1d
* **#37451 [quant] Generalizing _calculate_dynamic_qparams in quantized test**

Differential Revision: [D21286660](https://our.internmc.facebook.com/intern/diff/D21286660)",pytorch
37452,z-a-f,pr,2020-04-28T19:53:53Z,[quant] quantized reflection_pad1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37452 [quant] quantized reflection_pad1d**

Differential Revision: [D21286659](https://our.internmc.facebook.com/intern/diff/D21286659)",pytorch
37468,zasdfgbnm,pr,2020-04-28T23:33:53Z,Add overloads of std:: math functions for c10::complex [resubmit],"This reverts commit d167a7f6542ca751de0d5bd76653a587f97906f8.

",pytorch
37470,zasdfgbnm,pr,2020-04-29T00:16:06Z,Remove thrust_t from remainder_kernel_cuda,"complex is not supported, so no need to use thrust",pytorch
37494,z-a-f,pr,2020-04-29T08:24:28Z,[quant] Enable convolution tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37494 [quant] Enable convolution tests**

Differential Revision: [D21299442](https://our.internmc.facebook.com/intern/diff/D21299442)",pytorch
37497,z-a-f,pr,2020-04-29T11:21:05Z,[quant] QNNPACK factoring out common structs from conv/deconv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37497 [quant] QNNPACK factoring out common structs from conv/deconv**
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21301244](https://our.internmc.facebook.com/intern/diff/D21301244)",pytorch
37570,zasdfgbnm,pr,2020-04-30T05:21:25Z,Do not shadow operators in at::native,"Fixes https://github.com/pytorch/pytorch/issues/37563

This PR only fixes the case where `::operator` is defined before `TensorOperators.h` is included. I don't know how to generally fix it when `::operator` is defined after `TensorOperators.h`.",pytorch
37572,zasdfgbnm,pr,2020-04-30T07:20:28Z,Unshadow C++ operators for complex in namespace at and c10,"See: https://github.com/pytorch/pytorch/issues/37563

The eq and ne operator serves as a test for this",pytorch
37574,zasdfgbnm,pr,2020-04-30T08:50:08Z,Remove std::complex to std::complex casting specialization,"This is no longer needed because cuda copy kernel now uses `c10::complex`.

",pytorch
37605,zasdfgbnm,pr,2020-04-30T20:06:44Z,Overload c10::complex operators inside c10 namespace,"See:
https://github.com/pytorch/pytorch/issues/37563#issuecomment-622062118
http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Ro-namespace

Tested by eq and ne operator not failing",pytorch
37609,zasdfgbnm,pr,2020-04-30T21:28:16Z,[TESTING ONLY] DO NOT MERGE,Debugging #35771,pytorch
37632,zasdfgbnm,pr,2020-04-30T23:36:42Z,Remove std::complex from TypeMeta,,pytorch
37633,jeffdaily,pr,2020-04-30T23:44:16Z,fix ROCm bench CI by increasing first iter timeout,,pytorch
37643,zasdfgbnm,pr,2020-05-01T03:50:08Z,Migrate CUDA geometric kernels from std::complex to c10::complex,,pytorch
37647,zasdfgbnm,pr,2020-05-01T04:59:29Z,Migrate CUDA unary complex kernel to c10::complex,,pytorch
37648,zasdfgbnm,pr,2020-05-01T05:24:47Z,Migrate item() to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37651 Migrate CUDA fill kernel to c10::complex
* #37650 Migrate CUDA cat, scatter, gather, index, index_put to c10::complex
* #37649 Migrate CPU casting copy kernel to c10::complex
* **#37648 Migrate item() to c10::complex**

Differential Revision: [D21382318](https://our.internmc.facebook.com/intern/diff/D21382318)",pytorch
37649,zasdfgbnm,pr,2020-05-01T05:24:53Z,Migrate CPU casting copy kernel to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37651 Migrate CUDA fill kernel to c10::complex
* #37650 Migrate CUDA cat, scatter, gather, index, index_put to c10::complex
* **#37649 Migrate CPU casting copy kernel to c10::complex**
* #37648 Migrate item() to c10::complex

Differential Revision: [D21385430](https://our.internmc.facebook.com/intern/diff/D21385430)",pytorch
37650,zasdfgbnm,pr,2020-05-01T05:24:59Z,"Migrate CUDA cat, scatter, gather, index, index_put to c10::complex","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37651 Migrate CUDA fill kernel to c10::complex
* **#37650 Migrate CUDA cat, scatter, gather, index, index_put to c10::complex**
* #37649 Migrate CPU casting copy kernel to c10::complex
* #37648 Migrate item() to c10::complex

Differential Revision: [D21394299](https://our.internmc.facebook.com/intern/diff/D21394299)",pytorch
37651,zasdfgbnm,pr,2020-05-01T05:25:06Z,Migrate CUDA fill kernel to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37651 Migrate CUDA fill kernel to c10::complex**
* #37650 Migrate CUDA cat, scatter, gather, index, index_put to c10::complex
* #37649 Migrate CPU casting copy kernel to c10::complex
* #37648 Migrate item() to c10::complex

Differential Revision: [D21394351](https://our.internmc.facebook.com/intern/diff/D21394351)",pytorch
37676,zasdfgbnm,pr,2020-05-01T18:11:55Z,Move complex utilities out of Half.h,There is no reason to put complex utilities to half header.,pytorch
37677,zasdfgbnm,pr,2020-05-01T18:31:22Z,Add missing c10::complex::value_type,There is such a member type as in https://en.cppreference.com/w/cpp/numeric/complex,pytorch
37679,zasdfgbnm,pr,2020-05-01T18:49:02Z,Add vec256 for c10::complex,Just copy-paste the implementation of `std::complex`. The std:: version will be removed eventually.,pytorch
37689,zasdfgbnm,pr,2020-05-01T20:22:00Z,Math functions of c10::complex should be overloaded as const reference,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37690 Add vec256 for c10::complex
* **#37689 Math functions of c10::complex should be overloaded as const reference**

It has to be this way, otherwise, we will not be able to use it in vec256 because the function pointers declared there are using const reference.

Differential Revision: [D21394603](https://our.internmc.facebook.com/intern/diff/D21394603)",pytorch
37690,zasdfgbnm,pr,2020-05-01T20:22:07Z,Add vec256 for c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37690 Add vec256 for c10::complex**
* #37689 Math functions of c10::complex should be overloaded as const reference

Differential Revision: [D21394694](https://our.internmc.facebook.com/intern/diff/D21394694)",pytorch
37697,zasdfgbnm,pr,2020-05-01T22:37:43Z,Remove AT_DISPATCH_COMPLEX_TYPES_AND and AT_DISPATCH_ALL_TYPES_AND_HALF_AND_COMPLEX,These two macros only appear in `Dispatch.h`,pytorch
37710,z-a-f,pr,2020-05-02T07:13:17Z,[quant] quantized::conv_transpose2d_prepack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37759 [quant] quantized::conv1d_prepack and quantized::conv_transpose1d
* #37711 [quant] quantized::conv_transpose2d
* **#37710 [quant] quantized::conv_transpose2d_prepack**
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21368448](https://our.internmc.facebook.com/intern/diff/D21368448)",pytorch
37711,z-a-f,pr,2020-05-02T07:13:24Z,[quant] quantized::conv_transpose2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37761 [quant] quantized.ConvTranspose1d and quantized.ConvTranspose2d
* #37759 [quant] quantized::conv1d_prepack, quantized::conv_transpose1d_prepack, and quantized::conv_transpose1d
* **#37711 [quant] quantized::conv_transpose2d**
* #37777 [quant] quantized::conv_transpose2d_prepack
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21368447](https://our.internmc.facebook.com/intern/diff/D21368447)",pytorch
37759,z-a-f,pr,2020-05-04T10:07:53Z,"[quant] quantized::conv1d_prepack, quantized::conv_transpose1d_prepack, and quantized::conv_transpose1d","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37761 [quant] quantized.ConvTranspose1d and quantized.ConvTranspose2d
* **#37759 [quant] quantized::conv1d_prepack, quantized::conv_transpose1d_prepack, and quantized::conv_transpose1d**
* #37711 [quant] quantized::conv_transpose2d
* #37777 [quant] quantized::conv_transpose2d_prepack
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21378921](https://our.internmc.facebook.com/intern/diff/D21378921)",pytorch
37761,z-a-f,pr,2020-05-04T11:21:12Z,[quant] quantized.ConvTranspose1d and quantized.ConvTranspose2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37761 [quant] quantized.ConvTranspose1d and quantized.ConvTranspose2d**
* #37759 [quant] quantized::conv1d_prepack, quantized::conv_transpose1d_prepack, and quantized::conv_transpose1d
* #37711 [quant] quantized::conv_transpose2d
* #37777 [quant] quantized::conv_transpose2d_prepack
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing



Differential Revision: [D21379145](https://our.internmc.facebook.com/intern/diff/D21379145)",pytorch
37777,z-a-f,pr,2020-05-04T17:56:55Z,[quant] quantized::conv_transpose2d_prepack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37761 [quant] quantized.ConvTranspose1d and quantized.ConvTranspose2d
* #37759 [quant] quantized::conv1d_prepack, quantized::conv_transpose1d_prepack, and quantized::conv_transpose1d
* #37711 [quant] quantized::conv_transpose2d
* **#37777 [quant] quantized::conv_transpose2d_prepack**
* #37497 [quant] QNNPACK factoring out common structs from conv/deconv
* #36790 [quant] QNNPACK deconv kernel and tests
* #37405 [quant] QNNPACK deconvolution packing

Differential Revision: [D21386462](https://our.internmc.facebook.com/intern/diff/D21386462)",pytorch
37788,zasdfgbnm,pr,2020-05-04T19:22:16Z,Reduction should not coalesce_dimensions when splitting for 32bit indexing,"Fixes https://github.com/pytorch/pytorch/issues/37583
",pytorch
37849,jjsjann123,pr,2020-05-05T18:36:43Z,[CUDA_FUSER] Expand operation support for cuda fuser,"This PR added more supported operations in CUDA fuser. We are covering major point-wise operations supported in legacy fuser.

In an attempt to adapt to legacy executor:
1. added an naive shape propagation pass on pytorch JIT IR;
2. small refactor on graph partitioning;
3. fallback interpreter execution of fusion group;",pytorch
37875,zasdfgbnm,pr,2020-05-05T20:38:43Z,Migrate check_convert to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37878 Migrate some CUDA arithmetic kernels to c10::complex
* #37877 Migrate cpu kernel for index and index_put to c10::complex
* #37876 Migrate CPU sum, eq, and ne to c10::complex
* **#37875 Migrate check_convert to c10::complex**

Differential Revision: [D21426480](https://our.internmc.facebook.com/intern/diff/D21426480)",pytorch
37876,zasdfgbnm,pr,2020-05-05T20:38:50Z,"Migrate CPU sum, eq, and ne to c10::complex","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37878 Migrate some CUDA arithmetic kernels to c10::complex
* #37877 Migrate cpu kernel for index and index_put to c10::complex
* **#37876 Migrate CPU sum, eq, and ne to c10::complex**
* #37875 Migrate check_convert to c10::complex

Differential Revision: [D21426516](https://our.internmc.facebook.com/intern/diff/D21426516)",pytorch
37877,zasdfgbnm,pr,2020-05-05T20:38:56Z,Migrate cpu kernel for index and index_put to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37878 Migrate some CUDA arithmetic kernels to c10::complex
* **#37877 Migrate cpu kernel for index and index_put to c10::complex**
* #37876 Migrate CPU sum, eq, and ne to c10::complex
* #37875 Migrate check_convert to c10::complex

Differential Revision: [D21426613](https://our.internmc.facebook.com/intern/diff/D21426613)",pytorch
37878,zasdfgbnm,pr,2020-05-05T20:39:03Z,Migrate some CUDA arithmetic kernels to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37878 Migrate some CUDA arithmetic kernels to c10::complex**
* #37877 Migrate cpu kernel for index and index_put to c10::complex
* #37876 Migrate CPU sum, eq, and ne to c10::complex
* #37875 Migrate check_convert to c10::complex

Differential Revision: [D21426621](https://our.internmc.facebook.com/intern/diff/D21426621)",pytorch
37896,zasdfgbnm,pr,2020-05-06T01:09:22Z,"Migrate CUDA where, tril, triu to c10::complex","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37900 Migrate CUDA abs and neg to c10::complex
* #37899 Migrate CPU eye to c10::complex
* #37898 Migrate CPU unary ops to c10::complex
* #37897 Migrate CPU tril, triu, masked_fill to c10::complex
* **#37896 Migrate CUDA where, tril, triu to c10::complex**

Differential Revision: [D21442150](https://our.internmc.facebook.com/intern/diff/D21442150)",pytorch
37897,zasdfgbnm,pr,2020-05-06T01:09:29Z,"Migrate CPU tril, triu, masked_fill to c10::complex","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #37899 Migrate CPU eye to c10::complex
* #37898 Migrate CPU unary ops to c10::complex
* **#37897 Migrate CPU tril, triu, masked_fill to c10::complex**

Differential Revision: [D21442181](https://our.internmc.facebook.com/intern/diff/D21442181)",pytorch
37898,zasdfgbnm,pr,2020-05-06T01:09:37Z,Migrate CPU unary ops to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38459 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2
* #37899 Migrate CPU eye to c10::complex
* **#37898 Migrate CPU unary ops to c10::complex**

Differential Revision: [D21554156](https://our.internmc.facebook.com/intern/diff/D21554156)",pytorch
37899,zasdfgbnm,pr,2020-05-06T01:09:52Z,Migrate CPU eye to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38459 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2
* **#37899 Migrate CPU eye to c10::complex**
* #37898 Migrate CPU unary ops to c10::complex

Differential Revision: [D21554155](https://our.internmc.facebook.com/intern/diff/D21554155)",pytorch
37900,zasdfgbnm,pr,2020-05-06T01:10:00Z,Migrate CUDA abs and neg to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#37900 Migrate CUDA abs and neg to c10::complex**
* #37899 Migrate CPU eye to c10::complex
* #37898 Migrate CPU unary ops to c10::complex
* #37897 Migrate CPU tril, triu, masked_fill to c10::complex
* #37896 Migrate CUDA where, tril, triu to c10::complex

",pytorch
37952,jeffdaily,pr,2020-05-06T19:49:51Z,"[ROCm] in test_cuda.py, re-enable skipped tests","- test_stream_context
- test_cublas_multiple_threads_same_device
- test_cusparse_multiple_threads_same_device

These tests passed three rounds of CI.",pytorch
37954,jeffdaily,pr,2020-05-06T19:59:30Z,[ROCm] add support for HIP stream priorities,,pytorch
37977,zasdfgbnm,pr,2020-05-06T23:27:22Z,Migrate AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3 to c10::complex,"`AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND3` is removed
`AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3` is now using `c10::complex`",pytorch
38021,zasdfgbnm,pr,2020-05-07T16:30:16Z,Migrate CPU tensor factories to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38023 Migrate CPU cross and some elementwise to c10::complex
* #38022 Migrate CPU reduction to c10::complex
* **#38021 Migrate CPU tensor factories to c10::complex**

Differential Revision: [D21518263](https://our.internmc.facebook.com/intern/diff/D21518263)",pytorch
38022,zasdfgbnm,pr,2020-05-07T16:30:22Z,Migrate CPU reduction to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38023 Migrate CPU cross and some elementwise to c10::complex
* **#38022 Migrate CPU reduction to c10::complex**
* #38021 Migrate CPU tensor factories to c10::complex

Differential Revision: [D21518270](https://our.internmc.facebook.com/intern/diff/D21518270)",pytorch
38023,zasdfgbnm,pr,2020-05-07T16:30:29Z,Migrate CPU cross and some elementwise to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38463 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX
* **#38023 Migrate CPU cross and some elementwise to c10::complex**

Differential Revision: [D21518304](https://our.internmc.facebook.com/intern/diff/D21518304)",pytorch
38026,zasdfgbnm,pr,2020-05-07T18:06:40Z,Migrate CPU fill kernel to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38026 Migrate CPU fill kernel to c10::complex**

Differential Revision: [D21518318](https://our.internmc.facebook.com/intern/diff/D21518318)",pytorch
38037,zasdfgbnm,pr,2020-05-07T19:35:08Z,Refactor native/cpu/zmath.h,"There is now a `zmath.h` and `zmath_std.h`, where the latter is the copy-paste of the original `zmath.h` and supporting `std::complex`, and `zmath.h` is for supporting `c10::complex`. `zmath_std.h` will be removed eventually.",pytorch
38089,gmagogsfm,pr,2020-05-08T00:06:32Z,[TorchScript] Explicitly disallow del with more than 1 operand.,"Summary: del in python supports multiple operands, but PyTorch c++ frontend doesn't support that. To be consistent across different frontends, we decided to throw an exception when finding del with multiple operands inside torchscript.

Test Plan: Unit tests in test/jit/test_builtins.py

",pytorch
38144,zasdfgbnm,pr,2020-05-08T19:18:25Z,[Resubmit] Migrate AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3 to c10::complex,"This reverts commit 0c936f94d647a2c422d29cafaa923047dd243473.

",pytorch
38338,robieta,pr,2020-05-12T18:11:54Z,Prototype benchmarking util,"This is the prototype for the modular utils that we've been discussing. It is admittedly a large PR, but a good fraction of that is documentation and examples. I've trimmed a bit on the edges since we last discussed this design (for instance Timer is no longer Fuzzer aware), but it's mostly the same.

In addition to the library and hermetic examples, I've included `examples.end_to_end` which tests https://github.com/pytorch/pytorch/pull/38061 over a variety of shapes, dtypes, degrees of broadcasting, and layouts. (CC @crcrpar)  I only did CPU as I'm not set up on a GPU machine yet. [Results from my devserver](https://gist.github.com/robieta/d1a8e1980556dc3f4f021c9f7c3738e2)

Key takeaways:
  1) For contiguous Tensors, larger dtypes (fp32 and fp64) and lots of reuse of the mask due to broadcasting, improvements are significant. (Presumably due to better vectorization?)
  2) There is an extra ~1.5 us overhead, which dominates small kernels.
  3) Cases with lower write intensity (int8, lower mask fraction, etc) or non-contiguous seem to suffer.

Hopefully this demonstrates the proof-of-concept for how this tooling can be used to tune kernels and assess PRs. Looking forward to thoughts and feedback.",pytorch
38345,jjsjann123,pr,2020-05-12T19:01:21Z,[WIP][DO_NOT_REVIEW][SMOKE_TEST] swap fuser for smoke test,"Do not review, this is just using CI to get spot issues.",pytorch
38347,jeffdaily,pr,2020-05-12T19:16:47Z,"Revert ""[Resubmit] Migrate AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3 to c10::complex (#38144)""","This reverts commit 5077518c91562fbc5f52ef165fa8e30ceb6d4077.  (#38144)

The commit causes significant regressions for ROCm.",pytorch
38363,jeffdaily,pr,2020-05-12T22:42:32Z,Partial revert of #38144 to fix ROCm CI.,CC @ezyang @xw285cornell ,pytorch
38370,zasdfgbnm,pr,2020-05-13T01:16:44Z,Migrate min(dim=?) from THC to ATen,"Related issue: https://github.com/pytorch/pytorch/issues/36900

Since I feel this PR is already large enough, I didn't migrate `max` in this PR. Legacy code is not cleaned up either. All these remaining work will be done in later PRs after this is merged.

Benchmark on an extreme case

```python
import torch
print(torch.__version__)

t = torch.randn(100000, 2, device='cuda')

warmup = torch.arange(100000000)
torch.cuda.synchronize()

%timeit t.min(dim=0); torch.cuda.synchronize()
```
Before: 4ms; After: 24.5us.",pytorch
38380,zasdfgbnm,pr,2020-05-13T04:27:11Z,"Revert ""Partial revert of #38144 to fix ROCm CI. (#38363)""",The changes in this file broke ROCm and got reverted in #38363. This PR brings it back with ROCm fixed.,pytorch
38381,jeffdaily,pr,2020-05-13T04:28:28Z,[ROCm] add exact_dtype=False to bfloat16 test,"CC @rohithkrn @ezyang @xw285cornell 

Fixes
- TestNNDeviceTypeCUDA.test_activations_bfloat16_cuda
- TestNNDeviceTypeCUDA.test_pooling_bfloat16_cuda
- TestNNDeviceTypeCUDA.test_softmax_bfloat16_cuda",pytorch
38382,zasdfgbnm,pr,2020-05-13T05:32:27Z,Add complex support for torch.sum,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38382 Add complex support for torch.sum**

Differential Revision: [D21600127](https://our.internmc.facebook.com/intern/diff/D21600127)",pytorch
38425,jeffdaily,pr,2020-05-13T20:29:50Z,relax MAX_JOBS restriction for ROCm builds,"CC @ezyang @xw285cornell @sunway513

Forcing MAX_JOBS=4 was done 2 years ago.  We have tested up to MAX_JOBS=256.  OOM issues are no longer observed.",pytorch
38440,zasdfgbnm,pr,2020-05-13T21:55:41Z,Migrate min(dim=?) from THC to ATen and remove _min,"Related issue: #36900

Since I feel this PR is already large enough, I didn't migrate max in this PR. Legacy code is not cleaned up either. All these remaining work will be done in later PRs after this is merged.

Benchmark on an extreme case
```python
import torch
print(torch.__version__)

t = torch.randn(100000, 2, device='cuda')

warmup = torch.arange(100000000)
torch.cuda.synchronize()

%timeit t.min(dim=0); torch.cuda.synchronize()
```
Before: 4ms; After: 24.5us.",pytorch
38459,zasdfgbnm,pr,2020-05-14T06:46:07Z,Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38459 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2**
* #37899 Migrate CPU eye to c10::complex
* #37898 Migrate CPU unary ops to c10::complex

Differential Revision: [D21593870](https://our.internmc.facebook.com/intern/diff/D21593870)",pytorch
38460,zasdfgbnm,pr,2020-05-14T07:01:11Z,Migrate CPU clamp to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38462 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND
* #38461 Migrate CPU min max to c10::complex
* **#38460 Migrate CPU clamp to c10::complex**

Differential Revision: [D21663855](https://our.internmc.facebook.com/intern/diff/D21663855)",pytorch
38461,zasdfgbnm,pr,2020-05-14T07:08:04Z,Migrate CPU min max to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #38462 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND
* **#38461 Migrate CPU min max to c10::complex**
* #38460 Migrate CPU clamp to c10::complex

Differential Revision: [D21663871](https://our.internmc.facebook.com/intern/diff/D21663871)",pytorch
38462,zasdfgbnm,pr,2020-05-14T07:08:10Z,Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38462 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND**

Differential Revision: [D21663878](https://our.internmc.facebook.com/intern/diff/D21663878)",pytorch
38463,zasdfgbnm,pr,2020-05-14T07:14:54Z,Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38463 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX**
* #38023 Migrate CPU cross and some elementwise to c10::complex

",pytorch
38465,zasdfgbnm,pr,2020-05-14T09:04:08Z,Add std::log1p for c10::complex,"Note that `std::log1p` is not a thing for `std::complex`, so we have to manually implement it.",pytorch
38488,jeffdaily,pr,2020-05-14T17:14:02Z,skip test_torchbind_no_init on rocm,"CC @ezyang @xw285cornell 

Skip new test `test_torchbind_no_init` added by #37474 (0d220ef3810d1fa99e0dc18e426c1c89c3e4d00d).  This fixes ROCm CI.",pytorch
38544,gmagogsfm,pr,2020-05-15T09:17:07Z,Add SymbolicShape and replace all uses of VaryingShape<ShapeSymbol> with it,"Adding a SymbolicShape class to represent a generic tensor shape with ShapeSymbols.

Its core data structure is c10::optional<std::vector<ShapeSymbol>>. If has_value() == false, it represents an unranked tensor shape. At any dimension ShapeSymbol can contain dynamic size, checkable with ShapeSymbol::IsStatic method.

SymbolicShape now replaces all uses of VaryingShape<ShapeSymbol>, ie c10::optional<std::vector<c10::optional<ShapeSymbol>>>. The inner c10::optional wrapper around ShapeSymbol used to indicate dynamic shape, which overlaps with part of ShapeSymbol's representation.",pytorch
38551,jeffdaily,pr,2020-05-15T15:50:01Z,[ROCm] HIP version guard for occupancy API compatibility,"CC @ezyang @xw285cornell 

HIP from ROCm 3.5 renames `hipOccupancyMaxActiveBlocksPerMultiprocessor` to `hipModuleOccupancyMaxActiveBlocksPerMultiprocessor`.  In addition, the API parameter types now match CUDA.  Add these changes in a backwards-compatible manner.",pytorch
38593,zasdfgbnm,pr,2020-05-16T04:49:28Z,Refactor c10::complex and cleanup c10::Scalar,"**Main:**
- `c10::complex` is refactored: it no longer uses inheritance to specialize constructors, but using SFINAE instead. This implementation is cleaner and avoids some compiler bugs.
- `c10::Scalar` is cleaned up: it no longer needs to store complex as `double z[2]`, `c10::complex<double>` will work.

**Other cleanups:**
- `numeric_limits` of `c10::complex` is moved to `complex_utils.h`
- the variable in `c10::complex` storing real and imag is changed from `storage[2]` to `real_` and `imag_`
- remove the `c10::` before `complex` when in `c10` namespace",pytorch
38670,ppwwyyxx,pr,2020-05-18T18:20:47Z,Support paths with spaces when building ninja extension,"Generate the following `build.ninja` file and can successfully build:
```
cflags = -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA '-I/scratch/yuxinwu/space space/detectron2/layers/csrc' -I/private/home/yuxinwu/miniconda3/lib/python3.7
/site-packages/torch/include -I/private/home/yuxinwu/miniconda3/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/private/home/yuxinwu/miniconda3/lib/python3.7/site-packages/torc
h/include/TH -I/private/home/yuxinwu/miniconda3/lib/python3.7/site-packages/torch/include/THC -I/public/apps/cuda/10.1/include -I/private/home/yuxinwu/miniconda3/include/python3.7m -c
post_cflags = -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14
cuda_cflags = -DWITH_CUDA '-I/scratch/yuxinwu/space space/detectron2/layers/csrc' -I/private/home/yuxinwu/miniconda3/lib/python3.7/site-packages/torch/include -I/private/home/yuxinwu/miniconda3/li
b/python3.7/site-packages/torch/include/torch/csrc/api/include -I/private/home/yuxinwu/miniconda3/lib/python3.7/site-packages/torch/include/TH -I/private/home/yuxinwu/miniconda3/lib/python3.7/site
-packages/torch/include/THC -I/public/apps/cuda/10.1/include -I/private/home/yuxinwu/miniconda3/include/python3.7m -c
cuda_post_cflags = -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_
OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -ccbin=/public/apps/gcc/7.1.0/bin/gcc -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0
-gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_70,code=sm_70 -std=c++14
ldflags =

rule compile
  command = $cxx -MMD -MF $out.d $cflags -c $in -o $out $post_cflags
  depfile = $out.d
  deps = gcc

rule cuda_compile
  command = $nvcc $cuda_cflags -c $in -o $out $cuda_post_cflags



build /scratch/yuxinwu/space$ space/build/temp.linux-x86_64-3.7/scratch/yuxinwu/space$ space/detectron2/layers/csrc/vision.o: compile /scratch/yuxinwu/space$ space/detectron2/layers/csrc/vision.c$
p
build /scratch/yuxinwu/space$ space/build/temp.linux-x86_64-3.7/scratch/yuxinwu/space$ space/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.o: compile /scratch/yuxinwu/space$ space/de$
ectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.cpp
build /scratch/yuxinwu/space$ space/build/temp.linux-x86_64-3.7/scratch/yuxinwu/space$ space/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o: compile /scratch/yuxinwu/space$ space/de$
ectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp
build /scratch/yuxinwu/space$ space/build/temp.linux-x86_64-3.7/scratch/yuxinwu/space$ space/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.o: compile /scratch/yuxinwu/space$ space/detectron2$
layers/csrc/nms_rotated/nms_rotated_cpu.cpp
build /scratch/yuxinwu/space$ space/build/temp.linux-x86_64-3.7/scratch/yuxinwu/space$ space/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.o: compile /scratch/yuxinwu/space$ space/detectron2/layer$
/csrc/ROIAlign/ROIAlign_cpu.cpp

```",pytorch
38674,jjsjann123,pr,2020-05-18T19:51:48Z,Fix memory usage increase reported in #38568,"update to in-place version for bias add in convolution, this saves unnecessary memory allocation.",pytorch
38680,zasdfgbnm,pr,2020-05-18T21:54:47Z,Remove deprecated cuDNN API from caffe2,,pytorch
38689,jeffdaily,pr,2020-05-19T00:05:15Z,"Revert ""Updates assertEqual to use torch.isclose-like logic (#37294)""","This reverts commit 9cfc10d52e0d0a8576b0a5a347fa6fa8da86244a.

This is breaking ROCm CI. Notably, TestNCCL.

@mruberry @ezyang @xw285cornell ",pytorch
38724,jeffdaily,pr,2020-05-19T16:31:02Z,Add skipCUDAIfRocm to test_nn test_softmax_results.,"CC @ezyang @xw285cornell @sunway513

Commit 59d92e442b88eae51b84adc4e902e36e8f12a4db (#38557) has caused this test to regularly fail on ROCm CI gfx900 hosts.  Skipping test until root cause analysis can complete.",pytorch
38729,jjsjann123,pr,2020-05-19T18:24:13Z,fix the device inconsistency for import convert_sync_batchnorm,This fixes the device inconsistency reported in #37930 ,pytorch
38730,jeffdaily,pr,2020-05-19T18:42:49Z,add distributed/test_nccl to ROCM_BLACKLIST,"CC @ezyang @xw285cornell @sunway513

Work-around for recent ROCm CI failures due to 9cfc10d52e0d0a8576b0a5a347fa6fa8da86244a (#37294).  Replaces full revert suggested by PR #38689.",pytorch
38790,jeffdaily,pr,2020-05-20T16:37:34Z,add skipIfRocm to TestAutograd.test_memory_profiler,"CC @ezyang @xw285cornell @sunway513

Skip new test until triage of ROCm CI can be completed.

Test added by a94fb71b126001630d3d1e350347c20977f14ec0.",pytorch
38792,zasdfgbnm,pr,2020-05-20T17:02:27Z,Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#38792 Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX**

Differential Revision: [D21669629](https://our.internmc.facebook.com/intern/diff/D21669629)",pytorch
38814,zasdfgbnm,pr,2020-05-20T20:57:03Z,"Revert ""Revert D21593870: Kill AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2""",The failure was caused by cross merge conflicts. A new use of `AT_DISPATCH_ALL_TYPES_AND_C10_COMPLEX_AND2` at `ATen/native/cuda/TensorTransformations.cu` was added before the reverted PR merged. See https://github.com/pytorch/pytorch/pull/38814/commits/c73523a4c30a47caa5beba50f9120b227244ae91,pytorch
38850,zasdfgbnm,pr,2020-05-21T02:14:35Z,"Check illegal output dtype for torch.{min, max}",,pytorch
38881,zasdfgbnm,pr,2020-05-21T17:14:40Z,Migrate AT_DISPATCH_COMPLEX_TYPES to c10::complex,"This brings the whole `AT_DISPATCH_COMPLEX_TYPES` to `c10::complex`. There are many unnecessary
```C++
if (complex) {
   AT_DISPATCH_COMPLEX_TYPES
} else {
  AT_DISPATCH_...
}
```
logic, and I merged these cases when possible.

CPU kernels are fine, but some CUDA kernels are doing `c10::complex -> thrust::complex` casting, which will be removed later.",pytorch
38882,zasdfgbnm,pr,2020-05-21T17:35:11Z,Check reinterpret_cast of complex bidirectional,,pytorch
38892,zasdfgbnm,pr,2020-05-21T21:30:18Z,Migrate AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 to c10::complex,"No special changes are needed for CPU kernels, some CUDA kernels are still doing `c10::complex -> thrust::complex` casting, this will be cleaned up later. But for now, it will be good to just keep it as is, and change the dispatch macro first.",pytorch
38899,jeffdaily,pr,2020-05-21T23:49:47Z,Add ROCm-specific half_support_literal for JIT.,CC @ezyang @xw285cornell @sunway513 @lcskrishna ,pytorch
38993,jjsjann123,pr,2020-05-25T21:58:37Z,[nvFuser] add torch.jit.fuser context manager,"1. `torch.jit.fuser(str)` context manager facilitates switch between backend fusers:
  str - 'fuser0' enables only legacy fuser;
  str - 'fuser1' enables only NNC;
  str - 'fuser2' enables only nvFuser;
2. cleanup updated python tests.",pytorch
39029,zasdfgbnm,pr,2020-05-26T20:20:24Z,"Migrate minall, max, maxall from THC to ATen and cleanup THC",Fixes #36900 fixes https://github.com/pytorch/pytorch/issues/24594,pytorch
39042,zasdfgbnm,pr,2020-05-26T23:08:06Z,"Remove sumall from TH, THC, THCUNN",,pytorch
39043,zasdfgbnm,pr,2020-05-27T00:17:56Z,[WIP] Migrate AT_DISPATCH_COMPLEX_TYPES to c10::complex,"This brings the whole `AT_DISPATCH_COMPLEX_TYPES` to `c10::complex`. There are many unnecessary
```C++
if (complex) {
   AT_DISPATCH_COMPLEX_TYPES
} else {
  AT_DISPATCH_...
}
```
logic, and I merged these cases when possible.

CPU kernels are fine, but some CUDA kernels are doing `c10::complex -> thrust::complex` casting, which will be removed later.",pytorch
39045,zasdfgbnm,pr,2020-05-27T01:13:28Z,Migrate AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 to c10::complex,"No special changes are needed for CPU kernels, some CUDA kernels are still doing `c10::complex -> thrust::complex` casting, this will be cleaned up later. But for now, it will be good to just keep it as is, and change the dispatch macro first.",pytorch
39055,jjsjann123,pr,2020-05-27T08:02:50Z,Nearest interpolation gpu implementation fix [Resolves issue #38985],"fix nearest upsample dgrad bug, where window computation was wrong previously;
fix python test where previously GPU implementation was not tested;",pytorch
39097,jeffdaily,pr,2020-05-27T21:35:07Z,update circleci scripts for rocm ubuntu bionic support,CC @ezyang @xw285cornell @sunway513 ,pytorch
39106,jeffdaily,pr,2020-05-27T22:29:14Z,ROCm jenkins run distributed tests separately as new test3,"CC @ezyang @xw285cornell @sunway513 

Corresponds to https://github.com/pytorch/ossci-job-dsl/pull/65",pytorch
39107,jeffdaily,pr,2020-05-27T22:40:16Z,"Revert ""Simplify precision-specification in tests. (#37181)""","This reverts commit df4066bbb68739e2eb78e898e093525c9876b616.

This broke ROCm CI bfloat16 support.

CC @ezyang @nairbv @rohithkrn @sunway513 ",pytorch
39193,jeffdaily,pr,2020-05-28T19:52:21Z,use --no-binary for pip install of onnx,"onnx package requires a newer setuptools than the current docker build image provides.  Newer docker images are not yet available.  Work around the issue by upgrading setuptools prior to building caffe2.

This should restore ROCm CI caffe2 builds.

CC @ezyang @xw285cornell ",pytorch
39212,zasdfgbnm,pr,2020-05-29T01:00:59Z,Fix some bugs of argmin/argmax and min/max,"Partial fix of: https://github.com/pytorch/pytorch/issues/39060

There are actually two bugs:
1. `TensorIterator::get_dim_to_split` is asserting on what it shouldn't be.
2. `min_kernel_impl` and `max_kernel_impl` are setting `out_scalar_t` wrongly. `out_scalar_t` is used to compute indices for accumulation buffer, which is only used when the tensor is large enough.

Both are tested in `test_argminmax_large_axis_cuda`, but unfortunately, this test does not run on CI.

This PR makes `test_argminmax_large_axis_cuda` green, but this test is still not run on CI. I suggest keeping https://github.com/pytorch/pytorch/issues/39060 open until we figure out a way to run it.",pytorch
39253,jeffdaily,pr,2020-05-29T17:23:18Z,work around building onnx in older rocm docker images,CC @ezyang @xw285cornell @sunway513 @malfet ,pytorch
39262,jeffdaily,pr,2020-05-29T18:27:56Z,Add rocm image to circleci docker builder,CC @ezyang @sunway513 ,pytorch
39277,zasdfgbnm,pr,2020-05-29T22:18:25Z,"Initial support for building on Ampere GPU, CUDA 11, cuDNN 8","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39361 [WIP] Enable TF32 support for cuDNN
* #39283 [WIP] Enable TF32 support for cuBLAS
* **#39277 Initial support for building on Ampere GPU, CUDA 11, cuDNN 8**

This PR contains initial changes that makes PyTorch build with Ampere GPU, CUDA 11, and cuDNN 8.
TF32 related features will not be included in this PR.

Differential Revision: [D21832814](https://our.internmc.facebook.com/intern/diff/D21832814)",pytorch
39283,zasdfgbnm,pr,2020-05-30T00:28:30Z,[WIP] Enable TF32 support for cuBLAS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #39361 [WIP] Enable TF32 support for cuDNN
* **#39283 [WIP] Enable TF32 support for cuBLAS**

",pytorch
39285,zasdfgbnm,pr,2020-05-30T01:02:24Z,Migrate AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2 to c10::complex,"All the uses of `AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2` are for CUDA.

Dispatch macro comes first, cleanup of remaining `c10::complex --> thrust::complex` will be done later.",pytorch
39286,zasdfgbnm,pr,2020-05-30T02:03:57Z,Migrate pow kernel to c10::complex,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39286 Migrate pow kernel to c10::complex**

Differential Revision: [D21999454](https://our.internmc.facebook.com/intern/diff/D21999454)",pytorch
39293,zasdfgbnm,pr,2020-05-30T03:41:08Z,Clean up thrust::complex usage in geometric kernels,,pytorch
39294,zasdfgbnm,pr,2020-05-30T03:47:07Z,Clean up thrust::complex from rsqrt,,pytorch
39296,zasdfgbnm,pr,2020-05-30T04:28:01Z,Migrate AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES to c10::complex,,pytorch
39300,zasdfgbnm,pr,2020-05-30T08:53:02Z,Migrate AT_DISPATCH_COMPLEX_TYPES to c10::complex,,pytorch
39306,zasdfgbnm,pr,2020-05-31T01:23:52Z,"Reland Refactor c10::complex and cleanup c10::Scalar""","This reverts commit 8556664d6896a8e7f48f1c419e06e0568b9ee09e.

",pytorch
39336,ppwwyyxx,pr,2020-06-01T17:07:55Z,[pytorch] Let jit.unused ignore unsupported method signature,"Test Plan: next diff

Differential Revision: D21814656

",pytorch
39339,zasdfgbnm,pr,2020-06-01T18:08:10Z,[JIT] Make torch.unique_consecutive compatible,A `unique_consecutive` version of https://github.com/pytorch/pytorch/pull/38156,pytorch
39346,zasdfgbnm,pr,2020-06-01T19:12:53Z,[WIP] Allow c10::complex to be used in shared memory,,pytorch
39361,zasdfgbnm,pr,2020-06-01T22:41:56Z,[WIP] Enable TF32 support for cuDNN,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#39361 [WIP] Enable TF32 support for cuDNN**
* #39283 [WIP] Enable TF32 support for cuBLAS

",pytorch
39415,zasdfgbnm,pr,2020-06-02T22:19:00Z,[1.5.1] Make torch.unique_consecutive compatible,"Cherry pick of #39339

Summary:
A `unique_consecutive` version of https://github.com/pytorch/pytorch/pull/38156
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39339

Differential Revision: D21823997

Pulled By: eellison

fbshipit-source-id: d14596a36ba36497e296da5a344e0376cef56f1b

",pytorch
39416,zasdfgbnm,pr,2020-06-02T22:28:40Z,[1.5.1] Bug fix for argmin/argmax (#39212),"Cherry-pick of #39212, see https://github.com/pytorch/pytorch/issues/39104#issuecomment-635700502",pytorch
39433,zasdfgbnm,pr,2020-06-03T07:05:36Z,Remove tuple from reduction,,pytorch
39487,gmagogsfm,pr,2020-06-03T22:46:05Z,Support  torch.Tensor subclass (like Parameter) input.,"Currently torch.Tensor subclasses (like torch.nn.Parameter) isn't a supported type annotation to torch script inputs. This PR allows it to be treated like torch.Tensor for compilation.

Closes #38235 ",pytorch
39544,gmagogsfm,pr,2020-06-04T22:26:02Z,[JIT] Add Type::repr_str to return human-readable str,"Clearly expressing a type is inferred by PyTorch instead of explicitly annotated by user makes many error messages more user-friendly

Currently Type has two string conversion methods. str() for IR printing and python_str() for serialization and error message generation. If we want to include more information in type printing while maintaining serialization/deserialization correctness, we need to split python_str() into annotation_str() and repr_str(). 

annotation_str is solely responsible for serialization, it strictly matches format of python type annotation. repr_str() is responsible for generating a human-readable error message that includes information like ""this type is inferred, not explicitly annotated""

Closes #39449 ",pytorch
39553,zasdfgbnm,pr,2020-06-05T01:09:56Z,Migrate AT_DISPATCH_COMPLEX_TYPES to c10::complex,,pytorch
39563,zasdfgbnm,pr,2020-06-05T06:18:00Z,update,,pytorch
39564,zasdfgbnm,pr,2020-06-05T06:40:16Z,Migrate AT_DISPATCH_COMPLEX_TYPES to c10::complex,,pytorch
39568,zasdfgbnm,pr,2020-06-05T10:05:49Z,[JIT] Fix typing.Final for python 3.8,"fixes https://github.com/pytorch/pytorch/issues/39566

`typing.Final` is a thing since python 3.8, and on python 3.8, `typing_extensions.Final` is an alias of `typing.Final`, therefore, `ann.__module__ == 'typing_extensions'` will become False when using 3.8 and `typing_extensions` is installed.

~~I don't know why the test is skipped, seems like due to historical reason when python 2.7 was still a thing?~~ Edit: I know now, the `Final` for `<3.7` don't have `__origin__`",pytorch
39692,gmagogsfm,pr,2020-06-09T01:46:46Z,Add CHECK-SOURCE-HIGHLIGHTED to file check utils.,"Enhance FileCheck util to check for highlighted source ranges. This is useful when writing tests regarding generated error messages that require source code highlighting.

Here is how the error looks like in different cases:

- In case of needed source code token not found at all in input string:
```
RuntimeError: Expected to find ""invalid_token"" but did not find it
Searched string:

...  <--- HERE
def to_list_missing_type_annotation(x):
    # type: (torch.Tensor) -> List[float]
From CHECK-SOURCE-HIGHLIGHTED: invalid_token
```

- In case of source code token not highlighted:
```
Traceback (most recent call last):
  File ""test_range.py"", line 11, in <module>
    FileCheck().check_source_highlighted(""x.tolist()"").run(s)
RuntimeError: Expected to find ""~~~~~~~~~~"" but did not find it
Searched string:
    # type: (torch.Tensor) -> List[float]
    li = x.tolist()
         ~~~~~~~~~ <--- HERE
         ~~~~~~~~~~~~~~~~~~~...  <--- HERE
    return li
```

It is a bit confusing since both input text (usually an error message) and generated error messages have their highlighted portions, but this is consistent of previous behavior. Another option is to generate plain error messages without additional range highlighting on input text.

Test Plan:
Added unit test.

Closes #38698 
",pytorch
39714,z-a-f,pr,2020-06-09T07:46:52Z,[quant] Prep for conv_transpose packing,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* #40370 [quant] conv_transpose1d / conv_transpose2d
* #40360 [quant] conv_transpose1d_prepack / conv_transpose1d_unpack
* #40351 [quant] conv_transpose2d_prepack/conv_transpose2d_unpack
* **#39714 [quant] Prep for conv_transpose packing**



Differential Revision: [D22087071](https://our.internmc.facebook.com/intern/diff/D22087071)",pytorch
39735,jeffdaily,pr,2020-06-09T16:16:05Z,[ROCm] explicitly embed version within image name,"This commit also removes the clang7 install for ROCm images, and properly cleans up the apt cache after ROCm installation to reduce image sizes.

Embedding the ROCm version within the image name follows the precedent set by CUDA images and decouples image creation from ROCm implicitly installing the latest version when images are prepared.

CC @sunway513 @ezyang ",pytorch
39793,zasdfgbnm,pr,2020-06-10T18:24:06Z,Add at::tensor(complex) and torch::tensor(complex) overload,,pytorch
39794,robieta,pr,2020-06-10T18:44:03Z,update schema to reflect aliasing behavior,"Fixes: https://github.com/pytorch/pytorch/issues/38555

I did an audit of `native_functions.yaml` and found several functions in addition to `reshape` which were not reporting that they could alias:

```
@torch.jit.script
def foo(t: torch.Tensor):
    new_value = torch.tensor(1, dtype=t.dtype, device=t.device)

    t.flatten()[0] = new_value
    t.reshape(-1)[1] = new_value
    t.view_as(t)[2] = new_value
    t.expand_as(t)[3] = new_value
    t.reshape_as(t)[4] = new_value
    t.contiguous()[5] = new_value
    t.detach()[6] = new_value

    return t
```

Currently none of the values are assigned after dead code elimination, after this PR all are. (And the JIT output matches that of eager.)

I don't think this needs to be unit tested; presumably the generic machinery already is and this just brings these ops under the same umbrella.

**BC-breaking note**: This updates the native operator schema and the aliasing rules for autograd. JIT passes will no longer incorrectly optimize mutations on graphs containing these ops, and inplace ops on the result of `flatten` will now properly be tracked in Autograd and the proper backward graph will be created.",pytorch
39801,jeffdaily,pr,2020-06-10T20:18:14Z,Correct #39759 for HIP.,"Changes in PR #39759 broke HIP caffe2.
hipify for caffe2 renames CUDA to HIP; torch does not.
If caffe2 calls into torch, it needs to use CUDA-named functions.

CC @ezyang @xw285cornell @sunway513 @houseroad @dzhulgakov ",pytorch
39827,zasdfgbnm,pr,2020-06-10T23:44:42Z,Clean up thrust::complex from tanh_backward,,pytorch
39829,zasdfgbnm,pr,2020-06-11T00:03:06Z,Overload complex math functions on both :: and std::,Because ROCm has bug on std:: functions.,pytorch
39830,zasdfgbnm,pr,2020-06-11T00:23:13Z,Remove cpu vec256 for std::complex,std::complex is gone. We are now using c10::complex,pytorch
39831,zasdfgbnm,pr,2020-06-11T00:25:35Z,Remove std::complex from c10::Scalar,,pytorch
39833,zasdfgbnm,pr,2020-06-11T00:29:00Z,Remove std::complex from c10::Half,,pytorch
39834,zasdfgbnm,pr,2020-06-11T00:32:09Z,Remove LegacyComplex.h,All std::complex has been migrated to c10::complex,pytorch
39835,zasdfgbnm,pr,2020-06-11T00:35:11Z,Remove zmath_std.h,std::complex is gone,pytorch
39838,zasdfgbnm,pr,2020-06-11T01:32:51Z,Remove item and data_ptr for std::complex,,pytorch
39839,zasdfgbnm,pr,2020-06-11T01:41:57Z,Cleanup TensorIteratorDynamicCasting.h,"std::complex, and thrust::complex has gone",pytorch
39882,zasdfgbnm,pr,2020-06-11T22:19:24Z,Typo in Dispatch.h,"std::complex is gone, now we are using c10::complex on all dispatch macros.

",pytorch
39885,zasdfgbnm,pr,2020-06-11T22:39:50Z,Header rename complex_type.h -> complex.h,"This file should have been renamed as `complex.h`, but unfortunately, it was named as `complex_type.h` due to a name clash with FBCode. Is this still the case and is it easy to resolve the name clash? Maybe related to the comment at https://github.com/pytorch/pytorch/pull/39834#issuecomment-642950012",pytorch
39899,zasdfgbnm,pr,2020-06-11T23:28:59Z,Remove thrust::complex from reciprocal,,pytorch
39901,zasdfgbnm,pr,2020-06-11T23:36:38Z,Remove thrust::complex from sqrt,,pytorch
39902,zasdfgbnm,pr,2020-06-11T23:44:26Z,Kill thrust::complex from log kernels,,pytorch
39905,zasdfgbnm,pr,2020-06-11T23:59:12Z,Remove thrust casting from static_cast_with_inter_type,,pytorch
39906,zasdfgbnm,pr,2020-06-12T00:03:18Z,Rename is_complex_t -> is_complex,"`is_complex_t` is a bad name. For example in std, there are `std::is_same` but not `std::is_same_t`.",pytorch
39907,zasdfgbnm,pr,2020-06-12T00:26:34Z,"Kill meanall from TH, THC",,pytorch
39908,jeffdaily,pr,2020-06-12T00:31:33Z,Update jenkins caffe2 scripts for ROCm circleci images.,"Remove work-around to install conda locally for older ROCm jenkins images.
Remove use of sudo to install pip packages.
Install missing packages for caffe2 test.sh needs on ROCm.

CC @ezyang @xw285cornell @sunway513 ",pytorch
39995,robieta,pr,2020-06-13T23:22:44Z,Warn if `import torch` is called from the source root.,"This is a small developer quality of life improvement. I commonly try to run some snippet of python as I'm working on a PR and forget that I've cd-d into the local clone to run some git commands, resulting in annoying failures like:
`ImportError: cannot import name 'default_generator' from 'torch._C' (unknown location)`

This actually took a non-trivial amount of time to figure out the first time I hit it, and even now it's annoying because it happens just infrequently enough to not sit high in the mental cache.

This PR adds a check to `torch/__init__.py` and warns if `import torch` is likely resolving to the wrong thing:

```
WARNING:root:You appear to be importing PyTorch from a clone of the git repo:
  /data/users/taylorrobie/repos/pytorch
  This will prevent `import torch` from resolving to the PyTorch install
  (instead it will try to load /data/users/taylorrobie/repos/pytorch/torch/__init__.py)
  and will generally lead to other failures such as a failure to load C extensions.
```

so that the soon to follow internal import failure makes some sense. I elected to make this a warning rather than an exception because I'm not 100% sure that it's **always** wrong. (e.g. weird `PYTHONPATH` or `importlib` corner cases.)

EDIT: There are now separate cases for `cwd` vs. `PYTHONPATH`, and failure is an `ImportError`.",pytorch
40003,z-a-f,pr,2020-06-14T04:53:13Z,Adding a version serialization type to ConvPackedParam,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42941 [DO NOT MERGE] Trying to redefine the __setstate__ using def_unboxed
* **#40003 Adding a version serialization type to ConvPackedParam**

Differential Revision: [D22158980](https://our.internmc.facebook.com/intern/diff/D22158980)",pytorch
40021,z-a-f,pr,2020-06-15T09:08:41Z,[quant][graphmodel] linear_relu,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40021 [quant][graphmodel] linear_relu**

This replaces #36889 due to significant merge conflicts

Differential Revision: [D22087061](https://our.internmc.facebook.com/intern/diff/D22087061)",pytorch
40079,zasdfgbnm,pr,2020-06-16T05:36:52Z,[resubmit] Kill thrust::complex from log kernels,Use `::log` instead of `std::log` for better ROCm support.,pytorch
40160,zuoxingdong,pr,2020-06-17T13:05:54Z,[Update transforms.py]: use build-in `atanh` in TanhTransform,"Since `torch.atanh` is recently implemented in #38388, we should simply use it for `TanhTransform`. ",pytorch
40181,jeffdaily,pr,2020-06-17T20:01:14Z,ROCm thunk work-around for future transition to ROCm 3.5.1,"ROCm CI hosts will have their kernels upgraded first to ROCm 3.5.1.  CI images will follow soon after.  Due to the thunk/kernel mismatch during the interim, this PR will detect the mismatch and upgrade the thunk during the build.  This PR will be reverted once migration to ROCm 3.5.1 images is complete.

CC @ezyang @xw285cornell ",pytorch
40204,jeffdaily,pr,2020-06-18T00:16:04Z,add test_openmp to ROCM_BLACKLIST,"This test is flaky for rocm platform.  Add to blacklist until it can be further reviewed.

CC @ezyang @xw285cornell @sunway513 ",pytorch
40231,zasdfgbnm,pr,2020-06-18T18:10:21Z,Build docker for CUDA11,,pytorch
40271,z-a-f,pr,2020-06-19T05:52:53Z,[quant] Quantized adaptive_avg_pool3d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40271 [quant] Quantized adaptive_avg_pool3d**

Closes #40244

Differential Revision: [D22134318](https://our.internmc.facebook.com/intern/diff/D22134318)",pytorch
40286,jeffdaily,pr,2020-06-19T17:36:04Z,move ROCm 3.5 thunk upgrade from build.sh into test.sh,#40181 incorrectly placed the thunk work-around into the build.sh scripts.  It needed to be in test.sh.,pytorch
40351,z-a-f,pr,2020-06-21T02:30:43Z,[quant] conv_transpose2d_prepack/conv_transpose2d_unpack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* #40370 [quant] conv_transpose1d / conv_transpose2d
* #40360 [quant] conv_transpose1d_prepack / conv_transpose1d_unpack
* **#40351 [quant] conv_transpose2d_prepack/conv_transpose2d_unpack**
* #39714 [quant] Prep for conv_transpose packing

Differential Revision: [D22158983](https://our.internmc.facebook.com/intern/diff/D22158983)",pytorch
40360,z-a-f,pr,2020-06-21T21:59:22Z,[quant] conv_transpose1d_prepack / conv_transpose1d_unpack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* #40370 [quant] conv_transpose1d / conv_transpose2d
* **#40360 [quant] conv_transpose1d_prepack / conv_transpose1d_unpack**

Differential Revision: [D22158982](https://our.internmc.facebook.com/intern/diff/D22158982)",pytorch
40370,z-a-f,pr,2020-06-22T07:54:43Z,[quant] conv_transpose1d / conv_transpose2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* **#40370 [quant] conv_transpose1d / conv_transpose2d**

Differential Revision: [D22158979](https://our.internmc.facebook.com/intern/diff/D22158979)",pytorch
40371,z-a-f,pr,2020-06-22T09:24:55Z,[quant] ConvTranspose1d / ConvTranspose2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* **#40371 [quant] ConvTranspose1d / ConvTranspose2d**
* #40370 [quant] conv_transpose1d / conv_transpose2d

Differential Revision: [D22158981](https://our.internmc.facebook.com/intern/diff/D22158981)",pytorch
40385,jeffdaily,pr,2020-06-22T17:52:57Z,ROCm 3.5.1 image,,pytorch
40447,jeffdaily,pr,2020-06-23T18:20:27Z,[ROCm] restore jit tests,"Remove `skipIfRocm` from most jit tests and enable `RUN_CUDA_HALF` tests for ROCm.

These changes passed more than three rounds of CI testing against the ROCm CI.

CC @ezyang @xw285cornell @sunway513 ",pytorch
40452,zasdfgbnm,pr,2020-06-23T18:36:56Z,Add CUDA11 build and test,,pytorch
40454,zasdfgbnm,pr,2020-06-23T18:49:42Z,[WIP] build docker image for CUDA11,,pytorch
40481,z-a-f,pr,2020-06-24T01:37:51Z,[quant] Quantized adaptive_avg_pool3d (#40271),"Kernel for the Quantized 3D Adaptive Average Pooling
from #40271

",pytorch
40534,zasdfgbnm,pr,2020-06-24T21:08:04Z,build docker image for CUDA11,,pytorch
40557,gmagogsfm,pr,2020-06-25T07:37:31Z,Splitting embedding_bag to embedding_bag_forward_only and embedding_bag,"Currently embedding_bag's CPU kernel queries whether weight.requires_grad() is true. This violates layering of AutoGrad and Op Kernels, causing issues in third-party backends like XLA. See this [issue](https://github.com/pytorch/xla/issues/2215) for more details.

This PR hoists the query of weight.requires_grad() to Python layer, and splits embedding_bag into two separate ops, each corresponding to weight.requires_grad() == true and false. 


",pytorch
40577,jeffdaily,pr,2020-06-25T18:01:38Z,skip_if_rocm test_rnn in test_c10d_spawn.py,"Test was added a few months back in #36503 but recently became flaky for ROCm.

CC @ezyang @xw285cornell @sunway513 ",pytorch
40639,gmagogsfm,pr,2020-06-26T20:12:47Z,Add PyTorch Glossary,,pytorch
40737,zasdfgbnm,pr,2020-06-29T22:40:48Z,Enable TF32 support for cuDNN,"
",pytorch
40800,zasdfgbnm,pr,2020-06-30T19:07:44Z,Enable TF32 support for cuBLAS,"Benchmark on a fully connected network and torchvision models (time in seconds) on GA100:

| model              | batch size | forward(TF32) | forward(FP32) | backward(TF32) | backward(FP32) |
|--------------------|------------|---------------|---------------|----------------|----------------|
| FC 512-128-32-8    | 512        | 0.000211      | 0.000321      | 0.000499       | 0.000532       |
| alexnet            | 512        | 0.0184        | 0.0255        | 0.0486         | 0.0709         |
| densenet161        | 128        | 0.0665        | 0.204         | 0.108          | 0.437          |
| googlenet          | 256        | 0.0925        | 0.110         | 0.269          | 0.326          |
| inception_v3       | 256        | 0.155         | 0.214         | 0.391          | 0.510          |
| mnasnet1_0         | 512        | 0.108         | 0.137         | 0.298          | 0.312          |
| mobilenet_v2       | 512        | 0.114         | 0.294         | 0.133          | 0.303          |
| resnet18           | 512        | 0.0722        | 0.100         | 0.182          | 0.228          |
| resnext50_32x4d    | 256        | 0.170         | 0.237         | 0.373          | 0.479          |
| shufflenet_v2_x1_0 | 512        | 0.0463        | 0.0473        | 0.125          | 0.123          |
| squeezenet1_0      | 512        | 0.0870        | 0.0948        | 0.205          | 0.214          |
| vgg16              | 256        | 0.167         | 0.234         | 0.401          | 0.502          |
| wide_resnet50_2    | 512        | 0.186         | 0.310         | 0.415          | 0.638          |",pytorch
40811,z-a-f,pr,2020-06-30T20:11:37Z,[quant] Adding zero point type check for per channel quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#40811 [quant] Adding zero point type check for per channel quantization**

Differential Revision: [D22319417](https://our.internmc.facebook.com/intern/diff/D22319417)",pytorch
40898,jjsjann123,pr,2020-07-02T04:08:11Z,[WIP][DO NOT REVIEW] Smoke test2,debugging CI failure.,pytorch
41011,robieta,pr,2020-07-06T15:27:53Z,Add option to warn if elements in a Compare table are suspect,"This PR adds a `.highlight_warnings()` method to `Compare`, which will include a `(! XX%)` next to measurements with high variance to highlight that fact. For example:
```
[------------- Record function overhead ------------]
                      |    lstm_jit   |  resnet50_jit
1 threads: ------------------------------------------
      with_rec_fn     |   650         |  8600
      without_rec_fn  |   660         |  8000
2 threads: ------------------------------------------
      with_rec_fn     |   360         |  4200
      without_rec_fn  |   350         |  4000
4 threads: ------------------------------------------
      with_rec_fn     |   250         |  2100
      without_rec_fn  |   260         |  2000
8 threads: ------------------------------------------
      with_rec_fn     |   200 (! 6%)  |  1200
      without_rec_fn  |   210 (! 6%)  |  1100
16 threads: -----------------------------------------
      with_rec_fn     |   220 (! 8%)  |   900 (! 5%)
      without_rec_fn  |   200 (! 5%)  |  1000 (! 7%)
32 threads: -----------------------------------------
      with_rec_fn     |  1000 (! 7%)  |   920
      without_rec_fn  |  1000 (! 6%)  |   900 (! 6%)

Times are in milliseconds (ms).
(! XX%) Measurement has high variance, where XX is the median / IQR * 100.
```",pytorch
41131,gmagogsfm,pr,2020-07-08T17:47:33Z,"Fix a minor typo ""forget add"" -> ""forget to add""",,pytorch
41133,zasdfgbnm,pr,2020-07-08T18:01:05Z,Pull upstream select_compute_arch from cmake for Ampere,"This pulls the following merge requests from CMake upstream:
- https://gitlab.kitware.com/cmake/cmake/-/merge_requests/4979
- https://gitlab.kitware.com/cmake/cmake/-/merge_requests/4991

The above two merge requests improve the Ampere build:
- If `TORCH_CUDA_ARCH_LIST` is not set, it can now automatically pickup 8.0 as its part of its default value
- If `TORCH_CUDA_ARCH_LIST=Ampere`, it no longer fails with `Unknown CUDA Architecture Name Ampere in CUDA_SELECT_NVCC_ARCH_FLAGS`

Codes related to architecture < 3.5 are manually removed because PyTorch no longer supports it.

cc: @ngimel @ptrblck ",pytorch
41138,zasdfgbnm,pr,2020-07-08T18:51:46Z,Ampere has CUDA_MAX_THREADS_PER_SM == 2048,"See: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf
page 44, table 5
![image](https://user-images.githubusercontent.com/1032377/86958633-56051580-c111-11ea-94da-c726a61dc00a.png)
",pytorch
41163,jeffdaily,pr,2020-07-08T23:56:34Z,do not append to apt list nvidia-docker.list,"When setting up NVIDIA docker repo, each line was appending to the nvidia-docker.list file.  This results in the list growing every time setup_ci_environment.sh is run on the host.  Instead, create the file as new in the first line and subsequently append.

",pytorch
41180,Flamefire,pr,2020-07-09T09:45:01Z,Don't add NCCL dependency to gloo if system NCCL is used,"This avoids a (currently only) warning of cmake:
```
The dependency target ""nccl_external"" of target ""gloo_cuda"" does not exist.
Call Stack (most recent call first):
  CMakeLists.txt:411 (include)
```

This will be a real problem once Policy CMP0046 is set which will make this warning be an error",pytorch
41189,robieta,pr,2020-07-09T17:16:44Z,[DO_NOT_SUBMIT] Add cost based dispatch heuristic to conv2D,"This PR is just for testing. https://github.com/pytorch/pytorch/pull/40610 is the base PR and includes all discussion.

I will link the script that generated these constants shortly.",pytorch
41233,Flamefire,pr,2020-07-10T08:01:45Z,Define PSIMD_SOURCE_DIR when including FP16,"Avoids a superflous redownload when *NNPACK is not used (e.g. on Power)

Example: https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/1128/consoleFull   
Search for ""Downloading PSimd""

See also #41178",pytorch
41255,jeffdaily,pr,2020-07-10T15:11:28Z,generalize circleci docker build.sh and add centos support,"Add centos Dockerfile and support to circleci docker builds, and allow generic image names to be parsed by build.sh, so both hardcoded images and custom images can be built.  

Currently only adds a ROCm centos Dockerfile.

CC @ezyang @xw285cornell @sunway513 
",pytorch
41364,zasdfgbnm,pr,2020-07-13T20:14:34Z,Update docs about CUDA stream priority,,pytorch
41390,gmagogsfm,pr,2020-07-14T08:27:14Z,[1/N] Implement Enum JIT support,"* Add EnumType and AnyEnumType as first-class jit type
* Add Enum-typed IValue
* Enhanced aten::eq to support Enum

Supported:
Enum-typed function targuments
using Enum type and comparing them

TODO:
Add PyThon sugared value for Enum
Support getting name/value attrs of enums
Support Enum-typed return values
Support enum values of different types in same Enum class
Support serialization and deserialization

",pytorch
41397,Flamefire,pr,2020-07-14T12:43:24Z,Don't run tests with custom arguments with pytest,"This patch basically removes the `-m pytest` parameters when `extra_unittest_args` is used (e.g. `--subprocess`)

Fixes #41393

",pytorch
41398,Flamefire,pr,2020-07-14T12:43:54Z,Fix flaky test_stream_event_nogil due to missing event sync,"The test asserts that the stream is ""ready"" but doesn't wait for the
event to be ""executed"" which makes it fail on some platforms where the
`query` call occurs ""soon enough"".

Fixes #38807

",pytorch
41482,d4l3k,pr,2020-07-15T17:11:54Z,caffe2: add PIPELINE tag,"Summary: This adds a new tag for use with pipeline parallelism.

Test Plan: CI

Differential Revision: D22551487

",pytorch
41496,ppwwyyxx,pr,2020-07-15T20:33:22Z,Support @torch.jit.unused on a @torch.no_grad decorated function,"Summary: use the wrapped function (instead of the wrapper) to obtain argument names

Test Plan:
```
buck test mode/dev-nosan //caffe2/test:jit -- 'test_unused_decorator \(test_jit\.TestScript\)'
```

Before:
```
> Traceback (most recent call last):
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/test_jit.py"", line 3014, in test_unused_decorator
>     torch.jit.script(MyMod())
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_script.py"", line 888, in script
>     obj, torch.jit._recursive.infer_methods_to_compile
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py"", line 317, in create_script_module
>     return create_script_module_impl(nn_module, concrete_type, stubs_fn)
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py"", line 376, in create_script_module_impl
>     create_methods_from_stubs(concrete_type, stubs)
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/torch/jit/_recursive.py"", line 292, in create_methods_from_stubs
>     concrete_type._create_methods(defs, rcbs, defaults)
> RuntimeError:
> Non-static method does not have a self argument:
>   File ""/data/users/yuxinwu/fbsource2/fbcode/buck-out/dev/gen/caffe2/test/jit#binary,link-tree/test_jit.py"", line 3012
>             def forward(self, x):
>                 return self.fn()
>                        ~~~~~~~ <--- HERE
>
```

Differential Revision: D22554479

",pytorch
41498,zasdfgbnm,pr,2020-07-15T21:16:01Z,[reland] Enable TF32 support for cuBLAS,fix rocm,pytorch
41506,robieta,pr,2020-07-15T23:51:36Z,move benchmark utils into torch namespace,"Move the timing utils to `torch.utils._benchmark`. I couldn't figure out how to get setuptools to pick it up and put it under `torch` unless it is in the `torch` directory. (And I think it has to be for `setup.py develop` anyway.)

I also modified the record function benchmark since `Timer` and `Compare` should always be available now.",pytorch
41562,zasdfgbnm,pr,2020-07-16T21:36:08Z,[WIP],,pytorch
41583,Flamefire,pr,2020-07-17T08:38:58Z,Remove needless test duplication,"The test loops over `upper` but does not use it effectively running the same test twice which increases test times for no gain.

",pytorch
41813,jeffdaily,pr,2020-07-21T19:09:56Z,[ROCm] update hip library name,"With transition to hipclang, the HIP runtime library name was changed.  A symlink was added to ease the transition, but is going to be removed.  Conditionally set library name based on HIP compiler used.  Patch gloo submodule as part of build_amd.py script until its associated fix is available.

CC @ezyang @xw285cornell @sunway513 ",pytorch
41824,zasdfgbnm,pr,2020-07-21T23:09:30Z,Improve zero sized input for addmv,"fixes https://github.com/pytorch/pytorch/issues/41340

Unfortunately, I still can not get a K80 to verify the fix, but it should be working.",pytorch
41834,zasdfgbnm,pr,2020-07-22T02:05:49Z,Do not generate dynamic casting kernel if the op does not support it,"Binary size of `torch_cuda_generated_Activation.cu.o`: 38.8MB --> 30.7MB.
This PR only modifies activations, more are coming in later PRs.",pytorch
41843,gmagogsfm,pr,2020-07-22T07:14:40Z,Make runCleanUpPasses idempotent by reordering passes,"Currently constant pooling runs before const propagation, which can create more constants that need pooling. This can get in the way of serialization/deserialization stability because each time user serializes and deserializes a module, runCleanUpPasses is called upon it. Doing so multiple times would lead to different saved module.

This PR moves constant pooling after const propagation, which may slow down const propagation a little bit, but would otherwise side-step aforementioned problem.",pytorch
41891,gmagogsfm,pr,2020-07-23T00:08:34Z,Reduce instability in runCleanUpPasses by reordering passes.,"Currently constant pooling runs before const propagation, which can create more constants that need pooling. This can get in the way of serialization/deserialization stability because each time user serializes and deserializes a module, runCleanUpPasses is called upon it. Doing so multiple times would lead to different saved module.

This PR moves constant pooling after const propagation, which may slow down const propagation a little bit, but would otherwise side-step aforementioned problem.

test_constant_insertion in test_jit.py is also updated because after fixing the pass ordering, the number of constants is no longer a constant and it is extremely difficult to get the exact number with the current convoluted test structure. So for now, I changed the test to check only that CSE doesn't change number of ""prim::constant"" rather than comparing against a known number. Also left a TODO to improve this test.

ConstantPropagation pass is replaced by ConstantPropagationImmutableTypes because the latter is used in runCleanUpPasses. If not replaced, the former would create new CSE opportunities by folding more constants. This voids the purpose of the test case.",pytorch
41907,gmagogsfm,pr,2020-07-23T04:56:02Z,Support custom exception message,"Raise and assert used to have a hard-coded error message ""Exception"". User provided error message was ignored. This PR adds support to represent user's error message in TorchScript.

This breaks backward compatibility because now we actually need to script the user's error message, which can potentially contain unscriptable expressions. Such programs can break when scripting, but saved models can still continue to work.

Increased an op count in test_mobile_optimizer.py because now we need aten::format to form the actual exception message.

This is built upon an WIP PR:  https://github.com/pytorch/pytorch/pull/34112 by @driazati",pytorch
41924,Flamefire,pr,2020-07-23T13:52:13Z,Replace if(NOT ${var}) by if(NOT var),"As explained in #41922 using `if(NOT ${var})"" is usually wrong and can lead to issues like #41922 where the condition is wrongly evaluated to FALSE instead of TRUE. Instead the unevaluated variable name should be used in all cases, see the CMake docu for details.

This fixes the `NOT ${var}` cases by using a simple regexp replacement. It seems `pybind11_PREFER_third_party` is the only variable really prone to causing an issue as all others are set. However due to CMake evaluating unquoted strings in `if` conditions as variable names I recommend to never use unquoted `${var}` in an if condition. A similar regexp based replacement could be done on the whole codebase but as that does a lot of changes I didn't include this now. Also `if(${var})` will likely lead to a parser error if `var` is unset instead of a wrong result

Fixes #41922",pytorch
41925,Flamefire,pr,2020-07-23T14:35:42Z,Fix typo CXX_FLAGS -> CXXFLAGS in ASAN script and docu,The correct environment variable is CXXFLAGS not CXX_FLAGS which likely was influenced by CMakes naming: `CMAKE_CXX_FLAGS`. See e.g. https://en.wikipedia.org/wiki/CFLAGS,pytorch
41952,jeffdaily,pr,2020-07-23T22:00:39Z,restore at::Half support for caffe2 SumOp,"PR #40379 added long support but removed at::Half support.  Restore at::Half support.

CC @ezyang @xw285cornell @neha26shah",pytorch
41965,gmagogsfm,pr,2020-07-24T01:19:01Z,Add prim::EnumName and prim::EnumValue ops,"[2/N] Implement Enum JIT support
 
Add prim::EnumName and prim::EnumValue and their lowerings to support getting `name` and `value` attribute of Python enums.

Supported:
Enum-typed function targuments
using Enum type and comparing them
Support getting name/value attrs of enums

TODO:
Add PyThon sugared value for Enum
Support Enum-typed return values
Support enum values of different types in same Enum class
Support serialization and deserialization",pytorch
41982,Flamefire,pr,2020-07-24T06:06:01Z,"Replace ""if(NOT ${var}"" by ""if(NOT var""",Backport of #41924 to allow system pybind11 to be used,pytorch
41988,Flamefire,pr,2020-07-24T06:50:05Z,Fix flaky test_stream_event_nogil due to missing event sync,"The test asserts that the stream is ""ready"" but doesn't wait for the
event to be ""executed"" which makes it fail on some platforms where the
`query` call occurs ""soon enough"".

Backport of #41398
",pytorch
41989,Flamefire,pr,2020-07-24T06:52:02Z,Define PSIMD_SOURCE_DIR when including FP16,"Avoids a superflous redownload when *NNPACK is not used (e.g. on Power)

Backport of #41233
",pytorch
41991,Flamefire,pr,2020-07-24T07:12:34Z,Don't add NCCL dependency to gloo if system NCCL is used,"This avoids a (currently only) warning of cmake:
```
The dependency target ""nccl_external"" of target ""gloo_cuda"" does not exist.
Call Stack (most recent call first):
  CMakeLists.txt:411 (include)
```

Backport of #41180
",pytorch
41993,Flamefire,pr,2020-07-24T07:22:26Z,[PATCH] reenable test_tensorexpr,Backport of #41445,pytorch
41994,Flamefire,pr,2020-07-24T07:26:38Z,[RPC tests] Fix test_init_(rpc|pg)_then_(rpc|pg) not shutting down RPC,"The problem was due to non-deterministic destruction order of two global static variables: the mutexes used by glog and the RPC agent (which was still set because we didn't call `rpc.shutdown()`). When the TensorPipe RPC agent shuts down some callbacks may fire with an error and thus attempt to log something. If the mutexes have already been destroyed this causes a SIGABRT.

Backport of #41558",pytorch
41995,Flamefire,pr,2020-07-24T07:32:07Z,[PATCH] [RPC tests] Remove world_size and init_method from TensorPipe fixture,"Orignal message:

> This prepares the stack by simplifying the TensorPipe fixture. A comment says that the TensorPipe fixture cannot subclass the generic fixture class as that would lead to a diamond class hierarchy which Python doesn't support (whereas in fact it does), and therefore it copies over two properties that are defined on the generic fixture. However, each class that uses the TensorPipe fixture also inherits from the generic fixture, so there's no need to redefine those properties. And, in fact, by not redefining it we save ourselves some trouble when the TensorPipe fixture would end up overriding another override.

Backport of #40814

This fixes #41365",pytorch
41996,Flamefire,pr,2020-07-24T07:43:47Z,[PATCH] puts cuda test on cuda,"The test was always running on the CPU. This actually caused it to throw an error on non-MKL builds, since the CUDA test (which ran on the CPU) tried to execute but the test requires MKL (a requirement only checked for the CPU variant of the test).

Backport of #41523",pytorch
42010,Flamefire,pr,2020-07-24T15:01:25Z,Skip caffe2 unique op test for rocm3.5,"Summary:
unique op test failure in caffe2 blocks upgrading CI to rocm3.5.1. Skipping the test to unblock will re-enable after root causing and fixing the issue.

Backport of #41219

Fixes #42001",pytorch
42040,gmagogsfm,pr,2020-07-24T22:11:45Z,Enum python sugared value,"Fixes #{issue number}
",pytorch
42085,gmagogsfm,pr,2020-07-26T16:50:35Z,Implement Enum sugared value and Enum constant support,"[3/N] Implement Enum JIT support

* Add enum value as constant support
* Add sugared value for EnumClass

Supported:
Enum-typed function arguments
using Enum type and comparing them
Support getting name/value attrs of enums
Using Enum value as constant

TODO:
Add PyThon sugared value for Enum
Support Enum-typed return values
Support serialization and deserialization",pytorch
42197,jeffdaily,pr,2020-07-28T22:37:57Z,[ROCm] allow .jenkins/pytorch/test.sh to run on centos,"This doesn't fix any reported issue.  We validate ROCm PyTorch on ubuntu and centos.  For centos, we must modify the test.sh script to let it run on centos.",pytorch
42199,zasdfgbnm,pr,2020-07-28T23:12:41Z,Remove with_metaclass from torch,,pytorch
42200,zasdfgbnm,pr,2020-07-28T23:42:38Z,Update CUDA11 docker container,"- no more `-rc`
- add magma",pytorch
42250,robieta,pr,2020-07-29T20:17:43Z,Move op microbenchmarks to use fuzzed values,"Right now the current op microbenchmarks are quite limited. For instance, `add` only benchmarks contiguous 3D fp32 Tensors with power-of-two sizes and no broadcasting. For now I've only moved `AddBenchmark` so that we can discuss design. Once there is agreement there is will be fairly mechanical to move large swaths over.

# Changes
## Fuzzer
#### Random generation is keyed to an index. (And `__getitem__` is implemented.)
This is something that I've been meaning to do for a little while, as it means that to get the kth configuration you don't have to replay k-1 parameter generations. (And it also means that if there is some non-repeatability that creeps in it won't contaminate other indices.)

#### Added the `Fuzzer.params` property to get params without actually allocating Tensors.

#### Broke BinaryOpFuzzer into small, medium, and large variants.
The existing limits were far too large to be used for microbenchmarking, so they were preserved in a LARGE configuration, and microbenchmarks use SMALL and MEDIUM.


## Op microbenchmarks
One thing that I really like about the existing op microbenchmarks is that definitions are close to usage. As a result, for now I chose to leave the Tensor generation in the benchmark class rather than using Fuzzer's allocation. (Which does mean that all of the Tensors are contiguous, but I think that's fine for now.) Instead the fuzzer is simply used to generate more varied shapes. This means that `--list_tests` will generally be sufficient, and the fuzzer can be treated as little more than a repeatable black box. The ten short and ten long fuzzed configurations are:

```
# List of tests:
# add_X_SIZE(64,)_Y_SIZE(1,)_cpu
# add_X_SIZE(8,32)_Y_SIZE(8,32)_cpu
# add_X_SIZE(8,16)_Y_SIZE(1,16)_cpu
# add_X_SIZE(128,39,8)_Y_SIZE(128,39,8)_cpu
# add_X_SIZE(84,)_Y_SIZE(84,)_cpu
# add_X_SIZE(117,70,29)_Y_SIZE(1,70,1)_cpu
# add_X_SIZE(68,28,27)_Y_SIZE(68,28,27)_cpu
# add_X_SIZE(19,)_Y_SIZE(19,)_cpu
# add_X_SIZE(20,)_Y_SIZE(20,)_cpu
# add_X_SIZE(64,)_Y_SIZE(1,)_cpu
# add_X_SIZE(256,)_Y_SIZE(1,)_cpu
# add_X_SIZE(8,128)_Y_SIZE(8,128)_cpu
# add_X_SIZE(8,64)_Y_SIZE(1,64)_cpu
# add_X_SIZE(1024,132,8)_Y_SIZE(1024,132,8)_cpu
# add_X_SIZE(495,)_Y_SIZE(495,)_cpu
# add_X_SIZE(881,362,76)_Y_SIZE(1,362,1)_cpu
# add_X_SIZE(345,71,68)_Y_SIZE(345,71,68)_cpu
# add_X_SIZE(155,)_Y_SIZE(155,)_cpu
# add_X_SIZE(870,)_Y_SIZE(870,)_cpu
# add_X_SIZE(512,)_Y_SIZE(1,)_cpu
```

As you can see, this adds one, two, and three dim Tensors, non-power-of-two for some of the sizes (powers of two are still overrepresented since they are more important), and broadcasting. We should probably think some on how many tests there should be, as well as the distribution.

Thoughts?",pytorch
42352,zasdfgbnm,pr,2020-07-31T06:44:27Z,Remove VS2017 workaround for autocasting,"Because VS2017 is no longer supported after https://github.com/pytorch/pytorch/pull/42144
cc: @mcarilli ",pytorch
42398,gmagogsfm,pr,2020-08-01T01:19:46Z,Implement sort for string in aten,"Fixes #42375
",pytorch
42417,gmagogsfm,pr,2020-08-01T20:14:02Z,Fix check highlight in filecheck.,"* It originally failed to check for cases where highlight token appears more than once.
* Now it repeated tries to find highlight token if one doesn't seem correctly highlighted until end of error message.

",pytorch
42442,z-a-f,pr,2020-08-03T09:24:49Z,[quant] Quantization parameters in TensorIterator,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42443 [quant] quantized GLU and HardGLU
* #42728 HardGLU implementation
* #42727 Factoring out common code in the GLU
* **#42442 [quant] Quantization parameters in TensorIterator**

Differential Revision: [D23011803](https://our.internmc.facebook.com/intern/diff/D23011803)",pytorch
42443,z-a-f,pr,2020-08-03T09:24:57Z,[quant] quantized GLU and HardGLU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42443 [quant] quantized GLU and HardGLU**
* #42728 HardGLU implementation
* #42727 Factoring out common code in the GLU
* #42442 [quant] Quantization parameters in TensorIterator

Differential Revision: [D23011802](https://our.internmc.facebook.com/intern/diff/D23011802)",pytorch
42482,zasdfgbnm,pr,2020-08-03T20:27:00Z,[DO NOT MERGE] Test binary build for windows,Test for https://github.com/pytorch/builder/pull/463,pytorch
42615,zasdfgbnm,pr,2020-08-05T19:07:57Z,Fix cudnn version on build_environment of Windows CI,"
",pytorch
42623,gmagogsfm,pr,2020-08-05T19:39:36Z,Issue warning instead of error when parsing Enum while enum support is not enabled,"Returnning None rather than error matches previous behavior better.

Fixes https://fburl.com/yrrvtes3

",pytorch
42644,gmagogsfm,pr,2020-08-05T23:46:25Z,Support Enum as return value of scripted functions,"[4/N] Implement Enum JIT support

Add enum case in toPyObject
Register Enum classes in jit._state
Give test enum classes in test_enum.py distinct names so that don't clash in jit._state registration


Supported:
Enum-typed function arguments
using Enum type and comparing them
Support getting name/value attrs of enums
Using Enum value as constant
Support Enum-typed return values

TODO:
Support iterating through Enum class (enum value list)
Support serialization and deserialization",pytorch
42645,zasdfgbnm,pr,2020-08-06T00:06:52Z,Bump up NCCL to 2.7.6,Because 2.7.3 has some bug on GA100 which is fixed in 2.7.6,pytorch
42649,zasdfgbnm,pr,2020-08-06T01:23:07Z,CUDA reduction: allow outputs to have different strides,"Fixes https://github.com/pytorch/pytorch/issues/42364

Benchmark:
https://github.com/zasdfgbnm/things/blob/master/2020Q3/min-benchmark.ipynb
```python
import torch

print(torch.__version__)
print()

for i in range(100):
    torch.randn(1000, device='cuda')
    
for e in range(7, 15):
    N = 2 ** e
    input_ = torch.randn(N, N, device='cuda')
    torch.cuda.synchronize()
    %timeit input_.min(dim=0); torch.cuda.synchronize()
    input_ = torch.randn(N, N, device='cuda').t()
    torch.cuda.synchronize()
    %timeit input_.min(dim=0); torch.cuda.synchronize()
    print()
```
Before
```
1.7.0a0+5d7c3f9

21.7 µs ± 1.67 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.6 µs ± 773 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

22.5 µs ± 294 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.2 µs ± 250 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

26.4 µs ± 67 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.9 µs ± 316 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

33 µs ± 474 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
21.1 µs ± 218 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

84.2 µs ± 691 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
50.3 µs ± 105 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

181 µs ± 2.36 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
145 µs ± 149 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

542 µs ± 753 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
528 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

2.04 ms ± 9.74 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
2.01 ms ± 22.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
After
```
1.7.0a0+9911817

21.4 µs ± 695 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.6 µs ± 989 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

22.4 µs ± 153 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.5 µs ± 58.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

26.6 µs ± 147 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
20.9 µs ± 675 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

35.4 µs ± 560 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
21.7 µs ± 1.17 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)

86.5 µs ± 1.99 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
52.2 µs ± 1.57 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

195 µs ± 2.97 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
153 µs ± 4.46 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

550 µs ± 7.72 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
527 µs ± 3.04 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

2.05 ms ± 7.87 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
2 ms ± 4.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```",pytorch
42661,gmagogsfm,pr,2020-08-06T05:55:18Z,Support iterating through an Enum class,"[5/N] Implement Enum JIT support

Implement Enum class iteration
Add aten.ne for EnumType

Supported:
Enum-typed function arguments
using Enum type and comparing them
Support getting name/value attrs of enums
Using Enum value as constant
Support Enum-typed return values
Support iterating through Enum class (enum value list)

TODO:
Support serialization and deserialization",pytorch
42670,jeffdaily,pr,2020-08-06T15:58:05Z,pin numpy version to 1.18.5,"Using numpy 1.19.x instead of 1.18.x breaks certain unit tests.
Fixes #42561.  Likely also fixes #42583.

CC @ezyang @xw285cornell @sunway513 ",pytorch
42727,z-a-f,pr,2020-08-07T09:11:40Z,Factoring out common code in the GLU,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42443 [quant] quantized GLU and HardGLU
* #42728 HardGLU implementation
* **#42727 Factoring out common code in the GLU**
* #42442 [quant] Quantization parameters in TensorIterator

Differential Revision: [D23011804](https://our.internmc.facebook.com/intern/diff/D23011804)",pytorch
42728,z-a-f,pr,2020-08-07T09:11:48Z,HardGLU implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42443 [quant] quantized GLU and HardGLU
* **#42728 HardGLU implementation**
* #42727 Factoring out common code in the GLU
* #42442 [quant] Quantization parameters in TensorIterator

Differential Revision: [D23011805](https://our.internmc.facebook.com/intern/diff/D23011805)",pytorch
42758,z-a-f,pr,2020-08-07T22:03:40Z,[quant] Sorting the list of dispathes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42758 [quant] Sorting the list of dispathes**

Differential Revision: [D23011764](https://our.internmc.facebook.com/intern/diff/D23011764)",pytorch
42795,gmagogsfm,pr,2020-08-10T05:16:57Z,Enum serialization,"Fixes #{issue number}
",pytorch
42853,gmagogsfm,pr,2020-08-11T07:05:56Z,Fix incorrect aten::sorted.str return type,"aten::sorted.str output type was incorrectly set to bool[] due to a copy-paste error. This PR fixes it.

Fixes https://fburl.com/0rv8amz7
",pytorch
42872,gmagogsfm,pr,2020-08-11T19:14:01Z,Add Enum convert back to Python object support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42963 Add Enum TorchScript serialization and deserialization support
* #42874 Fix enum constant printing and add FileCheck to all Enum tests
* #42873 Add Enum convert back to Python object support
* **#42872 Add Enum convert back to Python object support**

",pytorch
42873,gmagogsfm,pr,2020-08-11T19:14:09Z,Add Enum convert back to Python object support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #42963 Add Enum TorchScript serialization and deserialization support
* #42874 Fix enum constant printing and add FileCheck to all Enum tests
* **#42873 Add Enum convert back to Python object support**
* #42872 Add Enum convert back to Python object support

",pytorch
42874,gmagogsfm,pr,2020-08-11T19:14:17Z,Fix enum constant printing and add FileCheck to all Enum tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43188 Enable Enum pickling/unpickling.
* #42963 Add Enum TorchScript serialization and deserialization support
* **#42874 Fix enum constant printing and add FileCheck to all Enum tests**
* #43121 Add Enum convert back to Python object support

Differential Revision: [D23222894](https://our.internmc.facebook.com/intern/diff/D23222894)",pytorch
42889,gmagogsfm,pr,2020-08-12T00:30:46Z,Improve sugared value's error message,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42889 Improve sugared value's error message**

I think most (if not all) cases where this code path is reached can be attributed to closing over a global variable.
Improving error message to make this clearer to users.

close #41288",pytorch
42904,z-a-f,pr,2020-08-12T07:48:02Z,[DO NOT MERGE] Fixing the BC breakage will be merged with the other PR,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* #40370 [quant] conv_transpose1d / conv_transpose2d
* #40360 [quant] conv_transpose1d_prepack / conv_transpose1d_unpack
* #40351 [quant] conv_transpose2d_prepack/conv_transpose2d_unpack
* #39714 [quant] Prep for conv_transpose packing
* **#42904 [DO NOT MERGE] Fixing the BC breakage will be merged with the other PR**
* #40003 Adding a version serialization type to ConvPackedParam

",pytorch
42941,z-a-f,pr,2020-08-12T20:19:17Z,[DO NOT MERGE] Trying to redefine the __setstate__ using def_unboxed,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#42941 [DO NOT MERGE] Trying to redefine the __setstate__ using def_unboxed**
* #40003 Adding a version serialization type to ConvPackedParam

",pytorch
42963,gmagogsfm,pr,2020-08-13T03:09:16Z,Add Enum TorchScript serialization and deserialization support,"* Adds code printing for enum type
* Enhance enum type to include all contained enum names and values
* Adds code parsing for enum type in deserialization
* Enabled serialization/deserialization test in most TestCases. (With a few dangling issues to be addressed in later PRs to avoid this PR grows too large)

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43188 Enable Enum pickling/unpickling.
* **#42963 Add Enum TorchScript serialization and deserialization support**
* #42874 Fix enum constant printing and add FileCheck to all Enum tests
* #43121 Add Enum convert back to Python object support

Differential Revision: [D23223281](https://our.internmc.facebook.com/intern/diff/D23223281)",pytorch
43074,zasdfgbnm,pr,2020-08-14T20:56:32Z,Compress fatbin to fit into 32bit indexing,"Fixes https://github.com/pytorch/pytorch/issues/39968

tested with `TORCH_CUDA_ARCH_LIST='3.5 5.2 6.0 6.1 7.0 7.5 8.0+PTX'`, before this PR, it was failing, and with this  PR, the build succeed.

With `TORCH_CUDA_ARCH_LIST='7.0 7.5 8.0+PTX'`, `libtorch_cuda.so` with symbols changes from 2.9GB -> 2.2GB

cc: @ptrblck @mcarilli @jjsjann123 ",pytorch
43092,zasdfgbnm,pr,2020-08-15T00:28:04Z,Add amax/amin,"Add a max/min operator that only return values.

## Some important decision to discuss
| **Question**                          | **Current State** |
|---------------------------------------|-------------------|
| Expose torch.max_values to python?    | No                |
| Remove max_values and only keep amax? | Yes               |
| Should amax support named tensors?    | Not in this PR    |

## Numpy compatibility

Reference: https://numpy.org/doc/stable/reference/generated/numpy.amax.html

| Parameter                                                                                                                                                                                                                                              | PyTorch Behavior                                                                  |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| `axis`:  None or int or tuple of ints, optional. Axis or axes along which to operate. By default, flattened input is used. If this is a tuple of ints, the maximum is selected over multiple axes, instead of a single axis or all the axes as before. | Named `dim`, behavior same as `torch.sum` (#29137)                                |
| `out`: ndarray, optional. Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.                                                                                                   | Same                                                                              |
| `keepdims`: bool, optional. If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.                                      | implemented as `keepdim`                                                          |
| `initial`: scalar, optional. The minimum value of an output element. Must be present to allow computation on empty slice.                                                                                                                              | Not implemented in this PR. Better to implement for all reductions in the future. |
| `where`: array_like of bool, optional. Elements to compare for the maximum.                                                                                                                                                                            | Not implemented in this PR. Better to implement for all reductions in the future. |

**Note from numpy:**
> NaN values are propagated, that is if at least one item is NaN, the corresponding max value will be NaN as well. To ignore NaN values (MATLAB behavior), please use nanmax.

PyTorch has the same behavior",pytorch
43093,ppwwyyxx,pr,2020-08-15T01:37:57Z,[jit] better error message,"Summary: without this it's hard to tell which module is going wrong

Test Plan:
```
> TypeError:
> 'numpy.int64' object in attribute 'Linear.in_features' is not a valid constant.
> Valid constants are:
> 1. a nn.ModuleList
> 2. a value of type {bool, float, int, str, NoneType, torch.device, torch.layout, torch.dtype}
> 3. a list or tuple of (2)
```

Differential Revision: D23148516

",pytorch
43121,gmagogsfm,pr,2020-08-16T05:22:53Z,Add Enum convert back to Python object support,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43188 Enable Enum pickling/unpickling.
* #42963 Add Enum TorchScript serialization and deserialization support
* #42874 Fix enum constant printing and add FileCheck to all Enum tests
* **#43121 Add Enum convert back to Python object support**

Differential Revision: [D23222628](https://our.internmc.facebook.com/intern/diff/D23222628)",pytorch
43183,z-a-f,pr,2020-08-18T01:50:28Z,[quant] Factoring out serialization versions,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43199 [quant] conv_transpose graph patterns
* #40371 [quant] ConvTranspose1d / ConvTranspose2d
* #40370 [quant] conv_transpose1d / conv_transpose2d
* #40360 [quant] conv_transpose1d_prepack / conv_transpose1d_unpack
* #40351 [quant] conv_transpose2d_prepack/conv_transpose2d_unpack
* #39714 [quant] Prep for conv_transpose packing
* **#43183 [quant] Factoring out serialization versions**

Differential Revision: [D23181621](https://our.internmc.facebook.com/intern/diff/D23181621)",pytorch
43188,gmagogsfm,pr,2020-08-18T05:06:03Z,Enable Enum pickling/unpickling.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43188 Enable Enum pickling/unpickling.**
* #42963 Add Enum TorchScript serialization and deserialization support
* #42874 Fix enum constant printing and add FileCheck to all Enum tests
* #43121 Add Enum convert back to Python object support

",pytorch
43199,z-a-f,pr,2020-08-18T09:50:25Z,[quant] conv_transpose graph patterns,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43199 [quant] conv_transpose graph patterns**
* #44844 [quant] Fix ConvTranspose mapping

Differential Revision: [D23228954](https://our.internmc.facebook.com/intern/diff/D23228954)",pytorch
43222,jjsjann123,pr,2020-08-18T20:27:59Z,patch enum class as hash key for gcc 5.4,"Fixes #43129 

where gcc doesn't support hash of enum class.",pytorch
43226,jeffdaily,pr,2020-08-18T21:33:44Z,remove thunk fix now that ROCm CI images are >= ROCm 3.5,"Also, relax BUILD_ENVIRONMENT exact match to rocm when installing pip packages for tests.

CC @ezyang @xw285cornell @sunway513 ",pytorch
43236,jeffdaily,pr,2020-08-18T23:43:14Z,update path in CI script to access ninja,"This relaxes the assumption that test.sh will be run in the CI environment by the CI user.

CC @ezyang @xw285cornell @sunway513 ",pytorch
43239,d4l3k,pr,2020-08-19T00:39:27Z,caffe2: expose CPUContext RandSeed for backwards compatibility with external RNG,"Summary:
This is an incremental step as part of the process to migrate caffe2 random number generator off of std::mt19937 and to instead use at::mt19937+at::CPUGeneratorImpl. The ATen variants are much more performant (10x faster).

This adds a way to get the CPUContext RandSeed for tail use cases that require a std::mt19937 and borrow the CPUContext one.

Test Plan: This isn't used anywhere within the caffe2 codebase. Compile should be sufficient.

Differential Revision: D23203280

",pytorch
43257,zasdfgbnm,pr,2020-08-19T07:04:02Z,Increase the memory requirement of test_reduction_split,This test is constantly failing on my 12GB GPU,pytorch
43303,z-a-f,pr,2020-08-19T23:40:38Z,[quant] fill_ path for quantized tensors,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43304 [quant] quantized path for ConstantPadNd
* **#43303 [quant] fill_ path for quantized tensors**

Differential Revision: [D23231947](https://our.internmc.facebook.com/intern/diff/D23231947)",pytorch
43304,z-a-f,pr,2020-08-19T23:40:45Z,[quant] quantized path for ConstantPadNd,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43304 [quant] quantized path for ConstantPadNd**
* #43303 [quant] fill_ path for quantized tensors

Differential Revision: [D23231946](https://our.internmc.facebook.com/intern/diff/D23231946)",pytorch
43305,jeffdaily,pr,2020-08-19T23:41:47Z,[ROCm] skip test_rpc in .jenkins/pytorch/test.sh,"#42636 added test_rpc, but this test binary is not built for ROCm.  Skip this test for ROCm builds.",pytorch
43343,jeffdaily,pr,2020-08-20T17:42:08Z,fix typo in test_dataloader test_multiprocessing_contexts,"#22990 added a multiprocessing_context argument to DataLoader, but a typo in the test causes the wrong DataLoader class to be used.",pytorch
43353,gmagogsfm,pr,2020-08-20T20:40:12Z,[DO NOT REVIEW/MERGE] import torch.classes before torch functional,"Fixes #{issue number}
",pytorch
43391,z-a-f,pr,2020-08-21T04:39:28Z,[quant] Fixing activation_post_process coming from a wrong fused batchnorm2d location,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43398 [quant][WIP] Fusion + tests + whole bunch of other things...
* #43393 [quant] BatchNorm1d
* #43392 [quant] Factored out common code in batchnorm.py
* **#43391 [quant] Fixing activation_post_process coming from a wrong fused batchnorm2d location**

Differential Revision: [D23301023](https://our.internmc.facebook.com/intern/diff/D23301023)",pytorch
43392,z-a-f,pr,2020-08-21T04:39:35Z,[quant] Factored out common code in batchnorm.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #43393 [quant] BatchNorm1d
* **#43392 [quant] Factored out common code in batchnorm.py**

Differential Revision: [D23301022](https://our.internmc.facebook.com/intern/diff/D23301022)",pytorch
43393,z-a-f,pr,2020-08-21T04:39:42Z,[quant] BatchNorm1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43393 [quant] BatchNorm1d**
* #43392 [quant] Factored out common code in batchnorm.py

Differential Revision: [D23301021](https://our.internmc.facebook.com/intern/diff/D23301021)",pytorch
43398,z-a-f,pr,2020-08-21T07:05:54Z,[quant][WIP] Fusion + tests + whole bunch of other things...,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43398 [quant][WIP] Fusion + tests + whole bunch of other things...**
* #43393 [quant] BatchNorm1d
* #43392 [quant] Factored out common code in batchnorm.py

This PR will be split into several, and is currently WIP.

Differential Revision: [D23301020](https://our.internmc.facebook.com/intern/diff/D23301020)",pytorch
43448,gmagogsfm,pr,2020-08-22T02:35:07Z,Implement sort for list of tuples,"* Implement tuple sort by traversing contained IValue types and generate a lambda function as comparator for sort.
* Tuple, class objects can now arbitrarily nest within each other and still be sortable

Fixes #43219
",pytorch
43460,gmagogsfm,pr,2020-08-23T00:21:23Z,Implement JIT Enum type serialization and deserialization,"[Re-review tips: nothing changed other than a type in python_ir.cpp to fix a windows build failure]

Adds code printing for enum type
Enhance enum type to include all contained enum names and values
Adds code parsing for enum type in deserialization
Enabled serialization/deserialization test in most TestCases. (With a few dangling issues to be addressed in later PRs to avoid this PR grows too large)

",pytorch
43519,zasdfgbnm,pr,2020-08-24T21:36:54Z,Evenly distribute output grad  into all matching inputs for min/max/median,"[bc-breaking note]. Previously, in case there were multiple max/min/median elements with the same value, gradient propagated only to the first element with this value, now gradient is evenly distributed between all the elements. This results in a minimum subnorm gradient. 
cc: @ngimel @mruberry ",pytorch
43566,zasdfgbnm,pr,2020-08-25T19:13:16Z,Skip SVD tests when no lapack,These tests are failing on one of my system that does not have lapack,pytorch
43583,zasdfgbnm,pr,2020-08-25T22:16:32Z,"Fix docs for kwargs, a-e","To reduce the chance of conflicts, not all ops are fixed. Ops starting with letter `f` will be fixed in separate PR.
",pytorch
43586,zasdfgbnm,pr,2020-08-25T22:48:48Z,"Fix docs for kwargs, f-p",,pytorch
43589,zasdfgbnm,pr,2020-08-25T23:28:10Z,"Fix docs for kwargs, q-z","
",pytorch
43681,z-a-f,pr,2020-08-27T01:19:11Z,Adding test to quantized copy for 'from float',"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#43681 Adding test to quantized copy for 'from float'**

Differential Revision: [D23364507](https://our.internmc.facebook.com/intern/diff/D23364507)",pytorch
43733,zasdfgbnm,pr,2020-08-27T19:47:55Z,Delete THCStream.cpp,"
",pytorch
43738,zasdfgbnm,pr,2020-08-27T20:06:16Z,Delete THCTensorMode.cu,"
",pytorch
43739,zasdfgbnm,pr,2020-08-27T20:09:03Z,Delete THCTensor.cu,"
",pytorch
43744,jeffdaily,pr,2020-08-27T21:14:39Z,Enable complex blas for ROCm.,"Revert ""Skips some complex tests on ROCm (#42759)"".  This reverts commit 55b1706775726418ddc5dd3b7756ea0388c0817c.

Use new cuda_to_hip_mappings.py from #43004.

Fixes https://github.com/pytorch/pytorch/pull/42383#issuecomment-670771922

CC @sunway513 ",pytorch
43794,zasdfgbnm,pr,2020-08-28T19:29:15Z,Add '--allow-run-as-root' to mpiexec to allow running distributed test inside a container,"Inside a container, the user is often root. We should allow this use case so that people can easily run `run_test.py` insider a container",pytorch
43797,zasdfgbnm,pr,2020-08-28T20:08:41Z,Use amax/maximum instead of max in optimizers,"
",pytorch
43808,zasdfgbnm,pr,2020-08-28T22:14:16Z,"Remove useless py2 compatibility import __future__, part 1","To avoid conflicts, this PR does not remove all imports. More are coming in further PRs.",pytorch
43819,zasdfgbnm,pr,2020-08-29T01:25:14Z,[resubmit] Add amax/amin,"**BC-breaking Note:**

This PR removes the min_values and max_values functions. These functions were undocumented and not accessible in Python.

**PR Summary:**

Resubmit for landing next week.",pytorch
43822,zasdfgbnm,pr,2020-08-29T02:28:40Z, Expand the coverage of test_blas_empty,"
",pytorch
43823,zasdfgbnm,pr,2020-08-29T03:38:58Z,Document the beta=0 behavior of BLAS functions,"
",pytorch
43827,zasdfgbnm,pr,2020-08-29T04:38:39Z,addmm/addmv should accept complex alpha and beta,,pytorch
43830,zasdfgbnm,pr,2020-08-29T07:56:47Z,Run function check and out check in TestTensorDeviceOps,,pytorch
43831,zasdfgbnm,pr,2020-08-29T08:32:23Z,Expand the coverage of test_addmm and test_addmm_sizes,"- This test is very fast and very important, so it makes no sense in marking it as slowTest
- This test is should also run on CUDA
- This test should check alpha and beta support
- This test should check `out=` support
- manual computation should use list instead of index_put because list is much faster
- precision for TF32 needs to be fixed. Will do it in future PR.",pytorch
43832,zasdfgbnm,pr,2020-08-29T08:50:34Z,Expand coverage of test_addmm_sizes,"
",pytorch
43835,zasdfgbnm,pr,2020-08-29T09:11:32Z,#include <string> in loopnest.h,This file is causing compiling failure on my gcc-10.1,pytorch
43841,zasdfgbnm,pr,2020-08-29T19:08:33Z,Fix bug in toComplexWithDefault,"I don't think this method is used anywhere, so I don't know how to test it. But the diff should justify itself.",pytorch
43842,zasdfgbnm,pr,2020-08-29T20:03:54Z,Fix THPVariable_float_scalar,"
",pytorch
43844,zasdfgbnm,pr,2020-08-29T20:30:59Z,Add __complex__,fixes https://github.com/pytorch/pytorch/issues/43833,pytorch
43936,gmagogsfm,pr,2020-09-01T05:06:12Z,Support capturing global value in TorchScript,"- Allows user to declare a global variable value as ""captured"" so that it is usable inside a torchscript function.
- Maintains a global singleton registry that keeps track of captured values at *runtime*. In other words, at compile time, no value is captured, just an Any-typed placeholder value.

A canonical usage example:

import torch

global_var = None 

def set_global_var_and_capture(v: str):
    global global_var
    global_var = v
    torch.jit.capture_global_constant_value(""global_var"")

def use_global_var():
    global global_var
    localized_var: str = str(global_var) #  <-- Assign to local var with cast to get actual type
    return localized_var

- Fixes #43397


",pytorch
43980,zasdfgbnm,pr,2020-09-01T20:21:30Z,"Further expand coverage of addmm/addmv, fix 0 stride","- test beta=0, self=nan
- test transposes
- fixes broadcasting of addmv
- not supporting tf32 yet, will do it in future PR together with other testing fixes",pytorch
43987,d4l3k,pr,2020-09-01T21:18:30Z,caffe2: use at::mt19937 instead of std::mt19937 (10x speedup),"Summary:
This replaces the caffe2 CPU random number (std::mt19937) with at::mt19937 which is the one currently used in pytorch. The ATen RNG is 10x faster than the std one and appears to be more robust given bugs in the std (https://fburl.com/diffusion/uhro7lqb)

For large embedding tables (10GB+) we see UniformFillOp taking upwards of 10 minutes as we're bottlenecked on the single threaded RNG. Swapping to at::mt19937 cuts that time to 10% of the current.

Test Plan:
buck run mode/opt //caffe2/caffe2/fb/async/comm/benchmarks:uniform_fill_benchmark -- --bm_min_iters 10 --bm_max_secs 5
  buck test //caffe2/caffe2:caffe2_test_cpu //caffe2/caffe2/python: //caffe2/caffe2/fb/operators:
  fbpkg build -E aml.dper2

the dper3 failures are fixed in the next diff D23376199

Differential Revision: D23219710

",pytorch
44041,robieta,pr,2020-09-02T17:51:42Z,Gh/taylorrobie/matmul optimize,"This applies two minor optimizations to native Conv2D.
1) Due to the `== 4` check [here](https://github.com/pytorch/pytorch/blob/70e00444bc687deeef280e2468653375c1546cfb/aten/src/ATen/native/ConvolutionMM2d.cpp#L157), slicing the input (and as a result reducing its dimensionality to three) was causing Im2Col to occur even for the 1x1 case where it is a no-op.
2) When `bias=False`, zeroing the input is wasteful as we can simply pass `beta=0` to the BLAS routine.

Benchmarks for this PR are fairly light; in both cases we're simply doing less work so performance should be strictly better. (Thus I didn't bother fuzzing.) To show a slightly different workflow, this script lets the user control gathering measurements rather than calling `subprocess`.

```
# benchmark_im2col_skip.py
import argparse
import os
import pickle

import torch
from torch.utils._benchmark import Timer, Compare

EXPECTED_ENVS = (""fbcode_warm"", ""branch"")
ENV = os.getenv(""CONDA_DEFAULT_ENV"")
assert ENV in EXPECTED_ENVS

RESULT_FILE_TEMPLATE = ""/tmp/im2col_skip_{env}.pkl""

REPEATS = 20
SIZES = [
    (64, 64, 1, 32),
    (512, 512, 1, 14),
    (1024, 1024, 1, 7),
    (3, 256, 1, 100),
    (256, 128, 8, 32),
]

def benchmark():
    torch._C._set_mkldnn_enabled(False)

    results = []
    for i, (c_in, c_out, b, size) in list(enumerate(SIZES)) * REPEATS:
        for kernel_size in [1, 3]:
            model = torch.nn.Conv2d(
                c_in, c_out, kernel_size=kernel_size, stride=1,
                padding=(0, 0), groups=1, bias=False
            )
            model.eval()
            timer = Timer(
                ""model(x)"",
                label=""Conv2D"",
                description=f""{kernel_size}x{kernel_size} [{i}]"",
                globals={
                    ""model"": model,
                    ""x"": torch.ones((b, c_in, size, size)),
                },
                env=ENV,
            )
            results.append(timer.blocked_autorange(min_run_time=0.5))

    with open(RESULT_FILE_TEMPLATE.format(env=ENV), ""wb"") as f:
        pickle.dump(results, f)


def process():
    results = []
    for env in EXPECTED_ENVS:
        with open(RESULT_FILE_TEMPLATE.format(env=env), ""rb"") as f:
            results.extend(pickle.load(f))
    results.sort(key=lambda r: r.description)

    cmp = Compare(results)
    cmp.trim_significant_figures()
    cmp.print()
    for i, (c_in, c_out, b, size) in enumerate(SIZES):
        print(f""[{i}]  c_in: {c_in:>4}{'':>8}c_out: {c_out:>4}{'':>8}batch: {b}{'':>8}Height/width: {size:>4}"")


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--process"", action=""store_true"")
    args = parser.parse_args()
    if args.process:
        process()
    else:
        benchmark()
```

```
$ conda deactivate && source activate branch && python examples/benchmark_im2col_skip.py
$ conda deactivate && source activate fbcode_warm && python examples/benchmark_im2col_skip.py
$ python examples/benchmark_im2col_skip.py --process
```

```
[-------------------------------------------------------------------- Conv2D -------------------------------------------------------------------]
                           |  1x1 [0]  |  1x1 [1]  |  1x1 [2]  |  1x1 [3]  |  1x1 [4]  |  3x3 [0]  |  3x3 [1]  |  3x3 [2]  |  3x3 [3]  |  3x3 [4]
1 threads: --------------------------------------------------------------------------------------------------------------------------------------
  (branch)       model(x)  |    229    |    1540   |    1950   |     994   |    8000   |    1210   |   10000   |   12000   |    3000   |   110000
  (fbcode_warm)  model(x)  |    275    |    1620   |    2000   |    1430   |    9700   |    1200   |   11000   |   12300   |    3650   |   110000

Times are in microseconds (us).


[0]  c_in:   64        c_out:   64        batch: 1        Height/width:   32
[1]  c_in:  512        c_out:  512        batch: 1        Height/width:   14
[2]  c_in: 1024        c_out: 1024        batch: 1        Height/width:    7
[3]  c_in:    3        c_out:  256        batch: 1        Height/width:  100
[4]  c_in:  256        c_out:  128        batch: 8        Height/width:   32
```

The effect, of course, varies with arithmetic intensity but improvement is non-trivial even when the number of channels is large.

TEST_PLAN: attached benchmark.",pytorch
44240,zasdfgbnm,pr,2020-09-05T02:11:33Z,Adjust TF32 tests,"- The thresholds of some tests are bumped up. Depending on the random generator, sometimes these tests fail with things like 0.0059 is not smaller than 0.005. I ran `test_nn.py` and `test_torch.py` for 10+ times to check these are no longer flaky.
- Add `@tf32_on_and_off` to new `matrix_exp` tests.
- Disable TF32 on test suites other than `test_nn.py` and `test_torch.py`

cc: @ptrblck ",pytorch
44243,gmagogsfm,pr,2020-09-05T05:45:50Z,Remove EXPERIMENTAL_ENUM_SUPPORT feature guard,"Fixes #41095
",pytorch
44278,Flamefire,pr,2020-09-07T10:42:09Z,Remove pybind11 from required submodules,"This can be taken from the system in which case it is not used from the submodule. Hence the check here limits the usage unnecessarily

ccing @malfet ",pytorch
44335,gmagogsfm,pr,2020-09-08T22:12:14Z,Support Python Slice class in TorchScript,"Implements support for[ Python Slice class](https://docs.python.org/3/c-api/slice.html) (not slice expression, which is already supported)

Slice object can be used in any place that supports slice expression, including multi-dim tensor slicing.

Fixes #43511
Fixes #43125
",pytorch
44369,Flamefire,pr,2020-09-09T06:47:46Z,Choose test affinity based on current affinity,"This avoids test failures in cgroup environments

Fixes #44368
",pytorch
44374,zasdfgbnm,pr,2020-09-09T07:56:07Z,Add support for NCCL alltoall,"In #42514, NCCL `alltoall_single` is already added. This PR adds NCCL `alltoall`.

The difference between `alltoall_single` and `alltoall` is: `alltoall_single`  works on a single tensor and send/receive slices of that tensor, while `alltoall` works on a list of tensor, and send/receive tensors in that list.

cc: @ptrblck @ngimel ",pytorch
44431,jeffdaily,pr,2020-09-09T22:15:17Z,[ROCm] fix cub hipify mappings,Fixes ROCm-specific workarounds introduced by #44259.  This adds new hipify mappings that properly handle cub outside of caffe2 sources.,pytorch
44474,zasdfgbnm,pr,2020-09-10T15:50:01Z,[DO NOT MERGE] debug windows cuda10.1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44478 [DO NOT MERGE] debug windows cuda10.1
* #44477 [DO NOT MERGE] debug windows cuda10.1
* #44476 [DO NOT MERGE] debug windows cuda10.1
* #44475 [DO NOT MERGE] debug windows cuda10.1
* **#44474 [DO NOT MERGE] debug windows cuda10.1**

",pytorch
44475,zasdfgbnm,pr,2020-09-10T15:50:10Z,[DO NOT MERGE] debug windows cuda10.1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44478 [DO NOT MERGE] debug windows cuda10.1
* #44477 [DO NOT MERGE] debug windows cuda10.1
* #44476 [DO NOT MERGE] debug windows cuda10.1
* **#44475 [DO NOT MERGE] debug windows cuda10.1**
* #44474 [DO NOT MERGE] debug windows cuda10.1

",pytorch
44476,zasdfgbnm,pr,2020-09-10T15:50:23Z,[DO NOT MERGE] debug windows cuda10.1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44478 [DO NOT MERGE] debug windows cuda10.1
* #44477 [DO NOT MERGE] debug windows cuda10.1
* **#44476 [DO NOT MERGE] debug windows cuda10.1**
* #44475 [DO NOT MERGE] debug windows cuda10.1
* #44474 [DO NOT MERGE] debug windows cuda10.1

",pytorch
44477,zasdfgbnm,pr,2020-09-10T15:50:30Z,[DO NOT MERGE] debug windows cuda10.1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #44478 [DO NOT MERGE] debug windows cuda10.1
* **#44477 [DO NOT MERGE] debug windows cuda10.1**
* #44476 [DO NOT MERGE] debug windows cuda10.1
* #44475 [DO NOT MERGE] debug windows cuda10.1
* #44474 [DO NOT MERGE] debug windows cuda10.1

",pytorch
44478,zasdfgbnm,pr,2020-09-10T15:50:39Z,[DO NOT MERGE] debug windows cuda10.1,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44478 [DO NOT MERGE] debug windows cuda10.1**
* #44477 [DO NOT MERGE] debug windows cuda10.1
* #44476 [DO NOT MERGE] debug windows cuda10.1
* #44475 [DO NOT MERGE] debug windows cuda10.1
* #44474 [DO NOT MERGE] debug windows cuda10.1

",pytorch
44512,z-a-f,pr,2020-09-10T22:14:38Z,removing conv filters from conv pattern matching,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44512 removing conv filters from conv pattern matching**

Differential Revision: [D23637409](https://our.internmc.facebook.com/intern/diff/D23637409)",pytorch
44513,z-a-f,pr,2020-09-10T22:21:31Z,[quant] Fixing the output shape for the linear,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44513 [quant] Fixing the output shape for the linear**

Differential Revision: [D23637508](https://our.internmc.facebook.com/intern/diff/D23637508)",pytorch
44553,jeffdaily,pr,2020-09-11T15:27:34Z,[ROCm] remove thrust workaround in ScanKernels,Remove ROCm workaround added in #39180.,pytorch
44579,zasdfgbnm,pr,2020-09-12T00:06:03Z,Cleanup workarounds for compiler bug of ROCm,,pytorch
44580,zasdfgbnm,pr,2020-09-12T00:30:51Z,Merge loops on GPU for ROCm and CUDA,"`ROCmLoops.cuh` is an older version of `CUDALoops`. In the past, there was only a single `Loops.cuh`, but when I was working on the vectorization of GPU loops, I have to create these two files due to two reasons:
1. ROCm was unable to handle complex well
2. ROCm had huge perf regression on the new kernel as reported by @iotamudelta 

Now 1 is resolved, I am not sure about 2, so @jeffdaily please check the perf of this PR on ROCm. If you like, you can reuse the previous script that I used for benchmarking: https://github.com/zasdfgbnm/things/blob/master/2020Q1/benchmark-unroll.ipynb

cc: @ngimel ",pytorch
44648,zasdfgbnm,pr,2020-09-14T18:33:04Z,Try fixing lda_cond for CPU addmv,"
",pytorch
44690,zasdfgbnm,pr,2020-09-15T05:04:07Z,Mention TF32 on related docs ,"cc: @ptrblck 

![image](https://user-images.githubusercontent.com/1032377/93168022-cbbfcb80-f6d6-11ea-8f6e-f2c8a15c5bea.png)
",pytorch
44717,robieta,pr,2020-09-15T15:54:58Z,Add callgrind collection to Timer,"This PR allows Timer to collect deterministic instruction counts for (some) snippets. Because of the intrusive nature of Valgrind (effectively replacing the CPU with an emulated one) we have to perform our measurements in a separate process. This PR writes a `.py` file containing the Timer's `setup` and `stmt`, and executes it within a `valgrind` subprocess along with a plethora of checks and error handling. There is still a bit of jitter around the edges due to the Python glue that I'm using, but the PyTorch signal is quite good and thus this provides a low friction way of getting signal. I considered using JIT as an alternative, but:

A) Python specific overheads (e.g. parsing) are important
B) JIT might do rewrites which would complicate measurement.

Consider the following bit of code, related to https://github.com/pytorch/pytorch/issues/44484:
```
from torch.utils._benchmark import Timer
counts = Timer(
    ""x.backward()"",
    setup=""x = torch.ones((1,)) + torch.ones((1,), requires_grad=True)""
).collect_callgrind()

for c, fn in counts[:20]:
    print(f""{c:>12}  {fn}"")
```

```
      812800  ???:_dl_update_slotinfo
      355600  ???:update_get_addr
      308300  work/Python/ceval.c:_PyEval_EvalFrameDefault'2
      304800  ???:__tls_get_addr
      196059  ???:_int_free
      152400  ???:__tls_get_addr_slow
      138400  build/../c10/core/ScalarType.h:c10::typeMetaToScalarType(caffe2::TypeMeta)
      126526  work/Objects/dictobject.c:_PyDict_LoadGlobal
      114268  ???:malloc
      101400  work/Objects/unicodeobject.c:PyUnicode_FromFormatV
       85900  work/Python/ceval.c:_PyEval_EvalFrameDefault
       79946  work/Objects/typeobject.c:_PyType_Lookup
       72000  build/../c10/core/Device.h:c10::Device::validate()
       70000  /usr/include/c++/8/bits/stl_vector.h:std::vector<at::Tensor, std::allocator<at::Tensor> >::~vector()
       66400  work/Objects/object.c:_PyObject_GenericGetAttrWithDict
       63000  ???:pthread_mutex_lock
       61200  work/Objects/dictobject.c:PyDict_GetItem
       59800  ???:free
       58400  work/Objects/tupleobject.c:tupledealloc
       56707  work/Objects/dictobject.c:lookdict_unicode_nodummy
```

Moreover, if we backport this PR to 1.6 (just copy the `_benchmarks` folder) and load those counts as `counts_1_6`, then we can easily diff them:
```
print(f""Head instructions: {sum(c for c, _ in counts)}"")
print(f""1.6 instructions:  {sum(c for c, _ in counts_1_6)}"")
count_dict = {fn: c for c, fn in counts}
for c, fn in counts_1_6:
    _ = count_dict.setdefault(fn, 0)
    count_dict[fn] -= c
count_diffs = sorted([(c, fn) for fn, c in count_dict.items()], reverse=True)
for c, fn in count_diffs[:15] + [["""", ""...""]] + count_diffs[-15:]:
    print(f""{c:>8}  {fn}"")
```

```
Head instructions: 7609547
1.6 instructions:  6059648
  169600  ???:_dl_update_slotinfo
  101400  work/Objects/unicodeobject.c:PyUnicode_FromFormatV
   74200  ???:update_get_addr
   63600  ???:__tls_get_addr
   46800  work/Python/ceval.c:_PyEval_EvalFrameDefault
   33512  work/Objects/dictobject.c:_PyDict_LoadGlobal
   31800  ???:__tls_get_addr_slow
   31700  build/../aten/src/ATen/record_function.cpp:at::RecordFunction::RecordFunction(at::RecordScope)
   28300  build/../torch/csrc/utils/python_arg_parser.cpp:torch::FunctionSignature::parse(_object*, _object*, _object*, _object**, bool)
   27800  work/Objects/object.c:_PyObject_GenericGetAttrWithDict
   27401  work/Objects/dictobject.c:lookdict_unicode_nodummy
   24115  work/Objects/typeobject.c:_PyType_Lookup
   24080  ???:_int_free
   21700  work/Objects/dictobject.c:PyDict_GetItemWithError
   20700  work/Objects/dictobject.c:PyDict_GetItem
          ...
   -3200  build/../c10/util/SmallVector.h:at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool)
   -3400  build/../aten/src/ATen/native/TensorIterator.cpp:at::TensorIterator::resize_outputs(at::TensorIteratorConfig const&)
   -3500  /usr/include/c++/8/x86_64-redhat-linux/bits/gthr-default.h:std::unique_lock<std::mutex>::unlock()
   -3700  build/../torch/csrc/utils/python_arg_parser.cpp:torch::PythonArgParser::raw_parse(_object*, _object*, _object**)
   -4207  work/Objects/obmalloc.c:PyMem_Calloc
   -4500  /usr/include/c++/8/bits/stl_vector.h:std::vector<at::Tensor, std::allocator<at::Tensor> >::~vector()
   -4800  build/../torch/csrc/autograd/generated/VariableType_2.cpp:torch::autograd::VariableType::add__Tensor(at::Tensor&, at::Tensor const&, c10::Scalar)
   -5000  build/../c10/core/impl/LocalDispatchKeySet.cpp:c10::impl::ExcludeDispatchKeyGuard::ExcludeDispatchKeyGuard(c10::DispatchKey)
   -5300  work/Objects/listobject.c:PyList_New
   -5400  build/../torch/csrc/utils/python_arg_parser.cpp:torch::FunctionParameter::check(_object*, std::vector<pybind11::handle, std::allocator<pybind11::handle> >&)
   -5600  /usr/include/c++/8/bits/std_mutex.h:std::unique_lock<std::mutex>::unlock()
   -6231  work/Objects/obmalloc.c:PyMem_Free
   -6300  work/Objects/listobject.c:list_repeat
  -11200  work/Objects/listobject.c:list_dealloc
  -28900  build/../torch/csrc/utils/python_arg_parser.cpp:torch::FunctionSignature::parse(_object*, _object*, _object**, bool)
```

Remaining TODOs:
  * Include a timer in the generated script for cuda sync.
  * Add valgrind to CircleCI machines and add a unit test.",pytorch
44735,zasdfgbnm,pr,2020-09-15T20:59:16Z,Remove py2 compatible future imports,,pytorch
44748,zasdfgbnm,pr,2020-09-15T22:20:13Z,CUDA bfloat compare ops,"
",pytorch
44750,zasdfgbnm,pr,2020-09-15T22:36:20Z,CUDA BFloat16 i0 support,"
",pytorch
44755,zasdfgbnm,pr,2020-09-15T23:03:46Z,CUDA BFloat16 TopK,,pytorch
44758,zasdfgbnm,pr,2020-09-15T23:28:48Z,"CUDA BFloat div, addcdiv, addcmul, mean, var",,pytorch
44760,zasdfgbnm,pr,2020-09-15T23:35:32Z,CUDA BFloat16 pow,"
",pytorch
44769,gmagogsfm,pr,2020-09-16T02:19:54Z,Fix misuse of PyObject_IsSubclass,"PyObject_IsSubclass may set python live exception bit if given object is not a class. `IsNamedTuple` is currently using it incorrectly, which may trip all following python operations in debug-build python. Normal release-build python is not affected because `assert` is no-op in release-build.

Fixes #43577",pytorch
44804,zasdfgbnm,pr,2020-09-16T18:52:25Z,CUDA BFloat16 and other improvements on abs,"Not sure if ROCm supports `std::abs` today, let's see the CI",pytorch
44813,zasdfgbnm,pr,2020-09-16T20:23:05Z,CUDA BFloat16 unary ops part 1,,pytorch
44824,zasdfgbnm,pr,2020-09-16T21:01:19Z,CUDA BFloat16 unary ops part 2,,pytorch
44834,zasdfgbnm,pr,2020-09-16T22:10:35Z,CUDA BFloat activations 1,,pytorch
44836,zasdfgbnm,pr,2020-09-16T22:23:25Z,CUDA BFloat Pooling,"
",pytorch
44837,zasdfgbnm,pr,2020-09-16T22:26:34Z,CUDA BFloat softmax,,pytorch
44844,z-a-f,pr,2020-09-16T22:57:05Z,[quant] Fix ConvTranspose mapping,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45078 [quant] conv_transpose graph patterns
* **#44844 [quant] Fix ConvTranspose mapping**

Differential Revision: [D23746466](https://our.internmc.facebook.com/intern/diff/D23746466)",pytorch
44847,z-a-f,pr,2020-09-16T23:15:18Z,[quant] Refactoring the mappings files,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#44847 [quant] Refactoring the mappings files**

Differential Revision: [D23747007](https://our.internmc.facebook.com/intern/diff/D23747007)",pytorch
44848,zasdfgbnm,pr,2020-09-16T23:29:04Z,CUDA BFloat embedding,"
",pytorch
44870,ppwwyyxx,pr,2020-09-17T08:09:46Z,[jit] stop parsing the block after seeing exit statements,"Summary: fix https://github.com/pytorch/pytorch/issues/44864

Test Plan: buck test mode/dev-nosan //caffe2/test:jit -- 'test_assert_is_script'

Differential Revision: D23755094

",pytorch
44891,gmagogsfm,pr,2020-09-17T17:28:58Z,Fix incorrect EnumValue serialization issue,"Previously, `prim::EnumValue` is serialized to `ops.prim.EnumValue`, which doesn't have the right implementation to refine return type. This diff correctly serializes it to enum.value, thus fixing the issue.

Fixes #44892
",pytorch
44918,zasdfgbnm,pr,2020-09-17T23:24:49Z,Enable bfloat16 random kernels on Windows,Fixes https://github.com/pytorch/pytorch/issues/33793,pytorch
44925,zasdfgbnm,pr,2020-09-18T01:21:09Z,CUDA BFloat16 infrastructure,"
",pytorch
44986,zasdfgbnm,pr,2020-09-18T21:50:59Z,"CUDA BFloat16 addmm, addmv",This PR was originally authored by @slayton58. I steal his implementation and added some tests.,pytorch
44994,zasdfgbnm,pr,2020-09-18T22:54:35Z,CUDA BFloat16 batchnorm (non-cuDNN),"
",pytorch
44997,zasdfgbnm,pr,2020-09-18T23:35:20Z,"CUDA BFloat16 gelu, hardswish, hardsigmoid","
",pytorch
44999,ppwwyyxx,pr,2020-09-18T23:46:14Z,[jit/detectron2] support @jit.ignore + properties,"Summary:
This way it should be backward-compatible (tested on PyTorch 1.4-6 at
https://app.circleci.com/pipelines/github/facebookresearch/detectron2/890/workflows/c843a73a-9ba1-4b3d-91dd-0a98ff82db22)

Test Plan: buck test mode/dev-nosan //vision/fair/detectron2/tests:test_anchor_generator

Differential Revision: D23797598

",pytorch
45002,zasdfgbnm,pr,2020-09-19T00:27:30Z,CUDA BFloat16 layernorm,"
",pytorch
45005,zasdfgbnm,pr,2020-09-19T00:33:35Z,CUDA BFloat16 Dropout,"
",pytorch
45007,zasdfgbnm,pr,2020-09-19T00:56:47Z,CUDA BFloat Conv (non-cuDNN),"
",pytorch
45011,zasdfgbnm,pr,2020-09-19T02:51:16Z,CUDA BFloat16 losses,"
",pytorch
45070,jeffdaily,pr,2020-09-21T16:48:51Z,IFU-master-2020-09-21,IFU-master-2020-09-21,pytorch
45078,z-a-f,pr,2020-09-21T19:12:30Z,[quant] conv_transpose graph patterns,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45078 [quant] conv_transpose graph patterns**

Differential Revision: [D23821580](https://our.internmc.facebook.com/intern/diff/D23821580)",pytorch
45081,z-a-f,pr,2020-09-21T19:43:02Z,[quant] ConvTranspose warnings,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45081 [quant] ConvTranspose warnings**

Differential Revision: [D23822449](https://our.internmc.facebook.com/intern/diff/D23822449)",pytorch
45087,zasdfgbnm,pr,2020-09-21T20:49:34Z,"Don't build cuDNN on HIP, don't build miopen on CUDA","Fixes #{issue number}
",pytorch
45097,jeffdaily,pr,2020-09-21T23:50:41Z,install ATen/native/cuda and hip headers,"The ATen/native/cuda headers were copied to torch/include, but then not included in the final package.  Further, add ATen/native/hip headers to the installation, as well.",pytorch
45132,jeffdaily,pr,2020-09-22T14:51:21Z,gitignore generated *.hip files and hip/ directories,Add *.hip files and hip/ directories to gitignore list.  Intentionally added to the `BEGIN NOT-CLEAN-FILES` section to avoid deleting the files when rebuilding.,pytorch
45151,zasdfgbnm,pr,2020-09-22T18:26:28Z,CUDA BFloat16 pooling,"
",pytorch
45152,z-a-f,pr,2020-09-22T18:26:33Z,[quant] torch.max_pool1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45175 [quant] Adding max_pool1d deps
* **#45152 [quant] torch.max_pool1d**

Differential Revision: [D23846473](https://our.internmc.facebook.com/intern/diff/D23846473)",pytorch
45155,zasdfgbnm,pr,2020-09-22T18:35:24Z,CUDA BFloat16 signal windows,Looks like this op is never tested for the support of different dtypes?,pytorch
45167,zasdfgbnm,pr,2020-09-22T21:35:56Z,CUDA BFloat16 batched gemm,"
",pytorch
45169,gmagogsfm,pr,2020-09-22T22:24:47Z,Exception ptr,Experiment torchscripting exception ptr,pytorch
45173,robieta,pr,2020-09-22T23:09:10Z,replace timer test with a mocked variant,"I noticed that the recently introduced adaptive_autorange tests occasionally timeout CI, and I've been meaning to improve the Timer tests for a while. This PR allows unit tests to swap the measurement portion of `Timer` with a deterministic mock so we can thoroughly test behavior without having to worry about flaky CI measurements. It also means that the tests can be much more detailed and still finish very quickly.

Test Plan: You're lookin' at it.",pytorch
45175,z-a-f,pr,2020-09-22T23:44:59Z,[quant] Adding max_pool1d deps,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45175 [quant] Adding max_pool1d deps**
* #45152 [quant] torch.max_pool1d

Differential Revision: [D23856566](https://our.internmc.facebook.com/intern/diff/D23856566)",pytorch
45218,jjsjann123,pr,2020-09-23T19:04:41Z,[nvFuser] Latency improvements for pointwise + reduction fusion,"A lot of changes are in this update, some highlights:

- Added Doxygen config file
- Split the fusion IR (higher level TE like IR) from kernel IR (lower level CUDA like IR)
- Improved latency with dynamic shape handling for the fusion logic
- Prevent recompilation for pointwise + reduction fusions when not needed
- Improvements to inner dimension reduction performance
- Added input -> kernel + kernel launch parameters cache, added eviction policy
- Added reduction fusions with multiple outputs (still single reduction stage)
- Fixed code generation bugs for symbolic tiled GEMM example
- Added thread predicates to prevent shared memory form being loaded multiple times
- Improved sync threads placements with shared memory and removed read before write race
- Fixes to FP16 reduction fusions where output would come back as FP32
",pytorch
45222,jeffdaily,pr,2020-09-23T19:30:58Z,add rocm 3.8 to nightly builds,Corresponding change in builder repo: https://github.com/pytorch/builder/pull/528.,pytorch
45240,zasdfgbnm,pr,2020-09-23T23:19:54Z,CUDA BFloat16 neg,"Fixes #{issue number}
",pytorch
45243,jeffdaily,pr,2020-09-23T23:36:12Z,IFU-master-2020-09-21,Relanding IFU master.,pytorch
45244,zasdfgbnm,pr,2020-09-23T23:43:49Z,BFloat16 support for torch.sign,Added BF16 support for torch.sign on CUDA,pytorch
45247,zasdfgbnm,pr,2020-09-24T00:16:25Z,"CUDA BFloat16 support of clamp, remainder, lshift, rshift","Add CUDA BFloat16 support of clamp, remainder, lshift, rshift
",pytorch
45297,d4l3k,pr,2020-09-24T20:05:19Z,caffe2/plan_executor: wait for 1 minute after exception and then abort,"Summary: If we have two concurrent substeps and one of them throws an exception and the other is blocking, we'll currently hang. This waits up to 1 minute for it to complete before terminating the process.

Test Plan: buck test caffe2/caffe2:caffe2_test_cpu -- PlanExecutorTest --stress-runs 100

Differential Revision: D20850851

",pytorch
45361,robieta,pr,2020-09-25T21:47:23Z,Gh/taylorrobie/timer cleanup,"This PR cleans up some of the rough edges around `Timer` and `Compare`
* Moves `Measurement` to be dataclass based
* Adds a bunch of type annotations. MyPy is now happy.
* Allows missing entries in `Compare`. This is one of the biggest usability issues with `Compare` right now, both from an API perspective and because the current failure mode is really unpleasant.
* Greatly expands the testing of `Compare`

Test plan: Changes to Timer are covered under existing tests, changes to `Compare` are covered by the expanded `test_compare` method.",pytorch
45382,gmagogsfm,pr,2020-09-26T22:16:23Z,Make sure each warnings.warn only executes once inside TorchScript.,"* Add a pass at end of runCleanupPasses to annotate `aten::warn` so that each has its unique id
* Enhanced interpreter so that it tracks which `aten::warn` has been executed before and skip them
* Improved insertInstruction so that it correctly checks for overflow

Fixes #45108
",pytorch
45478,gmagogsfm,pr,2020-09-29T02:22:36Z,Warn once node,"Fixes #{issue number}
",pytorch
45489,zasdfgbnm,pr,2020-09-29T06:35:27Z,CUDA RTX30 series support,I also opened a PR on cmake upstream: https://gitlab.kitware.com/cmake/cmake/-/merge_requests/5292,pytorch
45491,zasdfgbnm,pr,2020-09-29T07:12:37Z,[TESTING ONLY] OSS NCCL alltoall,"Fixes #{issue number}
",pytorch
45492,zasdfgbnm,pr,2020-09-29T07:23:09Z,Fix cholesky TF32 tests,"This test is changed one day before the landing of the tf32 tests PR, therefore the fix for this is not included in that PR.",pytorch
45511,robieta,pr,2020-09-29T17:08:35Z,Move torch.utils._benchmark to torch.utils.benchmark.,"Graduating namespace benchmark utils. Yay!

Test plan: Existing unit tests + search for occurrences of `_benchmark`.",pytorch
45533,z-a-f,pr,2020-09-29T21:37:04Z,[quant] torch.mean add path for unsupported QNNPACK modes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45533 [quant] torch.mean add path for unsupported QNNPACK modes**

Differential Revision: [D24030446](https://our.internmc.facebook.com/intern/diff/D24030446)",pytorch
45547,gmagogsfm,pr,2020-09-30T00:29:16Z,[Experimental][Partial] New implementation for torch.distributed APIs in C++,"This is an attempt at refactoring `torch.distributed` implementation. Goal is to push Python layer's global states (like _default_pg) to C++ layer such that `torch.distributed` becomes more TorchScript friendly.

This PR adds the skeleton of C++ implementation, at the moment it is not included in any build (and won't be until method implementations are filled in). If you see any test failures related, feel free to revert.
",pytorch
45582,jeffdaily,pr,2020-09-30T17:38:22Z,"passing all arguments to sccache wrapper script should be quoted as ""$@""",This fixes MIOpen runtime compilation since it passes quoted arguments to the clang compiler.  This change also makes the sccache wrapper scripts consistent with the nvcc wrapper.,pytorch
45586,robieta,pr,2020-09-30T18:23:44Z,Re-land: Add callgrind collection to Timer #44717,Test plan: The unit test has been softened to be less platform sensitive.,pytorch
45601,robieta,pr,2020-09-30T20:19:06Z,Cherrypick callgrind Timer into 1.7,"Cherry pick: `9b27e0926b9025d91c1df50bb6575a801a31f299`, as well as the test fix in https://github.com/pytorch/pytorch/pull/45586",pytorch
45688,z-a-f,pr,2020-10-01T19:20:06Z,[quant] slow path for the FBGEMM conv_transpose,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45688 [quant] slow path for the FBGEMM conv_transpose**
* #45913 [quant] Add check for asan and qnnpack in the conv tests

Differential Revision: [D24059960](https://our.internmc.facebook.com/intern/diff/D24059960)",pytorch
45706,jjsjann123,pr,2020-10-02T00:05:59Z,[nvFuser] Switching to `CudaFusionGuard` from `BailOut` for nvfuser,"1. Added `CudaFusionGuard` as the custom TypeCheck for nvfuser; enabled dynamic shape support with profiling executor.
2. dropped support for legacy fuser
3. re-enabled nvfuser tests.",pytorch
45730,jjsjann123,pr,2020-10-02T10:11:16Z,[DO NOT REVIEW] Smoke test pr to probe CI failure,"Fixes #{issue number}
",pytorch
45768,gmagogsfm,pr,2020-10-02T20:24:19Z,Add all remaining method declarations from torch.distributed Python API to C++,"
Also ran formatter on previous sections",pytorch
45811,jjsjann123,pr,2020-10-03T21:38:21Z,[DO NOT REVIEW] Smoke test pr to probe CI failure in parallel,"Fixes #{issue number}
",pytorch
45828,ppwwyyxx,pr,2020-10-05T03:20:31Z,[jit] support tracing tensor __setitem__ with dynamic shape,"Summary: fix https://github.com/pytorch/pytorch/issues/43548

Test Plan: buck test mode/dev-nosan //caffe2/test:jit -- 'test_trace_slice' --jobs 1

Differential Revision: D24106641

",pytorch
45913,z-a-f,pr,2020-10-06T18:16:53Z,[quant] Add check for asan and qnnpack in the conv tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #45688 [quant] slow path for the FBGEMM conv_transpose
* **#45913 [quant] Add check for asan and qnnpack in the conv tests**

Differential Revision: [D24142132](https://our.internmc.facebook.com/intern/diff/D24142132)",pytorch
45981,d4l3k,pr,2020-10-07T17:49:40Z,Recommit: caffe2/plan_executor: wait for 1 minute after exception and then abort,"Summary:
This is a recommit of previously reverted D20850851 (https://github.com/pytorch/pytorch/commit/3fbddb92b1be1f70edced886745116b8daeebb17).

TL;DR - combining condition_variables and atomics is a bad idea

https://stackoverflow.com/questions/49622713/c17-atomics-and-condition-variable-deadlock

Test Plan:
buck test mode/opt //caffe2/caffe2/python:hypothesis_test -- --stress-runs 1000 test_atomic_iter_with_concurrent_steps --timeout 120
  buck test mode/opt //caffe2/caffe2/python:hypothesis_test -- --stress-runs 100
  buck test mode/opt caffe2/caffe2:caffe2_test_cpu -- PlanExecutorTest --stress-runs 100

no timeouts https://www.internalfb.com/intern/testinfra/testconsole/testrun/7036874440059883/

will ensure no timeouts in OSS

Differential Revision: D24165505

",pytorch
46001,zasdfgbnm,pr,2020-10-07T22:31:05Z,Workaround for cublas bug for 45724,"Fixes #45724
",pytorch
46023,robieta,pr,2020-10-08T07:36:39Z,More Timer refinement,"This PR just adds more polish to the benchmark utils:

1) `common.py`, `timer.py`, and `valgrind_wrapper/timer_interface.py` are now MyPy strict compliant. (except for three violations due to external deps.) Compare and Fuzzer will be covered in a future PR.
2) `CallgrindStats` now uses `TaskSpec` rather than accepting the individual fields which brings it closer to `Measurement`.
3) Some `__repr__` logic has been moved into `TaskSpec` (which `Measurement` and `CallgrindStats` use in their own `__repr__`s) for a more unified feel and less horrible f-string hacking, and the repr's have been given a cleanup pass.
4) `Tuple[FunctionCount, ...]` has been formalized as the `FunctionCounts` class, which has a much nicer `__repr__` than just the raw tuple, as well as some convenience methods (`__add__`, `__sub__`, `filter`, `transform`) for easier DIY stat exploration. (I find myself using the latter two a lot now.) My personal experience is that manipulating `FunctionCounts` is massively more pleasant than the raw tuples of `FunctionCount`. (Though it's still possible to get at the raw data if you want.)
5) Better support for multi-line `stmt` and `setup`.
6) Compare now also supports rowwise coloring, which is often the more natural layout for A/B testing.
7) Limited support for `globals` in `collect_callgrind`. This should make it easier to benchmark JIT models. (CC @ZolotukhinM)
8) More unit tests, including extensive tests for the Callgrind stats manipulation APIs.
9) Mitigate issue with `MKL_THREADING_LAYER` when run in Jupyter. (https://github.com/pytorch/pytorch/issues/37377)

Test plan: changes should be covered by existing and new unit tests.",pytorch
46042,zasdfgbnm,pr,2020-10-08T17:46:19Z,[1.7] Workaround for cublas bug for 45724 (#46001),https://github.com/pytorch/pytorch/issues/45592#issuecomment-705257284,pytorch
46053,gmagogsfm,pr,2020-10-08T20:35:05Z,Implement all communication APIs in DistributedC10d new frontend,,pytorch
46179,Flamefire,pr,2020-10-12T11:42:50Z,Undefine bool and vector after including altivec.h,"Fixes #41261
",pytorch
46186,Flamefire,pr,2020-10-12T15:59:30Z,Workaround for bug in DistributedDataParallel,"Fix the DistributedDataParallelSingleProcessTest to work around a limitation in DistributedDataParallel where the batch_size needs to evenly divide by the number of GPUs used
See #46175
",pytorch
46218,z-a-f,pr,2020-10-12T22:02:38Z,[quant][refactor] Alphabetize the entries in the quantized import,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46218 [quant][refactor] Alphabetize the entries in the quantized import**

Differential Revision: [D24264414](https://our.internmc.facebook.com/intern/diff/D24264414)",pytorch
46228,z-a-f,pr,2020-10-13T01:17:43Z,[quant] Custom module fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* #46426 [quant] Allow replacement of the custom modules in the ""prepare"" step
* **#46228 [quant] Custom module fix**
* #46232 [quant] Compute scale and zero point automatically in testing::_quantize
* #46235 [quant] Quantized flip dispatch

Differential Revision: [D24270012](https://our.internmc.facebook.com/intern/diff/D24270012)",pytorch
46229,z-a-f,pr,2020-10-13T01:17:51Z,[quant] Statically quantized LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46229 [quant] Statically quantized LSTM**
* #47802 [quant][refactor] Adding fx to the 'quantization/__init__.py' to enable module import
* #47801 [quant][refactor] Moving the 'quantization/fx/utils.py' to 'quantization/utils.py'
* #47730 [quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX

Differential Revision: [D24270011](https://our.internmc.facebook.com/intern/diff/D24270011)",pytorch
46232,z-a-f,pr,2020-10-13T02:43:13Z,[quant] Compute scale and zero point automatically in testing::_quantize,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* #46426 [quant] Allow replacement of the custom modules in the ""prepare"" step
* #46228 [quant] Custom module fix
* **#46232 [quant] Compute scale and zero point automatically in testing::_quantize**
* #46235 [quant] Quantized flip dispatch

Differential Revision: [D24271061](https://our.internmc.facebook.com/intern/diff/D24271061)",pytorch
46235,z-a-f,pr,2020-10-13T06:07:46Z,[quant] Quantized flip dispatch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47220 [quant][WIP] Fixing the CM
* #46229 [quant] Statically quantized LSTM
* **#46235 [quant] Quantized flip dispatch**

Differential Revision: [D24689161](https://our.internmc.facebook.com/intern/diff/D24689161)",pytorch
46256,jeffdaily,pr,2020-10-13T17:02:11Z,[ROCm] update GPG key URL in circleci Dockerfile,,pytorch
46280,zasdfgbnm,pr,2020-10-13T20:39:56Z,Delete CUDAUnaryOps.cpp,This file is no longer used,pytorch
46283,zasdfgbnm,pr,2020-10-13T20:53:26Z,Add CUDA 11.1 docker build,"
",pytorch
46369,gmagogsfm,pr,2020-10-15T01:28:56Z,[JIT] Make InsertInstruction overflow check a warning instead of fatal,"This diff restores previous behavior of silently allow overflowing when inserting instructions. The behavior was changed recently in #45382. But it started to break some existing use cases that haver overflow problems. 

Restoring original behavior but throw a warning to to unblock existing use cases where overflowing happens.

Also fixes a but that incorrectly treats `N` in `instruction` as `int16` while it is `uint16`
",pytorch
46385,Flamefire,pr,2020-10-15T07:38:02Z,Workaround for bug in DistributedDataParallel,"Fix the DistributedDataParallelSingleProcessTest to work around a limitation in
DistributedDataParallel where the batch_size needs to evenly divide by the number of GPUs used
See #46175 

Backport of #46186",pytorch
46388,Flamefire,pr,2020-10-15T11:16:50Z,Remove Python version upper boundary check,"Summary:
This prevents setup.py from erroring out when Python-3.9 is used

Fixes https://github.com/pytorch/pytorch/issues/46314

Backport of  #46315",pytorch
46389,Flamefire,pr,2020-10-15T12:48:37Z,Distribute GPUs in round robin mode for distributed_test,"The ProcessGroupNCCL::barrier implementation assumes that when 1 GPU/rank is used the GPU-Index equals the rank. Due to NCCL communicator reuse this then leads to rank 0 using the (kinda) temporary communicator while the other processes might use other GPUs leading to them trying to create a new communicator and waiting for rank 0 until that creates a new (potentially unrelated) one.

See #46248 for details

**Note that this is a workaround only!** The real issue is much harder to solve and might affect more. See https://github.com/pytorch/pytorch/issues/46248#issuecomment-708510746",pytorch
46403,robieta,pr,2020-10-15T16:54:00Z,Ci all/taylorrobie/timer papercuts 1.7,"Important cleanup for benchmark utils.
",pytorch
46424,d4l3k,pr,2020-10-15T21:29:40Z,caffe2/plan_executor: propagate exceptions from reporter substeps,"Summary: Currently if an exception occurs in a reporter thread the process is killed via std::terminate. This adds support for handling the reporter exception if FLAGS_caffe2_handle_executor_threads_exceptions is set to true.

Test Plan: buck test mode/opt -c python.package_style=inplace //caffe2/caffe2/python:hypothesis_test //caffe2/caffe2:caffe2_test_cpu -- --stress-runs 100

Differential Revision: D24345027

",pytorch
46426,z-a-f,pr,2020-10-15T21:36:16Z,"[quant] Allow replacement of the custom modules in the ""prepare"" step","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* **#46426 [quant] Allow replacement of the custom modules in the ""prepare"" step**
* #46228 [quant] Custom module fix
* #46232 [quant] Compute scale and zero point automatically in testing::_quantize
* #46235 [quant] Quantized flip dispatch

",pytorch
46436,zasdfgbnm,pr,2020-10-15T23:06:22Z,Bump up windows cudnn version,"Fixes #{issue number}
",pytorch
46441,gmagogsfm,pr,2020-10-16T01:23:37Z,Support hashing of various data types by implementing generic hashing for IValues,"It used to be that TorchScript only supported hashing of `int`, `float` and `str`. This PR adds hashing for many other types including `Tuple`, `bool`, `device` by implementing generic hashing on IValue.

* Tensor hashing follows eager behavior, which is identity-based (hash according to pointer address rather than tensor content).

Fixes #44038

This is based on @suo's #44047, with some cleaning, more tests and fixing BC check issue.",pytorch
46452,jjsjann123,pr,2020-10-16T06:49:46Z,[nvFuser] Switching to `CudaFusionGuard` from `BailOut` for nvfuser - update 2,"1. Added CudaFusionGuard as the custom TypeCheck for nvfuser; enabled dynamic shape support with profiling executor;
2. dropped support for legacy fuser;
3. re-enabled nvfuser tests;
4. added registration for profiling record to allow profiling on user specified nodes.",pytorch
46461,Flamefire,pr,2020-10-16T13:20:09Z,Replace list(map(...)) constructs by list comprehensions,"As discussed in #46392 this makes the code more readable and possibly more performant.

It also fixes a bug detected by this where the argument order of `map` was confused: https://github.com/Flamefire/pytorch/commit/030a24906e3b5a1c8e7768e0444973d63d818fde#diff-5bb26bd3a23ee3bb540aeadcc0385df2a4e48de39f87ed9ea76b21990738fe98L1537-R1537

Fixes #46392
",pytorch
46462,Flamefire,pr,2020-10-16T13:22:45Z,Replace map(lambda constructs,"Follow-up of #46461 with a similar goal

Makes them more readable and possibly faster. Care has to be taken because `map` applies the function immediately while `(x for x in xs)` is a generator expression which gets evaluated later. This is a benefit in some cases where it is not required to actually create the list of values in memory (e.g. when passing to `tuple` or `extend` or `join`)
",pytorch
46494,gmagogsfm,pr,2020-10-16T21:13:41Z,Expose script_if_tracing as public API,"Fixes #45921

`@torch.jit._script_if_tracing` is still kept for BC",pytorch
46518,z-a-f,pr,2020-10-18T07:12:13Z,[quant][fix] Empty input to observer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46518 [quant][fix] Empty input to observer**

",pytorch
46576,gmagogsfm,pr,2020-10-20T06:48:34Z,Support doc_string for TorchBind custom classes,"With this PR, users can optionally provide a ""doc_string"" to describe a class or its method. doc_string for TorchBind classes and methods are stored as `doc_string` properties in `Function` and `ScriptClass`. These `dos_string` properties are then exposed in Python layer via PyBind for doc generation.

Fixes #46047
",pytorch
46603,jeffdaily,pr,2020-10-20T18:48:03Z,ProcessGroupNCCL::alltoall_base needs to call recordStream,"For similar reasons as documented in the `[Sync Streams]` note.  For a current example, `ProcessGroupNCCL::allgather` must also call `recordStream` and does so already.  

The output tensor is created on the default stream (by the application).  NCCL/RCCL internally uses another stream (i.e., ncclStream).  If we do not record the output tensor on the ncclStream, there is a chance that the output tensor might be deallocated while NCCL/RCCL is using it.

The application is not aware of the ncclStream since it's internal to ProcessGroupNCCL.  So, the application cannot record the output tensor on the ncclStream.

Patch originally developed by @sarunyap.",pytorch
46616,zasdfgbnm,pr,2020-10-20T21:25:49Z,Add CUDA 11.1 CI,"libtorch XImportant now runs on CUDA 11.1, ",pytorch
46689,robieta,pr,2020-10-21T22:57:48Z,Optimize comparison overhead from `_wrap_type_error_to_not_implemented`,"One consequence of https://github.com/pytorch/pytorch/pull/37091 is that comparison operators have to check for `__torch_function__`. However the import is called each time as part of the hot path, and while Python is lazy about re-import it is not free. This PR simply moves the import up one level to decorator construction time, which saves about 1.5 us:

```
from torch.utils.benchmark import Timer

timer = Timer(
    ""x == y"",
    ""x = torch.ones((1,)); y = torch.ones((1,))"",
)

print(timer.blocked_autorange(min_run_time=1), ""\n\n"")
print(timer.collect_callgrind())
```

Before:
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7fb9adec3d30>
x == y
setup: x = torch.ones((1,)); y = torch.ones((1,))
  Median: 7.00 us
  IQR:    0.27 us (6.89 to 7.16)
  143 measurements, 1000 runs per measurement, 1 thread


<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb9aded0400>
x == y
setup: x = torch.ones((1,)); y = torch.ones((1,))
                           All          Noisy symbols removed
    Instructions:      2499971                    2373429
    Baseline:             4401                       3981
100 runs per measurement, 1 thread
```

After:
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7f966fd02898>
x == y
setup: x = torch.ones((1,)); y = torch.ones((1,))
  Median: 5.43 us
  IQR:    0.25 us (5.34 to 5.59)
  177 measurements, 1000 runs per measurement, 1 thread


<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f966fd02ef0>
x == y
setup: x = torch.ones((1,)); y = torch.ones((1,))
                           All          Noisy symbols removed
    Instructions:      1759233                    1705173
    Baseline:             4335                       3954
100 runs per measurement, 1 thread
```

This drops instruction count by ~40% and overhead wall time by ~30%. Otherwise it should be a no-op. The check itself adds a further ~1us over the old state, but that is much harder to claw back.

Test plan: Existing unit tests should be sufficient.",pytorch
46691,gmagogsfm,pr,2020-10-21T23:09:23Z,Include new c10d frontend in build,"Fixes #{issue number}
",pytorch
46717,jeffdaily,pr,2020-10-22T15:32:22Z,[ROCm] update debug flags,Improves support for rocgdb when setting DEBUG=1 and building for ROCm.,pytorch
46747,z-a-f,pr,2020-10-22T23:42:38Z,[WIP] Numeric suite fix,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46747 [WIP] Numeric suite fix**

Differential Revision: [D24493498](https://our.internmc.facebook.com/intern/diff/D24493498)",pytorch
46852,jeffdaily,pr,2020-10-26T16:49:29Z,[ROCm] fix bug in miopen findAlgorithm.,"findAlgorithm should return if and only if a suitable algorithm is found.
The default algorithm is not guaranteed to have been cached.",pytorch
46857,jeffdaily,pr,2020-10-26T17:21:52Z,remove event not ready assertion from TestCuda.test_copy_non_blocking,"It is incorrect to assume that a newly recorded event will immediately query as False.
This test is flaky on ROCm due to this incorrect assumption.",pytorch
46858,z-a-f,pr,2020-10-26T17:33:51Z,[docs] Changing the wording on quantization versioning and support,"Fixes #46707 

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46858 [docs] Changing the wording on quantization versioning and support**

Differential Revision: [D24542598](https://our.internmc.facebook.com/intern/diff/D24542598)",pytorch
46880,robieta,pr,2020-10-26T22:19:41Z,expose Timer docs to PyTorch website.,"CC: @gchanan @jspisak @seemethere

I previewed the docs and they look reasonable. Let me know if I missed anything.",pytorch
46887,z-a-f,pr,2020-10-26T23:21:28Z,[docs] Changing the wording on quantization versioning and support,"Landed master PR: #46858

Fixes #{issue number} by changing the wording in the quantization documentation
",pytorch
46890,z-a-f,pr,2020-10-26T23:27:48Z,[docs] Changing the wording on quantization versioning and support,"
Original PR landed on master: #46858

Fixes #{issue number} by changing the wording in quantization docs
",pytorch
46941,zasdfgbnm,pr,2020-10-27T20:05:42Z,Fix some flaky tests in test_torch.py and test_nn.py,"Fixed test:
- `test_is_nonzero`, this is asserting exact match, which is flaky when `TORCH_SHOW_CPP_STACKTRACES=1`, I changed this to non-exact assert
- `test_pinverse` TF32
- `test_symeig` TF32
- `test_triangular_solve_batched_many_batches_cpu_float64` precision on CPU BLAS
- `test_qr` TF32, as well as the tensor factory forgets a `dtype=dtype`
- `test_lu` TF32
- `ConvTranspose2d` TF32
- `Conv3d_1x1x1_no_bias` TF32
- `Transformer*` TF32

",pytorch
47017,gmagogsfm,pr,2020-10-28T20:58:19Z,Split comm hooks into python-dependent hooks and others,"Fixes #{issue number}
",pytorch
47019,gmagogsfm,pr,2020-10-28T21:04:50Z,Split comm hooks into python-dependent hooks and others,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47019 Split comm hooks into python-dependent hooks and others**

This is needed because we plan to move most of c10d C++ implementation into `libtorch_*.so`, which can not have Python dependencies.",pytorch
47058,robieta,pr,2020-10-29T16:00:06Z,Add blas compare example,"Adds a standalone script which can be used to test different BLAS libraries. Right now I've deliberately kept it limited (only a couple BLAS libs and only GEMM and GEMV). It's easy enough to expand later.

CC @ngimel ",pytorch
47070,zasdfgbnm,pr,2020-10-29T19:02:06Z,Expand the test of torch.bmm on CUDA,"This is to satisfy the request at https://github.com/pytorch/pytorch/pull/42553#issuecomment-673673914, see also https://github.com/pytorch/pytorch/pull/47079 https://github.com/pytorch/pytorch/pull/47080",pytorch
47079,zasdfgbnm,pr,2020-10-29T20:29:59Z,Expand the test of torch.addbmm and torch.baddbmm,This is to satisfy the request at https://github.com/pytorch/pytorch/pull/42553#issuecomment-673673914. See also https://github.com/pytorch/pytorch/pull/47124,pytorch
47080,zasdfgbnm,pr,2020-10-29T20:57:29Z,Expand test of torch.test_baddbmm,This is to satisfy the request at https://github.com/pytorch/pytorch/pull/42553#issuecomment-673673914. See also https://github.com/pytorch/pytorch/pull/47070 https://github.com/pytorch/pytorch/pull/47079,pytorch
47097,zasdfgbnm,pr,2020-10-30T02:27:06Z,DO NOT MERGE,"These flags were added by myself in a previous PR. At that time, I thought these flags were not supported on Windows, but seems that I was wrong, and we should enable this on Windows as well.",pytorch
47107,gmagogsfm,pr,2020-10-30T08:02:48Z,Move most of C10D into libtorch,"Fixes #{issue number}
",pytorch
47121,jeffdaily,pr,2020-10-30T16:47:03Z,add rocm 3.9 to nightly builds,Corresponding pytorch builder repo update: https://github.com/pytorch/builder/pull/561.,pytorch
47124,zasdfgbnm,pr,2020-10-30T17:31:41Z,Expand the test of torch.bmm on CUDA,"basically https://github.com/pytorch/pytorch/pull/47070, enabled on all CI with `ci-all`",pytorch
47131,jeffdaily,pr,2020-10-30T20:47:09Z,[ROCm] enable kernel asserts,"Addresses missing ROCm feature indicated in #38943.
",pytorch
47136,jeffdaily,pr,2020-10-30T21:40:23Z,[ROCm] enable stream priorities,,pytorch
47212,jjsjann123,pr,2020-11-02T18:47:34Z,[DO NOT REVIEW] adding explicit in IValue::IValue(bool),"Boolean conversion for pointers seems strange in this case. https://en.cppreference.com/w/cpp/language/implicit_conversion.
Without explicit, we can wrap any pointer with IValue, which is a very confusing behavior.",pytorch
47220,z-a-f,pr,2020-11-02T20:17:49Z,[quant][WIP] Fixing the CM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47220 [quant][WIP] Fixing the CM**
* #46229 [quant] Statically quantized LSTM
* #46235 [quant] Quantized flip dispatch

",pytorch
47221,jjsjann123,pr,2020-11-02T20:31:38Z,pass strict_fuser_check for recursive fusion,We forgot to pass `strict_fuser_check` recursively to nested GraphFuser.,pytorch
47237,zasdfgbnm,pr,2020-11-02T22:31:31Z,The dimension being reduced should not be coalesced by TensorIterator,"Fixes https://github.com/pytorch/pytorch/issues/37583#issuecomment-720172838

Also add overload of `<<` for convenience of debugging.

This PR is tested by `test_reduction_split_cuda` which was added in https://github.com/pytorch/pytorch/pull/37788.

Reproduce
```python
import torch

a = torch.zeros(8, 1, 128, 1024, 1024)
a.cuda().sum(1)
```

Before

```
TensorIterator @ 0x7ffd05b10ba0 {
  ntensors() = 2
  noutputs() = 1
  shape() = [1073741824]
  strides(*) = {
    (0) = [4]
    (1) = [4]
  }
  dtype(*) = {
    (0) = Float
    (1) = Float
  }
  is_reduction_ = 1
}
```

After

```
TensorIterator @ 0x7fffc9051010 {
  ntensors() = 2
  noutputs() = 1
  shape() = [1, 1073741824]
  strides(*) = {
    (0) = [0, 4]
    (1) = [536870912, 4]
  }
  dtype(*) = {
    (0) = Float
    (1) = Float
  }
  is_reduction_ = 1
}
```",pytorch
47241,jeffdaily,pr,2020-11-02T23:38:06Z,[ROCm] remove use of HIP_PLATFORM,"Fixes deprecated use of the HIP_PLATFORM env var.  This env var is no longer needed to be set explicitly.  Instead, HIP_PLATFORM is automatically detected by hipcc.",pytorch
47245,gmagogsfm,pr,2020-11-03T00:13:20Z,Move most of C10D into libtorch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47245 Move most of C10D into libtorch**

",pytorch
47309,gmagogsfm,pr,2020-11-03T20:22:59Z,Move python-independent c10d implementations to torch/lib,"* This is a pre-step to build c10d into libtorch
* Includes a minor cleanup in c10d/CMakeLists.txt",pytorch
47331,zasdfgbnm,pr,2020-11-03T23:52:22Z,Add docs on how to toggle TF32 flags on C++,"I have been asked several times how to toggle this flag on libtorch. I think it would be good to mention it in the docs.
",pytorch
47344,ppwwyyxx,pr,2020-11-04T07:23:03Z,[jit] better message for bad type annotation,"Summary:
```
ValueError: Unknown type annotation: 'typing.Sequence[torch.Tensor]' at  File ""xxx.py"", line 223
        images = [x[""image""].to(self.device) for x in batched_inputs]
        images = [(x - self.pixel_mean) / self.pixel_std for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
                 ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        return images
```

Otherwise have no clue where the error is.

Test Plan: sandcastle

Differential Revision: D24723525

",pytorch
47428,robieta,pr,2020-11-05T06:31:53Z,Add C++ support to torch.utils.benchmark.Timer,"This PR makes pretty heavy use of `torch.utils.cpp_extension` to JIT compile snippets. The rest was fairly simple; Timer already defers to an underlying `_timer_cls` for its times, and `collect_callgrind` just needs something that it can subprocess execute with appropriate `CALLGRIND_TOGGLE_COLLECT`'s. I didn't initially intend to add the wall time portion, but it just kind of fell out naturally.

Key changes:
1) Expose `CXX_FLAGS` so the JIT'd code can match them. (Namely `-O*` and `-g`)
2) Add an option for `cpp_extension.load` to emit a standalone binary. Currently linux only: @malfet I wasn't sure what the windows analog to `-rpath` was.
3) Bring along the Valgrind headers as part of the wheel, as they're needed for the C++ measure loop.
4) Add `cpp_jit.py` and a couple templates. I tried to group as much of compilation bits as possible, though it has the curious effect of putting the `timeit` C++ under `valgrind_wrapper/`. I organized the intermediate files to be friendly to `cpp_extension`'s caching scheme while still not littering random artifacts, and repeated runs are reasonably snappy.
5) Add a `language=` argument to Timer that accepts `Python` or `C++`, and plumb accordingly.
6) Tie number of displayed lines to `PRINT_OPTS.threshold`. (This is just a papercut that's been bothering me for a while.)
7) Change compat bindings to not touch torch at all, which got rid of the strange behavior that compat bindings increased the instruction counts due to the inclusion of certain headers.

And now for the question that is doubtless on everyone's mind: Yes, the C++ instruction counts appear to be absolutely deterministic. (Yay!)

I've also added an example showing how instruction counts correlate to wall time, and how one could show the ""Python tax"" by diff-ing equivalent statements in Python and C++. 
",pytorch
47434,jjsjann123,pr,2020-11-05T08:45:25Z,switching from Code to GraphExecutor for gradient forward,We switch to GraphExecutor for autodiff forward graph. This is used to re-profile updated forward graph in order to profile it again.,pytorch
47464,ppwwyyxx,pr,2020-11-05T20:53:51Z,[jit] better message for bad type annotation,"Summary:
```
ValueError: Unknown type annotation: 'typing.Sequence[torch.Tensor]' at  File ""xxx.py"", line 223
        images = [x[""image""].to(self.device) for x in batched_inputs]
        images = [(x - self.pixel_mean) / self.pixel_std for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
                 ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        return images
```

Otherwise have no clue where the error is.

Test Plan: sandcastle

Differential Revision: D24764886

",pytorch
47475,d4l3k,pr,2020-11-06T00:08:52Z,[caffe2] improve core.Net cloning/init performance,"Summary: This improves the core.Net cloning/init performance by quite a bit. It makes set_input_record run in linear time instead of O(n) by checking the external_input map instead of regenerating the external inputs each time and then iterating over it.

Test Plan: unit tests + canary runs

Differential Revision: D24765346

",pytorch
47492,zasdfgbnm,pr,2020-11-06T06:28:26Z,Add constructor for ArgumentDef,"Fixes https://github.com/pytorch/pytorch/issues/47493
",pytorch
47494,zasdfgbnm,pr,2020-11-06T07:23:04Z,"Fix compiler warning variable ""num_ivalue_args"" was declared but never referenced detected during:","```
/home/gaoxiang/.local/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h(326): warning: variable ""num_ivalue_args"" was declared but never referenced
          detected during:
            instantiation of ""std::decay_t<c10::guts::infer_function_traits<Functor>::type::return_type> c10::impl::call_functor_with_args_from_stack_<Functor,AllowDeprecatedTypes,ivalue_arg_indices...>(Functor *, c10::Stack *, std::index_sequence<ivalue_arg_indices...>) [with Functor=c10::impl::WrapFunctionIntoRuntimeFunctor<std::decay_t<__nv_bool ()>>, AllowDeprecatedTypes=false, ivalue_arg_indices=<>]"" 
(346): here
            instantiation of ""std::decay_t<c10::guts::infer_function_traits<Functor>::type::return_type> c10::impl::call_functor_with_args_from_stack<Functor,AllowDeprecatedTypes>(Functor *, c10::Stack *) [with Functor=c10::impl::WrapFunctionIntoRuntimeFunctor<std::decay_t<__nv_bool ()>>, AllowDeprecatedTypes=false]"" 
(396): here
            instantiation of ""void c10::impl::make_boxed_from_unboxed_functor<KernelFunctor, AllowDeprecatedTypes>::call(c10::OperatorKernel *, const c10::OperatorHandle &, c10::Stack *) [with KernelFunctor=c10::impl::WrapFunctionIntoRuntimeFunctor<std::decay_t<__nv_bool ()>>, AllowDeprecatedTypes=false]"" 
/home/gaoxiang/.local/lib/python3.8/site-packages/torch/include/ATen/core/boxing/KernelFunction_impl.h(109): here
            instantiation of ""c10::KernelFunction c10::KernelFunction::makeFromUnboxedFunctor<AllowLegacyTypes,KernelFunctor>(std::unique_ptr<c10::OperatorKernel, std::default_delete<c10::OperatorKernel>>) [with AllowLegacyTypes=false, KernelFunctor=c10::impl::WrapFunctionIntoRuntimeFunctor<std::decay_t<__nv_bool ()>>]"" 
/home/gaoxiang/.local/lib/python3.8/site-packages/torch/include/ATen/core/boxing/KernelFunction_impl.h(175): here
            instantiation of ""c10::KernelFunction c10::KernelFunction::makeFromUnboxedRuntimeFunction(FuncType *) [with AllowLegacyTypes=false, FuncType=__nv_bool ()]"" 
/home/gaoxiang/.local/lib/python3.8/site-packages/torch/include/torch/library.h(92): here
            instantiation of ""torch::CppFunction::CppFunction(Func *, std::enable_if_t<c10::guts::is_function_type<Func>::value, std::nullptr_t>) [with Func=__nv_bool ()]"" 
/home/gaoxiang/.local/lib/python3.8/site-packages/torch/include/torch/library.h(457): here
            instantiation of ""torch::Library &torch::Library::def(NameOrSchema &&, Func &&) & [with NameOrSchema=const char (&)[23], Func=__nv_bool (*)()]"" 
/home/gaoxiang/extension-jit/test.cu(6): here
```",pytorch
47495,jjsjann123,pr,2020-11-06T09:01:33Z,[WIP] Profile ivalue pr,I have to smuggle this commit `Remove output used only by sizes (#448)` to avoid conflicts.,pytorch
47523,gmagogsfm,pr,2020-11-06T21:09:58Z,Implement Complex tensor support in all reduce and all gather,,pytorch
47524,zasdfgbnm,pr,2020-11-06T21:20:26Z,Test TORCH_LIBRARY in CUDA extension,"In the [official documentation](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html), it is recommended to use `TORCH_LIBRARY` to register ops for TorchScript. However, that code is never tested with CUDA extension and is actually broken (https://github.com/pytorch/pytorch/issues/47493). This PR adds a test for it. It will not pass CI now, but it will pass when the issue https://github.com/pytorch/pytorch/issues/47493 is fixed.",pytorch
47584,gmagogsfm,pr,2020-11-08T21:34:30Z,Test custom holder failure,"Fixes #{issue number}
",pytorch
47585,ppwwyyxx,pr,2020-11-08T23:00:33Z,fix extensions build flags on newer GPUs,"Fixes #47352
",pytorch
47613,d4l3k,pr,2020-11-09T18:54:08Z,[caffe2] plan_executor_test: add test case for should_stop loops,"Summary: This is to test some more cancellation edge cases that were missing before. It passes under the current code.

Test Plan: buck test mode/opt caffe2/caffe2:caffe2_test_cpu -- PlanExecutorTest --stress-runs 10

Differential Revision: D24836956

",pytorch
47656,z-a-f,pr,2020-11-10T07:12:41Z,[quant][refactor] Moving the quant_type into the QConfig,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47730 [quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX
* **#47656 [quant][refactor] Moving the quant_type into the QConfig**

The quant_type is used in custom module, which can utilize the QConfig
to prepare/convert. Also, this is the first PR in an attempt to enable
the hybrid quantization.",pytorch
47665,jjsjann123,pr,2020-11-10T12:04:28Z,Remove output used only by sizes (#448),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47668 profile ivalue for nvfuser
* #47667 refactor profiling optional
* #47666 adding profile_ivalue
* **#47665 Remove output used only by sizes (#448)**

Re-enabled the pass to remove outputs from fusion that is only used by aten::size;
Added size computation for reduction op via new operator prim::ReductionSizes;

Differential Revision: [D25254675](https://our.internmc.facebook.com/intern/diff/D25254675)",pytorch
47666,jjsjann123,pr,2020-11-10T12:04:37Z,adding profile_ivalue,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47668 profile ivalue for nvfuser
* #47667 refactor profiling optional
* **#47666 adding profile_ivalue**

Differential Revision: [D25255573](https://our.internmc.facebook.com/intern/diff/D25255573)",pytorch
47667,jjsjann123,pr,2020-11-10T12:04:45Z,refactor profiling optional,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47668 profile ivalue for nvfuser
* **#47667 refactor profiling optional**

Differential Revision: [D25255572](https://our.internmc.facebook.com/intern/diff/D25255572)",pytorch
47668,jjsjann123,pr,2020-11-10T12:04:52Z,profile ivalue for nvfuser,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47668 profile ivalue for nvfuser**
* #47667 refactor profiling optional

Differential Revision: [D25255571](https://our.internmc.facebook.com/intern/diff/D25255571)",pytorch
47683,jeffdaily,pr,2020-11-10T17:52:42Z,[ROCm] set ROCM_ARCH to gfx900 and gfx906 for CI builds,"This change adds the arch settings for caffe2 builds, fixes some typos,
and clarifies that this setting applies to both CircleCI and Jenkins.",pytorch
47700,zasdfgbnm,pr,2020-11-10T20:50:14Z,Add test for empty tensors for batch matmuls,"Fixes #{issue number}
",pytorch
47730,z-a-f,pr,2020-11-11T03:49:18Z,[quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* #47802 [quant][refactor] Adding fx to the 'quantization/__init__.py' to enable module import
* #47801 [quant][refactor] Moving the 'quantization/fx/utils.py' to 'quantization/utils.py'
* **#47730 [quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX**

Differential Revision: [D24270011](https://our.internmc.facebook.com/intern/diff/D24270011)",pytorch
47768,d4l3k,pr,2020-11-11T18:15:45Z,[caffe2] cache NextName indexes for faster name generation,"Summary: This stores the next ID for a given NextName(prefix, output_id) so repeated calls to NextName are significantly faster. This accounts for ~65% of time spent for large models.

Test Plan:
buck test //caffe2/caffe2/python/...

will launch canary job before landing to ensure no regressions + confirm speedup

Differential Revision: D24876961

",pytorch
47794,zasdfgbnm,pr,2020-11-11T22:22:41Z,"Update Windows CI to CUDA 11.1, cuDNN 8.0.5","Fixes #{issue number}
",pytorch
47801,z-a-f,pr,2020-11-11T23:09:30Z,[quant][refactor] Moving the 'quantization/fx/utils.py' to 'quantization/utils.py',"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* #47802 [quant][refactor] Adding fx to the 'quantization/__init__.py' to enable module import
* **#47801 [quant][refactor] Moving the 'quantization/fx/utils.py' to 'quantization/utils.py'**
* #47730 [quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX

This file has more ""generic"" utilities, and if we try to use it from the parent, it will cause circular dependency error.",pytorch
47802,z-a-f,pr,2020-11-11T23:09:38Z,[quant][refactor] Adding fx to the 'quantization/__init__.py' to enable module import,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46229 [quant] Statically quantized LSTM
* **#47802 [quant][refactor] Adding fx to the 'quantization/__init__.py' to enable module import**
* #47801 [quant][refactor] Moving the 'quantization/fx/utils.py' to 'quantization/utils.py'
* #47730 [quant] Add another keyword to the 'prepare_custom_config_dict' to be the same as in FX

",pytorch
47809,jeffdaily,pr,2020-11-12T00:06:50Z,[ROCm] add skipCUDAIfRocm to test_lingalg test_norm_fro_2_equivalence_old,This test started failing when ROCm CI moved to 3.9.  Skip until triage is complete.,pytorch
47819,gmagogsfm,pr,2020-11-12T01:27:35Z,Support using lambda function as TorchBind constructor,,pytorch
47861,robieta,pr,2020-11-12T17:39:51Z,Expose CXX_FLAGS through __config__,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47865 Enable callgrind collection for C++ snippets
* #47864 Add support for timing C++ snippets.
* #47863 Rework compat bindings.
* #47862 Add option for cpp_extensions to compile standalone executable
* **#47861 Expose CXX_FLAGS through __config__**

Differential Revision: [D25199263](https://our.internmc.facebook.com/intern/diff/D25199263)",pytorch
47862,robieta,pr,2020-11-12T17:40:04Z,Add option for cpp_extensions to compile standalone executable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47865 Enable callgrind collection for C++ snippets
* #47864 Add support for timing C++ snippets.
* #47863 Rework compat bindings.
* **#47862 Add option for cpp_extensions to compile standalone executable**
* #47861 Expose CXX_FLAGS through __config__

Differential Revision: [D25199265](https://our.internmc.facebook.com/intern/diff/D25199265)",pytorch
47863,robieta,pr,2020-11-12T17:40:17Z,Rework compat bindings.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47865 Enable callgrind collection for C++ snippets
* #47864 Add support for timing C++ snippets.
* **#47863 Rework compat bindings.**
* #47862 Add option for cpp_extensions to compile standalone executable
* #47861 Expose CXX_FLAGS through __config__

Differential Revision: [D25199261](https://our.internmc.facebook.com/intern/diff/D25199261)",pytorch
47864,robieta,pr,2020-11-12T17:40:29Z,Add support for timing C++ snippets.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #47865 Enable callgrind collection for C++ snippets
* **#47864 Add support for timing C++ snippets.**
* #47863 Rework compat bindings.
* #47862 Add option for cpp_extensions to compile standalone executable
* #47861 Expose CXX_FLAGS through __config__

Differential Revision: [D25199262](https://our.internmc.facebook.com/intern/diff/D25199262)",pytorch
47865,robieta,pr,2020-11-12T17:40:42Z,Enable callgrind collection for C++ snippets,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47865 Enable callgrind collection for C++ snippets**
* #47864 Add support for timing C++ snippets.
* #47863 Rework compat bindings.
* #47862 Add option for cpp_extensions to compile standalone executable
* #47861 Expose CXX_FLAGS through __config__

Differential Revision: [D25199264](https://our.internmc.facebook.com/intern/diff/D25199264)",pytorch
47873,zasdfgbnm,pr,2020-11-12T18:40:21Z,Batched matmul dtypes,"Fixes #{issue number}
",pytorch
47907,gmagogsfm,pr,2020-11-13T07:37:30Z,Add TorchBind-based Python and TorchScript binding for ProcessGroup,"Add TorchBind-binding for ProcessGroup class.

Currently there are a few limitation of TorchBind that prevents us from fully matching existing PyBind-binding of ProcessGroup:

- TorchBind doesn't support method overloading. Current PyBind binding uses overloading extensively to provide flexible API, but TorchBind (and TorchScript ClassType behind it) doesn't yet support it. Therefore, we can provide at most one version of API under each name.

- TorchBind doesn't support C++ enums yet. This prevents us from making real uses of XXXOptions, which is widely used in many APIs.
",pytorch
47944,jeffdaily,pr,2020-11-13T22:37:15Z,remove redundant sccache wrappers from build.sh scripts,,pytorch
47947,jeffdaily,pr,2020-11-13T23:05:08Z,[ROCm] remove sccache wrappers post build,"For ROCm, the CI images serve both the CI jobs as well as public releases. Without removing the sccache wrappers, end users are forced to use sccache. Our users have encountered sccache bugs when using our PyTorch images, so we choose to remove them after the CI build completes. Further, runtime compilation of MIOpen kernels still experiences errors due to sccache.",pytorch
47951,zasdfgbnm,pr,2020-11-14T00:13:08Z,Add tensor.view(dtype),"Fixes https://github.com/pytorch/pytorch/issues/42571

Note that this functionality is a subset of [`numpy.ndarray.view`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html):
- this only supports viewing a tensor as a dtype with the same number of bytes
- this does not support viewing a tensor as a subclass of `torch.Tensor`
",pytorch
47961,ppwwyyxx,pr,2020-11-14T05:21:17Z,[jit] print function in the error message,"Summary: otherwise don't know what is the problematic code

Test Plan:
```
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/jit/_script.py"", line 938, in script
    _compile_and_register_class(obj, _rcb, qualified_name)
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/jit/_script.py"", line 64, in _compile_and_register_cla
ss
    torch._C._jit_script_class_compile(qualified_name, ast, defaults, rcb)
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/jit/annotations.py"", line 332, in try_ann_to_type
    torch.jit._script._recursive_compile_class(ann, loc)
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/jit/_script.py"", line 1069, in _recursive_compile_clas
s
    rcb = _jit_internal.createResolutionCallbackForClassMethods(obj)
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/_jit_internal.py"", line 305, in createResolutionCallba
ckForClassMethods
    captures.update(get_type_hint_captures(fn))
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/_jit_internal.py"", line 288, in get_type_hint_captures
    annotation_to_type[get_annotation_str(f.returns)] = signature.return_annotation
  File ""/data/users/yuxinwu/fbsource/fbcode/buck-out/opt/gen/vision/fair/detectron2/tests/test_instances#binary,link-tree/torch/_jit_internal.py"", line 271, in get_annotation_str
    raise RuntimeError(f""Unexpected node type: {type(annotation)} in function {fn}"")
RuntimeError: Unexpected node type: <class '_ast.Str'> in function <function Boxes.clone at 0x7f1324577d40>
```

Differential Revision: D24971065

",pytorch
47963,zasdfgbnm,pr,2020-11-14T06:05:47Z,Ignore MSVC's pdb file,"These files are generated by MSVC when building with debug symbols `REL_WITH_DEB_INFO=1`:
```
PS C:\Users\Xiang Gao\source\repos\pytorch> git status
On branch master
Your branch is up to date with 'origin/master'.

Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
        torch/lib/asmjit.pdb
        torch/lib/c10.pdb
        torch/lib/c10_cuda.pdb
        torch/lib/caffe2_detectron_ops_gpu.pdb
        torch/lib/caffe2_module_test_dynamic.pdb
        torch/lib/caffe2_observers.pdb
        torch/lib/fbgemm.pdb
        torch/lib/shm.pdb
        torch/lib/torch_cpu.pdb
        torch/lib/torch_cuda.pdb

nothing added to commit but untracked files present (use ""git add"" to track)
```",pytorch
48018,robieta,pr,2020-11-16T17:17:41Z,move Tensor comparisons back to C,"It seems that the machinery to handle comparison method in C rather than Python already exists, unless I'm missing something. (There is a wrapper for `TypeError_to_NotImplemented_`, and Python code gen handles `__torch_function__` which are the two things `_wrap_type_error_to_not_implemented` is doing) The performance change is quite stark:

```
import torch
from torch.utils.benchmark import Timer

global_dict = {
    ""x"": torch.ones((2, 2)),
    ""y_scalar"": torch.ones((1,)),
    ""y_tensor"": torch.ones((2, 1)),
}

for stmt in (""x == 1"", ""x == y_scalar"", ""x == y_tensor""):
    print(Timer(stmt, globals=global_dict).blocked_autorange(min_run_time=5), ""\n"")
```

### Before:
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7f3d1289dc10>
x == 1
  Median: 12.86 us
  IQR:    0.65 us (12.55 to 13.20)
  387 measurements, 1000 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7f3d1289d1d0>
x == y_scalar
  Median: 6.03 us
  IQR:    0.33 us (5.91 to 6.24)
  820 measurements, 1000 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7f3d2b9e2050>
x == y_tensor
  Median: 6.34 us
  IQR:    0.33 us (6.16 to 6.49)
  790 measurements, 1000 runs per measurement, 1 thread
```


### After:
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7fbdba2a16d0>
x == 1
  Median: 6.88 us
  IQR:    0.40 us (6.74 to 7.14)
  716 measurements, 1000 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7fbdd2e07ed0>
x == y_scalar
  Median: 2.98 us
  IQR:    0.19 us (2.89 to 3.08)
  167 measurements, 10000 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7fbdd33e4510>
x == y_tensor
  Median: 3.03 us
  IQR:    0.13 us (2.97 to 3.10)
  154 measurements, 10000 runs per measurement, 1 thread
```

There's still a fair bit of work left. Equivalent NumPy is about 6x faster than the new overhead, and PyTorch 0.4 is about 1.25 us across the board. (No scalar cliff.) But it's a start.",pytorch
48025,zasdfgbnm,pr,2020-11-16T18:37:44Z,DO NOT MERGE,"Fixes #{issue number}
",pytorch
48029,zasdfgbnm,pr,2020-11-16T19:34:40Z,Bump up the CUDA OOM test memory size,"80GB is no longer large any more https://nvidianews.nvidia.com/news/nvidia-doubles-down-announces-a100-80gb-gpu-supercharging-worlds-most-powerful-gpu-for-ai-supercomputing

Hopefully, the new size could be OK until the end of Moore's Law :)",pytorch
48035,jeffdaily,pr,2020-11-16T20:05:59Z,centos now installs cmake from conda,"For the same reason that ubuntu builds need conda cmake to find mkl.

CC @jaglinux ",pytorch
48036,z-a-f,pr,2020-11-16T20:16:16Z,[quant] ReflectionPad2d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48051 [quant][refactor] Reflection pad now uses AT_DISPATCH_FLOATING_AND_QINT_TYPES
* #48050 [quant] AT_DISPATCH_FLOATING_AND_QINT_TYPES
* #48037 [quant] out-variant for the reflection pad
* **#48036 [quant] ReflectionPad2d**

Differential Revision: [D25000347](https://our.internmc.facebook.com/intern/diff/D25000347)",pytorch
48037,z-a-f,pr,2020-11-16T20:16:25Z,[quant] out-variant for the reflection pad,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48051 [quant][refactor] Reflection pad now uses AT_DISPATCH_FLOATING_AND_QINT_TYPES
* #48050 [quant] AT_DISPATCH_FLOATING_AND_QINT_TYPES
* **#48037 [quant] out-variant for the reflection pad**
* #48036 [quant] ReflectionPad2d

Differential Revision: [D25000345](https://our.internmc.facebook.com/intern/diff/D25000345)",pytorch
48050,z-a-f,pr,2020-11-16T22:15:41Z,[quant] AT_DISPATCH_FLOATING_AND_QINT_TYPES,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48051 [quant][refactor] Reflection pad now uses AT_DISPATCH_FLOATING_AND_QINT_TYPES
* **#48050 [quant] AT_DISPATCH_FLOATING_AND_QINT_TYPES**
* #48037 [quant] out-variant for the reflection pad
* #48036 [quant] ReflectionPad2d

The switch does not require testing -- if need to confirm if it works right, you can run the tests for the reflection padding: `python test/test_quantization.py TestPadding`

Differential Revision: [D25004159](https://our.internmc.facebook.com/intern/diff/D25004159)",pytorch
48051,z-a-f,pr,2020-11-16T22:15:48Z,[quant][refactor] Reflection pad now uses AT_DISPATCH_FLOATING_AND_QINT_TYPES,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48051 [quant][refactor] Reflection pad now uses AT_DISPATCH_FLOATING_AND_QINT_TYPES**
* #48050 [quant] AT_DISPATCH_FLOATING_AND_QINT_TYPES
* #48037 [quant] out-variant for the reflection pad
* #48036 [quant] ReflectionPad2d

Test Plan:

Run `python test/test_quantization.py TestPadding`

Differential Revision: [D25004158](https://our.internmc.facebook.com/intern/diff/D25004158)",pytorch
48058,jeffdaily,pr,2020-11-16T22:54:00Z,[ROCm] remove builds for ROCm versions less than 3.8,Minor cleanup of older ROCm docker builds.,pytorch
48086,gmagogsfm,pr,2020-11-17T09:47:08Z,Add newProcessGroup method to c10 new frontend.,"* Fix a few rebase errors from before
* Add frontend in build

Fixes #{issue number}
",pytorch
48105,z-a-f,pr,2020-11-17T17:21:16Z,[quant] Enabling the perchannel quant in the convTranspose,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48105 [quant] Enabling the perchannel quant in the convTranspose**

Differential Revision: [D25025384](https://our.internmc.facebook.com/intern/diff/D25025384)",pytorch
48118,jeffdaily,pr,2020-11-17T19:59:05Z,[ROCm] remove builds for versions less than 3.8,,pytorch
48146,gmagogsfm,pr,2020-11-18T01:51:39Z,Add c10d new frontend to build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48148 Add TorchBind-based Python and TorchScript binding for ProcessGroup
* #48147 Add process group creation logic in c10d new frontend
* **#48146 Add c10d new frontend to build**

Differential Revision: [D25073969](https://our.internmc.facebook.com/intern/diff/D25073969)",pytorch
48147,gmagogsfm,pr,2020-11-18T01:51:48Z,Add process group creation logic in c10d new frontend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48148 Add TorchBind-based Python and TorchScript binding for ProcessGroup
* **#48147 Add process group creation logic in c10d new frontend**
* #48146 Add c10d new frontend to build

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)",pytorch
48148,gmagogsfm,pr,2020-11-18T01:57:06Z,Add TorchBind-based Python and TorchScript binding for ProcessGroup,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48148 Add TorchBind-based Python and TorchScript binding for ProcessGroup**
* #48147 Add process group creation logic in c10d new frontend
* #48146 Add c10d new frontend to build

",pytorch
48164,zasdfgbnm,pr,2020-11-18T07:33:45Z,Install magma on CUDA 11.1,cc: @xwang233 @janeyx99 ,pytorch
48271,d4l3k,pr,2020-11-19T20:33:06Z,caffe2/core.Net: is_external_input rebuild lookup tables when necessary,"Summary: is_external_input doesn't check if the lookup tables are valid. Calling .Proto() should invalidate all lookup tables and have them rebuilt on call to any methods depending on them. This adds this check to is_external_input.

Test Plan: internal unit tests

Differential Revision: D25100464

",pytorch
48333,gmagogsfm,pr,2020-11-20T21:33:14Z,Implement JIT serialization of ProcessGroup,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48538 Bind all of frontend
* **#48333 Implement JIT serialization of ProcessGroup**

",pytorch
48340,d4l3k,pr,2020-11-20T23:06:13Z,caffe2: refactor context to allow being typed,"Summary:
This changes the context managed classes from using a decorator to define them to using inheritance. Inheritance allows the python static type checking to work correctly.

```
context.define_context()
class Bar(object): ...

context.define_context(allow_default=True)
class Foo(object): ...
```

becomes
```
class Foo(context.Managed): ...

class Bar(context.DefaultManaged): ...
```

Behavior differences:
* arg_name has been removed since it's not used anywhere
* classes need to call `super()` in `__enter__/__exit__` methods if they override (none do)

This also defines a context.pyi file to add types for python3.

Test Plan:
ci

  buck test //caffe2/caffe2/python:context_test

Differential Revision: D25133469

",pytorch
48369,gmagogsfm,pr,2020-11-22T08:42:37Z,Implement JIT serialization of ProcessGroup,"ghstack-source-id: 32b2bc5a703125ce89f7146d6084ffee2f98fa1d
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48333

Fixes #{issue number}
",pytorch
48379,gmagogsfm,pr,2020-11-22T20:39:22Z,Implement JIT serialization of ProcessGroup,"ghstack-source-id: 32b2bc5a703125ce89f7146d6084ffee2f98fa1d
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48333

Fixes #{issue number}
",pytorch
48405,jeffdaily,pr,2020-11-23T23:29:31Z,work around #47028 until a proper fix is identified,"Otherwise, this test will appear flaky for ROCm even though it is a generic PyTorch issue.

CC @albanD ",pytorch
48424,jeffdaily,pr,2020-11-24T20:18:27Z,Allow ROCm CI to use non-default stream.,Revert #26394. Fixes #27356.  Not all MIOpen handles were setting their stream to the current stream prior to running the op.,pytorch
48431,jeffdaily,pr,2020-11-24T22:33:27Z,[ROCm] restore autograd tests,"Fixes #30845.
",pytorch
48455,jjsjann123,pr,2020-11-25T15:40:28Z,fix nvrtc PTX architecture cap for CUDA toolkit,"Fixes #48200 

CUDA 11.0 only supports < sm_80 (https://docs.nvidia.com/cuda/archive/11.0/nvrtc/#group__options)

Note: NVRTC documentation is not a reliable source to query supported architecture. Rule of thumb is that nvrtc supports the same set of arch for nvcc, so the best way to query that is something like `nvcc -h | grep -o ""compute_[0-9][0-9]"" | sort | uniq`",pytorch
48522,gmagogsfm,pr,2020-11-27T21:42:58Z,Gmagogsfm/frontend switch,"Fixes #{issue number}
",pytorch
48538,gmagogsfm,pr,2020-11-28T20:36:58Z,Bind all of frontend,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48538 Bind all of frontend**
* #48333 Implement JIT serialization of ProcessGroup

",pytorch
48542,gmagogsfm,pr,2020-11-28T21:53:13Z,[CI-ALL]Implement JIT serialization of ProcessGroup,"ghstack-source-id: 32b2bc5a703125ce89f7146d6084ffee2f98fa1d
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48333

Fixes #{issue number}
",pytorch
48544,gmagogsfm,pr,2020-11-28T22:07:32Z,Implement JIT serialization of ProcessGroup,"This diff enables JIT serialization of `ProcessGroup`, including both base `ProcessGroup` class and derived classes like `ProcessGroupNCCL`.

If a `ProcessGroup` is created via high-level APIs like `dist_c10d.frontend().new_process_group_helper()`, they are automatically serializable. If a `ProcessGroup` is created via its derived class TorchBind APIs like `dist_c10d.ProcessGroupNCCL()`, then it has to be given a name and registered with `dist_c10d.frontend().register_process_group_name` to be uniquely identifiable and serializable. 

* Fixed a minor bug in new dist_c10d frontend which fails to check whether a process group is used or not
* Fixed an issue where `test_jit_c10d.py` wasn't really run due to a configuration bug. Now tests are run as a slow test (need ci-all/* branch)
",pytorch
48604,gmagogsfm,pr,2020-11-30T19:23:15Z,Gmagogsfm/frotend switch 2,"Fixes #{issue number}
",pytorch
48720,robieta,pr,2020-12-02T16:40:02Z,"Revert ""Revert D25199264: Enable callgrind collection for C++ snippets""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48720 Revert ""Revert D25199264: Enable callgrind collection for C++ snippets""**

This reverts commit 6646ff122d3215b77909f669fc26cf6a927030db.

Differential Revision: [D25273994](https://our.internmc.facebook.com/intern/diff/D25273994)",pytorch
48723,jeffdaily,pr,2020-12-02T16:51:59Z,[reland] [ROCm] remove versions less than 3.8,First attempt to land #48118 failed due to the problem fixed by #48722.,pytorch
48726,robieta,pr,2020-12-02T17:24:25Z,[DO_NOT_SUBMIT] Slow tests for C++ Timer support,,pytorch
48776,robieta,pr,2020-12-03T08:11:06Z,Optimize dispatch for non `__torch_function__` calls,"This PR adds several optimizations:
1) ~~Define `torch._C._all_tensors` method which is much faster than `all(type(t) is Tensor for t in tensors)` (50 ns vs. 250 ns / element), and roll the fast path bailout into `has_torch_function`~~

2) ~~Define `torch._C._is_tensor` method to expose `THPVariable_CheckExact` in Python. This is marginally faster than `type(t) is Tensor` (50 ns vs 85 ns), and it's worth keeping `not _is_tensor(t) and has_torch_function((t,))` because it lets us skip a function call and tuple construction, which works out to about 50 ns savings over just `has_torch_function`. (And there are a lot of unary functions.)~~

3) ~~Reorder the checks in `torch.functional` and `torch.nn.functional`, as in most cases `torch.jit.is_scripting()` is False (and we are more overhead sensitive in eager), so for the eager fast path we can save >100 ns by checking that after.~~

4) Stop importing `torch.overrides` inside functions now that the circular import is gone.

**EDIT: See below for updated changes.**

This PR is pretty big, but also mostly mechanical changes.

The test script assumes two envs: `ref` which is built from master, and `fast_torch_fn_check` which is built from this PR. It's really hard to get stable overhead measurements, so I went for the slightly more complicated but much more precise ""throw compute at it"" approach. This PR covers several dozen ops, so I selected one of each code path (e.g. `has_torch_function(tensors)`, `not _is_tensor(input) and has_torch_function((input,))`, etc.) and annotated them appropriately.

<details>

<summary> test.py </summary>

```
import argparse
import multiprocessing
import multiprocessing.dummy
import os
import pickle
import queue
import random
import sys
import subprocess
import tempfile
import time

import torch
from torch.utils.benchmark import Timer, Compare


NUM_CORES = multiprocessing.cpu_count()
ENVS = (
    ""ref"",
    ""fast_torch_fn_check""
)


MIN_RUN_TIME = 1
REPLICATES = 300
SETUP = """"""
x = torch.ones((1, 1))
y = torch.ones((1, 1))
""""""

TASKS = {
    ""tensor.py: _wrap_type_error_to_not_implemented `__floordiv__`"": ""x // y"",
    ""tensor.py: method          `__hash__`"": ""hash(x)"",
    ""functional.py: (unary)     `unique`"": ""torch.functional.unique(x)"",
    ""functional.py: (args)      `atleast_1d`"": ""torch.functional.atleast_1d((x, y))"",
    ""nn/functional.py: (unary)  `relu`"": ""torch.nn.functional.relu(x)"",
    ""nn/functional.py: (args)   `linear`"": ""torch.nn.functional.linear(x, y)"",
}


def worker_main(argv):
    parser = argparse.ArgumentParser()
    parser.add_argument(""--output_file"", type=str)
    output_file = parser.parse_args(argv).output_file

    env = os.path.split(os.getenv(""CONDA_PREFIX""))[1]
    assert env in ENVS

    results = []
    for i, stmt in enumerate(TASKS.values()):
        timer = Timer(
            stmt=stmt,
            setup=SETUP,
            env=env,
            sub_label="" "",
            description=f""[{i}]"",
        )
        results.append(timer.blocked_autorange(min_run_time=MIN_RUN_TIME))

    with open(output_file, ""wb"") as f:
        pickle.dump(results, f)


def main():
    num_workers = int(NUM_CORES // 2)
    tasks = list(ENVS * REPLICATES)
    random.shuffle(tasks)
    task_queue = queue.Queue()
    for t in tasks:
        task_queue.put(t)
    results = []

    def map_fn(worker_id):
        core = str(worker_id * 2)
        _, output_file = tempfile.mkstemp(suffix="".pkl"")
        try:
            while True:
                try:
                    env = task_queue.get_nowait()
                except queue.Empty:
                    break

                subprocess.run(
                    "" "".join([
                        ""source"", ""activate"", env, ""&&"",
                        ""taskset"", ""--cpu-list"", core,
                        ""python"", os.path.abspath(__file__),
                        ""--mode"", ""worker"",
                        ""--output_file"", output_file
                    ]),
                    shell=True,
                    check=True,
                )

                # We don't need a lock, as the GIL is enough.
                with open(output_file, ""rb"") as f:
                    results.extend(pickle.load(f))
        finally:
            os.remove(output_file)

    with multiprocessing.dummy.Pool(num_workers) as pool:
        st, eta, n_total = time.time(), """", len(tasks) * len(TASKS)
        map_job = pool.map_async(map_fn, range(num_workers))
        while not map_job.ready():
            n_complete = len(results)
            if n_complete:
                sec_per_element = (time.time() - st) / n_complete
                n_remaining = n_total - n_complete
                eta = f""ETA: {n_remaining * sec_per_element:.0f} sec""

            print(f""\r{n_complete} / {n_total}   {eta}"".ljust(40), end="""")
            sys.stdout.flush()
            time.sleep(2)
        print()

    compare = Compare(results)
    compare.trim_significant_figures()
    compare.colorize()
    compare.print()
    for i, k in enumerate(TASKS.keys()):
        print(f""[{i}] {k}"")
    print()


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--mode"", type=str, choices=(""main"", ""worker""), default=""main"")
    args, remaining = parser.parse_known_args()

    if args.mode == ""main"":
        assert not remaining
        main()
    else:
        worker_main(remaining)

```

</details>

<img width=""676"" alt=""Screen Shot 2020-12-03 at 12 04 09 AM"" src=""https://user-images.githubusercontent.com/13089297/100981059-1cd3a480-34fb-11eb-89c1-78d73f7700f2.png"">

CC @ezyang @bhosmer @xwang233 @mcarilli @mruberry ",pytorch
48791,jeffdaily,pr,2020-12-03T16:56:19Z,[ROCm] add 3.10 docker image,Add a ROCm 3.10 docker image for CI.  Keep the 3.9 image and remove the 3.8 image.  Plan is to keep two ROCm versions at a time.,pytorch
48797,zasdfgbnm,pr,2020-12-03T19:01:26Z,"Reland ""Add test for empty tensors for batch matmuls""","This reverts commit c7746adbc6e6ace9d4c2b54e32c8d36a7b7b0e31.

Fixes #{issue number}
",pytorch
48801,zasdfgbnm,pr,2020-12-03T20:36:43Z,Enable BF16 for indexing on CUDA,"Fixes #{issue number}
",pytorch
48805,zasdfgbnm,pr,2020-12-03T21:27:34Z,Reenable some BF16 tests on CUDA,"Fixes #{issue number}
",pytorch
48806,zasdfgbnm,pr,2020-12-03T21:27:46Z,CUDA BF16 norm,"Fixes #{issue number}
",pytorch
48807,zasdfgbnm,pr,2020-12-03T21:28:01Z,CUDA BF16 sparse,"Fixes #{issue number}
",pytorch
48809,zasdfgbnm,pr,2020-12-03T22:39:05Z,CUDA BF16 backwards,"Looks like there's no test?
",pytorch
48810,zasdfgbnm,pr,2020-12-03T22:43:18Z,Kill AT_SKIP_BFLOAT16_IF_NOT_ROCM,"Dependency:
https://github.com/pytorch/pytorch/pull/48809 https://github.com/pytorch/pytorch/pull/48807 https://github.com/pytorch/pytorch/pull/48806 https://github.com/pytorch/pytorch/pull/48805 https://github.com/pytorch/pytorch/pull/48801 https://github.com/pytorch/pytorch/pull/44994 https://github.com/pytorch/pytorch/pull/44848",pytorch
48866,jeffdaily,pr,2020-12-04T22:19:04Z,[ROCm] add 3.10 to nightly builds,Depends on https://github.com/pytorch/builder/pull/603.,pytorch
48963,robieta,pr,2020-12-07T23:35:11Z,treat Parameter the same way as Tensor,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48966 Treat has_torch_function and object_has_torch_function as static False when scripting
* #48965 move has_torch_function to C++, and make a special case object_has_torch_function
* #48964 clean up imports for tensor.py
* **#48963 treat Parameter the same way as Tensor**

This PR makes the binding code treat `Parameter` the same way as `Tensor`, unlike all other `Tensor` subclasses. This does change the semantics of `THPVariable_CheckExact`, but it isn't used much and it seemed to make sense for the half dozen or so places that it is used.

Test plan: Existing unit tests. Benchmarks are in #48966

Differential Revision: [D25590733](https://our.internmc.facebook.com/intern/diff/D25590733)",pytorch
48964,robieta,pr,2020-12-07T23:35:25Z,clean up imports for tensor.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48966 Treat has_torch_function and object_has_torch_function as static False when scripting
* #48965 move has_torch_function to C++, and make a special case object_has_torch_function
* **#48964 clean up imports for tensor.py**
* #48963 treat Parameter the same way as Tensor

Stop importing overrides within methods now that the circular dependency is gone, and also organize the imports while I'm at it because they're a jumbled mess.

Test plan: Existing unit tests. Benchmarks are in #48966

Differential Revision: [D25590730](https://our.internmc.facebook.com/intern/diff/D25590730)",pytorch
48965,robieta,pr,2020-12-07T23:35:37Z,"move has_torch_function to C++, and make a special case object_has_torch_function","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #48966 Treat has_torch_function and object_has_torch_function as static False when scripting
* **#48965 move has_torch_function to C++, and make a special case object_has_torch_function**
* #48964 clean up imports for tensor.py
* #48963 treat Parameter the same way as Tensor

This PR pulls `__torch_function__` checking entirely into C++, and adds a special `object_has_torch_function` method for ops which only have one arg as this lets us skip tuple construction and unpacking. We can now also do away with the Python side fast bailout for `Tensor` (e.g. `if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors)`) because they're actually slower than checking with the Python C API.

Test plan: Existing unit tests. Benchmarks are in #48966

Differential Revision: [D25590732](https://our.internmc.facebook.com/intern/diff/D25590732)",pytorch
48966,robieta,pr,2020-12-07T23:35:51Z,Treat has_torch_function and object_has_torch_function as static False when scripting,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#48966 Treat has_torch_function and object_has_torch_function as static False when scripting**
* #48965 move has_torch_function to C++, and make a special case object_has_torch_function
* #48964 clean up imports for tensor.py
* #48963 treat Parameter the same way as Tensor

This PR lets us skip the `if not torch.jit.is_scripting():` guards on `functional` and `nn.functional` by directly registering `has_torch_function` and `object_has_torch_function` to the JIT as statically False.

**Benchmarks**

The benchmark script is kind of long. The reason is that it's testing all four PRs in the stack, plus threading and subprocessing so that the benchmark can utilize multiple cores while still collecting good numbers. Both wall times and instruction counts were collected. This stack changes dozens of operators / functions, but very mechanically such that there are only a handful of codepath changes. Each row is a slightly different code path (e.g. testing in Python, testing in the arg parser, different input types, etc.)

<details>

<summary> Test script </summary>

```
import argparse
import multiprocessing
import multiprocessing.dummy
import os
import pickle
import queue
import random
import sys
import subprocess
import tempfile
import time

import torch
from torch.utils.benchmark import Timer, Compare, Measurement


NUM_CORES = multiprocessing.cpu_count()
ENVS = {
    ""ref"": ""HEAD (current)"",
    ""torch_fn_overhead_stack_0"": ""#48963"",
    ""torch_fn_overhead_stack_1"": ""#48964"",
    ""torch_fn_overhead_stack_2"": ""#48965"",
    ""torch_fn_overhead_stack_3"": ""#48966"",
}

CALLGRIND_ENVS = tuple(ENVS.keys())


MIN_RUN_TIME = 3
REPLICATES = {
    ""longer"": 1_000,
    ""long"": 300,
    ""short"": 50,
}

CALLGRIND_NUMBER = {
    ""overnight"": 500_000,
    ""long"": 250_000,
    ""short"": 10_000,
}

CALLGRIND_TIMEOUT = {
    ""overnight"": 800,
    ""long"": 400,
    ""short"": 100,
}

SETUP = """"""
    x = torch.ones((1, 1))
    y = torch.ones((1, 1))
    w_tensor = torch.ones((1, 1), requires_grad=True)
    linear = torch.nn.Linear(1, 1, bias=False)
    linear_w = linear.weight
""""""

TASKS = {
    ""C++: unary                 `.t()`"": ""w_tensor.t()"",
    ""C++: unary  (Parameter)    `.t()`"": ""linear_w.t()"",
    ""C++: binary (Parameter)    `mul` "": ""x + linear_w"",
    ""tensor.py: _wrap_type_error_to_not_implemented `__floordiv__`"": ""x // y"",
    ""tensor.py: method          `__hash__`"": ""hash(x)"",
    ""Python scalar              `__rsub__`"": ""1 - x"",
    ""functional.py: (unary)     `unique`"": ""torch.functional.unique(x)"",
    ""functional.py: (args)      `atleast_1d`"": ""torch.functional.atleast_1d((x, y))"",
    ""nn/functional.py: (unary)  `relu`"": ""torch.nn.functional.relu(x)"",
    ""nn/functional.py: (args)   `linear`"": ""torch.nn.functional.linear(x, w_tensor)"",
    ""nn/functional.py: (args)   `linear (Parameter)`"": ""torch.nn.functional.linear(x, linear_w)"",
    ""Linear(..., bias=False)"": ""linear(x)"",
}


def _worker_main(argv, fn):
    parser = argparse.ArgumentParser()
    parser.add_argument(""--output_file"", type=str)
    parser.add_argument(""--single_task"", type=int, default=None)
    parser.add_argument(""--length"", type=str)
    args = parser.parse_args(argv)
    single_task = args.single_task

    conda_prefix = os.getenv(""CONDA_PREFIX"")
    assert torch.__file__.startswith(conda_prefix)

    env = os.path.split(conda_prefix)[1]
    assert env in ENVS

    results = []
    for i, (k, stmt) in enumerate(TASKS.items()):
        if single_task is not None and single_task != i:
            continue

        timer = Timer(
            stmt=stmt,
            setup=SETUP,
            sub_label=k,
            description=ENVS[env],
        )
        results.append(fn(timer, args.length))

    with open(args.output_file, ""wb"") as f:
        pickle.dump(results, f)


def worker_main(argv):
    _worker_main(
        argv,
        lambda timer, _: timer.blocked_autorange(min_run_time=MIN_RUN_TIME)
    )


def callgrind_worker_main(argv):
    _worker_main(
        argv,
        lambda timer, length: timer.collect_callgrind(number=CALLGRIND_NUMBER[length], collect_baseline=False))


def main(argv):
    parser = argparse.ArgumentParser()
    parser.add_argument(""--long"", action=""store_true"")
    parser.add_argument(""--longer"", action=""store_true"")
    args = parser.parse_args(argv)

    if args.longer:
        length = ""longer""
    elif args.long:
        length = ""long""
    else:
        length = ""short""
    replicates = REPLICATES[length]

    num_workers = int(NUM_CORES // 2)
    tasks = list(ENVS.keys()) * replicates
    random.shuffle(tasks)
    task_queue = queue.Queue()
    for _ in range(replicates):
        envs = list(ENVS.keys())
        random.shuffle(envs)
        for e in envs:
            task_queue.put((e, None))

    callgrind_task_queue = queue.Queue()
    for e in CALLGRIND_ENVS:
        for i, _ in enumerate(TASKS):
            callgrind_task_queue.put((e, i))

    results = []
    callgrind_results = []

    def map_fn(worker_id):
        # Adjacent cores often share cache and maxing out a machine can distort
        # timings so we space them out.
        callgrind_cores = f""{worker_id * 2}-{worker_id * 2 + 1}""
        time_cores = str(worker_id * 2)
        _, output_file = tempfile.mkstemp(suffix="".pkl"")
        try:
            loop_tasks = (
                # Callgrind is long running, and then the workers can help with
                # timing after they finish collecting counts.
                (callgrind_task_queue, callgrind_results, ""callgrind_worker"", callgrind_cores, CALLGRIND_TIMEOUT[length]),
                (task_queue, results, ""worker"", time_cores, None))

            for queue_i, results_i, mode_i, cores, timeout in loop_tasks:
                while True:
                    try:
                        env, task_i = queue_i.get_nowait()
                    except queue.Empty:
                        break

                    remaining_attempts = 3
                    while True:
                        try:
                            subprocess.run(
                                "" "".join([
                                    ""source"", ""activate"", env, ""&&"",
                                    ""taskset"", ""--cpu-list"", cores,
                                    ""python"", os.path.abspath(__file__),
                                    ""--mode"", mode_i,
                                    ""--length"", length,
                                    ""--output_file"", output_file
                                ] + ([] if task_i is None else [""--single_task"", str(task_i)])),
                                shell=True,
                                check=True,
                                timeout=timeout,
                            )
                            break

                        except subprocess.TimeoutExpired:
                            # Sometimes Valgrind will hang if there are too many
                            # concurrent runs.
                            remaining_attempts -= 1
                            if not remaining_attempts:
                                print(""Too many failed attempts."")
                                raise
                            print(f""Timeout after {timeout} sec. Retrying."")

                    # We don't need a lock, as the GIL is enough.
                    with open(output_file, ""rb"") as f:
                        results_i.extend(pickle.load(f))

        finally:
            os.remove(output_file)

    with multiprocessing.dummy.Pool(num_workers) as pool:
        st, st_estimate, eta, n_total = time.time(), None, """", len(tasks) * len(TASKS)
        map_job = pool.map_async(map_fn, range(num_workers))
        while not map_job.ready():
            n_complete = len(results)
            if n_complete and len(callgrind_results):
                if st_estimate is None:
                    st_estimate = time.time()
                else:
                    sec_per_element = (time.time() - st_estimate) / n_complete
                    n_remaining = n_total - n_complete
                    eta = f""ETA: {n_remaining * sec_per_element:.0f} sec""

            print(
                f""\r{n_complete} / {n_total}  ""
                f""({len(callgrind_results)} / {len(CALLGRIND_ENVS) * len(TASKS)})   ""
                f""{eta}"".ljust(40), end="""")
            sys.stdout.flush()
            time.sleep(2)
    total_time = int(time.time() - st)
    print(f""\nTotal time: {int(total_time // 60)} min, {total_time % 60} sec"")

    desc_to_ind = {k: i for i, k in enumerate(ENVS.values())}
    results.sort(key=lambda r: desc_to_ind[r.description])

    # TODO: Compare should be richer and more modular.
    compare = Compare(results)
    compare.trim_significant_figures()
    compare.colorize(rowwise=True)

    # Manually add master vs. overall relative delta t.
    merged_results = {
        (r.description, r.sub_label): r
        for r in Measurement.merge(results)
    }

    cmp_lines = str(compare).splitlines(False)
    print(cmp_lines[0][:-1] + ""-"" * 15 + ""]"")
    print(f""{cmp_lines[1]} |{'':>10}\u0394t"")
    print(cmp_lines[2] + ""-"" * 15)
    for l, t in zip(cmp_lines[3:3 + len(TASKS)], TASKS.keys()):
        assert l.strip().startswith(t)
        t0 = merged_results[(ENVS[""ref""], t)].median
        t1 = merged_results[(ENVS[""torch_fn_overhead_stack_3""], t)].median
        print(f""{l} |{'':>5}{(t1 / t0 - 1) * 100:>6.1f}%"")
    print(""\n"".join(cmp_lines[3 + len(TASKS):]))


    counts_dict = {
        (r.task_spec.description, r.task_spec.sub_label): r.counts(denoise=True)
        for r in callgrind_results
    }

    def rel_diff(x, x0):
        return f""{(x / x0 - 1) * 100:>6.1f}%""

    task_pad = max(len(t) for t in TASKS)
    print(f""\n\nInstruction % change (relative to `{CALLGRIND_ENVS[0]}`)"")
    print("" "" * (task_pad + 8)  + ("" "" * 7).join([ENVS[env] for env in CALLGRIND_ENVS[1:]]))
    for t in TASKS:
        values = [counts_dict[(ENVS[env], t)] for env in CALLGRIND_ENVS]

        print(t.ljust(task_pad + 3) + ""  "".join([
            rel_diff(v, values[0]).rjust(len(ENVS[env]) + 5)
            for v, env in zip(values[1:], CALLGRIND_ENVS[1:])]))

        print(""\033[4m"" + ""    Instructions per invocation"".ljust(task_pad + 3) + ""  "".join([
            f""{v // CALLGRIND_NUMBER[length]:.0f}"".rjust(len(ENVS[env]) + 5)
            for v, env in zip(values[1:], CALLGRIND_ENVS[1:])]) + ""\033[0m"")
        print()

    import pdb
    pdb.set_trace()


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--mode"", type=str, choices=(""main"", ""worker"", ""callgrind_worker""), default=""main"")
    args, remaining = parser.parse_known_args()

    if args.mode == ""main"":
        main(remaining)

    elif args.mode == ""callgrind_worker"":
        callgrind_worker_main(remaining)

    else:
        worker_main(remaining)

```

</details>

**Wall time**
<img width=""1178"" alt=""Screen Shot 2020-12-12 at 12 28 13 PM"" src=""https://user-images.githubusercontent.com/13089297/101994419-284f6a00-3c77-11eb-8dc8-4f69a890302e.png"">

<details>

<summary> Longer run (`python test.py --long`) is basically identical. </summary>

<img width=""1184"" alt=""Screen Shot 2020-12-12 at 5 02 47 PM"" src=""https://user-images.githubusercontent.com/13089297/102000425-2350e180-3c9c-11eb-999e-a95b37e9ef54.png"">

</details>

**Callgrind**
<img width=""936"" alt=""Screen Shot 2020-12-12 at 12 28 54 PM"" src=""https://user-images.githubusercontent.com/13089297/101994421-2e454b00-3c77-11eb-9cd3-8cde550f536e.png"">

Test plan: existing unit tests.

Differential Revision: [D25590731](https://our.internmc.facebook.com/intern/diff/D25590731)",pytorch
49011,z-a-f,pr,2020-12-08T10:43:57Z,[quant] Add reflection padding to conv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49011 [quant] Add reflection padding to conv**

Differential Revision: [D25394384](https://our.internmc.facebook.com/intern/diff/D25394384)",pytorch
49109,zasdfgbnm,pr,2020-12-09T20:50:27Z,Refactor cudnn convolution,"cuDNN v7 API has been deprecated, so we need to migrate to cuDNN v8 API. The v8 API does not exist on cuDNN 7, so there will be a long time both API should exist.

This is step 0 of adding cuDNN v8 API. There is no real code change in this PR. It just copy-pastes existing code. The original `Conv.cpp` is split into `ConvPlaceholders.cpp`, `ConvShared.cpp`, `ConvShared.h`, `Conv_v7.cpp`, `Conv_v8.cpp`. Currently `Conv_v8.cpp` is empty, and will be filled in the future.

The `ConvPlaceholders.cpp` contains placeholder implementation of cudnn convolution when cudnn is not enabled. These operators only raise errors and do no real computation. This file also contains deprecated operators. These operators are implemented using current operators.

The `ConvShared.cpp` and `ConvShared.h` contains code that will be shared by the v7 and v8 API, these include the definition of struct `ConvolutionParams` and `ConvolutionArgs`. As well as ATen exposed API like `cudnn_convolution` and intermediate `cudnn_convolution_forward`. These exposed functions will call raw API like `raw_cudnn_convolution_forward_out` in `Conv_v7.cpp` or `Conv_v8.cpp` for the real implementation.

The `Conv_v7.cpp`, `Conv_v8.cpp` contains the implementation of raw APIs, and are different for v7 and v8.",pytorch
49274,zasdfgbnm,pr,2020-12-12T01:11:20Z,Bugfix nightly checkout tool to work on Windows,"I am submitting this PR on behalf of Janne Hellsten(@nurpax) from @NVIDIA, for the convenience of CLA. Thanks Janne a lot for the contribution!

This fixes the bug when running `
./tools/nightly.py checkout -b my-nightly-branch` on windows. Before this fix, this command gets the following error on Windows.

```
ERROR:root:Fatal exception
Traceback (most recent call last):
  File ""./tools/nightly.py"", line 166, in logging_manager
    yield root_logger
  File ""./tools/nightly.py"", line 644, in main
    install(
  File ""./tools/nightly.py"", line 552, in install
    spdir = _site_packages(pytdir.name, platform)
  File ""./tools/nightly.py"", line 325, in _site_packages
    os.path.join(pytdir.name, ""Lib"", ""site-packages"")
NameError: name 'pytdir' is not defined
log file: d:\pytorch\nightly\log\2020-12-11_16h10m14s_6867a21e-3c0e-11eb-878e-04ed3363a33e\nightly.log
```",pytorch
49313,gmagogsfm,pr,2020-12-14T06:39:04Z,Add flag torch_jit_disable_warning_prints to allow disabling all warnings.warn,"Adding a flag `torch_jit_disable_warning_prints` to optimize interpreter performance by suppressing (potentially large amount) of warnings.warn.

This is to work around TorchScript's warning behavior mismatch with Python. Python by default triggers a warning once per location but TorchScript doesn't support it. This causes same warning to trigger and print once per inference run, hurting performance.",pytorch
49344,zasdfgbnm,pr,2020-12-14T18:53:04Z,Fix CUDA extension ninja build,"I am submitting this PR on behalf of Janne Hellsten(@nurpax) from @NVIDIA, for the convenience of CLA. Thanks Janne a lot for the contribution!

Currently, the ninja build decides whether to rebuild a .cu file or not pretty randomly. And there are actually two issues:

First, the arch list in the building command is ordered randomly. When the order changes, it will unconditionally rebuild regardless of the timestamp.

Second, the header files are not included in the dependency list, so if the header file changes, it is possible that ninja will not rebuild.

This PR fixes both issues. The fix for the second issue requires nvcc >= 10.2. nvcc < 10.2 can still build CUDA extension as it used to be, but it will be unable to see the changes in header files.",pytorch
49356,zasdfgbnm,pr,2020-12-14T21:06:56Z,Add BFloat16 support for isinf and isfinite,"Also fix some tests.
",pytorch
49397,gmagogsfm,pr,2020-12-15T07:10:42Z,"Add norm(dim, p) overload",,pytorch
49430,jjsjann123,pr,2020-12-15T21:35:55Z,fixing autodiff to support Optional[Tensor] on inputs,"This PR fixes two local issue for me:

1. Assert failure when passing `None` to `Optional[Tensor]` input that requires gradient in autodiff
2. Wrong vjp mapping on inputs when `requires_grad` flag changes on inputs stack.

This PR is to support autodiff on layer_norm.",pytorch
49591,robieta,pr,2020-12-18T04:52:41Z,Remove deadlines for Caffe2 hypothesis_test when running on GPU.,"Summary:
A bunch of these tests are marked flaky, and have been since time immemorial. (Read: as far back as Buck will build.) However closer inspection reveals that they fail if and only if run on a GPU worker. What seems to be going on is that there are more jobs than GPUs, so the contention causes waits which registers as timeouts on the test.

This diff is kind of hacky, but it basically just drops deadlines if a GPU is present. Because Caffe2 is going away I'm not too terribly concerned about a beautiful solution, but we may as well keep some test coverage if it's easy.

CC Sebastian, Ilia, Min, and Hongzheng who also have tasks for what seems to be the same flakiness.

Test Plan: Turn the tests back on and see if they fall over. (The failure repros reliably on an OnDemand GPU and is fixed by this change, so it's not really just a hail Mary.)

Reviewed By: ngimel

Differential Revision: D25632981

",pytorch
49624,jeffdaily,pr,2020-12-19T00:34:07Z,[ROCm] enable kernel asserts,Addresses missing ROCm feature indicated in #38943.,pytorch
49632,jeffdaily,pr,2020-12-19T04:04:23Z,[ROCm] add 4.0 to nightly builds,Depends on https://github.com/pytorch/builder/pull/614.,pytorch
49670,z-a-f,pr,2020-12-21T08:50:23Z,[quant][feature] Adding an ability to skip immediate observation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #49671 [quant] Quantizable LSTM
* **#49670 [quant][feature] Adding an ability to skip immediate observation**

Differential Revision: [D25663851](https://our.internmc.facebook.com/intern/diff/D25663851)",pytorch
49671,z-a-f,pr,2020-12-21T08:50:31Z,[quant] Quantizable LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #49866 [quant] Quantizable MultiheadAttention
* **#49671 [quant] Quantizable LSTM**

- Introduces the `torch.nn.quantizable` namespace
- Adds the `torch.nn.quantizable.LSTM` module

The point of the `quantizable` namespace is to segregate the purely quantized modules with the modules that could be quantized through a normal quantization flow, but are not using the quantized kernels explicitly.
That means the quantizable modules are functionally and numerically equivalent to the FP ones and can be used instead of the FP ones without any loss.

The main difference between the `torch.nn.LSTM` and the `torch.nn.quantizable.LSTM` is that the former one does not support observation for the linear layers, because all the computation is internal to the `aten` namespace.
The `torch.nn.quantizable.LSTM`, however, uses explicit linear layers that can be observed for further quantization.

Test Plan:

```
python test/test_quantization.py TestQuantizedOps.test_custom_module_lstm
```

Differential Revision: [D25663870](https://our.internmc.facebook.com/intern/diff/D25663870)",pytorch
49839,ppwwyyxx,pr,2020-12-24T21:27:08Z,[pytorch] [WIP] traceable advanced indexing,"Differential Revision: D25701773

",pytorch
49853,ppwwyyxx,pr,2020-12-25T13:05:47Z,[pytorch] make is_tracing scriptable,"Summary: fix https://github.com/pytorch/pytorch/issues/47379

Test Plan: buck test mode/dev-nosan //caffe2/test:jit -- 'test_script_is_tracing'

Differential Revision: D25704315

",pytorch
49866,z-a-f,pr,2020-12-26T10:14:49Z,[quant] Quantizable MultiheadAttention,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51169 [quant][refactor] Factor out MHA code dup from nn and nn.quantizable
* #50459 [quant] Factoring out the list of no_observers
* **#49866 [quant] Quantizable MultiheadAttention**

- Adds the `torch.nn.quantizable.MultiheadAttention`

The quantizable version can serve as a fully equivalent to `torch.nn.MultiheadAttention` module.
The main difference is that it allows for linear units observation after the `prepare` step in the quantization flow.

Note: The `from_observed` method (called during the `convert`) removes the `bias_k` and `bias_v` parameters, and resets them as attributes.
This is done to avoid an error of assigning a quantized tensor to the `torch.nn.Parameter`.

Test Plan:

```
python test/test_quantization.py TestQuantizedOps.test_custom_module_multi_head_attention
```

Differential Revision: [D25706179](https://our.internmc.facebook.com/intern/diff/D25706179)",pytorch
49934,ppwwyyxx,pr,2020-12-29T19:33:50Z,Update update_s3_htmls.yml,"It is now running for forks, and generates a lot of failure message to owner of forks.
",pytorch
49964,z-a-f,pr,2020-12-30T09:07:31Z,[quant] Mapping for the `_LinearWithBias`,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49964 [quant] Mapping for the `_LinearWithBias`**

`torch.nn.modules.linear._LinearWithBias` is only used in the transformers, and is completely identical to the `torch.nn.Linear`.
This PR creates a mapping so that this module would be treated the same as the Linear.

Test Plan:

```
python test/test_quantization.py TestDynamicQuantizedModule TestStaticQuantizedModule
```

Differential Revision: [D25731589](https://our.internmc.facebook.com/intern/diff/D25731589)",pytorch
49965,z-a-f,pr,2020-12-30T09:14:57Z,[quant] Backend string for the quantized types,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#49965 [quant] Backend string for the quantized types**

Without this checking the type of the quantized tensor using `type` would throw an error.

After this PR running the `type(qx)`, where `qx` is a quantized tensor would show something like `torch.quantized.QUInt8`.

Test Plan: Not needed -- this is just a string description for the quantized tensors

Differential Revision: [D25731594](https://our.internmc.facebook.com/intern/diff/D25731594)",pytorch
49967,ppwwyyxx,pr,2020-12-30T12:23:47Z,Support scripting classmethod called with object instances,"Currentlt classmethods are compiled the same way as methods - the first argument is self.
Adding a fake statement to assign the first argument to the class.
This is kind of hacky, but that's all it takes.",pytorch
50132,z-a-f,pr,2021-01-06T07:53:04Z,Adding MyPy daemon status file to gitignore,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50132 Adding MyPy daemon status file to gitignore**

When running mypy command using `dmypy run`, it creates a status file.
This PR adds the file to the ignore list.

Differential Revision: [D25834504](https://our.internmc.facebook.com/intern/diff/D25834504)",pytorch
50223,jeffdaily,pr,2021-01-07T20:26:39Z,caffe2 test.sh pip might not need sudo if pip is root,"Update logic in MAYBE_SUDO check. Assumption was incorrect that if pip
was installed as user then sudo is needed. pip could be installed as
root and run as root. Assumption was initially pip was root and user was
non root.
",pytorch
50227,zasdfgbnm,pr,2021-01-07T21:13:53Z,Enable skipped test for c10::complex on CUDA >= 11.2,"That test was skipped due to a compiler bug. That bug should be fixed in 11.2, so we should enable it.",pytorch
50250,zasdfgbnm,pr,2021-01-08T02:41:18Z,Kill non-bool version of maskedXXX from TH and THC,"dependency: https://github.com/pytorch/pytorch/pull/47951

Now we can easily view a byte tensor as a bool tensor at no cost, there is no reason to keep a separate kernel for these non-bool versions of masked operations.

My only concern:
Is it safe to assume `sizeof(bool) == 1` is always true? Or is PyTorch already assuming this? I checked with godbot on a few different compilers on a few different platforms (arm, avr, x86, powerpc64e, risc-v), seem that this assumption is safe.",pytorch
50297,z-a-f,pr,2021-01-08T22:59:52Z,[quant][bug] Fixing the mapping getter to return a copy,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #50304 [quant][refactor] Minor refactor of some typos
* **#50297 [quant][bug] Fixing the mapping getter to return a copy**

Current implementation has a potential bug: if a user modifies the quantization mappings returned by the getters, the changes will propagate.
For example, the bug will manifest itself if the user does the following:

```
my_mapping = get_default_static_quant_module_mappings()
my_mapping[nn.Linear] = UserLinearImplementation
model_A = convert(model_A, mapping=my_mapping)

default_mapping = get_default_static_quant_module_mappings()
model_B = convert(model_B, mapping=default_mapping)
```

In that case the `model_B` will be quantized with with the modified mapping.

Differential Revision: [D25855753](https://our.internmc.facebook.com/intern/diff/D25855753)",pytorch
50304,z-a-f,pr,2021-01-09T00:21:20Z,[quant][refactor] Minor refactor of some typos,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#50304 [quant][refactor] Minor refactor of some typos**
* #50297 [quant][bug] Fixing the mapping getter to return a copy

Does not include any functional changes -- purely for fixing minor typos in the `fuser_method_mappings.py`

Differential Revision: [D25857248](https://our.internmc.facebook.com/intern/diff/D25857248)",pytorch
50319,jjsjann123,pr,2021-01-09T06:22:01Z,patch nvrtc API for cuda TK >= 11.1,"CUDA TK >= 11.1 provides ptxjitcompiler that emits SASS instead of PTX.
1. This gives better backward-compatibility that allows future TK to work with older driver, which might not necessarily be able to load generated PTX through JIT compile and would error out at runtime;
https://docs.nvidia.com/deploy/cuda-compatibility/#using-ptx
2. Meanwhile, SASS doesn't provide good future compatibility, so for unsupported arch, we fallback to PTX to support future device.
https://docs.nvidia.com/deploy/cuda-compatibility/index.html#cubin-compatibility",pytorch
50355,robieta,pr,2021-01-10T19:44:22Z,DO_NOT_SUBMIT: All CI for https://github.com/pytorch/pytorch/pull/48966,"N/A
",pytorch
50405,zasdfgbnm,pr,2021-01-12T00:20:13Z,Fix test_jit_cuda_archflags on machine with more than one arch,"This fixes the following flaky test on machine with gpus of different arch:
```
_________________________________________________________________________________________________________________ TestCppExtensionJIT.test_jit_cuda_archflags __________________________________________________________________________________________________________________

self = <test_cpp_extensions_jit.TestCppExtensionJIT testMethod=test_jit_cuda_archflags>

    @unittest.skipIf(not TEST_CUDA, ""CUDA not found"")
    @unittest.skipIf(TEST_ROCM, ""disabled on rocm"")
    def test_jit_cuda_archflags(self):
        # Test a number of combinations:
        #   - the default for the machine we're testing on
        #   - Separators, can be ';' (most common) or ' '
        #   - Architecture names
        #   - With/without '+PTX'
    
        capability = torch.cuda.get_device_capability()
        # expected values is length-2 tuple: (list of ELF, list of PTX)
        # note: there should not be more than one PTX value
        archflags = {
            '': (['{}{}'.format(capability[0], capability[1])], None),
            ""Maxwell+Tegra;6.1"": (['53', '61'], None),
            ""Pascal 3.5"": (['35', '60', '61'], None),
            ""Volta"": (['70'], ['70']),
        }
        if int(torch.version.cuda.split('.')[0]) >= 10:
            # CUDA 9 only supports compute capability <= 7.2
            archflags[""7.5+PTX""] = (['75'], ['75'])
            archflags[""5.0;6.0+PTX;7.0;7.5""] = (['50', '60', '70', '75'], ['60'])
    
        for flags, expected in archflags.items():
>           self._run_jit_cuda_archflags(flags, expected)

test_cpp_extensions_jit.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_cpp_extensions_jit.py:158: in _run_jit_cuda_archflags
    _check_cuobjdump_output(expected[0])
test_cpp_extensions_jit.py:134: in _check_cuobjdump_output
    self.assertEqual(actual_arches, expected_arches,
../../.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:1211: in assertEqual
    super().assertEqual(len(x), len(y), msg=self._get_assert_msg(msg, debug_msg=debug_msg))
E   AssertionError: 2 != 1 : Attempted to compare the lengths of [iterable] types: Expected: 2; Actual: 1.
E   Flags: ,  Actual: ['sm_75', 'sm_86'],  Expected: ['sm_86']
E   Stderr: 
E   Output: ELF file    1: cudaext_archflags.1.sm_75.cubin
E   ELF file    2: cudaext_archflags.2.sm_86.cubin

```",pytorch
50407,zasdfgbnm,pr,2021-01-12T00:59:34Z,Make test_dataloader.py compatible with pytest,"Currently when running with pytest, it fails with

```
__________________________________________________________________________________________________________________ ERROR at setup of test_worker_info_init_fn __________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_dataloader.py, line 670
  def test_worker_info_init_fn(worker_id):
E       fixture 'worker_id' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_dataloader.py:670
```
This is because pytest considers anything starting with test_ as a test, so I renamed it to _test_... to prevent this from happening.",pytorch
50408,zasdfgbnm,pr,2021-01-12T01:12:34Z,Make test_multiprocessing_spawn.py compatible with pytest,"This file is currently failing with

```
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 13
  def test_success_func(i):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:13
________________________________________________________________________________________________________________ ERROR at setup of test_success_single_arg_func ________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 17
  def test_success_single_arg_func(i, arg):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:17
_________________________________________________________________________________________________________________ ERROR at setup of test_exception_single_func _________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 22
  def test_exception_single_func(i, arg):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:22
__________________________________________________________________________________________________________________ ERROR at setup of test_exception_all_func ___________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 28
  def test_exception_all_func(i):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:28
_________________________________________________________________________________________________________________ ERROR at setup of test_terminate_signal_func _________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 33
  def test_terminate_signal_func(i):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:33
__________________________________________________________________________________________________________________ ERROR at setup of test_terminate_exit_func __________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 39
  def test_terminate_exit_func(i, arg):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:39
___________________________________________________________________________________________________________ ERROR at setup of test_success_first_then_exception_func ___________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 45
  def test_success_first_then_exception_func(i, arg):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:45
___________________________________________________________________________________________________________________ ERROR at setup of test_nested_child_body ___________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 52
  def test_nested_child_body(i, ready_queue, nested_child_sleep):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:52
_____________________________________________________________________________________________________________________ ERROR at setup of test_infinite_task _____________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 57
  def test_infinite_task(i):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:57
_____________________________________________________________________________________________________________________ ERROR at setup of test_process_exit ______________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 62
  def test_process_exit(idx):
E       fixture 'idx' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py:62
________________________________________________________________________________________________________________________ ERROR at setup of test_nested _________________________________________________________________________________________________________________________
file /home/gaoxiang/pytorch-tf32/test/test_multiprocessing_spawn.py, line 66
  def test_nested(i, pids_queue, nested_child_sleep, start_method):
E       fixture 'i' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, include_metadata_in_junit_xml, json_metadata, metadata, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.
```
when running with pytest. This is because pytest considers anything starting with `test_` as a test, so I renamed it to `_test_...` to prevent this from happening.",pytorch
50435,zasdfgbnm,pr,2021-01-12T17:28:07Z,Fix fft slow tests,"The failure is:
```
______________________________________________________________________________________________________ TestCommonCUDA.test_variant_consistency_jit_fft_rfft_cuda_float64 _______________________________________________________________________________________________________
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:889: in wrapper
    method(*args, **kwargs)
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:889: in wrapper
    method(*args, **kwargs)
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py:267: in instantiated_test
    if op is not None and op.should_skip(generic_cls.__name__, name,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f7375f9b550>, cls_name = 'TestCommon', test_name = 'test_variant_consistency_jit', device_type = 'cuda', dtype = torch.float64

    def should_skip(self, cls_name, test_name, device_type, dtype):
>       for si in self.skips:
E       TypeError: 'NoneType' object is not iterable

../.local/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:186: TypeError

```",pytorch
50440,zasdfgbnm,pr,2021-01-12T18:21:14Z,Fix TestOpInfoCUDA.test_unsupported_dtypes_addmm_cuda_bfloat16 on ampere,"The `TestOpInfoCUDA.test_unsupported_dtypes_addmm_cuda_bfloat16` in `test_ops.py` is failing on ampere. This is because addmm is supported on Ampere, but the test is asserting that it is not supported.
",pytorch
50442,zasdfgbnm,pr,2021-01-12T19:58:07Z,Enable BFloat support for gemms on arch other than ampere,"Fixes #{issue number}
",pytorch
50453,zasdfgbnm,pr,2021-01-13T01:36:11Z,Fix TF32 failures in test_linalg.py,"On Ampere GPU, matmuls are computed by default with TF32 when the dtype is `torch.float`:  https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices, which results in reduced precision in results. However, linear algebra usually need higher precision, therefore lots of tests in `test_linalg.py` are failing on Ampere GPU because of precision issue.

To fix this issue:
- Most linear algebra methods, except for matmuls, should add `NoTF32Guard`
- Expected results in unit tests should compute matmuls using numpy instead of pytorch cuda.",pytorch
50459,z-a-f,pr,2021-01-13T05:41:00Z,[quant] Factoring out the list of no_observers,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51169 [quant][refactor] Factor out MHA code dup from nn and nn.quantizable
* **#50459 [quant] Factoring out the list of no_observers**
* #49866 [quant] Quantizable MultiheadAttention

Some of the custom modules cannot have the observers be inserted automatically. This PR factors out that list into a separate function.

Test is not required, as it is covered by the unittests for those modules.

Differential Revision: [D26092531](https://our.internmc.facebook.com/intern/diff/D26092531)",pytorch
50467,jjsjann123,pr,2021-01-13T09:53:39Z,LayerNorm Support in autodiff:,"1. extend autodiff by adding entry for layer_norm in symbolic script, we now use native_layer_norm_backward
2. added backward function `layernorm_double_backward` for `native_layer_norm_backward`, preserves double backward support for LayerNorm in autodiff/ScriptModule
3. added python test to verify autodiff on layer_norm with various configuration of optional tensors; (verify the fix in #49430)

Co-authored-by: Ryan Spring <rspring@nvidia.com>
Co-authored-by: Jie <jiej@nvidia.com>",pytorch
50508,jeffdaily,pr,2021-01-13T23:05:29Z,[ROCm] warn unsupported PYTORCH_CUDA_FUSER_DISABLE_FMA,nvcc's `--fmad=false` is not valid for the HIP compiler.  Upcoming ROCm releases will start treating unrecognized compiler flags as an error.,pytorch
50509,zasdfgbnm,pr,2021-01-13T23:22:58Z,Fix test_dispatch.py when running with TORCH_SHOW_CPP_STACKTRACES=1,"`test_dispatch.py` has many asserts about the error message. When running with `TORCH_SHOW_CPP_STACKTRACES=1`, the error message is different from when `TORCH_SHOW_CPP_STACKTRACES=0`, which makes many tests in `test_dispatch.py` fail. This PR fixes these failures when running with `TORCH_SHOW_CPP_STACKTRACES=1`.
",pytorch
50550,zasdfgbnm,pr,2021-01-14T20:29:47Z,Fix nan compare for complex,"Currently, the complex number comparison is done by comparing real and imaginary separately. This is not always correct, because, for example, for `x = -inf+nanj`, on CUDA 11.2, `asin(x) = nan-infj` and on gcc 10.2, `asin(x) = nan+infj`. They are both correct and I don't think one is more correct than the other. However, this way of comparing causes two test failures on my machine:
```
FAILED test/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_reference_numerics_asin_cuda_complex128 - AssertionError: False is not true : Scalars failed to compare as equal! Comparing the imaginary part -inf and inf gives a difference of inf, but the allowed difference...
FAILED test/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_reference_numerics_asin_cuda_complex64 - AssertionError: False is not true : Scalars failed to compare as equal! Comparing the imaginary part -inf and inf gives a difference of inf, but the allowed difference ...
```
This PR fixes these failures by adding a `relaxed` mode for complex comparing. In the relax mode, as long as any of the real or imag is nan, then the whole number is considered as nan and considered as equal. That is `nan + 2j == 3 + nan j`.
",pytorch
50558,zasdfgbnm,pr,2021-01-14T22:56:45Z,Fix some JIT tests on python 3.9.1,"Python 3.9.1 can no longer get the source code if you define a class inside a method using

```python
def f():
    global SomeClass
    class SomeClass:
        pass
```
we should now do
```python
class SomeClass:
    pass
def f():
    global SomeClass
    class SomeClass:
        pass
```
This is causing test failures like:
```
FAILED test/test_jit_legacy.py::TestClassType::test_cast_overloads - OSError: Can't get source for <class 'jit.test_class_type.Foo'>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.
FAILED test/test_jit_legacy.py::TestClassType::test_class_sorting - OSError: Can't get source for <class 'jit.test_class_type.Foo'>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.
FAILED test/test_jit_legacy.py::TestClassType::test_class_specialization - OSError: Can't get source for <class 'jit.test_class_type.Foo'>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.
FAILED test/test_jit_legacy.py::TestClassType::test_class_type_as_param - OSError: Can't get source for <class 'jit.test_class_type.FooTest'>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.
FAILED test/test_jit_legacy.py::TestClassType::test_classmethod - OSError: Can't get source for <class 'jit.test_class_type.ClassWithClassMethod'>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.

```",pytorch
50560,zasdfgbnm,pr,2021-01-15T00:10:57Z,Fix get_annotation_str for python 3.9,"On python 3.9, `Subscript.slice` is no longer an `ast.Index` object
",pytorch
50603,robieta,pr,2021-01-15T20:38:30Z,add utility to back test Timer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* **#50603 add utility to back test Timer**

This PR is kind of dodgy, but given that I've tried to keep it as clean and localized as possible under the circumstances. In effect, it copies the local copy of `torch/utils` that `Timer` needs to a shadow folder in the target install. Where possible I've put the compat shims in `utils/benchmark` code, however some code mods are needed to support very old versions. I've tested `Timer` with Python snippets back to 0.4, and C++ snippets back to 1.0. This is code debt, it will break and have to be fixed as PyTorch gets updated; however it's only for a small set of devs and we can delete / prune as the historic analysis effort wraps up.",pytorch
50604,robieta,pr,2021-01-15T20:38:43Z,add global_setup for C++ includes,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* **#50604 add global_setup for C++ includes**
* #50603 add utility to back test Timer

In Python, it's fine to import inside a function. E.g.
```
def my_fn():
  import foo
  return foo.bar()
```

whereas in C++ the equivalent:

```
auto my_fn() {
  #include <foo>
  return foo::bar();
}
```

is at best scandalous and at more often just illegal. However it is sometimes necessary to add top level setup like `#include`s to snippets. This PR adds a C++ only `global_setup` argument to Timer.",pytorch
50605,robieta,pr,2021-01-15T20:38:56Z,Instruction count benchmark (1): Types and interfaces,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* **#50605 Instruction count benchmark (1): Types and interfaces**
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This PR sets the core interfaces the the suite uses. The entire suite leans heavily on dataclasses and enums for type safety, and it is all MyPy strict compliant. (Though I still need to set up unit tests, since it lives in `benchmarks/` rather than `torch/`.) The other main source of documentation is the README in https://github.com/pytorch/pytorch/pull/50610.",pytorch
50606,robieta,pr,2021-01-15T20:39:14Z,Instruction count benchmark (2): Initial benchmark definitions.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* **#50606 Instruction count benchmark (2): Initial benchmark definitions.**
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This PR adds some of the utils around benchmark definition, and adds some basic benchmarks in `definitions/standard.py`. Much more comprehensive benchmarks are added in https://github.com/pytorch/pytorch/pull/50608, but I elected to start with an example set in this PR just to make the whole stack more manageable to review. ",pytorch
50607,robieta,pr,2021-01-15T20:39:28Z,Instruction count benchmark (3): Enable TorchScript benchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* **#50607 Instruction count benchmark (3): Enable TorchScript benchmarks**
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

Add some of the machinery to automatically TorchScript benchmarks. Other portions live in `core/api_impl.py`, which is added in https://github.com/pytorch/pytorch/pull/50608.",pytorch
50608,robieta,pr,2021-01-15T20:39:41Z,Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* **#50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.**
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This really should be two PRs, but I made the stack when the code looked somewhat different, so anywayyyyyy...

1) `core/api_impl.py`

`core/api.py` defines the basic interface and docs for how to define a grouped benchmark, but `api_impl` does the heavy lifting of actually unpacking them into the various configurations. I've made it as comprehensible as possible (lots of dataclasses and enums, asserts, comments, etc.) but at the end of the day it is still 300 lines of fiddly string munging. I can't think of any natural way to further break it up without hurting more than it helps.

2) Vastly extend the standard benchmarks

As of writing, there are 156 benchmarks spanning factory function, TensorIterator ops, indexing, nn Modules, Python, C++, Eager, Torchscript, AutoGrad, and anything else (microbenchmark scale) that seemed worth testing. There are still some more to add (e.g. `clip_grad_norm_`, mesoscale nn Module benchmarks), but the coverage is pretty good. The number one goal was readability: someone should be able to read `definitions/standard.py` and `definitions/setup.py` and get a very good idea of what's going on. An example run using the suite can be found in https://github.com/pytorch/pytorch/pull/50610.",pytorch
50609,robieta,pr,2021-01-15T20:39:54Z,Instruction count benchmark (5): Add benchmark runner.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* **#50609 Instruction count benchmark (5): Add benchmark runner.**
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This PR adds the infrastructure to actually drive the benchmark collection. At a high level it's a pretty straightforward pool of subprocesses loop, but it aims to provide a very smooth user experience by:
 - Using most of the cores on a machine to speed up collection time. (Minus a few to keep from overloading the machine.)
 - Isolating jobs to contiguous cores in the same NUMA node. Wall times aren't particularly stable, but may as well stabilize them as much as possible.
 - Plumbing worker failures (and the **worker's** stack trace) to the main process, as well as info about which job failed.
 - Handling timeout and retry. (Sometimes Valgrind like to hang if too many concurrent jobs are running.)
 - Printing progress and ETA.
 - Making sure processes clean up after themselves.
 - Checking that Valgrind is producing stable instruction counts, and warning if it isn't. (Implemented in https://github.com/pytorch/pytorch/pull/50610)",pytorch
50610,robieta,pr,2021-01-15T20:40:07Z,"Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* **#50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README**
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This PR adds:
1) A README. (Maybe it should be a separate PR. I can shuffle it if need be.)
Right now it's primarily focused on quick starts (running the standard suite, and defining ad-hoc benchmarks), with some notes on the high level system design after that.

2) A frontend to actually collect measurements.
This part is very much WIP. (But it needs a frontend in order to start experimenting.) 

`frontend/run.py` is in reasonable shape, though some of the exclusions for backtesting are somewhat suspect. 

`frontend/display.py` is very similar to `torch.utils.benchmark.Compare`, but richer. My plan is to move most of the core table code from `display` into the benchmark utils to both make `Compare` safer and more extensible, and reduce the complexity of this suite. However for now it's a good place to experiment.

`main.py` is again, just some code to enable testing. There is currently an `A/B` mode and a single env mode. (So we can start experimenting with wiring instruction counts into CI.)

# End result: A/B testing example
The following A/B tests 1.7 and HEAD on my devserver:
```
python main.py \
  --mode A/B \
  --A ""source activate backtest_1_7"" \
  --patch_a \
  --B ""source activate ab_ref"" \
  --display_time \
  --colorize
```

First sub-row is # of instructions, and the second is wall time. (Dimmed out if variance makes it suspect.) As you can see, there is some noise in the wall times but over such a large range of optimizations the improvement is much larger than the noise. (And simply omit `--display_time` to only show instructions.)

<img width=""1282"" alt=""Screen Shot 2021-01-21 at 12 15 29 PM"" src=""https://user-images.githubusercontent.com/13089297/105407381-84949a80-5be2-11eb-8d58-c5d5bd7b6193.png"">
<img width=""1285"" alt=""Screen Shot 2021-01-21 at 12 15 56 PM"" src=""https://user-images.githubusercontent.com/13089297/105407398-88c0b800-5be2-11eb-842f-47c33d694259.png"">
<img width=""1286"" alt=""Screen Shot 2021-01-21 at 12 16 14 PM"" src=""https://user-images.githubusercontent.com/13089297/105407412-8bbba880-5be2-11eb-94ef-7979798e88dd.png"">
<img width=""1290"" alt=""Screen Shot 2021-01-21 at 12 16 23 PM"" src=""https://user-images.githubusercontent.com/13089297/105407427-8eb69900-5be2-11eb-90cd-d041efcd1f7e.png"">
",pytorch
50827,zasdfgbnm,pr,2021-01-20T18:17:48Z,More about cudnn refactor,"- Resolves @ngimel's review comments in https://github.com/pytorch/pytorch/pull/49109
- Move `ConvolutionArgs` from `ConvShared.h` to `Conv_v7.cpp`, because cuDNN v8 uses different descriptors therefore will not share the same `ConvolutionArgs`.
- Refactor the `ConvolutionParams` (the hash key for benchmark):
  - Remove `input_stride`
  - Add `input_dim`
  - Add `memory_format`
- Make `repro_from_args` to take `ConvolutionParams` instead of `ConvolutionArgs` as arguments so that it can be shared for v7 and v8
- Rename some `layout` to `memory_format`. `layout` should be sparse/strided and `memory_format` should be contiguous/channels_last. They are different things.
",pytorch
50856,jjsjann123,pr,2021-01-21T00:09:24Z,Exposing linear layer to fuser,"1. enabling linear in autodiff;
2. remove control flow in python for linear;",pytorch
50892,zasdfgbnm,pr,2021-01-21T18:24:02Z,Add device id to ConvolutionParams,"Fixes https://github.com/pytorch/pytorch/issues/50844
",pytorch
50917,jeffdaily,pr,2021-01-21T22:58:48Z,[ROCm] rename HIP_HCC_FLAGS to HIP_CLANG_FLAGS,"ROCm 3.5 replaced hcc with hip-clang and deprecated HIP_HCC_FLAGS.
HIP_CLANG_FLAGS should be used moving forward. HIP_HCC_FLAGS will
be removed soon.",pytorch
50927,zasdfgbnm,pr,2021-01-22T02:08:11Z,Rewrite kron with broadcasting at::mul,"Because it is shorter, faster, and does not have TF32 issue.

Benchmark: https://github.com/zasdfgbnm/things/blob/master/2021Q1/kron.ipynb",pytorch
50970,jeffdaily,pr,2021-01-22T23:09:13Z,[ROCm] work around compiler issue for IGammaKernel.cu,Add const to static variable inside `__host__ __device__` function.,pytorch
51045,zasdfgbnm,pr,2021-01-25T19:17:36Z,Followup of kron PR,Followup of https://github.com/pytorch/pytorch/pull/50927,pytorch
51121,robieta,pr,2021-01-26T17:52:21Z,"Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* #51122 Intermezzo: Add Timer support for callgrind repeats
* **#51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)**
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

This PR adds a couple tweaks that came up in the course of building this ubenchmark suite, but aren't large enough to warrant their own PR:
1) C++ wall time should release the GIL
2) Add option to retain `callgrind.out` contents. This will allow processing with kCachegrind for more detailed analysis.
3) Changed the default of `collect_baseline`. This is mostly there so I can check that the harness isn't too expensive, but as Python measurements get more reliable it is liable to be a distraction / source of noise for other users.
4) Add a `__mul__` overload to function counts. e.g. suppose `c0` was run with `number=100`, and `c1` was run with `number=200`, then `c0 * 2 - c1` is needed to properly diff them. (Obviously there are correctness concerns, but I think it's fine as a caveat emptor convenience method.)",pytorch
51122,robieta,pr,2021-01-26T17:52:31Z,Intermezzo: Add Timer support for callgrind repeats,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #51123 Instruction count benchmark (7): Use  when collecting callgrind.
* **#51122 Intermezzo: Add Timer support for callgrind repeats**
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

A lot of the cost of Callgrind is in startup and shutdown. `import torch` alone takes ~20 seconds when run under Valgrind. Moreover, there is sometimes warmup that needs to happen beyond just lazy init. (e.g. warming up malloc arenas or unicode caches.) This PR uses the `CALLGRIND_DUMP_STATS` macro to add a `repeats` arg to `Timer.collect_callgrind`. For most snippets where `stmt` is modest and `number` isn't too large, executing the body of the benchmark loop is a minority of the overall time and repeats are nearly free.

Empirically, this also does a great deal to stabilize Python since spurious events like hitting a `gc` collection or jitter in the interpreter will generally be the minority, and most of the repeats will have the same number of instructions. It is, however, worth noting that the asymptotic instruction counts for Python processes tends to vary by process, so there is still some noise in A/B testing.

Some of these warmups (such as malloc warmup) are also present in C++, so sequential repeats are not identical. However this behavior is still deterministic.

One minor finesse is the addition of `--threshold=100` to the `callgrind_annotate` call. The default value is 99%, so previously we have been inadvertently doing some filtering. ",pytorch
51123,robieta,pr,2021-01-26T17:52:41Z,Instruction count benchmark (7): Use  when collecting callgrind.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51123 Instruction count benchmark (7): Use  when collecting callgrind.**
* #51122 Intermezzo: Add Timer support for callgrind repeats
* #51121 Intermezzo: quality of life tweaks to Timer. (Retain callgrind.out, change collect_baseline default, add __mul__ for FunctionCounts)
* #50610 Instruction count benchmark (6): Add frontend to drive microbenchmarks, and start README
* #50609 Instruction count benchmark (5): Add benchmark runner.
* #50608 Instruction count benchmark (4): Extend the benchmarks in both coverage and complexity.
* #50607 Instruction count benchmark (3): Enable TorchScript benchmarks
* #50606 Instruction count benchmark (2): Initial benchmark definitions.
* #50605 Instruction count benchmark (1): Types and interfaces
* #50604 add global_setup for C++ includes
* #50603 add utility to back test Timer

Switch the instruction count benchmarks to use repeats rather than long Callgrind runs. This is both lower variance (many Python microbenchmarks are now properly deterministic) and takes less time to execute.",pytorch
51169,z-a-f,pr,2021-01-27T01:05:29Z,[quant][refactor] Factor out MHA code dup from nn and nn.quantizable,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51169 [quant][refactor] Factor out MHA code dup from nn and nn.quantizable**
* #50459 [quant] Factoring out the list of no_observers
* #49866 [quant] Quantizable MultiheadAttention

Quantizable MHA shares some code with its FP equivalent.
This factors out common parts.

Test Plan:

Explicit testing is not available, as there is no functional tests for the nn.MHA.
However, the functional equivalence of this PR was tested across commits by comparing the numerics of the pre-change and post-change MHA. Similar was done for the quantizable counterpart.

Differential Revision: [D26092530](https://our.internmc.facebook.com/intern/diff/D26092530)",pytorch
51238,jeffdaily,pr,2021-01-27T21:19:53Z,[ROCm] add hipMAGMA support,"Fixes #48831.  

- CI image is updated to build hipMAGMA from source and set env MAGMA_HOME.
- CMake is updated to separate different requirements for CUDA versus ROCm MAGMA.
- Some unit tests that become enabled with MAGMA are currently skipped for ROCm due to failures.  Fixing these failures will be follow-on work.",pytorch
51240,jjsjann123,pr,2021-01-27T21:56:12Z,fixing index clamping for upsample nearest kernel backward,Fixes #51036 ,pytorch
51257,jeffdaily,pr,2021-01-28T01:07:10Z,[ROCm] add 4.0.1 to nightly builds,Depends on https://github.com/pytorch/builder/pull/628.,pytorch
51390,zasdfgbnm,pr,2021-01-30T00:23:10Z,step 0 of cuDNN v8 convolution API integration,"This PR is step 0 of adding PyTorch convolution bindings using the cuDNN frontend. The cuDNN frontend is the recommended way of using cuDNN v8 API. It is supposed to have faster release cycles, so that, for example, if people find a specific kernel has a bug, they can report it, and that kernel will be blocked in the cuDNN frontend and frameworks could just update that submodule without the need for waiting for a whole cuDNN release.

The work is not complete, and this PR is only step 0.

**What this PR does:**
- Add cudnn-frontend as a submodule.
- Modify cmake to build that submodule.
- Add bindings for convolution forward in `Conv_v8.cpp`, which is disabled by a macro by default.
- Tested manually by enabling the macro and run `test_nn.py`. All tests pass except those mentioned below.

**What this PR doesn't:**
- Only convolution forward, no backward. The backward will use v7 API.
- No 64bit-indexing support for some configuration. This is a known issue of cuDNN, and will be fixed in a later cuDNN version. PyTorch will not implement any workaround for issue, but instead, v8 API should be disabled on problematic cuDNN versions.
- No test beyond PyTorch's unit tests.
  - Not tested for correctness on real models.
  - Not benchmarked for performance.
- Benchmark cache is not thread-safe. (This is marked as `FIXME` in the code, and will be fixed in a follow-up PR)
- cuDNN benchmark is not supported.
- There are failing tests, which will be resolved later:
  ```
  FAILED test/test_nn.py::TestNNDeviceTypeCUDA::test_conv_cudnn_nhwc_cuda_float16 - AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.001 and atol=1e-05, found 32 element(s) (out of 32) whose difference(s) exceeded the margin of error (in...
  FAILED test/test_nn.py::TestNNDeviceTypeCUDA::test_conv_cudnn_nhwc_cuda_float32 - AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1.3e-06 and atol=1e-05, found 32 element(s) (out of 32) whose difference(s) exceeded the margin of error (...
  FAILED test/test_nn.py::TestNNDeviceTypeCUDA::test_conv_large_cuda - RuntimeError: CUDNN_BACKEND_OPERATION: cudnnFinalize Failed cudnn_status: 9
  FAILED test/test_nn.py::TestNN::test_Conv2d_depthwise_naive_groups_cuda - AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0 and atol=1e-05, found 64 element(s) (out of 64) whose difference(s) exceeded the margin of error (including 0 an...
  FAILED test/test_nn.py::TestNN::test_Conv2d_deterministic_cudnn - RuntimeError: not supported yet
  FAILED test/test_nn.py::TestNN::test_ConvTranspose2d_groups_cuda_fp32 - RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM
  FAILED test/test_nn.py::TestNN::test_ConvTranspose2d_groups_cuda_tf32 - RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM
  ```

Although this is not a complete implementation of cuDNN v8 API binding, I still want to merge this first. This would allow me to do small and incremental work, for the ease of development and review.",pytorch
51507,jeffdaily,pr,2021-02-01T23:19:38Z,[ROCm] add 4.0.1 docker image,"Add a ROCm 4.0.1 docker image for CI. Keep the 3.10 image.
Keep the 3.9 image until the 3.9 image is no longer needed.
Plan is to keep two ROCm versions at a time.",pytorch
51543,gmagogsfm,pr,2021-02-02T08:52:36Z,Remove duplicate check for THPLayout in toSugaredValue,,pytorch
51545,gmagogsfm,pr,2021-02-02T09:28:29Z,Handle repeated jit.script calls on function gracefully,Repeated calls on `class` is not handled since `class`'s compilation process will change soon in #44324,pytorch
51584,zasdfgbnm,pr,2021-02-02T21:42:25Z,DO NOT REVIEW,"Fixes #{issue number}
",pytorch
51604,zasdfgbnm,pr,2021-02-03T01:20:07Z,Improve test_reference_numerics,"Fixes https://github.com/pytorch/pytorch/issues/50749
ci-all version of https://github.com/pytorch/pytorch/pull/50550
",pytorch
51613,jjsjann123,pr,2021-02-03T03:30:44Z,Linear autodiff reland,"add backward for aten::linear to SymbolicScript to make aten::linear differentiable in TorchScript
make torch.nn.functional.linear call aten::linear to avoid python control flow for that op
onnx changes that are needed to support this

reland of  PR #50856 ",pytorch
51624,gmagogsfm,pr,2021-02-03T07:22:21Z,Tolerate `torch.jit.script` call to Enum classes,,pytorch
51628,gmagogsfm,pr,2021-02-03T08:01:44Z,[Usability] Add explicit and early check for unsupported script targets,"Previously, we don't explicitly check if argument passed to `torch.jit.script` is of supported type. It sometimes leads to ugly error messages when user pass in unsupported objects.  This PR makes the check explicit and improves error messaging.",pytorch
51664,robieta,pr,2021-02-03T20:12:06Z,Expand benchmark utils docs,"Add some much needed documentation on the Timer callgrind output format, and expand what is shown on the website.
",pytorch
51675,gmagogsfm,pr,2021-02-03T23:02:24Z,Support mypy ignore annotation with particular rule specified,"Previously TorchScript allows a ignore-all type check suppression rule that looks like 
```
code code code  # type: ignore
```

But a more common use case is 
```
code code code  # type: ignore[specific-rule]
```
This PR allows the more common use case

Fixes #48643
",pytorch
51682,z-a-f,pr,2021-02-03T23:32:22Z,[quant] Conv1d initialized wight shape,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51682 [quant] Conv1d initialized wight shape**

",pytorch
51703,gmagogsfm,pr,2021-02-04T04:49:54Z,"Fix clang-tidy warnings in python_sugared_value.{h,cpp}","
",pytorch
51713,jjsjann123,pr,2021-02-04T08:11:19Z,fixing mkldnn_linear & backward with silent error,"mkldnn_linear & mkldnn_linear_backward_input gives wrong result when weight is non contiguous.

Issue exposed in PR #51613",pytorch
51733,robieta,pr,2021-02-04T18:21:52Z,raise windows tol to 30%,"Up the Windows tolerance set by https://github.com/pytorch/pytorch/pull/35818, as CI is still showing some flakes.

Test Plan: CI",pytorch
51775,gmagogsfm,pr,2021-02-05T07:33:38Z,[Usability] Capture argument names for traced functions and modules,"Previously `torch.jit.trace` relies on AutoGrad hooks to infer name of tensors in computation, including those of function/method arguments. This often doesn't work out because:

- These names often do not exist
- Tracer uses argument name of first tensor operation on each tensor as inferred argument names. These tensor operations have programmatically-generated names like `argument_1`

This PR extracts argument names directly from Python functions and pass them down to tracer, which then assigns them to correct graph inputs. This way, we always have the correct argument names captured in IR. 

This is useful for both debugging and supporting using `InterfaceType` to represent traced modules.",pytorch
51824,zasdfgbnm,pr,2021-02-06T03:15:37Z,Remove unused include in TensorIteratorDynamicCasting.h,"In the past, this file included `thrust/complex.h` because the `thrust::complex` --> `c10::complex` migration was not done. Today, this task has been done for a while but seems that this include was not deleted.
",pytorch
51913,ppwwyyxx,pr,2021-02-08T22:59:01Z,1.8 cherrypick: Add metacompile of Ternary if (#51789),cherrypick of https://github.com/pytorch/pytorch/pull/51789 into 1.8 release,pytorch
51915,jeffdaily,pr,2021-02-08T23:36:59Z,[ROCm] skip more magma tests,Additional magma tests have been identified as failing after integrating hipMAGMA into the ROCm builds.  Skipping is necessary until they can be fixed properly.  This is blocking migration of ROCm CI to 4.0.1.,pytorch
51993,robieta,pr,2021-02-09T20:53:49Z,Better re-raise handling for custom exceptions,"When dataloader raises a custom exception with a complex constructor, we fail to re-raise and all useful info is lost. This PR tries to retain as much information as possible to give the user at least some indication of what is going on.

Test plan: Added unit test.",pytorch
52048,jjsjann123,pr,2021-02-10T14:53:06Z,[WIP] Native layer norm backwards,"Fixes #{issue number}
",pytorch
52064,jeffdaily,pr,2021-02-10T18:32:13Z,[ROCm] skip one more magma test that is flaky,Skipped hipMAGMA tests are tracked in https://github.com/pytorch/pytorch/issues/51303.,pytorch
52124,robieta,pr,2021-02-11T01:06:28Z,Gh/taylorrobie/import timer fbcode,"`torch.__config__._cxx_flags` gets called on import, but this means that Timer can't be used if it fails. (Even just the wall time parts.) This is needlessly restrictive.",pytorch
52136,jjsjann123,pr,2021-02-11T11:20:33Z,Ge v1,This is a second attempt to use graph executor to run forward on a gradient. This allows a secondary chance to profile intermediate tensor introduced by autodiff.,pytorch
52346,gmagogsfm,pr,2021-02-17T05:18:36Z,Optimize `setDebugName` time complexity,"`setDebugName` maintains an invariant that all debug names of values in same graph must be distinct. This is achieved by appending numeric suffixes to requested debug names. However, the implementation was slow (O(N^2)) when there are a lot of name conflicts. This PR fixes the problem by adding more book-keeping logic so that time complexity is brought down to O(1) on average. 

Tested on a module with about 1k+ submodules, tracing time reduced from ~90s to ~55s",pytorch
52472,jeffdaily,pr,2021-02-19T00:23:04Z,[ROCm] missing template declarations for complex blas,,pytorch
52704,zasdfgbnm,pr,2021-02-23T23:06:39Z,[do not review]Update CMakeLists.txt,"Fixes #{issue number}
",pytorch
52713,zasdfgbnm,pr,2021-02-24T01:37:48Z,Add 64bit indexing support for softmax,"fixes https://github.com/pytorch/pytorch/issues/52715 https://github.com/pytorch/pytorch/issues/52716

split across batch dimension",pytorch
52756,jeffdaily,pr,2021-02-24T17:53:00Z,Fix hipify_python,Fixes the torchvision build failure on ROCm reported in #52645.,pytorch
52830,gmagogsfm,pr,2021-02-25T08:05:05Z,Language Ref Python Builtin Functions and Values,,pytorch
52871,zasdfgbnm,pr,2021-02-25T20:54:06Z,Fixes new tf32 failures in test_nn.py,Also modify the `tf32_on_and_off` decorator to make it support function without `device` argument.,pytorch
52890,zasdfgbnm,pr,2021-02-26T00:37:37Z,Remove useless test_reference_numerics skip infos,"These are no longer useful. Let's wait for a few days before merging this, just in case somebody finds failures in them.",pytorch
52892,d4l3k,pr,2021-02-26T00:51:58Z,[caffe2] EnforceFinite: log blobs finiteness in workspace on error,"Summary:
When an EnforceFinite check fails this logs all of the tensors in the workspace and whether they are finite or not.

This is a little bit hacky since it uses the aten APIs. I've `ifdef`ed the implementation so it should compile fine on xplat and mobile. It's also accessing the workspace directly but since this is a logging op it seems fine to bend the rules.

Test Plan:
$ buck test //caffe2/caffe2/python/operator_test:enforce_finite_op_test

  $ buck-out/gen/caffe2/caffe2/python/operator_test/enforce_finite_op_test#binary.par
  I0225 16:29:46.166507 311548 enforce_finite_op.h:62] blob X isfinite=false

Differential Revision: D26626336

",pytorch
52941,zasdfgbnm,pr,2021-02-26T20:42:54Z,Disable TF32 on DDP tests,"When a system has an ampere and a non-ampere card, lots of tests will fail, because results on different cards are differnet.",pytorch
52971,gmagogsfm,pr,2021-02-27T16:00:38Z,[DO NOT SUBMIT] Debug trace interface,"Fixes #{issue number}
",pytorch
53031,z-a-f,pr,2021-03-01T19:01:54Z,[quant][fix] MHA tensor assignment fix,"Summary: During the module conversion, the weight was assigned directly to the linear layer inside the quantizable MHA. Instead the weight must be assigned to the `layer.weight`.

Test Plan:
`buck test mode/opt //caffe2/test:quantization -- test_custom_module_multi_head_attention`

```
Building: finished in 6.9 sec (100%) 7316/7316 jobs, 3 updated
  Total time: 7.4 sec
More details at https://www.internalfb.com/intern/buck/build/914cb095-806e-4891-8822-e2644283f05c
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: fcccbd0b-a887-4874-8455-d1cf8411be1d
Trace available for this run at /tmp/tpx-20210301-004359.492205/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/1688849910412609
    ✓ ListingSuccess: caffe2/test:quantization - main (2.440)
    ✓ Pass: caffe2/test:quantization - test_custom_module_multi_head_attention (quantization.test_quantized_op.TestQuantizedOps) (5.672)
Summary
  Pass: 1
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/1688849910412609
```

Differential Revision: D26720500

",pytorch
53052,gmagogsfm,pr,2021-03-01T22:26:15Z,Fix jit.trace mis-handling of InterfaceType,"`jit.trace` recursively gathers all named attributes in module at beginning of
tracing. This is fine in a pure-tracing environment, but breaks when a
scripted module that contains an InterfaceType'd submodule is involved.
Because InterfaceType, by design, is not allowed to have any attribute,
thus some of the gathered attributes will turn into fatal errors in
following some graph rewrite passes.

This PR fixes this bug by distinguishing InterfaceType'd submodules from
normal ClassType'd submodules.",pytorch
53221,z-a-f,pr,2021-03-04T00:02:44Z,[sparsity][refactor] Rename row/col to out/in features,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Names such as `row_block_size` and `col_block_size` might be ambiguous, especially if different engines use different tensor layouts (i.e. rows=output features, etc.). Having names such as `out_features_block_size` and `in_features_block_size` makes more sense

Differential Revision: [D26747065](https://our.internmc.facebook.com/intern/diff/D26747065/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26747065/)!",pytorch
53222,z-a-f,pr,2021-03-04T00:02:52Z,[sparsity] Moving model_optimization C++ files to OSS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Migrating the FB code to the OSS. Starting with the C++ code.
This diff also introduces the `mo` folder under the `ATen/native`. The planned folder structure (as per discussion here: https://fb.quip.com/8uL7A1bgb1AP):

```
./mo
├── pruning  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
├── quantization  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
└── sparsity
    ├── cpu
    ├── cuda
    └── ...
```

The new c++ namespaces are `torch::mo` (see `qlinear.cpp` for example). The torch ops are registered under the `sparsity` namespace (i.e. `""sparsity::sparse_qlinear""`

Differential Revision: [D26749445](https://our.internmc.facebook.com/intern/diff/D26749445/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26749445/)!",pytorch
53223,z-a-f,pr,2021-03-04T00:03:58Z,[sparsity] Moving model_optimization C++ files to OSS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Migrating the FB code to the OSS. Starting with the C++ code.
This diff also introduces the `mo` folder under the `ATen/native`. The planned folder structure (as per discussion here: https://fb.quip.com/8uL7A1bgb1AP):

```
./mo
├── pruning  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
├── quantization  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
└── sparsity
    ├── cpu
    ├── cuda
    └── ...
```

The new c++ namespaces are `torch::mo` (see `qlinear.cpp` for example). The torch ops are registered under the `sparsity` namespace (i.e. `""sparsity::sparse_qlinear""`

Differential Revision: [D26749445](https://our.internmc.facebook.com/intern/diff/D26749445/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26749445/)!",pytorch
53225,z-a-f,pr,2021-03-04T00:11:29Z,[sparsity] Moving model_optimization C++ files to OSS,"Summary:
Migrating the FB code to the OSS. Starting with the C++ code.
This diff also introduces the `mo` folder under the `ATen/native`. The planned folder structure (as per discussion here: https://fb.quip.com/8uL7A1bgb1AP):

```
./mo
├── pruning  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
├── quantization  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
└── sparsity
    ├── cpu
    ├── cuda
    └── ...
```

The new c++ namespaces are `torch::mo` (see `qlinear.cpp` for example). The torch ops are registered under the `sparsity` namespace (i.e. `""sparsity::sparse_qlinear""`

Test Plan:
`buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`

```
Action graph will be rebuilt because files have been added or removed.
Parsing buck files: finished in 1.8 sec
Building: finished in 5.8 sec (100%) 7317/7317 jobs, 0 updated
  Total time: 7.6 sec
More details at https://www.internalfb.com/intern/buck/build/1d1fcb21-d712-4eae-8b7a-de63c70a3d30
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: a842b1dc-4ead-4299-bb26-b8c1b9e9dc13
Trace available for this run at /tmp/tpx-20210302-145129.778562/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/1407375066410600
    ✓ ListingSuccess: caffe2/torch/fb/model_optimization:sparsity_test - main (1.711)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseKernels) (1.556)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear_serdes (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.785)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.790)
Summary
  Pass: 3
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/1407375066410600
```

Differential Revision: D26749445

",pytorch
53226,z-a-f,pr,2021-03-04T00:18:53Z,[sparsity] Migrated python codes to OSS,"Summary:
Moving the python files from the internal codebase to the OSS.

**Breaking changes**

If you were using the internal implementation of the sparsity, the location changed to `nn/mo/*`

Test Plan:
- Internal: `buck test mode/opt //caffe2/test:mo`
- OSS: `python test/test_mo.py`

Differential Revision: D26794879

",pytorch
53236,gmagogsfm,pr,2021-03-04T03:10:59Z,Adds torch.* API section for TorchScript Lang Ref,,pytorch
53258,z-a-f,pr,2021-03-04T11:33:40Z,[sparsity] Base sparsifier class,"Summary: Introducing the base sparsifier class as described in https://docs.google.com/document/d/1Tr3OYnv7RdTDmnLNxUF_8cwpBofPTGY7cTrROXKA_yw/edit?usp=sharing

Test Plan:
`buck test mode/opt //caffe2/test:mo -- TestBaseSparsifierClass`

```
Building: finished in 4.8 sec (100%) 7385/7385 jobs, 0 updated
  Total time: 5.2 sec
More details at https://www.internalfb.com/intern/buck/build/11697063-377e-456d-a072-bb5f99cbca5b
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: 89ba2d03-b789-4a64-a4a9-af8f2f433f45
Trace available for this run at /tmp/tpx-20210304-032515.471916/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/562950135301094
    ✓ ListingSuccess: caffe2/test:mo - main (0.850)
    ✓ Pass: caffe2/test:mo - test_add_group (mo.sparsity.test_sparsifier.TestBaseSparsifierClass) (0.883)
    ✓ Pass: caffe2/test:mo - test_getstate_setstate (mo.sparsity.test_sparsifier.TestBaseSparsifierClass) (1.036)
Summary
  Pass: 2
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/562950135301094
```

Differential Revision: D26814581

",pytorch
53293,robieta,pr,2021-03-04T21:47:06Z,"Add global setup to timer to allow #include, etc.","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #53296 Definition infrastructure for instruction count ubenchmarks
* #53295 Add repeats to Timer.collect_callgrind(...)
* #53294 Quality of life improvements to Timer
* **#53293 Add global setup to timer to allow #include, etc.**

Instruction count benchmarks need some includes for IValues, but this is also just generally useful. (Unlike Python where you can just drop imports anywhere, C++ will get very upset if you `#include` in a function body...)

Differential Revision: [D26906684](https://our.internmc.facebook.com/intern/diff/D26906684)",pytorch
53294,robieta,pr,2021-03-04T21:47:12Z,Quality of life improvements to Timer,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #53296 Definition infrastructure for instruction count ubenchmarks
* #53295 Add repeats to Timer.collect_callgrind(...)
* **#53294 Quality of life improvements to Timer**
* #53293 Add global setup to timer to allow #include, etc.

Just a bunch of little things, none of which are big enough to need a full PR.

1) C++ wall time should release the GIL
2) Add option to retain `callgrind.out` contents. This will allow processing with kCachegrind for more detailed analysis.
3) Stop subtracting the baseline instruction counts. (People just found it confusing when they saw negative instruction counts.) There is a finesse in #53295 that drops the baseline to ~800 instructions for `number=100`, and at that level it's not worth correcting.
4) Add a `__mul__` overload to function counts. e.g. suppose `c0` was run with `number=100`, and `c1` was run with `number=200`, then `c0 * 2 - c1` is needed to properly diff them. (Obviously there are correctness concerns, but I think it's fine as a caveat emptor convenience method.)
5) Tweak the `callgrind_annotate` call, since by default it filters very small counts.
6) Move some args to kwargs only since types could be ambiguous otherwise.
7) Don't omit rows from slices. It was annoying to print something like `stats[:25]` and have `__repr__` hide the lines in the middle.

Differential Revision: [D26906715](https://our.internmc.facebook.com/intern/diff/D26906715)",pytorch
53295,robieta,pr,2021-03-04T21:47:17Z,Add repeats to Timer.collect_callgrind(...),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #53296 Definition infrastructure for instruction count ubenchmarks
* **#53295 Add repeats to Timer.collect_callgrind(...)**

A lot of the time spent in `collect_callgrind` is spinning up Valgrind and executing the initial `import torch`. In most cases the actual run loop is a much smaller fraction. As a result, we can reuse the same process to do multiple replicates and do a much better job amortizing that startup cost. This also tends to result in more stable measurements: the kth run is more repeatable than the first because everything has been given a chance to settle into a steady state. The instruction microbenchmarks lean heavily on this behavior. I found that in practice doing several `n=100` replicates to be more reliable than one monolithic 10,000+ iteration run. (Since rare cases like memory consolidation will just contaminate that one replicate, as opposed to getting mixed into the entire long run.)

Differential Revision: [D26907093](https://our.internmc.facebook.com/intern/diff/D26907093)",pytorch
53296,robieta,pr,2021-03-04T21:47:23Z,Definition infrastructure for instruction count ubenchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53296 Definition infrastructure for instruction count ubenchmarks**
* #54484 Revert ""Revert D26907093: Add repeats to Timer.collect_callgrind(...)""

Part 1 of the instruction count microbenchmarks. This PR is focused on benchmark definition machinery. (Though you can run `main.py` to see it in action.) A summary of the system is given in the README.

Differential Revision: [D26907092](https://our.internmc.facebook.com/intern/diff/D26907092)",pytorch
53426,jeffdaily,pr,2021-03-05T23:43:57Z,add --run-parallel 4 to shard1 and shard2 tests,Also corrects logic for running tests in the RUN_PARALLEL_BLOCKLIST.,pytorch
53460,ppwwyyxx,pr,2021-03-06T21:21:34Z,[pytorch] use correct warning type for tracer warnings,"Summary:
We have code to ignore this category of warnings and found this one is incorrect.

Use `stacklevel=2`, otherwise the warning is always filtered by TracerWarning.ignore_lib_warnings()

Test Plan: sandcastle

Differential Revision: D26867290

",pytorch
53558,z-a-f,pr,2021-03-08T22:54:46Z,[sparsity] Moving model_optimization C++ files to OSS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

Migrating the FB code to the OSS. Starting with the C++ code.
This diff also introduces the `mo` folder under the `ATen/native`. The planned folder structure (as per discussion here: https://fb.quip.com/8uL7A1bgb1AP):

```
./mo
├── pruning  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
├── quantization  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
└── sparsity
    ├── cpu
    ├── cuda
    └── ...
```

The new c++ namespaces are `torch::mo` (see `qlinear.cpp` for example). The torch ops are registered under the `sparsity` namespace (i.e. `""sparsity::sparse_qlinear""`

Differential Revision: [D26749445](https://our.internmc.facebook.com/intern/diff/D26749445/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26749445/)!",pytorch
53559,z-a-f,pr,2021-03-08T22:55:43Z,[sparsity] Moving model_optimization C++ files to OSS,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53559 [sparsity] Moving model_optimization C++ files to OSS**

Migrating the FB code to the OSS. Starting with the C++ code.
This diff also introduces the `mo` folder under the `ATen/native`. The planned folder structure (as per discussion here: https://fb.quip.com/8uL7A1bgb1AP):

```
./mo
├── pruning  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
├── quantization  # TODO
│   ├── cpu
│   ├── cuda
│   └── ...
└── sparsity
    ├── cpu
    ├── cuda
    └── ...
```

The new c++ namespaces are `torch::mo` (see `qlinear.cpp` for example). The torch ops are registered under the `sparsity` namespace (i.e. `""sparsity::sparse_qlinear""`

Differential Revision: [D26749445](https://our.internmc.facebook.com/intern/diff/D26749445/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D26749445/)!",pytorch
53576,gmagogsfm,pr,2021-03-09T01:30:07Z,Support parsing Ellipsis in JIT frontend,"De-sugars `Ellipsis` into dots (`...`)

Fixes #53517",pytorch
53667,gmagogsfm,pr,2021-03-09T23:49:05Z,[Cherrypick] Support parsing Ellipsis in JIT frontend ,"This fixes a bug in FX+TorchScript integration

Link to landed master PR (if applicable): #53576
Link to release branch PR: self
Criteria Category:`3. Fixes to new features introduced in the most recent minor release (e.g. 1.8.1 for 1.8.x release)",pytorch
53672,gmagogsfm,pr,2021-03-10T01:09:41Z,Rewrite functional.tensordot to be TorchScript-able,"Fixes #53487
",pytorch
53766,gmagogsfm,pr,2021-03-10T23:22:16Z,Cherrypick #53576 into release/1.8,"This fixes a bug in FX+TorchScript integration

Link to landed master PR (if applicable): #53576
Link to release branch PR: self
Criteria Category:`3. Fixes to new features introduced in the most recent minor release (e.g. 1.8.1 for 1.8.x release)
",pytorch
53841,zasdfgbnm,pr,2021-03-11T20:43:10Z,Replace thrust with cub in randperm,"Benchmark of
```python
%timeit torch.randperm(100000, device='cuda'); torch.cuda.synchronize()
```
thrust:
```
5.76 ms ± 42.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
cub:
```
3.02 ms ± 32.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

sync in thrust sort is removed

Warning:
Thrust supports 64bit indexing, but cub doesn't, so this is a functional regression. However, `torch.randperm(2**31, device='cuda')` fails with OOM on 40GB A100, and `torch.randperm(2**32, device='cuda')` fails with OOM on 80GB A100, so I think this functional regression has low impact and is acceptable.",pytorch
54010,robieta,pr,2021-03-15T16:58:29Z,Refactor Compare to be more modular and extensible,"Right now `Compare` is kind of a mess of comprehensions and string manipulation, and extending it is quite hard. (For instance, even adding the ability to print callgrind results would be a non-trivial amount of work.) This PR replaces the old implementation with a much more composable implementation:

1) Cells can produce arbitrary output (including newlines, ANSI color codes, etc.) based on what is appropriate. (And have reduction methods to coordinate with adjacent values.)

2) The Table handles layout and alignment so that cell implementers don't have to concern themselves with it.

Not only does it make the Compare source much more readable (and also now MyPy strict compliant), but it means that users have a lot more control over how to lay out the data. (For instance we go over a bandwidth example in the new README.) I moved the source to a `visualize` internal namespace just to try to keep things tidy.

Test plan: Existing unit tests should suffice. The layout changes slightly, but in my opinion for the better.

I've added Peter, Alexander, and Bert as ""power user reviewers""; I'm curious to hear if this would make your life easier.",pytorch
54081,robieta,pr,2021-03-16T19:09:17Z,address PR comments and add variant benchmark definition,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54081 address PR comments and add variant benchmark definition**
* #53296 Definition infrastructure for instruction count ubenchmarks
* #53295 Add repeats to Timer.collect_callgrind(...)

",pytorch
54113,zasdfgbnm,pr,2021-03-17T00:13:29Z,Remove sync for randperm on small tensors.,"For small tensors, it is known that GPU operates slower than CPU. However, offloading to CPU causes host <--> device sync. As a result, although offloading to CPU has better microbenchmarks, it often hurts instead of benefits the end-to-end performance, and it could be a blocker for CUDA graphs. After discussion with @mcarilli and @ptrblck, we think it might be good to just remove this piece of code and let it be slow.

Microbenchmarks:

```python
def run50_sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

torch.cuda.synchronize()
%timeit run50_sync(lambda: torch.randperm(3, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(30, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(300, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(3000, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(30000, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(300000, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(3000000, device='cuda'))
%timeit run50_sync(lambda: torch.randperm(30000000, device='cuda'))
```

Before this PR:
```
5.79 ms ± 51.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.78 ms ± 92.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
6.17 ms ± 87.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9.65 ms ± 69.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.6 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
21 ms ± 120 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
104 ms ± 880 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
944 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

After this PR:
```
7.22 ms ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.28 ms ± 9.03 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.25 ms ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9.19 ms ± 5.83 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
9.76 ms ± 162 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
12.3 ms ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
69.3 ms ± 42.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
716 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",pytorch
54161,jeffdaily,pr,2021-03-17T16:01:59Z,add --gpu-max-threads-per-block=256 to hipMAGMA build,"As of ROCm version 4.0.1, the HIP compiler default for max threads per block is 256 but is subject to change in future releases.  To protect against changes, hipMAGMA should be built with the previously-assumed default.  This change is necessary here in PyTorch until upstream magma project utilizes `__launch_bounds__` or some other means of controlling launch bounds.",pytorch
54176,zasdfgbnm,pr,2021-03-17T16:45:21Z,Make index_add take a scalar argument alpha,"```
index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
```
now becomes
```
index_add(Tensor self, int dim, Tensor index, Tensor source, Scalar alpha=1) -> Tensor
```
Generally, this sounds useful and harmless, and inside PyTorch, we are already needing this feature in `add_out_dense_sparse_cuda`, see the `SparseCUDATensorMath.cu` change in this PR.

**Test not added yet. Will add if after discussion we believe this is a good idea.**
- [ ] TODO: add test",pytorch
54231,zasdfgbnm,pr,2021-03-18T02:39:13Z,DO NOT MERGE,"Fixes #{issue number}
",pytorch
54249,z-a-f,pr,2021-03-18T10:12:04Z,[quant] Fixing converting of the custom LSTM module,"- Fixes the `from_observed` that didn't work properly because the it was not a class method.
- After `convert` is done, the name of the ""Quantizable"" lstm is now changed to ""Quantized""

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54249 [quant] Fixing converting of the custom LSTM module**

Differential Revision: [D27154862](https://our.internmc.facebook.com/intern/diff/D27154862)",pytorch
54341,jeffdaily,pr,2021-03-19T18:15:59Z,[ROCm] allow PYTORCH_ROCM_ARCH in cpp_extension.py,Allows extensions to override ROCm gfx arch targets.  Reuses the same env var used during cmake build for consistency.,pytorch
54350,jeffdaily,pr,2021-03-19T20:26:54Z,[ROCm] use hiprtc precompiled header,"HIP's runtime compiler (hiprtc) is adding support for precompiled HIP headers in the ROCm 4.2 release.  Conditionally add support for this feature.  Using this feature will improve the ROCm torch wheel user experience; users will no longer need to install HIP headers separately to use torch JIT features.

The use of this feature is conditionalized on a new ROCM_VERSION macro.",pytorch
54484,robieta,pr,2021-03-23T05:19:12Z,"Revert ""Revert D26907093: Add repeats to Timer.collect_callgrind(...)""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #53296 Definition infrastructure for instruction count ubenchmarks
* **#54484 Revert ""Revert D26907093: Add repeats to Timer.collect_callgrind(...)""**

Re-land of https://github.com/pytorch/pytorch/pull/53295. (With fixed unit tests.)

This reverts commit 0dc5abfaa9cac9266791788839d896b14600d123.

Differential Revision: [D27255201](https://our.internmc.facebook.com/intern/diff/D27255201)",pytorch
54485,gmagogsfm,pr,2021-03-23T05:28:11Z,Add documentation for torch.jit.Attribute and torch.jit.annotate,This is to prepare for new language reference spec that needs to describe `torch.jit.Attribute` and `torch.jit.annotate`,pytorch
54511,jeffdaily,pr,2021-03-23T16:07:58Z,"[ROCm] add MAGMA_HOME env var hint to cmake, centos-rocm Dockerfile","MAGMA_HOME was previously set for the ubuntu-rocm/Dockerfile.  However, this missed centos builds as well as any builds that do not use the CI image environments.",pytorch
54514,robieta,pr,2021-03-23T16:16:06Z,address PR comments,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54514 address PR comments**
* #53296 Definition infrastructure for instruction count ubenchmarks
* #54484 Revert ""Revert D26907093: Add repeats to Timer.collect_callgrind(...)""

",pytorch
54517,robieta,pr,2021-03-23T16:27:18Z,Ci all/instruction count ci,"Pre-land check for https://github.com/pytorch/pytorch/pull/53296

Test plan: DO_NOT_SUBMIT",pytorch
54626,zasdfgbnm,pr,2021-03-24T21:11:06Z,"`sort`: Partially migrate from THC to ATen, replace the thrust path with cub","The thrust path of `torch.sort` in THC is rewritten and replaced with cub in ATen. The original algorithm is followed, but since cub does not offer custom compare operator, I have to change it a bit to 2 sort + gather.

Note: tensor larger than 2^31 elements is supported, but the dimension being sorted can not go beyond 2^31.

Related: https://github.com/pytorch/pytorch/pull/50887 https://github.com/pytorch/pytorch/issues/24637

Benchmark:

```python
import torch
import itertools

for i in range(1000):
    torch.arange(100000, device='cuda')

def run50_sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

for i, j in itertools.product([512, 4096, 8192], repeat=2):
    print(i,j)
    t = torch.randn(i, j, device='cuda')
    torch.cuda.synchronize()
    %timeit run50_sync(lambda: torch.sort(t))
    torch.cuda.synchronize()
    %timeit run50_sync(lambda: torch.sort(t, dim=0))
    print()
```

Before
```
512 512
3.91 ms ± 8.53 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
4.87 ms ± 5.06 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

512 4096
70.5 ms ± 29.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
32.7 ms ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

512 8192
142 ms ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
64.4 ms ± 94.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 512
26.8 ms ± 1.68 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
82.2 ms ± 13.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 4096
606 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
722 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

4096 8192
1.28 s ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.54 s ± 500 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 512
53.5 ms ± 73.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
168 ms ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

8192 4096
1.28 s ± 236 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.54 s ± 272 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 8192
2.69 s ± 741 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
3.28 s ± 549 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

After
```
512 512
4.02 ms ± 28.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5 ms ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

512 4096
40.7 ms ± 74.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
33.9 ms ± 186 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

512 8192
71.7 ms ± 636 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
66.4 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 512
27.6 ms ± 27.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
46.6 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 4096
262 ms ± 1.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
321 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

4096 8192
520 ms ± 5.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
661 ms ± 853 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 512
54.6 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
83.2 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

8192 4096
521 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
645 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 8192
1.04 s ± 2.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.34 s ± 541 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",pytorch
54628,jeffdaily,pr,2021-03-24T21:26:48Z,[ROCm] add 4.1 docker image,"Add a ROCm 4.1 docker image for CI.  Plan is to keep two ROCm versions at a time, however we still need the 3.9 image due to some CI jobs depending on it.  Keep the 4.0.1 and 3.10 images, in addition to the 3.9 image until the 3.9 image is no longer needed.",pytorch
54635,jeffdaily,pr,2021-03-24T23:25:10Z,[ROCm] add 4.1 to nightly builds,Depends on https://github.com/pytorch/builder/pull/685.,pytorch
54651,robieta,pr,2021-03-25T03:48:21Z,extend benchmarks,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #54652 Add runner for instruction count benchmarks.
* **#54651 extend benchmarks**

This PR fleshes out the benchmarks to everything I could come up with. (166 individual cases when all is said and done.) If there's anything you feel warrants a spot in CI that I've missed, by all means let me know.

Differential Revision: [D27537824](https://our.internmc.facebook.com/intern/diff/D27537824)",pytorch
54652,robieta,pr,2021-03-25T03:48:27Z,Add runner for instruction count benchmarks.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#54652 Add runner for instruction count benchmarks.**
* #54651 extend benchmarks

This PR adds a fairly robust runner for the instruction count microbenchmarks. Key features are:

* Timeout and retry. (In rare cases, Callgrind will hang under heavy load.)
* Robust error handling and keyboard interrupt support.
* Benchmarks are pinned to cores. (Wall times still won't be great, but it's something.)
* Progress printouts, including a rough ETA.

Differential Revision: [D27537823](https://our.internmc.facebook.com/intern/diff/D27537823)",pytorch
54687,jeffdaily,pr,2021-03-25T15:57:09Z,add sndfile yum package to centos dockerfile,"Fixes error when running torch test suite inside a centos CI image.  As described by https://pypi.org/project/SoundFile/0.10.3.post1/, `On Linux, you need to install libsndfile using your distribution’s package manager`.  This was missing from the centos CI image.

```
python test_spectral_ops.py -v
...
Traceback (most recent call last):
  File ""test_spectral_ops.py"", line 25, in <module>
    import librosa
  File ""/opt/conda/lib/python3.6/site-packages/librosa/__init__.py"", line 211, in <module>
    from . import core
  File ""/opt/conda/lib/python3.6/site-packages/librosa/core/__init__.py"", line 6, in <module>
    from .audio import *  # pylint: disable=wildcard-import
  File ""/opt/conda/lib/python3.6/site-packages/librosa/core/audio.py"", line 8, in <module>
    import soundfile as sf
  File ""/opt/conda/lib/python3.6/site-packages/soundfile.py"", line 142, in <module>
    raise OSError('sndfile library not found')
OSError: sndfile library not found
```",pytorch
54727,jeffdaily,pr,2021-03-25T20:30:57Z,[ROCm] utilize PUBLIC vs PRIVATE linking to avoid incorrect dependencies,"Fixes the build of projects that depend on torch, such as torchaudio.  Otherwise torchaudio will complain that gloo_hip is missing.",pytorch
54910,zasdfgbnm,pr,2021-03-29T20:21:24Z,DO NOT MERGE,Empty `randperm_handle_duplicate_keys`,pytorch
54990,zasdfgbnm,pr,2021-03-30T21:39:43Z,DO NOT MERGE 2,"With RNG update, no kernel",pytorch
54992,zasdfgbnm,pr,2021-03-30T21:43:56Z,DO NOT MERGE 3,"with kernel, no write",pytorch
54994,zasdfgbnm,pr,2021-03-30T22:02:39Z,DO NOT MERGE 4,"no cleanup, always 64 bits
",pytorch
55015,zasdfgbnm,pr,2021-03-31T03:15:26Z,DO NOT MERGE 5,"Fixes #{issue number}
",pytorch
55069,jeffdaily,pr,2021-03-31T17:39:55Z,"[ROCm] if TEST_WITH_ROCM, only instantiate GPU device tests",Improves ROCm CI throughput by instantiating only for device tests that exercise the AMD GPU devices.,pytorch
55139,jjsjann123,pr,2021-04-01T10:05:44Z,patching graph_for,"Allows individual DifferentiableGraphOp to display optimized forward graph. This improves user visibility to graph mutation via optimization pass, especially fusion.",pytorch
55175,zasdfgbnm,pr,2021-04-01T21:29:36Z,DO NOT MERGE 6,"Fixes #{issue number}
",pytorch
55263,zasdfgbnm,pr,2021-04-02T23:20:16Z,DO NOT MERGE 7,"Fixes #{issue number}
",pytorch
55264,zasdfgbnm,pr,2021-04-02T23:23:22Z,DO NOT MERGE 8,"Fixes #{issue number}
",pytorch
55268,gmagogsfm,pr,2021-04-03T01:53:26Z,[JIT] Allow unpacking tuple and assign their values to SELECT-type expressions,"Fixes #51176
",pytorch
55308,zasdfgbnm,pr,2021-04-05T17:04:26Z,TEST ONLY,"Fixes #{issue number}
",pytorch
55428,robieta,pr,2021-04-06T23:27:11Z,Collect instruction counts (and wall times) for CI,"This PR add a `--mode` flag and a script to collect microbenchmarks in a single JSON file. I also added a version check since benchmarks are expected to evolve; this also turned up a determinism bug in `init_from_variants`. (`set` is not ordered, unlike `dict`)

Test plan: Run in CI

CC: @ngimel @wconstab @ezyang @bhosmer ",pytorch
55475,gmagogsfm,pr,2021-04-07T17:35:24Z,[Hackathon] Add error source range highlighting check in test_recursive_script.py,,pytorch
55482,gmagogsfm,pr,2021-04-07T18:01:14Z,[Hackathon] Add error source range highlighting check in test_scriptmod_ann,,pytorch
55487,gmagogsfm,pr,2021-04-07T18:05:43Z,[Hackathon] Add source range highlighting check to test_slice,,pytorch
55491,gmagogsfm,pr,2021-04-07T18:18:23Z,[Hackathon] Add source range highligh check to test_string_formatting,,pytorch
55495,gmagogsfm,pr,2021-04-07T18:26:18Z,[Hackathon] Add source highlight check in test_torchbind,,pytorch
55498,gmagogsfm,pr,2021-04-07T18:35:12Z,[Hackathon] Add source highlighting check in test_type_sharing,,pytorch
55501,gmagogsfm,pr,2021-04-07T18:39:53Z,[Hackathon] Add source highlighting check to test_unsupported_ops,"
",pytorch
55513,gmagogsfm,pr,2021-04-07T18:52:22Z,[Hackathon] Add source range highlight check to test_with,,pytorch
55565,jjsjann123,pr,2021-04-07T22:08:29Z,fixing Optional[Tensor] type in autodiff,"Fixes #54783

We need to be extra careful with the pattern to legitimately use `unchecked_unwrap_optional` in autodiff.
This would at least allow us to start support `Optional[Tensor]` in autodiff, which is quite common in composite layers.",pytorch
55693,zasdfgbnm,pr,2021-04-09T17:55:00Z,Migrate thrust->cub for index put,"64bit indexing is not supported, because if `num_indices = 2^31`, then 4 long tensors of `num_indices` elements will take 64GB RAM. I don't think anybody will be interested in running `index_put` with 64GB GPU RAM.

Benchmark on CUDA 11.3 RTX3090:
```python
import torch
import itertools


def run50_sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

    
run50_sync(lambda: torch.randperm(1000000, device='cuda'))

    
def benchmark(M, L):
    a = torch.randn(M, device='cuda')
    i1 = torch.randint(M, (L,), dtype=torch.long, device='cuda')
    v = torch.randn(L, device='cuda')

    torch.cuda.synchronize()

    %timeit run50_sync(lambda:a.index_put_((i1,), v, True))

for M, L in itertools.product((100, 100000, 10000000), repeat=2):
    print(M, L)
    benchmark(M, L)
```

Before
```
100 100
5.13 ms ± 91 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100 100000
30.2 ms ± 471 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
100 10000000
3.17 s ± 14.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
100000 100
5.19 ms ± 61.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000 100000
11.9 ms ± 200 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000 10000000
712 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
10000000 100
5.07 ms ± 66.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000 100000
12.1 ms ± 76.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000 10000000
627 ms ± 7.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

After
```
100 100
3.75 ms ± 49.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100 100000
26.2 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
100 10000000
2.81 s ± 23.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
100000 100
3.85 ms ± 16.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000 100000
9.74 ms ± 40.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000 10000000
444 ms ± 1.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
10000000 100
3.85 ms ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000 100000
10.7 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000 10000000
396 ms ± 2.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",pytorch
55701,jjsjann123,pr,2021-04-09T19:09:11Z,patching requires_grad on DifferentiableGraph,"The retrieval of profile node is much easier prior to inserting guard node.
test cases updated to reflect the patch on a previously failing cases.",pytorch
55758,z-a-f,pr,2021-04-11T23:59:42Z,[quant][refactor] Dedup the batch norm code,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55761 [quant] ConvBNReLU1d
* #55760 [quant][refactor] Renaming layer names in the batch norm test
* #55759 [quant][refactor] BatchNorm factor out common code
* **#55758 [quant][refactor] Dedup the batch norm code**

Batch norm 2d and 3d were carbon copies of each other. This factors out the duplicated portions

Differential Revision: [D27702396](https://our.internmc.facebook.com/intern/diff/D27702396/)",pytorch
55759,z-a-f,pr,2021-04-11T23:59:49Z,[quant][refactor] BatchNorm factor out common code,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55761 [quant] ConvBNReLU1d
* #55760 [quant][refactor] Renaming layer names in the batch norm test
* **#55759 [quant][refactor] BatchNorm factor out common code**
* #55758 [quant][refactor] Dedup the batch norm code

Batch norm had some duplicate code shared with the intrinsic. This factors out the common portions of it

Differential Revision: [D27701951](https://our.internmc.facebook.com/intern/diff/D27701951/)",pytorch
55760,z-a-f,pr,2021-04-11T23:59:56Z,[quant][refactor] Renaming layer names in the batch norm test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #55761 [quant] ConvBNReLU1d
* **#55760 [quant][refactor] Renaming layer names in the batch norm test**
* #55759 [quant][refactor] BatchNorm factor out common code
* #55758 [quant][refactor] Dedup the batch norm code

The layer names used to be sequential, but after many iterations are just layers named as conv1, conv3, etc., but not following the numbering sequence. This refactors the test for easier debug

Differential Revision: [D27702366](https://our.internmc.facebook.com/intern/diff/D27702366/)",pytorch
55761,z-a-f,pr,2021-04-12T00:00:02Z,[quant] ConvBNReLU1d,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#55761 [quant] ConvBNReLU1d**
* #55760 [quant][refactor] Renaming layer names in the batch norm test
* #55759 [quant][refactor] BatchNorm factor out common code
* #55758 [quant][refactor] Dedup the batch norm code

Although the 1d conv BN is already supported, this introduces `BNReLU1d` and `BatchNorm1d` for consistency.

Differential Revision: [D27702386](https://our.internmc.facebook.com/intern/diff/D27702386/)",pytorch
55767,jjsjann123,pr,2021-04-12T01:13:44Z,fixing DifferentiableGraph output with wrong requires_grad flag,"Fixing requires_grad on outputs from DifferentiableGraph, the proper flag is
retrieved from profiling information. We previously only retrieves the profiling
information on the first profile node in all its uses. However, in case where
control flows are present, we need to iteratively search for profile node with
profiling information available, in case the first use is in an inactive code
path.

e.g.
```
  graph(%0 : Tensor,
        %1 : Bool):
  ..., %2 : Tensor = prim::DifferentiableGraph_0(%0)
  %3 : Tensor = prim::If(%1)
    block0():
      %4 : Tensor = prim::DifferentiableGraph_1(%2)
      -> (%4)
    block1():
      %5 : Tensor = prim::DifferentiableGraph_2(%2)
      -> (%5)
  -> (%3)
with prim::DifferentiableGraph_0 = graph(%0 : Tensor):
  ...
  %out : Tensor = aten::operation(...)
  ...
  return (..., %out)
with prim::DifferentiableGraph_1 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Tensor](%0)
  ...
with prim::DifferentiableGraph_2 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Float(...)](%0)
  ...
```

Fixes #{issue number}
",pytorch
55896,Flamefire,pr,2021-04-13T13:00:36Z,Increase default test timeout for distributed tests,"When running on clusters the startup time for the subprocesses might be much higher which leads to spurious failures.
So increase this to 500s similar to torch/testing/_internal/distributed/distributed_test.py

FTR: In our tests we even use ridiculously high values such as 3000s to avoid this, so this is even conservative. Maybe an env variable to allow to override this easily would be a good idea?
",pytorch
55897,Flamefire,pr,2021-04-13T13:07:32Z,Fix passing of jit_opt_level,"The jit_opt_level must be passed as void* directly not by reference.
This avoids failures and miscompilations as the level will be kind of random instead of one of the valid values.

Fixes #52147
See the issue for more details",pytorch
55901,Flamefire,pr,2021-04-13T13:44:18Z,Deduplicate codegenOutputQuery to query maximum CUDA compute capabilities,"There were 2 versions of the same code which were slightly different although functionally equivalent.
When adding support for another CUDA / device version both would need to be changed and kept in sync. So it is better to have only 1 version of it as the unique source of truth.

I chose the implementation which looks cleaner and easier to read and added some minor enhancements and comments to further increase readability.
",pytorch
55904,Flamefire,pr,2021-04-13T14:14:45Z,Handle JIT test failure when the GPU is newer than the CUDA compiler,"The test uses the CUDA compute capabilities of the current device to compile an extension. If nvcc is older than the device, it will fail with a message like ""Unsupported gpu architecture 'compute_80'"" resulting in a `RuntimeError: Error building extension 'cudaext_archflags'` ultimately failing the test.

This checks for this case and allows execution to continue

Fixes #51950
",pytorch
55963,gmagogsfm,pr,2021-04-13T21:55:25Z,Workaround for jit.trace + TorchBind issue,"Fixes #{issue number}
",pytorch
55965,jeffdaily,pr,2021-04-13T22:23:00Z,[reland][ROCm] use hiprtc precompiled header,"Revert ""Revert D27449031: [pytorch][PR] [ROCm] use hiprtc precompiled header"".  Reland PR #54350.

This reverts commit 204ac21bf1457022caab197001788239720b96d6.

The original PR was reverted under suspicion that it was causing CI instability, but it was instead due to a hardware failure.",pytorch
55984,zasdfgbnm,pr,2021-04-14T03:04:57Z,Fix cxx11 abi,"Fixes https://github.com/pytorch/pytorch/issues/55829
",pytorch
56084,jeffdaily,pr,2021-04-14T22:06:52Z,[ROCm] miopen immediate mode,Implements the ability to skip the benchmarking of convolution kernels for ROCm/MIOpen.,pytorch
56141,Flamefire,pr,2021-04-15T12:42:09Z,Fix segmentation fault due to access to destroyed CudaIPCGlobalEntities instance,"There is an instance of the static destruction order fiasco where cuda_ipc_global_entities may be accessed after it is destroyed. See #51961

This change uses a flag and avoids accesses to the destroyed class when it is set to false.

Fixes #51961

This removes the function to clear shared_blocks introduced by #53080 which had multiple issues: Unprotected access to a shared structure and modification of the vector which is being cleared by the destructors of the objects contained.
I.e. what happened was:

- `CudaIPCSentDataLimbo_.clear_shared_blocks();` is called from the destructor of CudaIPCGlobalEntities as of your PR
- This deletes instances of `CudaIPCSentData` which hold `at::DataPtr` created by `GetNewRefCountedSentData`
- This means `CudaIPCSentDataDelete` is called with still active pointers
- Hence `CudaIPCSentDataLimbo_.add` is called adding a new value to `shared_blocks_`

",pytorch
56236,zasdfgbnm,pr,2021-04-16T06:06:39Z,DO NOT MERGE,"Fixes #{issue number}
",pytorch
56332,zasdfgbnm,pr,2021-04-17T22:46:36Z,do not merge 2,"Fixes #{issue number}
",pytorch
56337,zasdfgbnm,pr,2021-04-18T05:29:41Z,do not merge 3,"Fixes #{issue number}
",pytorch
56343,zasdfgbnm,pr,2021-04-18T17:19:28Z,do not merge 4,"Fixes #{issue number}
",pytorch
56344,zasdfgbnm,pr,2021-04-18T17:20:32Z,do not merge 5,"Fixes #{issue number}
",pytorch
56349,zasdfgbnm,pr,2021-04-19T03:41:03Z,do not merge 6,"Fixes #{issue number}
",pytorch
56379,zasdfgbnm,pr,2021-04-19T14:38:45Z,do not merge: Randperm debug8,"Fixes #{issue number}
",pytorch
56466,jjsjann123,pr,2021-04-20T11:32:09Z,Avoid DifferentiableGraph operation mutate requires_grad flag on input tensor list,"Fixes #55609 

Revert ""Manual revert of D27369251 (#56080)""

This reverts commit 92a09fb.",pytorch
56553,z-a-f,pr,2021-04-21T01:11:58Z,[sparsity] Moving only the C++ files from internal to OSS,"Summary:
This splits the previous diff into multiple parts. This introduces only the c++ files.

The unittests pass as part of the internal build. Will be put in the OSS in the later PRs

Test Plan:
`buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`

```
Parsing buck files: finished in 2.0 sec
Creating action graph: finished in 16.4 sec
Building: finished in 55.0 sec (100%) 20264/20264 jobs, 16 updated
  Total time: 01:13.6 min
More details at https://www.internalfb.com/intern/buck/build/c9c5e69e-ce00-4560-adce-58b68bc43e47
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: 1e678a07-0689-45b4-96f3-54d0a3181996
Trace available for this run at /tmp/tpx-20210415-161113.966600/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/3096224795029304
    ✓ ListingSuccess: caffe2/torch/fb/model_optimization:sparsity_test - main (4.186)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.752)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseKernels) (1.884)
    ✓ Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear_serdes (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (2.013)
Summary
  Pass: 3
  ListingSuccess: 1
```

Reviewed By: ailzhang

Differential Revision: D27833226

",pytorch
56555,z-a-f,pr,2021-04-21T01:43:33Z,"[sparsity][refactor] Remove ""Sparsity"" from the function names","Summary: Remove the ""sparse"" and ""sparsity"" from the function/variable names

Test Plan: `buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`

Reviewed By: raghuramank100

Differential Revision: D27812205

",pytorch
56617,z-a-f,pr,2021-04-21T19:55:40Z,[sparsity] Moving the sparsity python files to OSS,"Summary: This migrates the sparsity to the open source

Test Plan: `buck test mode/opt //caffe2/test:ao`

Reviewed By: raghuramank100

Differential Revision: D27812207

",pytorch
56711,robieta,pr,2021-04-22T17:43:37Z,Allow instruction counting to use shared memory as a staging ground. (And a couple other tweaks.),"This is actually something I discovered a while ago with the wall of serotonin. It was really easy for large scale runs to get bottlenecked on disk access. I have a hack in the working files of that machine to use `/dev/shm`, but I figured I should formalize and actually make a respectable utility.

I also added a param to tweak the run cadence and print when a CorePool is created; these are just to make the CI logs a bit nicer. (A printout each second on a 40 minute CI job is a bit much...)",pytorch
56750,zasdfgbnm,pr,2021-04-23T00:44:30Z,Migrate masked_scatter to use cub instead of thrust,"Benchmark:

```python
import torch
import itertools


def run50_sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

    
run50_sync(lambda: torch.randperm(1000000, device='cuda'))

    
def benchmark(M):
    a = torch.randn(M, device='cuda')
    m = torch.randint(1, (M,), dtype=torch.long, device='cuda').bool()
    v = torch.randn(M, device='cuda')

    torch.cuda.synchronize()

    %timeit run50_sync(lambda:a.masked_scatter_(m, v))

for M in (100, 1000, 100000, 10000000):
    print(M)
    benchmark(M)
```

Before:
```
100
8.65 ms ± 80.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
1000
8.75 ms ± 72.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000
9.27 ms ± 87.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000
33.6 ms ± 358 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

After
```
100
8.04 ms ± 37.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
1000
8.09 ms ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
100000
8.63 ms ± 76.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
10000000
31.9 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```",pytorch
56821,zasdfgbnm,pr,2021-04-23T20:49:49Z,Implement torch.sort with cub::DeviceSegmentedRadixSort,"Benchmark:
```python
import torch
import itertools

for i in range(1000):
    torch.arange(100000, device='cuda')

def run50_sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

for i, j in itertools.product([512, 4096, 8192], repeat=2):
    print(i,j)
    t = torch.randn(i, j, device='cuda')
    torch.cuda.synchronize()
    %timeit run50_sync(lambda: torch.sort(t))
    torch.cuda.synchronize()
    %timeit run50_sync(lambda: torch.sort(t, dim=0))
    print()
```

Before
```
512 512
4.02 ms ± 28.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5 ms ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

512 4096
40.7 ms ± 74.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
33.9 ms ± 186 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

512 8192
71.7 ms ± 636 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
66.4 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 512
27.6 ms ± 27.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
46.6 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 4096
262 ms ± 1.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
321 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

4096 8192
520 ms ± 5.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
661 ms ± 853 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 512
54.6 ms ± 133 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
83.2 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

8192 4096
521 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
645 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 8192
1.04 s ± 2.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.34 s ± 541 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

After
```
512 512
4.65 ms ± 62.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.75 ms ± 62.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

512 4096
30.3 ms ± 261 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
39.4 ms ± 421 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

512 8192
59.7 ms ± 344 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
77 ms ± 601 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 512
32.2 ms ± 376 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
37.1 ms ± 211 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

4096 4096
204 ms ± 471 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
272 ms ± 1.87 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

4096 8192
422 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
562 ms ± 4.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 512
63.1 ms ± 595 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
72.7 ms ± 532 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

8192 4096
401 ms ± 3.08 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
573 ms ± 2.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

8192 8192
831 ms ± 7.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
1.2 s ± 9.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",pytorch
56995,gmagogsfm,pr,2021-04-27T06:51:34Z,"Remove dtype hack for randperm, tril_indices and triu_indices","Fixes #{issue number}
",pytorch
57064,zasdfgbnm,pr,2021-04-27T21:23:20Z,cub block sort,"Fixes #{issue number}
",pytorch
57105,gmagogsfm,pr,2021-04-28T07:10:41Z,"Fix default dtype for randperm, triu/tril_indices inside TorchScript","Fixes #56676
",pytorch
57154,gmagogsfm,pr,2021-04-28T15:59:00Z,Remove duplicate entry for filter in language ref v2,"
",pytorch
57204,zasdfgbnm,pr,2021-04-28T22:25:46Z,"Fix NVRTC versioning for CUDA 11.X (X>=3), CUDA 12 and later","NVRTC versioning has changed starting 11.3, and will change again for CUDA 12.X. See comment in code for detail. As a result, jit on CUDA 11.3 is broken.

Also, the error message is misleading: When both `libname` and `alt_libname` are non-empty, the error message is only reporting `alt_libname`, it should report both.

To reproduce the error, you can use:

```python
import torch

torch._C._jit_set_profiling_mode(False)
torch._C._jit_set_profiling_executor(False)
torch._C._jit_override_can_fuse_on_cpu(True)
torch._C._jit_override_can_fuse_on_gpu(True)

@torch.jit.script
def jit_relu_dropout(x, prob) :
    # type: (Tensor, float) -> Tensor
    x = torch.nn.functional.relu(x)
    x = torch.nn.functional.dropout(x, p=prob, training=True)
    return x

x = torch.randn((64, 40, 12, 1024), device=""cuda:0"", dtype=torch.float16, requires_grad=True)
y = jit_relu_dropout(x, 0.5)
```
with CUDA 11.3, and you will see
```
Traceback (most recent call last):
  File ""/home/gaoxiang/misc/nvrtc-failure.py"", line 16, in <module>
    y = jit_relu_dropout(x, 0.5)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: Error in dlopen or dlsym: libnvrtc-8aa72235.so.11.3: cannot open shared object file: No such file or directory
```",pytorch
57218,zasdfgbnm,pr,2021-04-29T00:24:46Z,Remove debugging print in randperm,Sorry that I forget to delete this. Thank @xwang233 for finding this.,pytorch
57297,zasdfgbnm,pr,2021-04-29T20:16:10Z,DEBUG,"Fixes #{issue number}
",pytorch
57321,jjsjann123,pr,2021-04-29T23:36:27Z,Re-enable BatchNorm autodiff,"Turns on BN in autodiff:

1. outputs an empty tensor for running stats to by pass autodiff issue on None;
2. fixing BN inference backward in cudnn & miopen, where backward falls back to native batchnorm kernel instead;",pytorch
57364,zasdfgbnm,pr,2021-04-30T16:46:35Z,[resubmit] Remove sync for randperm on small tensors. (#54113),- [x] check MaskRCNN,pytorch
57400,jeffdaily,pr,2021-04-30T22:48:31Z,[ROCm] fix JIT codegen,"Fixes upcoming changes that are part of ROCm 4.2 and affect PyTorch JIT.

- ROCM_VERSION macro must be available to both device and host compilation passes.  
- Unifies some of CUDA and HIP differences in the code generated, specifically NAN / POS_INFINITY / NEG_INFINITY
- Differentiates bf16 codegen for HIP.
- Optionally provides missing macros when using hiprtc precompiled header feature.",pytorch
57496,gmagogsfm,pr,2021-05-03T21:28:19Z,"Revert ""Remove unused forward AD flag (#57058)"" and ""Improve perf for forward AD view handling (#57057)""","

This reverts commit 95dc2b6e9b419ddbd88cd974ad3e999773eb9f8f and
83f186717bbacad1be9f3ab44305d60f99e22137.
",pytorch
57506,jeffdaily,pr,2021-05-03T22:14:46Z,[ROCm] libtorch nightly builds,"Enables libtorch builds for ROCm, starting with ROCm version 4.5.2.",pytorch
57571,zasdfgbnm,pr,2021-05-04T21:52:55Z,Fix internal assert in CUDA caching allocator when trying to allocate ~2^64 memory,"When the memory requested is huge, some internal logic in CUDA caching allocator could overflow. The result of the overflow is the caching allocator gives a confusing error message.

For example:

```python
import torch
import torch.nn as nn
from torch.utils import cpp_extension
cuda_source = """"""
#include <c10/cuda/CUDACachingAllocator.h>
void my_fun(void)
{
    size_t temp_storage_bytes = 18446744073708433663UL;
    auto& caching_allocator = *::c10::cuda::CUDACachingAllocator::get();
    auto temp_storage = caching_allocator.allocate(temp_storage_bytes);
    return;
}
""""""
cpp_source = """"""
    void my_fun(void);
""""""
module = torch.utils.cpp_extension.load_inline(
    name=""cuda_test_extension"",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=""my_fun"",
    extra_cuda_cflags=[""--extended-lambda""],
    verbose=True,
)
module.my_fun()
print('done')
```

gives

```
Traceback (most recent call last):
  File ""/home/gaoxiang/misc/caching-allocator.py"", line 26, in <module>
    module.my_fun()
RuntimeError: p.block != nullptr && p.block->ptr != nullptrINTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":991, please report a bug to PyTorch. 
Exception raised from alloc_block at ../c10/cuda/CUDACachingAllocator.cpp:991 (most recent call first):
frame #0: <unknown function> + 0x83e93 (0x7f424f05ee93 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x83bf9 (0x7f424f05ebf9 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0x839bd (0x7f424f05e9bd in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const + 0x4c (0x7f428a3350a2 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x40 (0x7f424f05dc34 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #5: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x97 (0x7f424f05c42f in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #6: <unknown function> + 0x6948b4 (0x7f42978fd8b4 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x22373 (0x7f424f0e2373 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: <unknown function> + 0x1fa6c (0x7f424f0dfa6c in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #9: <unknown function> + 0x2337a (0x7f424f0e337a in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x23f18 (0x7f424f0e3f18 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #11: my_fun() + 0x4b (0x7f4200338f74 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #12: torch::detail::wrap_pybind_function_impl_<void (&)()>(void (&)(), std::integer_sequence<unsigned long>)::{lambda()#1}::operator()() const + 0x3f (0x7f420031e575 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #13: <unknown function> + 0x570f2 (0x7f42003350f2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #14: <unknown function> + 0x536e2 (0x7f42003316e2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #15: <unknown function> + 0x4ef2f (0x7f420032cf2f in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #16: <unknown function> + 0x4ef93 (0x7f420032cf93 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
frame #17: <unknown function> + 0x3e7f2 (0x7f420031c7f2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)
<omitting python frames>
frame #30: __libc_start_main + 0xd5 (0x7f42c60bab25 in /usr/lib/libc.so.6)
```
",pytorch
57574,jjsjann123,pr,2021-05-04T22:50:45Z,fixing DifferentiableGraphOp updating requires_grad on input tensor list; python test added to verify the test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #57575 Rework requires_grad on DifferentiableGraphOp
* **#57574 fixing DifferentiableGraphOp updating requires_grad on input tensor list; python test added to verify the test**

Differential Revision: [D29038774](https://our.internmc.facebook.com/intern/diff/D29038774)",pytorch
57575,jjsjann123,pr,2021-05-04T22:50:50Z,Rework requires_grad on DifferentiableGraphOp,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57575 Rework requires_grad on DifferentiableGraphOp**
* #57574 fixing DifferentiableGraphOp updating requires_grad on input tensor list; python test added to verify the test

This PR does two things:

1. reverts ""Manual revert of D27369251 (#56080)"" in commit
   92a09fb87a567100122b872613344d3a422abc9f.

2. fixing DifferentiableGraph output with wrong requires_grad flag

Fixing requires_grad on outputs from DifferentiableGraph, the proper flag is
retrieved from profiling information. We previously only retrieves the profiling
information on the first profile node in all its uses. However, in case where
control flows are present, we need to iteratively search for profile node with
profiling information available, in case the first use is in an inactive code
path.

e.g.
```
  graph(%0 : Tensor,
        %1 : Bool):
  ..., %2 : Tensor = prim::DifferentiableGraph_0(%0)
  %3 : Tensor = prim::If(%1)
    block0():
      %4 : Tensor = prim::DifferentiableGraph_1(%2)
      -> (%4)
    block1():
      %5 : Tensor = prim::DifferentiableGraph_2(%2)
      -> (%5)
  -> (%3)
with prim::DifferentiableGraph_0 = graph(%0 : Tensor):
  ...
  %out : Tensor = aten::operation(...)
  ...
  return (..., %out)
with prim::DifferentiableGraph_1 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Tensor](%0)
  ...
with prim::DifferentiableGraph_2 = graph(%0 : Tensor):
  %temp : Tensor = prim::profile[profiled_type=Float(...)](%0)
  ...
```

Differential Revision: [D29038773](https://our.internmc.facebook.com/intern/diff/D29038773)",pytorch
57751,jeffdaily,pr,2021-05-06T17:16:31Z,[ROCm] ubuntu version check in install_rocm.sh,In preparation for ROCm 4.2 release changing the apt repo name from xenial to ubuntu.,pytorch
57884,gmagogsfm,pr,2021-05-08T02:18:24Z,Preserve custom state_dict save/load methods in TorchScript,"Previously, methods like `_save_to_state_dict` and `_load_from_state_dict` are incorrectly overwritten when we copy methods from `nn.Module` to `RecursiveScriptModule`. We should instead check if user module defines those methods and exported them to conditionally copy methods from `nn.Module` over to `RecursiveScriptModule`.

In this PR, I temporarily limit the fix to state_dict related methods to limit blast radius since this change is technically BC-breaking. 

Fixes #45225
",pytorch
57886,gmagogsfm,pr,2021-05-08T06:02:34Z,Add tests for custom state_dict save/load methods in TorchScript,"
",pytorch
57962,zasdfgbnm,pr,2021-05-10T17:48:16Z,Fix UB in library.h,"The function name and return type both are called `class_`, therefore they are ambiguous and this is UB and does not work on NVCC. See the tests for the failure case.
 
Thanks for the help of Thibaut Lutz from NVIDIA's compiler team.

cc: @yueyericardo @ptrblck ",pytorch
57984,zasdfgbnm,pr,2021-05-10T21:00:00Z,Add tests for bfloat16 math functions on CUDA,"I copy paste `cuda_half_test.cu` to `cuda_bfloat16_test.cu`  and change it to test bfloat16. It do find a few issues:
- `CUDA_VERSION` sometimes doesn't work on c10 (I don't know the reason), I changed it to use `__CUDACC_VER_MAJOR__` instead
- The `operator __nv_bfloat16()` of `c10::BFloat16` should not be explicit
- PyTorch should be built with `-D__CUDA_NO_BFLOAT16_OPERATORS__` to be consistent with half behavior
- There is a `assert(::abs(::atanh(Half(1.0)) - ::atanh(1.0f)) <= threshold);` in the test, this doesn't make sense, because `atanh(1)` is inf.",pytorch
58132,Flamefire,pr,2021-05-12T09:54:22Z,Fix alias violation in binary ops,"This removes an invalid reinterpret_cast (cast of unrelated types is undefined behavior) and uses the proper load/store instead

Fixes #58031
",pytorch
58143,jeffdaily,pr,2021-05-12T15:39:58Z,[ROCm] add rocm4.2 to nightly builds,Depends on https://github.com/pytorch/builder/pull/764.,pytorch
58196,zasdfgbnm,pr,2021-05-12T23:13:58Z,BFloat16 support for torch.sort,"
",pytorch
58478,Flamefire,pr,2021-05-18T13:15:17Z,Fix usage of TORCH_INTERNAL_ASSERT with message ,"Using only a string as the argument for TORCH_INTERNAL_ASSERT will never trigger a failure as a string is always a truethy value.
This hides actual bugs and makes users and devs think all worked while it did not.
Change to use `TORCH_INTERNAL_ASSERT(false, ""msg"")`

I found this because on PPC (where there is no XNNPACK) one of the tests failed:

```
FAIL: test_preserve_bundled_inputs_methods (__main__.TestOptimizer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_mobile_optimizer.py"", line 328, in test_preserve_bundled_inputs_methods
    self.assertFalse(hasattr(incomplete_bi_module_optim, 'get_all_bundled_inputs'))
AssertionError: True is not false
```

This happens because the assertion is not fired and the module is returned unchanged: https://github.com/pytorch/pytorch/compare/master...Flamefire:assert_fix?expand=1#diff-5d18cdc66f45034594ef1f12d8ed1e9e21c21ae75fb55eb88713f26e49f8eb74R266

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
58553,Flamefire,pr,2021-05-19T11:23:45Z,Fix arange functions for VSX specializations of Vec256,"Need a templated 2nd parameter to support e.g. double steps even for int vectors.

This extends https://github.com/pytorch/pytorch/pull/34555 x86 specific fix to VSX instruction set.

Fixes #58551
",pytorch
58564,Flamefire,pr,2021-05-19T15:52:51Z,Only sync CUDA if the operation is run on GPU,"This fixes test failures when PyTorch is build without CUDA

Fixes #58563

I used the same is_cuda check that is used in test_nn.py",pytorch
58703,z-a-f,pr,2021-05-20T21:20:21Z,[sparse] Add the AO namespace to torch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparse] Sparsifier class
* #58705 [sparsity] Sparse linear layers
* **#58703 [sparse] Add the AO namespace to torch**

Differential Revision: [D28970962](https://our.internmc.facebook.com/intern/diff/D28970962)",pytorch
58704,z-a-f,pr,2021-05-20T21:20:27Z,[sparsity] Sparsifier class,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* **#58704 [sparsity] Sparsifier class**
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

Implements the base sparsifier class based on the #59835 RFC documents.

This PR implements the base class for the sparsification. Specifically, the prepare method is implemented.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D28970958](https://our.internmc.facebook.com/intern/diff/D28970958)",pytorch
58705,z-a-f,pr,2021-05-20T21:20:32Z,[sparsity] Sparsity parametrization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* **#58705 [sparsity] Sparsity parametrization**
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

The basic demo for this particular implementation can be found here:
https://gist.github.com/z-a-f/1d06ae8d5a509d3c9c1596dcb924afe0

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D28970959](https://our.internmc.facebook.com/intern/diff/D28970959)",pytorch
58706,z-a-f,pr,2021-05-20T21:20:38Z,[sparsity] Sparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #58707 [sparsity][refactor] Import factoring out
* **#58706 [sparsity] Sparsifier**
* #58705 [sparsity] Sparse linear layers
* #58704 [sparse] Sparsifier class
* #58703 [sparse] Add the AO namespace to torch

",pytorch
58707,z-a-f,pr,2021-05-20T21:20:43Z,[sparsity][refactor] Import factoring out,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* **#58707 [sparsity][refactor] Import factoring out**
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

Minor refactor that changes the format of the import.
This is done to avoid accidental circular dependencies.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D28970961](https://our.internmc.facebook.com/intern/diff/D28970961)",pytorch
58865,zasdfgbnm,pr,2021-05-24T17:16:23Z,[resubmit] masked_scatter thrust->cub,See ae7760cf50bb2cddff4663a07b9d68decf4b6c75 for the fix,pytorch
58955,z-a-f,pr,2021-05-25T22:03:51Z,[sparsity] WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* **#58955 [sparsity] WeightNormSparsifier**
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

Implements the weight norm sparsifier.
This type of sparsifier computes the norm of the weights, sorts them, and zeroes-out the target fraction of them.

The main imeplemented method is `update_mask`, which holds the main logic of changing the masks.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D28970960](https://our.internmc.facebook.com/intern/diff/D28970960)",pytorch
59043,z-a-f,pr,2021-05-26T21:53:13Z,[quant] Documentation on custom modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59043 [quant] Documentation on custom modules**

Differential Revision: [D28726000](https://our.internmc.facebook.com/intern/diff/D28726000)",pytorch
59182,gmagogsfm,pr,2021-05-30T05:20:41Z,[WIP] Computed GOTO optimization in TS interpreter,"Fixes #{issue number}
",pytorch
59220,Flamefire,pr,2021-05-31T14:36:33Z,Pass WITH_BLAS option from environment to CMake,"Allows to choose the BLAS backend with Eigen. Previously this was a CMake option only and the env variable was ignored.

Related to https://github.com/pytorch/pytorch/commit/f1f3c8b0fad9d647454a4d0507a2db4381563c8e

The claimed options `BLAS=BLIS WITH_BLAS=blis` are misleading: When `BLAS=BLIS` is set the `WITH_BLAS` option does not matter at all, it would only matter for `BLAS=Eigen` hence this issue went undetected so far.
",pytorch
59343,zasdfgbnm,pr,2021-06-02T22:51:08Z,Add AT_DISPATCH_ALL_OPAQUE_TYPES,"@ngimel Do you like this idea? If yes, I will open some follow-up PRs to migrate other ops to this dispatch macro.
",pytorch
59382,Flamefire,pr,2021-06-03T16:54:27Z,Fix vectorized calculations on POWER,"This fixes multiple bugs introduced by the VSX optimized code in https://github.com/pytorch/pytorch/pull/41541

- min/max/clamp now consistently return nan when any value is NaN as on other architectures
- The non-complex angle functions return PI for negative values now
- The complex angle functions have been corrected and optimized
- The float32-log function implementation returned a wrong result when inf was passed (and maybe other inputs), replaced by the sleef function just as for float64

Fixes #59248
Fixes #57537
",pytorch
59389,robieta,pr,2021-06-03T17:53:27Z,Add no-op benchmarks to test overhead,"This stack is focused on overhead, but it's not straightforward to actually measure our current overhead. So the first step is to add noop kernels that exercise the machinery, and where effectively 100% of the work can be characterized as ""overhead"".

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59393
* #59392
* #59391
* **#59389**

",pytorch
59390,robieta,pr,2021-06-03T17:53:33Z,Optimize AutoDispatchBelowADInplaceOrView,"The DispatchKeyGuards have to play XOR games to work around the zero initialization of POD locals, but if there is no bit collision (which is known at compile time and can enforce with a static_assert) we can skip the transform and just use the ""normal"" bit manipulation formulas.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59395 proof of concept for redispatch inline optimization
* #59394 Mirror shouldRunRecordFunction state in central TLS
* #59393 optimize checks
* #59392 move requires grad into central TLS
* #59391 move local dispatch key storage to a TLS seperate from the LocalDispatchKeySet.h
* **#59390 Optimize AutoDispatchBelowADInplaceOrView**
* #59389 Add no-op benchmarks to test overhead
",pytorch
59391,robieta,pr,2021-06-03T17:53:39Z,move local dispatch key storage to a TLS seperate from the LocalDispatchKeySet.h,"There are currently four TLS lookups on the autograd dispatch key path:
1) A lookup to fetch the local dispatch key set for initial call
2) A lookup to check the RecordFunction state to determine if we need to record the call
3) A lookup inside the autograd kernel to check if grad mode is active
4) A lookup of the same keyset from (1) for AutoDispatchBelowADInplaceOrView

If we coalesce thread local state into a single struct, we can reuse the pointer and skip TLS address lookups. This is complicated, however, by the requirement that this thread local data be plain old data (POD). This PR factors the dispatch key state into a standalone TLS struct and adapts the local dispatch key machinery to use external storage (the central TLS); this means that if one already has the TLS pointer on hand operations on the local dispatch key set can skip the lookup work. Subsequent PRs in the stack add additional fields and replicate the pattern. The reason I started with a degenerate (single field)  case was to validate that separating the storage and accessor did not impact perf.

There is an additional finesse in this PR. Due to the POD nature of the TLS, we have to play some XOR games with the storage. However in common special cases (which are known at compile time) there is no bit overlap and we can skip the XOR conversion and still maintain correctness.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59393
* #59392
* **#59391**
* #59389
",pytorch
59392,robieta,pr,2021-06-03T17:53:45Z,move requires grad into central TLS,"Coalesce grad mode enabled into the new central TLS. This allows us to skip a redundant TLS lookup in the Autograd kernels. (Since we will need the TLS for AutoDispatchBelowADInplaceOrView as well.) We also inline that function, since it is tiny and on the hot path.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #59393
* **#59392**
* #59391
* #59389
",pytorch
59393,robieta,pr,2021-06-03T17:53:51Z,optimize checks,"This PR contains several thematically similar micro optimizations:
* Replace `unpack` and `throw_error_for_complex_autograd` with macros. We're currently slinging around compile-time-known strings for the error messages, because C++ isn't good about optimizing that away.
* Kill `unpack_opt`. It is never used.
* `checked_cast_variable` is a relic of the Tensor/Variable merge, and is no longer meaningful.
* Add a simple `Tensor&` overload for `isFwGradDefined` (we're currently wrapping it in an optional, and then unwrapping that in the body), and inline this overload in the header to shave a couple more instructions.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59393**
* #59392
* #59391
* #59389
",pytorch
59394,robieta,pr,2021-06-03T17:53:57Z,Mirror shouldRunRecordFunction state in central TLS,"This PR will probably warrant some discussion. The current RecordFunction TLS is non-trivial (it contains a vector of classes), and is therefore not appropriate to move into the central TLS. That said, `shouldRunRecordFunction` doesn't need visibility into all of that detail; it just checks if there are any callbacks registered. This is a strawman which mirrors some invariants into the central TLS, with the idea that updates are infrequent and reuse of the TLS is useful. That said, this PR does violate Edward's law: ""thou shalt not make an invalid state representable.""

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59394 Mirror shouldRunRecordFunction state in central TLS**
* #59393 optimize checks
* #59392 move requires grad into central TLS
* #59391 move local dispatch key storage to a TLS seperate from the LocalDispatchKeySet.h
* #59389 Add no-op benchmarks to test overhead

",pytorch
59395,robieta,pr,2021-06-03T17:54:03Z,proof of concept for redispatch inline optimization,"Right now the `::redispatch` functions are in a cpp file, which costs about 30 instructions relative to inlining them. (Or defining extern TypedOperatorDefs) This PR isn't really for submission, but rather just to start a discussion about how we can collect this perf. (Since it will be really easy to run afoul of subtle linker / binary size / SOIF issues if we just start codegen-ing massive numbers of static defs.)

Perf: (instructions in C++)
```
function unary             504 -> 395*  (22%)
function binary            634 -> 473*	(25%)
function (manual) unary    489 -> 344	(30%)
function (manual) binary   613 -> 395	(35%)
method unary               504 -> 395*	(22%)
method binary              634 -> 473*	(25%)
method (manual) unary      489 -> 344	(30%)
method (manual) binary     613 -> 395	(35%)

*Does not inline redispatch, but could.
```

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#59395 proof of concept for redispatch inline optimization**
* #59394 Mirror shouldRunRecordFunction state in central TLS
* #59393 optimize checks
* #59392 move requires grad into central TLS
* #59391 move local dispatch key storage to a TLS seperate from the LocalDispatchKeySet.h
* #59389 Add no-op benchmarks to test overhead
",pytorch
59443,Flamefire,pr,2021-06-04T12:48:50Z,Add dump function for all Vectorized specializations,"This really helps when debugging issues hence introduce a generic function instead of the per-instantiation dump member function so it can be used consistently.

I needed this for my work on the POWER issues and noticed only some classes/specializations of the `vec.dump()` function, hence the added `dump(vec)` free function which avoids the need to add one for each new specialization.

I'm sure this can be helpful to others too and would even suggest to dump the `dump` member functions (Pun intended ;-) )
",pytorch
59518,gmagogsfm,pr,2021-06-06T03:54:13Z,Merge script and script_pdt methods,"Fixes #{issue number}
",pytorch
59622,Flamefire,pr,2021-06-08T10:34:33Z,Use a world size of 2 for test_zero_redundancy_optimizer,"This is the only working configuration for those tests at the moment.

Fixes #59548

See https://github.com/pytorch/pytorch/commit/787854ce413e7b73614fac60f9af767ea6dac3be which already limits it to 2-4 but https://github.com/pytorch/pytorch/issues/53322#issuecomment-821551901 reported that it also fails with 3, which I can confirm (see #59548). At least 1 test also fails with 4 GPUs, hence use 2 here which always works.
",pytorch
59624,Flamefire,pr,2021-06-08T10:38:37Z,Fix accuraccy failures when running test_nn on A100s,"Make sure tests run explicitely without TF32 don't use TF32 operations

Fixes https://github.com/pytorch/pytorch/issues/52278

After the tf32 accuracy tolerance was increased to 0.05 this is the only remaining change required to fix the above issue (for TestNN.test_Conv3d_1x1x1_no_bias_cuda)
",pytorch
59659,gmagogsfm,pr,2021-06-08T20:30:51Z,Correctly model aliasing in pin_memory op schema,"Fixes #59652
",pytorch
59770,z-a-f,pr,2021-06-10T02:14:55Z,[sparsity] Base sparsity level scheduler class,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* **#59770 [sparsity] Base sparsity level scheduler class**
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

Implements the base scheduler class for changing the sparsity levels in the sparsifier.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D29070603](https://our.internmc.facebook.com/intern/diff/D29070603)",pytorch
59771,z-a-f,pr,2021-06-10T02:15:00Z,[sparsity] Lambda Scheduler,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* **#59771 [sparsity] Lambda Scheduler**
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

Implements a specific sparsity scheduler, that uses a user-provided lambda's to change the levels.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D29070604](https://our.internmc.facebook.com/intern/diff/D29070604)",pytorch
59981,jjsjann123,pr,2021-06-14T21:08:21Z,fixing illegal memory access on NHWC BN kernel,adding an early exit in the kernel to avoid reading out of bound.,pytorch
60032,z-a-f,pr,2021-06-15T18:18:49Z,[sparsity][refactor] Restructure the tests folders,"Summary: There will be more sparse tests coming. This PR creates a separate folder for the sparse tests

Test Plan: `python test/test_ao.py`

Differential Revision: D29139265

",pytorch
60256,Flamefire,pr,2021-06-18T09:11:43Z,Copy Tensor for tests to avoid in-place transform modifying the original tensor,"Fixes #48591
",pytorch
60409,jeffdaily,pr,2021-06-21T21:55:56Z,restore JOB_BASE_NAME for test1 and test2 in test.sh,"JOB_BASE_NAME for test1 and test2 were removed by #60124.  This caused the ROCm CI to run all tests for both test1 and test2.  Restore the use of JOB_BASE_NAME.

Fixes #60377.
",pytorch
60450,Flamefire,pr,2021-06-22T09:41:12Z,Fix test failures with some glibc libraries,"Large complex values lead to nan/inf results when using some glibc
implementations of atanh/acos
- Skip test_reference_numerics_hard instead of ""normal""
- Test the edge values only for cdouble where the stdlib/glibc implementations support those large values

Fixes #60259
",pytorch
60451,Flamefire,pr,2021-06-22T09:46:43Z,Increase some tolerances for tf32 for Conv3d tests,"Allow those tests to pass on A100 GPUs which support tf32


Basically follow-up to https://github.com/pytorch/pytorch/pull/52871 which also increased some precisions to 0.05

For reference these are the failures I see (only ones in testnn with 1.9.0):
```
FAIL: test_Conv3d_pad_same_cuda_tf32 (__main__.TestNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""test_nn.py"", line 11296, in with_tf32_on
    test.test_cuda(self, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_nn.py"", line 5103, in test_cuda
    test_case.assertEqualIgnoreType(cpu_d_i, gpu_d_i, atol=self.precision, rtol=0)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1254, in assertEqualIgnoreType
    return self.assertEqual(*args, exact_dtype=False, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1355, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0 and atol=0.005, found 161 element(s) (out of 288) whose difference(s) exceeded the margin of error (including 0 nan compariso
ns). The greatest difference was 0.032408137116391345 (-33.45570601919647 vs. -33.42329788208008), which occurred at index (2, 0, 0, 1, 0).

======================================================================
FAIL: test_Conv3d_pad_same_dilated_cuda_tf32 (__main__.TestNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""test_nn.py"", line 11296, in with_tf32_on
    test.test_cuda(self, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_nn.py"", line 5103, in test_cuda
    test_case.assertEqualIgnoreType(cpu_d_i, gpu_d_i, atol=self.precision, rtol=0)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1254, in assertEqualIgnoreType
    return self.assertEqual(*args, exact_dtype=False, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1355, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0 and atol=0.005, found 111 element(s) (out of 288) whose difference(s) exceeded the margin of error (including 0 nan compariso
ns). The greatest difference was 0.024654212557543076 (35.104286017977465 vs. 35.07963180541992), which occurred at index (3, 0, 0, 0, 2).

======================================================================
FAIL: test_Conv3d_pad_valid_cuda_tf32 (__main__.TestNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1033, in wrapper
    method(*args, **kwargs)
  File ""test_nn.py"", line 11296, in with_tf32_on
    test.test_cuda(self, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_nn.py"", line 5103, in test_cuda
    test_case.assertEqualIgnoreType(cpu_d_i, gpu_d_i, atol=self.precision, rtol=0)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1254, in assertEqualIgnoreType
    return self.assertEqual(*args, exact_dtype=False, **kwargs)
  File ""/tmp/easybuild-tmp/eb-ED4M3d/tmpqOhUjN/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 1355, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0 and atol=0.005, found 41 element(s) (out of 288) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.010903167642320355 (8.074376869119371 vs. 8.06347370147705), which occurred at index (0, 0, 1, 0, 0).

```",pytorch
60458,Flamefire,pr,2021-06-22T12:12:44Z,Increase tolerance for test_grad_scaling_clipping,"This makes it pass on A100 and with e.g. torch.manual_seed(6) called before running this test.

Fixes #60455",pytorch
60462,Flamefire,pr,2021-06-22T14:39:16Z,Increase tolerance for some distributed tests to 5e-5,"On A100 GPUs 10 tests fail due to slightly higher deviations.
This fixes those.

Note that rtol is still the default and atol was increased by a factor of 5 (from 1e-5)

The failing tests were:

- test_accumulate_gradients_module
- test_accumulate_gradients_module_with_grad_is_view
- test_ddp_checkpointing_once
- test_ddp_checkpointing_twice
- test_ddp_checkpointing_unused_params
- test_ddp_checkpointing_weight_sharing
- test_nccl_backend_1gpu_module_device_ids_integer_list
- test_nccl_backend_1gpu_module_device_ids_torch_device_list
- test_nccl_backend_single_device_module_device_ids_None
- test_nccl_backend_single_device_module_empty_device_id",pytorch
60602,jeffdaily,pr,2021-06-23T22:56:00Z,[ROCm] allow user to override PYTORCH_ROCM_ARCH,"Restores the ability of a user to call .jenkins/pytorch/build.sh while
also setting PYTORCH_ROCM_ARCH. Otherwise, with IN_CI=1 as the new
default, it will forcibly ignore user settings when build.sh is used
outside of CI.",pytorch
60607,jeffdaily,pr,2021-06-23T23:19:50Z,use explicitly non-returning GPU atomics,"Enables an important performance optimization for ROCm, in light of the discussion in #41028.

CC @jithunnair-amd @sunway513",pytorch
60714,jjsjann123,pr,2021-06-25T03:00:50Z,clang-tidy patch,"Two changes made here:
1. Set `LANG=C.UTF-8` for clang-tidy so we can properly decode symbols in comment;
2. In case of file removed, `end` could be null and we should skip the chunk/file;
3. tiny bug fix for the loop indent.",pytorch
60728,z-a-f,pr,2021-06-25T08:01:39Z,[quant][sparsity] Generic convert function,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60728 [quant][sparsity] Generic convert function**
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* #60887 [sparsity] Add sparsity tests to run_test.py

This is a wrapper that implements a common convert function for use with quantization and sparsity.
Currently, only the whole model/module conversion is supported.

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D29465899](https://our.internmc.facebook.com/intern/diff/D29465899)",pytorch
60850,z-a-f,pr,2021-06-28T06:05:52Z,[sparsity][refactor] Changing linear row/col control,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* **#60850 [sparsity][refactor] Changing linear row/col control**
* #60887 [sparsity] Add sparsity tests to run_test.py

Test Plan:

```
python test/test_ao_sparsity.py
```

Differential Revision: [D29465900](https://our.internmc.facebook.com/intern/diff/D29465900)",pytorch
60887,z-a-f,pr,2021-06-28T17:53:10Z,[sparsity] Add sparsity tests to run_test.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #60728 [quant][sparsity] Generic convert function
* #59771 [sparsity] Lambda Scheduler
* #59770 [sparsity] Base sparsity level scheduler class
* #58955 [sparsity] WeightNormSparsifier
* #58707 [sparsity][refactor] Import factoring out
* #58704 [sparsity] Sparsifier class
* #58705 [sparsity] Sparsity parametrization
* #60850 [sparsity][refactor] Changing linear row/col control
* **#60887 [sparsity] Add sparsity tests to run_test.py**

Test Plan:

```
./test/run_test.py -i test_ao_sparsity
```

Differential Revision: [D29465834](https://our.internmc.facebook.com/intern/diff/D29465834)",pytorch
60969,z-a-f,pr,2021-06-29T17:37:58Z,Enable jit tracing to parametrization and add jit tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#60969 Enable jit tracing to parametrization and add jit tests**

This PR fixes the tracing in the parametrizations.
The current resolution is that when tracing is performed while caching is enabled, we throw an error.
Without caching, the tracing should work properly (tests added).

Currently, the parametrizations don't support scripting.
This PR introduces the same logic as with the tracing (throw error if caching).
However, the scripting itself cannot enabled due to the use of the generator expressions in the parametrizations.
Added TODO to fix it.

Differential Revision: [D29462887](https://our.internmc.facebook.com/intern/diff/D29462887)",pytorch
61028,robieta,pr,2021-06-30T07:01:46Z,const-ify LocalDispatchKeySet members,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#61028**
* #59393
* #59392
* #59391
* #59389

",pytorch
61331,gmagogsfm,pr,2021-07-07T03:49:16Z,Allow dims=0 in torch.tensordot call,"In one of my previous PRs that rewrite `tensordot` implementation, I mistakenly take empty value of `dims_a` and `dims_b` as illegal values. This turns out to be not true. Empty `dims_a` and `dims_b` are supported, in fact common when `dims` is passed as an integer. This PR removes the unnecessary check.

Fixes #61096
",pytorch
61554,jeffdaily,pr,2021-07-12T21:38:50Z,ignore and clear cudaErrorNotReady errors,"Follow-up to #18584. This PR covers the remaining places where event or stream query might result in not ready errors.
",pytorch
61875,zasdfgbnm,pr,2021-07-19T23:50:08Z,Adding warning on isend about modifying after send,"This is a standard limitation on communication collective libraries. For example:

https://www.open-mpi.org/doc/v4.0/man3/MPI_Isend.3.php
```
A nonblocking send call indicates that the system may start copying data out of the send buffer. The sender should not modify any part of the send buffer after a nonblocking send operation is called, until the send completes.
```

http://openucx.github.io/ucx/api/latest/html/group___u_c_p___c_o_m_m.html#ga8323878b60f426c630d4ff8996ede3cc
```
The user should not modify any part of the buffer after this operation is called, until the operation completes.
```",pytorch
61943,ppwwyyxx,pr,2021-07-21T02:21:10Z,[pytorch] don't LOG by default when everything is fine,"Summary:
These logs are added in D24183463 (https://github.com/pytorch/pytorch/commit/c83314e9829822fcdc765e0b1eb9585a6b114c83) for debugging.

However in fbcode they appear in everyone's job. A normal job that runs fine should ideally not print extra logs. So I changed some that I observed to VLOG.

Test Plan: no longer see these logs when training

Differential Revision: D29810706



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
62270,zasdfgbnm,pr,2021-07-27T16:18:16Z,[DO NOT REVIEW YET] Allow NCCL pg to send and recv CPU tensors with UCC,cc: @ptrblck ,pytorch
62458,robieta,pr,2021-07-30T06:19:24Z,Initial commit for Timer refactor: worker + task + frontend model,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* #62462
* #62461
* #62460
* #62459
* __->__ #62458
* #62740

# NOTE: This is a rough proposal, and is not feature complete.

This stack is a prototype proposal to refactor `torch.utils.benchmark.Timer` into a series of modular, composable components.

## Worker
The lowest level of this proposed architecture is the Worker. At a high level, the idea is to have a very restricted set of semantics (effectively just an interactive Python shell), and then build complexity from higher levels. In this PR, the worker in question is simply using `compile` and `exec` to run code in the current interpreter, just like `timeit.Timer`. However the Worker interface is much more restrictive, and later entries in this stack will add workers with different properties. (And show the benefits of being able to swap them out.)

## Task
This is where most of the ""smarts"" are expected to live. Currently there is only a simple timed loop, but later PRs will introduce the more complex existing methods like `blocked_autorange` and `adaptive_autorange`. The API of a task is pretty simple: just a single property to expose the worker that actually runs task logic. (This is to support #62460) Otherwise, details such as initialization and calling convention are up to the author on a case by case basis.

Tasks are expected to be the primary avenue of ""hackability"" in the new Timer structure. For instance, if I wanted to make a timer that would automatically JIT `stmt` that would be done by subclassing or forking an existing task and adjusting it accordingly.

## Timer (frontend)
In this proposed architecture, much of the code which currently lives in Timer moves to tasks. Consequently, the `Timer` class becomes a simple frontend, providing helpful error messages and wrapping results with metadata. Other applications, such as a benchmark runner, may choose to eschew `Timer` and deal directly in Tasks and Workers.",pytorch
62459,robieta,pr,2021-07-30T06:19:29Z,"Add formal mechanism for using stmt, setup, and global_setup in code. (And support C++)","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* #62462
* #62461
* #62460
* __->__ #62459
* #62458
* #62740

In #62458, we implemented our measurement loop (`_timeit_task_inner_f`) using raw string manipulation. It works, but it isn't very easy to read or write and doesn't scale to more complex logic. Moreover, `Timer` supports C++ snippets, so we need to handle that case as well.

This PR adds templates (`template.py` and `template.cpp`) and a compilation layer. To implement some logic, simply write it in the appropriate template file and then use `templates/jit` to get the code to make a runnable object. (Which can then be passed to a worker.) Specifically:
  - Logic that runs in the parent should live in a `Task`.
  - Logic that runs in the worker should live in the templates. (e.g. the inner measurement loop)
 ",pytorch
62460,robieta,pr,2021-07-30T06:19:35Z,"Introduce ""run_in_worker"" to bridge the remote worker abstraction","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* #62462
* #62461
* __->__ #62460
* #62459
* #62458
* #62740

This PR is very magical, but there is a good reason. One of the sharp edges of workers which expect strings of source code is that writing big string literals is not ergonomic and loses modern quality of life benefits: syntax highlighting, type checking, etc. This PR adds a decorator for tasks that allows methods to be written as normal code, but run in the worker by extracting the arguments and source and automatically piping it to the worker. We also add a file with the type information of the compiled template so we can access it in a type safe manner.

I was quite careful to make `run_in_worker` robust. (Including limiting what it supports.) So while some things won't work (like passing a non-marshallable object), there shouldn't be any silent correctness issues.",pytorch
62461,robieta,pr,2021-07-30T06:19:41Z,Add runtime_utils to handle num_threads and cuda sync,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* #62462
* __->__ #62461
* #62460
* #62459
* #62458
* #62740

One of the primary motivations for #62460 was to allow us to write more complex Task level logic. (Including logic that needs to run in the worker.) A good application of that is CUDA synchronization. In the current Timer implementation, we CUDA sync conservatively if PyTorch is built with CUDA and a GPU is present. But with Kineto, we can do better and actually check if the GPU is used. We also need to control the number of threads; up until this PR we've just been ignoring `num_threads`.",pytorch
62462,robieta,pr,2021-07-30T06:19:46Z,"Add docstrings to Timer, and add blocked_autorange and adaptive_autorange","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* __->__ #62462
* #62461
* #62460
* #62459
* #62458
* #62740

This is a fairly mechanical port of the existing `Timer` code to the wall time task. In the current implementation, everything keys off of `self._timer.timeit(...)`, which is effectively 1:1 with `TimeitTask.measure`. (Note that I haven't ported `Measurement`, so it's just raw numbers for now.) This PR is mostly to show that more complex algorithms are naturally implemented by making a new Task or extending an existing one.",pytorch
62463,robieta,pr,2021-07-30T06:19:52Z,"Add SubprocessWorker (and derived NoisePoliceWorker), as well as Timers for them","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* __->__ #62463
* #62462
* #62461
* #62460
* #62459
* #62458
* #62740

Now for the fun part. Workers are deliberately conceived with very simple semantics and a very restricted interface. This allows us to define a subprocess based worker and use it in place of an in-process worker. The implementation is much more complicated because we have to handle communication (moving data, signaling completion, etc.), logging, and error propagation. However the distinction between in-process and subprocess is entirely encapsulated in the worker; the Task is entirely indifferent.

One of the more exciting aspects of a subprocess worker is that we can use existing tools more or less for free. For instance, to use Noise Police we just have to prefix the worker launch command appropriately. While not implemented here, I expect there are lots of tools that this will allow to be folded into `Timer`. (e.g. `perf`)",pytorch
62464,robieta,pr,2021-07-30T06:19:58Z,Add initial Callgrind support using SubprocessWorker,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62464
* #62463
* #62462
* #62461
* #62460
* #62459
* #62458
* #62740

In the existing Timer implementation, Callgrind support is quite intricate. All of the logic for generating the loop script, spawning the process, and collecting the results is intermingled and non-reusable. By contrast, in this factored prototype adding (admittedly rudimentary) callgrind support is as simple as making a worker with an appropriate launch command, and writing a (very simply) Task to drive the loop. ",pytorch
62495,zasdfgbnm,pr,2021-07-30T18:26:47Z,Migrate Embedding thrust sort to cub sort,"This PR only migrates sort. Other thrust operations will be migrated in followup PRs

Benchmark `num_embeddings` pulled from https://github.com/huggingface/transformers/tree/master/examples by
```
grep -P 'vocab_size.*(=|:)\s*[0-9]+' -r transformers/examples/
grep -P 'hidden_size.*(=|:)\s*[0-9]+' -r transformers/examples/
```
to get `vocab_size = 119547, 50265, 32000, 8000, 3052` (similar size omitted) and `hidden_size = 512, 768`

Code:
```python
import torch
import itertools

num_embeddings = (119547, 50265, 32000, 8000, 3052)
num_tokens = (4096, 16384)
hidden_sizes = (512, 768)

for ne, nt, nh in itertools.product(num_embeddings, num_tokens, hidden_sizes):
    print(f""Embedding size: {ne}, Tokens: {nt}, Hidden size: {nh}"")
    embedding = torch.nn.Embedding(ne, nh).cuda()
    input_ = torch.randint(ne, (nt,), device='cuda')
    out = embedding(input_)
    torch.cuda.synchronize()
    %timeit out.backward(out, retain_graph=True); torch.cuda.synchronize()
```

## On CUDA 11.3.1

Before:
```
Embedding size: 119547, Tokens: 4096, Hidden size: 512
1.43 ms ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 4096, Hidden size: 768
2.07 ms ± 56.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 512
1.61 ms ± 2.29 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 768
2.32 ms ± 8.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 512
738 µs ± 1.38 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 768
1.02 ms ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 512
913 µs ± 3.89 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 768
1.27 ms ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 512
559 µs ± 860 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 768
743 µs ± 630 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 512
713 µs ± 969 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 768
977 µs ± 884 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 512
301 µs ± 8.02 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 768
383 µs ± 4.36 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 512
409 µs ± 1.39 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 768
515 µs ± 766 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 512
215 µs ± 1.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 768
250 µs ± 320 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 512
271 µs ± 888 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 768
325 µs ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

After:
```
Embedding size: 119547, Tokens: 4096, Hidden size: 512
1.42 ms ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 4096, Hidden size: 768
2.05 ms ± 9.93 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 512
1.6 ms ± 3.19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 768
2.3 ms ± 3.67 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 512
730 µs ± 811 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 768
1.01 ms ± 2.71 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 512
887 µs ± 1.08 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 768
1.25 ms ± 2.74 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 512
556 µs ± 1.86 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 768
744 µs ± 4.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 512
691 µs ± 570 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 768
957 µs ± 2.02 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 512
309 µs ± 2.84 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 768
376 µs ± 2.18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 512
381 µs ± 1.49 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 768
487 µs ± 2.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 512
202 µs ± 383 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 768
239 µs ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 512
243 µs ± 1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 768
340 µs ± 2.28 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

## On CUDA 11.1

Before:
```
Embedding size: 119547, Tokens: 4096, Hidden size: 512
1.41 ms ± 14.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 4096, Hidden size: 768
2.05 ms ± 7.61 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 512
1.61 ms ± 1.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 768
2.32 ms ± 2.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 512
743 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 768
1.02 ms ± 2.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 512
912 µs ± 5.91 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 768
1.28 ms ± 6.17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 512
555 µs ± 2.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 768
743 µs ± 655 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 512
714 µs ± 1.89 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 768
980 µs ± 1.52 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 512
312 µs ± 396 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 768
386 µs ± 2.32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 512
413 µs ± 3.19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 768
512 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 512
209 µs ± 585 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 768
271 µs ± 776 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 512
297 µs ± 1.11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 768
377 µs ± 3.87 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

After:
```
Embedding size: 119547, Tokens: 4096, Hidden size: 512
1.46 ms ± 12 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 4096, Hidden size: 768
2.09 ms ± 4.31 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 512
1.64 ms ± 4.48 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 119547, Tokens: 16384, Hidden size: 768
2.35 ms ± 2.54 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 512
782 µs ± 2.12 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 4096, Hidden size: 768
1.06 ms ± 596 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 512
945 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 50265, Tokens: 16384, Hidden size: 768
1.31 ms ± 553 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 512
603 µs ± 856 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 4096, Hidden size: 768
789 µs ± 500 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 512
752 µs ± 7.56 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 32000, Tokens: 16384, Hidden size: 768
1.01 ms ± 4.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 512
323 µs ± 7.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 4096, Hidden size: 768
398 µs ± 765 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 512
412 µs ± 544 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 8000, Tokens: 16384, Hidden size: 768
519 µs ± 614 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 512
229 µs ± 1.17 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 4096, Hidden size: 768
263 µs ± 417 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 512
274 µs ± 576 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Embedding size: 3052, Tokens: 16384, Hidden size: 768
354 µs ± 1.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```",pytorch
62518,jeffdaily,pr,2021-07-31T00:10:47Z,cast return of cudaGetLastError() to void when discarding,"Fixes #62511.
",pytorch
62597,zasdfgbnm,pr,2021-08-02T19:41:15Z,Cleanup CUDA 10.1 and 10.0 support on CI,10.1 is removed in https://github.com/pytorch/pytorch/pull/56056,pytorch
62609,zasdfgbnm,pr,2021-08-02T21:25:39Z,Remove CUDA10.2 + gcc 9 in CI,"This is an invalid combination because CUDA10.2 does not support gcc>8
",pytorch
62721,zasdfgbnm,pr,2021-08-04T13:56:12Z,DO NOT MERGE,"running ci-all tests to collect some data
",pytorch
62740,robieta,pr,2021-08-04T18:20:22Z,Introduce hermetic workers which can be used for benchmark isolation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #62464
* #62463
* #62462
* #62461
* #62460
* #62459
* #62458
* __->__ #62740

",pytorch
62805,z-a-f,pr,2021-08-05T09:16:32Z,[sparsity] Remove the pack_param from the sparsifier state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

That was the original design, that we decided to simplify by removing the packing in the sparsifier.
The state of the sparsifier is saved directly, and the old behavior accidentally bled through to the current version.
This change removes the `_pack_params` method, and changes the state_dict to include the state directly.
We don't have to change the load_state_dict, as it will work with either the old or the new format.",pytorch
62806,z-a-f,pr,2021-08-05T09:17:12Z,[sparsity] Remove the pack_param from the sparsifier state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65291
* #62809
* #62808
* #62807
* __->__ #62806

That was the original design, that we decided to simplify by removing the packing in the sparsifier.
The state of the sparsifier is saved directly, and the old behavior accidentally bled through to the current version.
This change removes the `_pack_params` method, and changes the state_dict to include the state directly.
We don't have to change the load_state_dict, as it will work with either the old or the new format.",pytorch
62807,z-a-f,pr,2021-08-05T09:17:23Z,[sparsity] Fix for accumulation bug in WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65291
* #62809
* #62808
* __->__ #62807
* #62806

This fixes a bug in the WeightNormSparsifier, where the mask is being multiplied by the newly computed mask.
Because the mask elements are binary 0/1, this accumulates the mask over every iteration, eventually collapsing the mask to zero.
This bug accidentally bled through from old versions.",pytorch
62808,z-a-f,pr,2021-08-05T09:17:34Z,[sparsity][doc] Docstring for WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65291
* #62809
* __->__ #62808
* #62807
* #62806

This adds the docstring documentation to the WeightNormSparsifier and adds the typehints for the constructor args.
Note, this does not require testing as only the doc is changed.",pytorch
62809,z-a-f,pr,2021-08-05T10:02:13Z,[sparsity] Add m-out-of-n support in the WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65291
* __->__ #62809
* #62808
* #62807
* #62806

The m-out-of-n is implemented as follows:

1. Compute the blocks that need to be sparsified using the weight-norm criterion
2. Within each block below the threshold find the smallest absolute value elements
3. Zero out only the smallest values within each block

m-out-of-n describes sparsification scheme where in a block with ""n"" elements, only ""m"" of them would be zeroed-out.
Block sparsity, with the whole block being all zeros, is a special case of m-out-n: If m==n, the whole block is reset.

This echoes the implementation described in the https://github.com/pytorch/pytorch/issues/59835,
as well as meets the support of the nVidia cusparselt requirements.
To support the CUDA sparsity (2/4), one would need to set the sparsity_level to 1.0.
That translates to all blocks of shape 1x4 within a tensor will sprasify with 2-out-4 scheme.",pytorch
63002,zasdfgbnm,pr,2021-08-09T21:54:24Z,[DO NOT MERGE] Invoke NCCL through UCC,"Fixes #{issue number}
",pytorch
63022,Flamefire,pr,2021-08-10T08:51:32Z,Add Github action to upload full source releases,"Those release tarballs include the submodules.
The action is run on every tag, master-branch push but will not upload anything.
This makes sure nothing is broken when an actual release happens.

On created releases the action runs and uploads the tarball

Fixes #62708

As I don't have access rights here and testing is obviously hard (as a new release needs to be published), I set up a test at https://github.com/Flamefire/pytorch/releases/tag/testtag
See also the run(s) at https://github.com/Flamefire/pytorch/actions/workflows/create_release.yml
",pytorch
63042,zasdfgbnm,pr,2021-08-10T19:49:26Z,Embedding thrust->cub: unique,Followup of https://github.com/pytorch/pytorch/pull/62495,pytorch
63112,zasdfgbnm,pr,2021-08-11T22:28:53Z,Fix batch_isend_irecv tests for err case,"- `batch_isend_irecv` returns a list of requests instead of a single request
- remove some unused variables",pytorch
63231,gmagogsfm,pr,2021-08-13T16:24:56Z,Remove left-over print in test_diff_graph_inline_threshold,,pytorch
63351,jeffdaily,pr,2021-08-16T20:19:08Z,"Revert ""[ROCm] Update HIP_VERSION to TORCH_HIP_VERSION (#62786)""","This reverts commit ab7a47298039da3b60b4e33cbb70c5fc06e1a315.

Fixes #{issue number}
",pytorch
63354,jeffdaily,pr,2021-08-16T20:37:19Z,"Revert ""Allow TransformerEncoder and TransformerDecoder to accept 0-d…","…im batch sized tensors. (#62800)""

This reverts commit 809e1e7457b011f3e6528304d9f8461b1cb3475b.
",pytorch
63374,robieta,pr,2021-08-17T00:13:56Z,change with_callable_args to return a fresh _PartialWrapper,"Fixes https://github.com/pytorch/pytorch/issues/63326

Currently `get_callable_args` has the side effect of mutating the input _PartialWrapper. When that input is one of the global defaults, there are all sorts of lifetime issues that crop up. (Details in the linked issue.) So far as I can tell, we only need to make a constructor which is module (and by extension device) aware, so making a fresh one should have the same effect without leaking the last call's module.

Test plan: the repro in #63326 now reports no leaked Tensors, and all quantization tests pass locally.
",pytorch
63401,jeffdaily,pr,2021-08-17T15:33:12Z,[DO NOT MERGE] disable CUDA/HIP caching allocator for all tests,"Let's see what happens when we disable the caching allocator.

We've seen some ROCm unit test failures when not using the caching allocator and got curious if CUDA behaved similar.",pytorch
63484,jeffdaily,pr,2021-08-18T15:13:23Z,[ROCm] hipFree() may not not synchronize non-blocking streams,"hipFree() may not synchronize non-blocking streams.  When disabling the caching allocator, it will result in memory access faults for tests that create and destroy many intermediate tensors because the memory is recovered before the kernels run.  Here we introduce a safer uncached allocator to use when disabling the caching allocator.  Further, the caching allocator will synchronize the stream associated with the cached block before calling hipFree().",pytorch
63508,jeffdaily,pr,2021-08-18T19:50:32Z,add distributed/_sharded_tensor/test_sharded_tensor to ROCM_BLOCKLIST,"Fixes current ROCm CI test2 brokenness until tensorpipe is fully supported by ROCm.
",pytorch
63528,jeffdaily,pr,2021-08-18T22:59:02Z,empty caching allocator before test_avg_pool2d large subtest,"Otherwise, unrecoverable OOM occurs on MI25.  Fixes broken ROCm CI test1.",pytorch
63745,jjsjann123,pr,2021-08-23T05:04:41Z,nvfuser update,"Syncing nvfuser code base from devel branch, Listing a few of our development since last sync:

- Extends support to normalization and reduction kernels.
- Multiple kernel launch for single `CudaFusionGroup`. Hierarchical caching system has been updated to cache graph segmentation.
- profile_ivalue is enabled to convert dynamic scalar into compile time constants, which are required by the codegen. (e.g. reduction axes).

To keep this PR simple and relatively review-free. We stripped most external changes and submitted them as separate PRs, so this gigantic PR is easier to handle.

internal updates are files located in:
1. updates in nvfuser codegen `torch/csrc/jit/coddgen/cuda`
2. added nvfuser specific benchmarks `benchmarks/cpp/nvfuser`
3. nvfuser jit cpp tests `test/cpp/jit/test_gpu.cpp` `test/cpp/jit/test_gpu_shift.cpp` `test/cpp/jit/test_gpu_validator.h`

updates affecting integration:

1. profile_ivalue enabled for nvfuser. related changes are in `torch/csrc/jit/runtime/*`, 
2. exposed a few more symbols `aten/src/ATen/core/*` used by codegen",pytorch
63806,zasdfgbnm,pr,2021-08-23T21:05:16Z,[Reland] Embedding thrust->cub migration,"Fixes https://github.com/pytorch/pytorch/issues/63427
",pytorch
63879,z-a-f,pr,2021-08-24T19:32:59Z,[quant] Fixing the conversion of the quantizable RNN,"Summary:
Quantizable RNN had a bug, where the `from_observed` was an instance method, instead of a class method. This caused the `tq.convert` to fail. This fixes the issue by making the `from_observed` a classmethod.

The tests were passing before because the unittests were not using the custom module path, but a conventional `from_float`, which is also supported.

Test Plan:
`buck test mode/dev //caffe2/test:quantization -- test_custom_module_lstm`

```
buck test mode/dev //caffe2/test:quantization -- test_custom_module_lstm
Parsing buck files: finished in 0.5 sec
Downloaded 0/2 artifacts, 0.00 bytes, 100.0% cache miss (for updated rules)
Building: finished in 9.2 sec (100%) 12622/12622 jobs, 2/12622 updated
  Total time: 9.7 sec
More details at https://www.internalfb.com/intern/buck/build/0d87b987-649f-4d06-b0e2-97b5077
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: cb99305f-65c9-438b-a99f-a0a2a3089778
Trace available for this run at /tmp/tpx-20210824-115652.540356/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/5066549645030046
    ✓ ListingSuccess: caffe2/test:quantization - main (12.550)
    ✓ Pass: caffe2/test:quantization - test_custom_module_lstm (quantization.core.test_quantized_op.TestQuantizedOps) (174.867)
Summary
  Pass: 1
  ListingSuccess: 1
If you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/5066549645030046
```

Reviewed By: mtl67

Differential Revision: D30520473

",pytorch
63937,jjsjann123,pr,2021-08-25T09:12:58Z,Add native_dropout,"Adds native_dropout to have a reasonable target for torchscript in auto diff. native_dropout has scale and train as arguments in its signature, this makes native_dropout more consistent with other operators and removes conditionals in the autodiff definition.

cc @gmagogsfm",pytorch
63938,jjsjann123,pr,2021-08-25T09:13:47Z,Autodiff update for Add operator,"Avoids extra operators being generated in simple add cases, for example bias addition.

cc @gmagogsfm",pytorch
63939,jjsjann123,pr,2021-08-25T09:16:02Z,Add fp16/fp32 autocasting to JIT/TorchScript,"Adds mixed precision autocasting support between fp32/fp16 to torchscript/JIT. More in depth descriptoin can be found at [torch/csrc/jit/JIT-AUTOCAST.md](https://github.com/pytorch/pytorch/pull/63939/files#diff-1f1772aaa508841c5bb58b74ab98f49a1e577612cd9ea5c386c8714a75db830b)

This PR implemented an autocast optimization pass that inserts casting ops per AMP rule (torch/csrc/jit/passes/autocast.cpp), that mimics the behavior of eager autocast. The pass also takes into consideration the context of `torch.cuda.amp.autocast` and only inserts casting ops within the enabled context manager, giving feature parity as with eager amp autocast.

We currently provide JIT AMP autocast as a prototyping feature, so it is default off and could be turned on via `torch._C._jit_set_autocast_mode(True)`

The JIT support for autocast is subject to different constraints compared to the eager mode implementation (mostly related to the fact that TorchScript is statically typed), restriction on the user facing python code is described in doc torch/csrc/jit/JIT-AUTOCAST.md

This is a prototype, there are also implementation limitation that's necessary to keep this PR small and get something functioning quickly on upstream, so we can iterate on designs.

Few limitation/challenge that is not properly resolved in this PR:
1. Autocast inserts cast operation, which would have impact on scalar type of output tensor feeding downstream operations. We are not currently propagating the updated scalar types, this would give issues/wrong results on operations in promotion rules.

2. Backward for autodiff in JIT misses the casting of dgrad to input scalar type, as what autograd does in eager. This forces us to explicitly mark the casting operation for certain operations (e.g. binary ops), otherwise, we might be feeding dgrad with mismatch scalar type to input. This could potentially break gradient function consuming dgrad. (e.g. gemm backwards, which assumes grad_output to be of same scalar type as input')

3. `torch.autocast` api has an optional argument `dtype` which is not currently supported in the JIT autocast and we require a static value.

Credit goes mostly to:
@tlemo
@kevinstephano",pytorch
63940,jjsjann123,pr,2021-08-25T09:18:23Z,fixing sorting in stride indices,"Updating `computeStrideProps` logic to break ties on stride_indices.

For two dimension with identical stride, the dimension with size-1 should be considered as the faster dimension. Otherwise, its stride should be the product of existing stride and the size of the other dimension.

Note that there's still inconsistency between eager memory_format and stride_properties in JIT, this is a design issue due to the ambiguity on size-1 stride. One example showing this failing test has been disabled in the added cpp test",pytorch
63941,jjsjann123,pr,2021-08-25T09:20:18Z,specializeGradSumToSize patch - propagate profile_none through profile_ivalue,"simply propagate profile_none_ value through profile_ivalue nodes inserted by nvfuser.

Without the propagation, profile_ivalue inserted by other passes would block the optimization on no-op sum_to_size.

cc @gmagogsfm",pytorch
63942,jjsjann123,pr,2021-08-25T09:22:00Z,Add softplus support to autodiff,"Add softplus definition to autodiff.

cc @gmagogsfm",pytorch
64083,gmagogsfm,pr,2021-08-27T05:11:27Z,More robust check of whether a class is defined in torch,"This would prevent bugs for classes that
1) Are defined in a module that happens to start with `torch`, say `torchvision`
2) Are defined in torch but with an import alias like `import torch as th`",pytorch
64085,gmagogsfm,pr,2021-08-27T05:41:50Z,Remove outdated warning about RecursiveScriptModule not being copiable,RecursiveScriptModule has its customized `__copy__` and `__deepcopy__` defined. The warning/error  that says it is not copiable is outdated,pytorch
64086,z-a-f,pr,2021-08-27T06:07:40Z,[quant] AO migration of the `quantize.py`,"Summary:
AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly.

This migrates the `quantize.py` from torch.quantization to `torch.ao.quantization`.

At this point both locations will be supported. Eventually the torch.quantization will be deprecated.

Test Plan: `buck test mode/opt //caffe2/test:quantization`

Differential Revision: D30055886

",pytorch
64127,zasdfgbnm,pr,2021-08-28T00:02:08Z,Bring back old algorithm for sorting on small number of segments,"Fixes https://github.com/pytorch/pytorch/issues/63456
The code was copy-pasted from the previous commit without modification. 
",pytorch
64287,jeffdaily,pr,2021-08-31T17:34:31Z,replace C10_WARP_SIZE with warpSize,"`warpSize` and `C10_WARP_SIZE` were inconsistently used within ATen.  Since `warpSize` is defined by both CUDA and ROCm builds, use `warpSize` instead of hard-coded `C10_WARP_SIZE` to pick up any future changes to `warpSize` that would be platform or arch specific.",pytorch
64302,jeffdaily,pr,2021-08-31T19:35:39Z,[ROCm] define C10_WARP_SIZE to warpSize HIP constant,"warpSize is defined as a constexpr in HIP headers.  It is incorrect to assume warpSize 64.  This change fixes the C10_WARP_SIZE definition in torch sources similar to [how it was done in caffe2](https://github.com/pytorch/pytorch/blob/master/caffe2/utils/GpuDefs.cuh#L10-L14).

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport",pytorch
64304,zasdfgbnm,pr,2021-08-31T20:28:14Z,Fixes reduction launch config,"Fixes #48573 
See also https://github.com/pytorch/pytorch/pull/64194
Improves reduction performance for non-contiguous tensors that previously selected very suboptimal launch configuration (see benchmarks in #64194, they also apply here). ",pytorch
64341,gmagogsfm,pr,2021-09-01T05:12:36Z,Use `detach` after `clone` when deepcopying IValue Tensors,"Previously `torch.clone` is used to copy IValue Tensors, this has the
unintended effect of creating a gradient link between old and new
tensors. This is not desired effect for deepcopy. Using `detach` after
 `clone` would avoid this problem.

",pytorch
64437,z-a-f,pr,2021-09-02T17:47:56Z,"Back out ""[quant] AO migration of the `quantize.py`""","Summary:
Based on the discussion in the AO team meeting, a decision was made to migrate the files in a such a manner that preseerves the blame history. This diff will be split into two diffs that keep the blame.

Original commit changeset: 8ef7470f9fa6

Test Plan: `buck test mode/dev //caffe2/test:quantization`

Reviewed By: jerryzh168

Differential Revision: D30732757

",pytorch
64438,z-a-f,pr,2021-09-02T17:55:36Z,[quant] Enable jit tracing on quantizable LSTM,"Summary:
The quantizable LSTM didn't support jit tracing because it had several non taceable paths. We sacrifice some of the user experience to enable the tracing.

The main UX feature removed is a user-friendly message when trying to access the backwards path in a bidirectional LSTM: When the bidirectional flag is `False`, we used to throw a nice error message when the user tried accessing backwards weights. Now the message is default (removed properties).

Test Plan: `buck test mode/dev //caffe2/test:quantization -- test_custom_module_lstm`

Differential Revision: D30732630

",pytorch
64445,z-a-f,pr,2021-09-02T19:28:45Z,[quant] AO migration of the `quantize.py` (resubmission),"Summary:
AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly.
This migrates the quantize.py from torch.quantization to torch.ao.quantization.
At this point both locations will be supported. Eventually the torch.quantization will be deprecated.

Test Plan: `buck test mode/dev //caffe2/test:quantization`

Differential Revision: D30734870

",pytorch
64498,zasdfgbnm,pr,2021-09-03T17:26:35Z,EmbeddingBag sort thrust->cub,"Partially fixes https://github.com/pytorch/pytorch/issues/57505

Also fixes a warning I found when compiling:
```
/home/gaoxiang/pytorch-cub/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu(7): warning: inline qualifier ignored for ""__global__"" function
```
I also updated the bfloat16 guard to CUDA 11.5",pytorch
64583,robieta,pr,2021-09-07T16:41:11Z,Convert profiler schedule to a functor for serialization,"Fixes #64492

Functions generated from factories are not pickleable, but classes are. The one behavior change from this PR is that we no longer enforce kwargs for the ctor. Dataclasses are getting keyword-only support, but only in 3.10. (Which means we won't be able to use it until 3.13 or 3.14...)

Test plan: existing unit tests, plus a line for pickling.
",pytorch
64638,z-a-f,pr,2021-09-08T09:05:27Z,[quant] Enable jit tracing on quantizable LSTM (resubmission),"Summary:
The quantizable LSTM didn't support jit tracing because it had several non taceable paths. We sacrifice some of the user experience to enable the tracing.
The main UX feature removed is a user-friendly message when trying to access the backwards path in a bidirectional LSTM: When the bidirectional flag is False, we used to throw a nice error message when the user tried accessing backwards weights. Now the message is default (removed properties).

Test Plan: `buck test mode/dev //caffe2/test:quantization -- test_custom_module_lstm`

Differential Revision: D30803753

",pytorch
64833,z-a-f,pr,2021-09-10T18:06:04Z,[quant] Member variable access in quantizable LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64834
* __->__ #64833

The quantizable LSTM stores it's weight/bias variables deeper in the hierarchy. To access the weights/biases, one must either use `module.named_parameters()` or dive deeper (s.a. `module.layers[0].layer_fw.cell.weight`). However, in the original `nn.LSTM`, those variables are accessible from the top level (i.e. `module.weight_ih_l0`).

This PR creates an alias on the top level to be able to access the weights/biases directly from the top. Note that because during the `prepare`/`convert` steps those variables change ID, we have to ""reattach"" them on the top level. Hence, the support method `_set_weight_bias_variables`, which is called during construction, `from_float`, and `from_observed`.

Major note: Once the module is quantized, the weight/bias variables are packed. That means that accessing them directly is impossible, as the variables return unpacking functions.

Differential Revision: [D30870715](https://our.internmc.facebook.com/intern/diff/D30870715/)",pytorch
64834,z-a-f,pr,2021-09-10T18:06:08Z,[quant] Correct name for quantized LSTM,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64834
* #64833

Once quantized, the quantizable LSTM is still showing up as ""QuantizableLSTM"". It would be more correct to change it to ""QuantizedLSTM"". Note: This works for both converting with custom module and converting without the custom module (QUantizableLSTM supports both ways).

Differential Revision: [D30872072](https://our.internmc.facebook.com/intern/diff/D30872072/)",pytorch
64910,z-a-f,pr,2021-09-13T09:34:01Z,[quant] Removing unnecessary import from torch/quantization/quantize.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64985
* #64912
* #64911
* __->__ #64910

This bled through from the original location. Removing it is not just refactoring, but also prevents potential recursive imports.

Differential Revision: [D30882924](https://our.internmc.facebook.com/intern/diff/D30882924/)",pytorch
64911,z-a-f,pr,2021-09-13T09:34:06Z,[quant] Add imports to the torch/ao/quantization/__init__.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64911

The import statements that involve the `quantize.py` were not added to the module level __init__ file. Those imports are necessary to mimic the behavior of the old import locations. Otherwise, the user would need to change their import statements to `from torch.ao.quantization.quantize import quantize` (instead of `from torch.ao.quantization import quantize`.

Differential Revision: [D30897663](https://our.internmc.facebook.com/intern/diff/D30897663/)",pytorch
64912,z-a-f,pr,2021-09-13T09:34:10Z,[quant][refactor] Change the structure of the ao migration tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64919
* #64917
* #64916
* #64913
* __->__ #64912
* #64910

The test naming was confusing and ambiguous. The file was changed to reflect the framework that is being migrated (""quantization"" instead of ""quantize""). Also, the common testing class was extracted out

Differential Revision: [D30898214](https://our.internmc.facebook.com/intern/diff/D30898214/)",pytorch
64913,z-a-f,pr,2021-09-13T09:34:23Z,[quant] AO migration of the `fuse_modules.py` (phase 1),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64919
* #64917
* #64916
* __->__ #64913
* #64912
* #64910

AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly.
This migrates the fuse_module.py from torch.quantization to torch.ao.quantization.
At this point both locations will be supported. Eventually the torch.quantization will be deprecated.

Differential Revision: [D30882819](https://our.internmc.facebook.com/intern/diff/D30882819/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30882819/)!",pytorch
64916,z-a-f,pr,2021-09-13T09:47:42Z,[quant] [quant] AO migration of the `quant_types.py` (phase 1),"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64919
* #64917
* __->__ #64916
* #64913
* #64912
* #64910

AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly.
This migrates the quant_type.py from torch.quantization to torch.ao.quantization.
At this point both locations will be supported. Eventually the torch.quantization will be deprecated.

Differential Revision: [D30898422](https://our.internmc.facebook.com/intern/diff/D30898422/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30898422/)!",pytorch
64917,z-a-f,pr,2021-09-13T10:05:58Z,"[quant] AO migration of the `_correct_bias.py`, `_equalize.py`, and `_learnable_fake_quantize.py`","Stack from [ghstack](https://github.com/ezyang/ghstack):
* #64919
* __->__ #64917
* #64916
* #64913
* #64912
* #64910

AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly.
This migrates from torch.quantization to torch.ao.quantization the following files:
- `_correct_bias.py`
- `_equalize.py`
- `_learnable_fake_quantize.py`

**Note:** These file are migrated completely without any warning. The old location is thus silently deprecated.

Differential Revision: [D30898565](https://our.internmc.facebook.com/intern/diff/D30898565/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30898565/)!",pytorch
64919,z-a-f,pr,2021-09-13T10:34:55Z,[quant] AO migration of the `torch/quantization/utils.py`,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #64919

AO Team is migrating the existing torch.quantization into torch.ao.quantization. We are doing it one file at a time to make sure that the internal callsites are updated properly. This migrates the quantization utilities.

**Note:** This file is migrated completely without any warning. The old location is thus silently deprecated.

Differential Revision: [D30899082](https://our.internmc.facebook.com/intern/diff/D30899082/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D30899082/)!",pytorch
65127,gmagogsfm,pr,2021-09-16T06:50:27Z,Test lazy source range,"Fixes #{issue number}
",pytorch
65137,jjsjann123,pr,2021-09-16T09:16:46Z,"Revert ""Revert D30752939: [pytorch][PR] nvfuser update""","This reverts commit 03389dc851db6f3ca52f9a4455ce2090c64a223d.

Attempt again for PR: #63745
Fixes the windows build failure.",pytorch
65202,jjsjann123,pr,2021-09-17T07:36:47Z,[DO NOT REVIEW] Nvfuser revert smoke test 2,attempt in parallel for ci debugging. :cry: ,pytorch
65291,z-a-f,pr,2021-09-18T09:33:00Z,[sparsity] Change API to take FQNs as configuration,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65291
* #62809
* #62808
* #62807
* #62806

",pytorch
65292,z-a-f,pr,2021-09-18T09:46:44Z,[sparsity] Remove the pack_param from the sparsifier state_dict,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65295
* #65296
* #65294
* #65293
* __->__ #65292

That was the original design, that we decided to simplify by removing the packing in the sparsifier.
The state of the sparsifier is saved directly, and the old behavior accidentally bled through to the current version.
This change removes the `_pack_params` method, and changes the state_dict to include the state directly.
We don't have to change the load_state_dict, as it will work with either the old or the new format.

The main reason for this PR is the simplification. The original design didn't achieve anything useful by packing the sparsification parameters.

Differential Revision: [D31186826](https://our.internmc.facebook.com/intern/diff/D31186826)",pytorch
65293,z-a-f,pr,2021-09-18T09:46:47Z,[sparsity] Fix for accumulation bug in WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65295
* #65296
* #65294
* __->__ #65293
* #65292

This fixes a bug in the WeightNormSparsifier, where the mask is being multiplied by the newly computed mask.
Because the mask elements are binary 0/1, this accumulates the mask over every iteration, eventually collapsing the mask to zero.
This bug accidentally bled through from old versions.

Differential Revision: [D31186829](https://our.internmc.facebook.com/intern/diff/D31186829)",pytorch
65294,z-a-f,pr,2021-09-18T09:46:51Z,[sparsity][doc] Docstring for WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65295
* #65296
* __->__ #65294
* #65293
* #65292

This adds the docstring documentation to the WeightNormSparsifier and adds the typehints for the constructor args.
Note, this does not require testing as only the doc is changed.

Differential Revision: [D31186827](https://our.internmc.facebook.com/intern/diff/D31186827)",pytorch
65295,z-a-f,pr,2021-09-18T09:46:54Z,[sparsity] Add m-out-of-n support in the WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65295
* #65296
* #65294
* #65293
* #65292

The m-out-of-n is implemented as follows:

1. Compute the blocks that need to be sparsified using the weight-norm criterion
2. Within each block below the threshold find the smallest absolute value elements
3. Zero out only the smallest values within each block

m-out-of-n describes sparsification scheme where in a block with ""n"" elements, only ""m"" of them would be zeroed-out.
Block sparsity, with the whole block being all zeros, is a special case of m-out-n: If m==n, the whole block is reset.

This echoes the implementation described in the https://github.com/pytorch/pytorch/issues/59835,
as well as meets the support of the nVidia cusparselt requirements.
To support the CUDA sparsity (2/4), one would need to set the sparsity_level to 1.0.
That translates to all blocks of shape 1x4 within a tensor will sprasify with 2-out-4 scheme.

Differential Revision: [D31186828](https://our.internmc.facebook.com/intern/diff/D31186828)",pytorch
65296,z-a-f,pr,2021-09-18T09:46:57Z,[sparsity] Change API to take FQNs as configuration,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65295
* __->__ #65296
* #65294
* #65293
* #65292

The original API described in the https://github.com/pytorch/pytorch/issues/59835
assumed that the per-layer configuration would take a module/layer
reference. However, a more useful approach is to refer to the layers
by their fully qualified names (FQN). That allows us to store the
configuration in a file without serializing the models.

We define a layer's FQN as it's ""path"" within a model. For example,
if one can refer to a model using `model.layer0.sublayerX`, the FQN
of the sublayerX is `'layer0.sublayerX'`.

Test Plan:
```
python test/test_ao_sparsity.py -- TestBaseSparsifier
buck test @mode/opt //caffe2:test -- TestBaseSparsifier
```

Differential Revision: [D31186830](https://our.internmc.facebook.com/intern/diff/D31186830)",pytorch
65309,gmagogsfm,pr,2021-09-19T07:49:24Z,Add `SourceView` which doesn't own source text as base class of `Source`,"This would save the cost copying text from stack to heap in some cases (like
parsing function schema during loading phase of libtorch.so)

",pytorch
65570,z-a-f,pr,2021-09-23T20:01:00Z,[quant] Fix applying non-zero offset 1 to null pointer in quantized interpolation,"Summary: Although this is not an issue that could pop-up in practice, LLVM-12 throws an error about this issue if not checked.

Test Plan: `buck test mode/dev //caffe2/test:quantization -- --exact 'caffe2/test:quantization - test_empty_batch (quantization.core.test_quantized_op.TestQuantizedOps)'`

Reviewed By: r-barnes

Differential Revision: D31151681

",pytorch
65885,z-a-f,pr,2021-09-30T07:04:42Z,[ao_migration] torch/ao/nn migraiton: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65889
* #65888
* #65887
* #65886
* __->__ #65885
* #66035

This changes the imports in the `caffe2/torch/ao/nn` to include the new import locations.

Differential Revision: [D31299690](https://our.internmc.facebook.com/intern/diff/D31299690/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31299690/)!",pytorch
65886,z-a-f,pr,2021-09-30T07:04:47Z,[ao_migration] torch/ao/ns: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65889
* #65888
* #65887
* __->__ #65886
* #65885
* #66035

This changes the imports in the `caffe2/torch/ao/ns` to include the new import locations.

Differential Revision: [D31299692](https://our.internmc.facebook.com/intern/diff/D31299692/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31299692/)!",pytorch
65887,z-a-f,pr,2021-09-30T07:04:52Z,[ao_migration] torch/ao/ns/fx: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65889
* #65888
* __->__ #65887
* #65886
* #65885
* #66035

This changes the imports in the `caffe2/torch/ao/ns/fx` to include the new import locations.

Differential Revision: [D31299693](https://our.internmc.facebook.com/intern/diff/D31299693/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31299693/)!",pytorch
65888,z-a-f,pr,2021-09-30T07:04:57Z,[ao_migration] torch/ao/quantization: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65889
* __->__ #65888
* #65887
* #65886
* #65885
* #66035

This changes the imports in the `caffe2/torch/ao/nn` to include the new import locations.

Differential Revision: [D31299691](https://our.internmc.facebook.com/intern/diff/D31299691/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31299691/)!",pytorch
65889,z-a-f,pr,2021-09-30T07:05:02Z,[ao_migration] torch/ao/quantization/fx: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65889
* #65888
* #65887
* #65886
* #65885
* #66035

This changes the imports in the `caffe2/torch/ao/nn` to include the new import locations.

Differential Revision: [D31299684](https://our.internmc.facebook.com/intern/diff/D31299684/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31299684/)!",pytorch
65892,z-a-f,pr,2021-09-30T07:34:07Z,[ao_migration] torch/nn/quantized: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/quantized` to include the new import locations.

```
codemod -d torch/nn/quantized --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301193](https://our.internmc.facebook.com/intern/diff/D31301193/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301193/)!",pytorch
65893,z-a-f,pr,2021-09-30T07:34:11Z,[ao_migration] torch/nn/quantizable: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/quantizable` to include the new import locations.

```
codemod -d torch/nn/quantizable --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301194](https://our.internmc.facebook.com/intern/diff/D31301194/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301194/)!",pytorch
65894,z-a-f,pr,2021-09-30T07:34:15Z,[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/qat` to include the new import locations.

```
codemod -d torch/nn/qat --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301196](https://our.internmc.facebook.com/intern/diff/D31301196/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301196/)!",pytorch
65895,z-a-f,pr,2021-09-30T07:34:19Z,[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/intrinsic` to include the new import locations.

```
codemod -d torch/nn/intrinsic --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301195](https://our.internmc.facebook.com/intern/diff/D31301195/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301195/)!",pytorch
65896,z-a-f,pr,2021-09-30T07:35:17Z,[ao_migration] torch/nn/quantized: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/quantized` to include the new import locations.

```
codemod -d torch/nn/quantized --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301193](https://our.internmc.facebook.com/intern/diff/D31301193/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301193/)!",pytorch
65897,z-a-f,pr,2021-09-30T07:35:21Z,[ao_migration] torch/nn/quantizable: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/quantizable` to include the new import locations.

```
codemod -d torch/nn/quantizable --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301194](https://our.internmc.facebook.com/intern/diff/D31301194/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301194/)!",pytorch
65898,z-a-f,pr,2021-09-30T07:35:25Z,[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/qat` to include the new import locations.

```
codemod -d torch/nn/qat --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301196](https://our.internmc.facebook.com/intern/diff/D31301196/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301196/)!",pytorch
65899,z-a-f,pr,2021-09-30T07:35:29Z,[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* (to be filled)

This changes the imports in the `caffe2/torch/nn/intrinsic` to include the new import locations.

```
codemod -d torch/nn/intrinsic --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301195](https://our.internmc.facebook.com/intern/diff/D31301195/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301195/)!",pytorch
65900,z-a-f,pr,2021-09-30T07:37:11Z,[ao_migration] torch/nn/quantized: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65903
* #65902
* #65901
* __->__ #65900

This changes the imports in the `caffe2/torch/nn/quantized` to include the new import locations.

```
codemod -d torch/nn/quantized --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301193](https://our.internmc.facebook.com/intern/diff/D31301193/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301193/)!",pytorch
65901,z-a-f,pr,2021-09-30T07:37:15Z,[ao_migration] torch/nn/quantizable: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65903
* #65902
* __->__ #65901
* #65900

This changes the imports in the `caffe2/torch/nn/quantizable` to include the new import locations.

```
codemod -d torch/nn/quantizable --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301194](https://our.internmc.facebook.com/intern/diff/D31301194/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301194/)!",pytorch
65902,z-a-f,pr,2021-09-30T07:37:20Z,[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65903
* __->__ #65902
* #65901
* #65900

This changes the imports in the `caffe2/torch/nn/qat` to include the new import locations.

```
codemod -d torch/nn/qat --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301196](https://our.internmc.facebook.com/intern/diff/D31301196/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301196/)!",pytorch
65903,z-a-f,pr,2021-09-30T07:37:23Z,[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #65903
* #65902
* #65901
* #65900

This changes the imports in the `caffe2/torch/nn/intrinsic` to include the new import locations.

```
codemod -d torch/nn/intrinsic --extensions py 'torch.quantization' 'torch.ao.quantization'
```

Differential Revision: [D31301195](https://our.internmc.facebook.com/intern/diff/D31301195/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D31301195/)!",pytorch
66035,z-a-f,pr,2021-10-01T22:21:01Z,[quant] Add quantization import in the ao/__init__,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #65889
* #65888
* #65887
* #65886
* #65885
* __->__ #66035

When importing the package __init__.py it is important to include the subpackages as imports in the __init__. However, during the migration, that was not done. This fixes it

Differential Revision: [D31354228](https://our.internmc.facebook.com/intern/diff/D31354228/)",pytorch
66057,z-a-f,pr,2021-10-03T01:20:09Z,[quant] Fixing the hypothesis test for topk,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66058
* __->__ #66057

The current test is creating the sets that are too slow.
This will cause either ""Filtering too much"" or ""Timeout"" errors in the future versions of hypothesis.
This PR preemptively fixes the issue.

Test plan:

`python test/test_quantization.py`

Differential Revision: [D31366065](https://our.internmc.facebook.com/intern/diff/D31366065)",pytorch
66058,z-a-f,pr,2021-10-03T01:20:12Z,[quant] Fix the parts that were missing after initial migration,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66058
* #66057

After the initial migration from `torch.quantization` to `torch.ao.quantization`, some of the files did not change.
This happened because the migration was done in parallel, and some of the files were landed while the others were still in the original location.
This is the last fix in the AO migration phase 1, which completely enables the ao.quantization namespace.

Test Plan:

`python test/test_quantization.py`

Differential Revision: [D31366066](https://our.internmc.facebook.com/intern/diff/D31366066)",pytorch
66089,zasdfgbnm,pr,2021-10-04T17:52:15Z,Fix gcc11 build error in ir_emitter.cpp,"```
../torch/csrc/jit/frontend/ir_emitter.cpp: In lambda function:
../torch/csrc/jit/frontend/ir_emitter.cpp:1681:76: error: 'this' pointer is null [-Werror=nonnull]
 1681 |               << "" elements, which were unified to "" << candidate->repr_str();
      |                                                         ~~~~~~~~~~~~~~~~~~~^~
cc1plus: some warnings being treated as errors
```",pytorch
66091,zasdfgbnm,pr,2021-10-04T19:52:32Z,Remove sync in Embedding caused by unique,,pytorch
66158,z-a-f,pr,2021-10-05T19:26:20Z,[quant] Remove hypothesis from qtopk,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66158

qtopk used hypothesis which created flaky tests. In addition to that the tests generated were not representative, and would not catch the cases that we are interested in.

This diff removes the hypothesis from the qtopk and merges the qtopk and qtopk_nhwc tests. We now use specific testcases.

Differential Revision: [D31401341](https://our.internmc.facebook.com/intern/diff/D31401341/)",pytorch
66176,jjsjann123,pr,2021-10-05T23:42:47Z,"Revert ""Revert D31227448: [pytorch][PR] fixing sorting in stride indices""",enabling #63940 ,pytorch
66219,zasdfgbnm,pr,2021-10-06T19:06:54Z,Refactor cub namespace handling,"This PR is to update PyTorch with the following cub changes:
- Starting cub 1.13.1, cub requires users to define `CUB_NS_QUALIFIER` if `CUB_NS_PREFIX` is also defined. Besides that, a new mechanism `CUB_WRAPPED_NAMESPACE` is added.

And I do the following change to PyTorch:
- Starting CUDA 11.5, define `CUB_WRAPPED_NAMESPACE` globally as an nvcc flag.
- Fix caffe2 failures caused by the above change.
- Add a `aten/src/ATen/cuda/cub_definitions.cuh` that defines helper macros about feature availability.",pytorch
66273,jjsjann123,pr,2021-10-07T18:54:17Z,dropout update in autodiff,"1. Unifies dropout op in autodiff
2. Removes dropout inference support in autodiff",pytorch
66411,z-a-f,pr,2021-10-11T10:41:45Z,[sparsity] Fix and enable the pruning tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66416
* #66415
* #66414
* #66413
* #66412
* __->__ #66411

The original tests were disabled, and had some bugs. This fixes those unittests.

Differential Revision: [D31590678](https://our.internmc.facebook.com/intern/diff/D31590678)",pytorch
66412,z-a-f,pr,2021-10-11T10:41:48Z,[sparsity] Fix GPU training for sparsity,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66416
* #66415
* #66414
* #66413
* __->__ #66412
* #66411

The GPU training was not supported in the sparsifier.
The reason was that when the sparsifier was created the masks would default to the CPU.
Attaching a GPU model to the sparsifier would throw an error.
The solution is to create the masks on the same device as the weight.

Differential Revision: [D31590675](https://our.internmc.facebook.com/intern/diff/D31590675)",pytorch
66413,z-a-f,pr,2021-10-11T10:41:51Z,[sparsity] Make pruner follow the sparsifier API,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66416
* #66415
* #66414
* __->__ #66413
* #66412
* #66411

The pruner API divelged from the sparsifier API.
We would like to attach the  configuration of the layers by their FQNs.

Differential Revision: [D31590673](https://our.internmc.facebook.com/intern/diff/D31590673)",pytorch
66414,z-a-f,pr,2021-10-11T10:41:54Z,[sparsity] Remove mask from modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66416
* #66415
* __->__ #66414
* #66413
* #66412
* #66411

Original API attached the masks to the modules.
According to the [API](https://github.com/pytorch/pytorch/issues/59835), the mask ownership should belong to the parametrizations.
This removes the attachments of the masks to the modules.

Note: We still keep the removal of the masks in the `squash_mask`

Differential Revision: [D31590676](https://our.internmc.facebook.com/intern/diff/D31590676)",pytorch
66415,z-a-f,pr,2021-10-11T10:41:58Z,[sparsity] Add axis support to the pruning parametrization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66416
* __->__ #66415
* #66414
* #66413
* #66412
* #66411

We would like to allow the users to use any axis for pruning.
THis allows the base pruner to attach axis config to the pruners.

Differential Revision: [D31590677](https://our.internmc.facebook.com/intern/diff/D31590677)",pytorch
66416,z-a-f,pr,2021-10-11T10:42:01Z,[sparsity] Add weight norm pruner,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66416
* #66415
* #66414
* #66413
* #66412
* #66411

The weight norm pruner uses the weight norm statistics to identify the indices that need to be removed.

Tests: TBD

Differential Revision: [D31590674](https://our.internmc.facebook.com/intern/diff/D31590674)",pytorch
66469,zasdfgbnm,pr,2021-10-12T02:55:20Z,Use libcudacxx instead of thrust for complex when possible,Because including thrust and cub in the same file can be tricky. Including `thrust/complex.h` and `cub/cub.cuh` in the same file cause compilation failures for the latest `main` branch of thrust and cub.,pytorch
66566,zasdfgbnm,pr,2021-10-13T19:45:53Z,EmbeddingBackward exclusive_scan thrust->cub,"Fixes #{issue number}
",pytorch
66580,zasdfgbnm,pr,2021-10-13T21:49:47Z,Use cub 1.15's latest scan-by-key algorithm to replace thrust for Embedding.cu and EmbeddingBag.cu,,pytorch
66639,jjsjann123,pr,2021-10-14T18:03:58Z,Dropout update in autodiff take2,restore reverted PR #66273 ,pytorch
66657,jeffdaily,pr,2021-10-14T21:55:10Z,[ROCm] enable test_distributed() in test.sh,"Restores tests for ROCm CI that used to run prior to #63147.

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport",pytorch
66670,gmagogsfm,pr,2021-10-15T00:48:57Z,Make several methods of SharedParserData private,,pytorch
66711,zasdfgbnm,pr,2021-10-15T18:58:49Z,Use `cub::FutureValue` to simplify 64bit indexing split of cub scan,"https://github.com/NVIDIA/cub/pull/305 has landed to cub 1.15. This is ready to review and land. This PR contains https://github.com/pytorch/pytorch/pull/66219, please land that PR first before review.
",pytorch
66777,z-a-f,pr,2021-10-18T07:11:36Z,[sparsity] Add ability to keep sparsity parameters in modules,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66779
* #66778
* __->__ #66777
* #69415

Sometimes one might need to keep the sparsity parameters after the sparsifier is detached.
This saves the parameters in the `sparse_params`.
There are two ways of keeping the sparsifier params:

1. Tuple[str, ...]: A tuple of all the parameters that need to be stored.
2. Dict[str, Tuple[str, ...]]: A dict of layer keys and parameters. In this case only specified layers will have the parameters attached to.

For example:

```
>>> # This will keep params in every module
>>> sparsifier.squash_mask(keep_sparse_params=('sparse_block_shape',))
>>> print(model.submodule.linear1.sparse_params)
{'sparse_block_shape': (1, 4)}
>>> print(model.submodule.linear2.sparse_params)
{'sparse_block_shape': (1, 4)}
```

```
>>> # This will keep params only in specific modules
>>> sparsifier.squash_mask(keep_sparse_params={'submodule.linear1': ('sparse_block_shape',)})
>>> print(model.submodule.linear1.sparse_params)
{'sparse_block_shape': (1, 4)}
>>> print(model.submodule.linear2.sparse_params)
AttributeError: 'Linear' object has no attribute 'sparse_params'
```

Differential Revision: [D31835722](https://our.internmc.facebook.com/intern/diff/D31835722)",pytorch
66778,z-a-f,pr,2021-10-18T07:11:40Z,[sparsity] Convert function for sparse kernels without a context manager,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66779
* __->__ #66778
* #66777
* #69415

This removes the hack of the context manager that would communicate the zeros block shape to the quantization convert.
The conversion will assume that the converted modules have `sparse_params` (which is added by the sparsifier).

Differential Revision: [D31835721](https://our.internmc.facebook.com/intern/diff/D31835721)",pytorch
66779,z-a-f,pr,2021-10-18T07:11:44Z,[sparsity] Enable FX quantization for sparse linear,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #66779
* #66778
* #66777
* #69415

Tests: TBD

Differential Revision: [D31835720](https://our.internmc.facebook.com/intern/diff/D31835720)",pytorch
66942,jeffdaily,pr,2021-10-20T16:17:31Z,add support for ubuntu 20.04 to CI docker images,"Some minor changes are needed to the .circleci docker scripts to support ubuntu 20.04.  One edit updates the packages needed for all images (.circleci/docker/common/install_base.sh), while the other edit is specific to ROCm support.

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH @seemethere @malfet @pytorch/pytorch-dev-infra",pytorch
67258,zasdfgbnm,pr,2021-10-26T17:02:15Z,Don't #define NUM_THREADS,"PyTorch doesn't compile with the latest `main` branch of cub again. The root cause is, PyTorch defines a macro `NUM_THREADS`, and cub added some code like
```C++
template<...., int NUM_THREADS, ...>
```
and these two mess up with each other.",pytorch
67300,jjsjann123,pr,2021-10-27T00:01:32Z,removing annotation on conv.bias,"Remove annotation on `conv.bias`

This allows Profiling Executor to profile bias input to convolution, which is needed for fuser optimization to fuser bias add to later ops.",pytorch
67407,robieta,pr,2021-10-28T01:45:46Z,Python tracer for profiler,"This PR instruments the CPython interpreter and integrates the resulting trace into the PyTorch profiler. 

The python tracing logic works by enabling `PyEval_SetProfile`, and then logging the minimal information to track every time python calls or returns from a function. A great deal of care has gone into keeping this process very lightweight; the `RawEvent` struct is only two words and doesn't do anything fancy. When a python function is called, we have to do extra work. If the call is to `nn.Module.__call__`, we simply incref to extend the life of the module. Otherwise we check if we have seen the function before, and if not go through the (somewhat expensive) task of saving the strings which we then cache.

To actually get a useful timeline, we have to replay the events to determine the state of the python stack at any given point. A second round of stack replay is needed to figure out what the last python function was for each torch op so we can reconstruct the correct python stack. All of this is done during post processing, so while we want to be reasonably performant it is no longer imperative to shave every last bit.

I still need to do a bit of refinement (particularly where the tracer interfaces with the profiler), but this should give a good sense of the general structure.

Test plan:
```
import torch


class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(2, 2)
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        x = self.linear(x)
        return self.relu(x)


def call_module():
    m = MyModule()
    for _ in range(4):
        m(torch.ones((2, 2)))


def top_level_fn():
    with torch.profiler.profile(with_stack=True) as p:
        call_module()

    p.export_chrome_trace(""test_trace.json"")


top_level_fn()
```
<img width=""1043"" alt=""Screen Shot 2021-10-27 at 6 43 18 PM"" src=""https://user-images.githubusercontent.com/13089297/139171803-f95e70f3-24aa-45e6-9d4b-6d437a3f108d.png"">

PS: I've tried to comment liberally, particularly around some of the more magical parts. However I do plan on doing another linting and commenting pass. Hopefully it's not too bad right now.
",pytorch
67421,gmagogsfm,pr,2021-10-28T05:27:59Z,Allow PDT to generate `Union` type instead of `Any`,"PDT used to generate `Any` type whenever an argument has one or more types that can not be unified. Now that we have `Union` type support, we could make the type definition tighter by generating Union type.

I had to make extensive changes to the existing tests because of following design decisions made by us and Python

1. UnionType as a return type of function can not be inferred automatically to guard against accidental type inconsistencies. Thus I had to manually annotate return type of some functions under test as `Union` or `Any`.
2. There is a bug that prevents more than 1 variables being refined in a single if statement in the form of `if isinstance(a, int) and isinstance(b, int)`. Thus I had to replace such pattern with refinement of a single variable.
3. In some build environment with Python 3.6, bool is a subtype of int. This is an unfortunate design choice made by CPython. Therefore, `Union[bool, int]` is not a legal type there since bool < int. Thus, I had to modify test cases that involves `Union[bool, int]` into other types.

",pytorch
67648,jjsjann123,pr,2021-11-01T22:51:03Z,autodiff fix for autocast_to_xxx,"Fixes autocast + autodiff issue where `RuntimeError: grad_inputs.size() == node->inputs().size()INTERNAL ASSERT FAILED at ""../torch/csrc/jit/runtime/autodiff.cpp"":426, please report a bug to PyTorch.`",pytorch
67928,zasdfgbnm,pr,2021-11-05T20:42:34Z,[WIP] Implement unique_by_key by fancy iterators,"Fixes #{issue number}
",pytorch
67943,jjsjann123,pr,2021-11-06T01:00:42Z,Nvfuser code bump 11 5,"nvfuser code update:
1. Tuning heuristics on schedulers for reduction/normalization kernels;
2. bfloat16 on IO tensor support;
3. Refactored memory format support, now we can support dimension collapsing with non-coherent input tensors with different memory format. e.g. channels last tensor input to batch normalization. Note that we are currently limiting memory format to only Contiguous and Channels last;
4. Refactored nvfuser graph partitioning in `graph_fuser.cpp`, separated node merge and profile node API. Updated `profiling_record.cpp`.

Things that are reverted from our local branch:
1. changes on some entries in autodiff
2. aten::gelu with approximation
3. native_dropout(_backward)",pytorch
68008,zasdfgbnm,pr,2021-11-08T17:37:11Z,[resubmit] Don't #define NUM_THREADS,"This reverts commit 9e8016d8c48e9c99addad93112f99d3375a0fbc7.

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
68019,zasdfgbnm,pr,2021-11-08T19:53:10Z,Tune test_reference_numerics_normal,"Fixes #{issue number}
",pytorch
68074,d4l3k,pr,2021-11-09T19:15:28Z,torch.monitor - Initial C++ Stats,"This is the first step of many PRs towards implementing the `torch.monitor` RFC https://github.com/pytorch/rfcs/pull/30

This defines the aggregation types, the `Stat` class and provides some simple collection of the stats. 

This doesn't match the RFC exactly as it incorporates some of the comments on the RFC as well as a few changes for performance. 

Changes:
* added window_size to the stats. If specified it will always compute the stat using the `window_size` number of values. If there aren't enough values within that window it reports the previous stats.
* This doesn't include the push metrics yet (will be coming). 
  After more discussion it looks like the best way to handle this is to support a hybrid where the metric can set how frequently it'll be logged. For fixed window_size metrics it'll be logged each time it hits the window size. This will allow performant counters as well as lower frequency push counters (window_size=1).

Performance considerations:
* Updating the stats acquires a lock on that Stat object. This should be performant unless there's many-many threads writing to the same stat. Single thread will typically use futex so should be quite fast.
* Adding/removing/fetching all stats sets a global lock on the stat list -- this shouldn't be an issue since these events happen infrequently.
* Fetching stats accesses one stat at a time instead of a global lock. This means the exported values are linearizable but not serializable across multiple stats but I don't expect this to be an issue.

Next steps:
1. Add StatCollector interface for push style metrics
1. Add pybind interfaces to expose to Python
1. Add default metric providers
1. Integrate into Kineto trace view

Differential Revision: D32266032
",pytorch
68325,robieta,pr,2021-11-14T22:23:24Z,[Reland] Python tracer.,"There were two issues with the original PR:
1) My assumption that bound C functions could be trusted to stay alive was not valid. I'm still not entirely sure what was dying, but I've just added a cache so that the first time I see a function I collect the repr just like I was already doing with Python functions.

2) `std::regex` is known to be badly broken and prone to segfaults. Because I'm just doing a very simple prefix prune it's fine to do it manually; see `trimPrefix`. Long term we should move all of PyTorch to `re2` as the internal lint suggests, but CMake is hard and I couldn't get it to work.",pytorch
68376,zasdfgbnm,pr,2021-11-15T19:45:01Z,Use cub::DeviceSelect::UniqueByKey for EmbeddingBackward,"https://github.com/NVIDIA/cub/pull/405 is still under review, API might change before it finally lands into cub 1.16, please wait for https://github.com/NVIDIA/cub/pull/405 before merging this. Tested locally and tests pass. 
",pytorch
68484,jeffdaily,pr,2021-11-16T22:51:39Z,add rocm 4.3.1 GHA workflow,cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH,pytorch
68552,jeffdaily,pr,2021-11-17T23:25:43Z,add rocm GHA workflow,cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH,pytorch
68632,zasdfgbnm,pr,2021-11-19T02:56:10Z,Implement topk with sort for some cases,"Benchmark that compares original implementation and the sort implementation (this code should run on a branch without this patch):
```python
import torch
import timeit

def tune_dtype(f):
    def ret(*args, **kwargs):
        for dtype in [torch.int8, torch.half, torch.float, torch.double]:
            f(*args, **kwargs, dtype=dtype)
    return ret

def tune_slice(f):
    def ret(*args, **kwargs):
        slice = 1
        while slice <= 256:
            f(*args, **kwargs, slice=slice)
            slice *= 2
    return ret

def tune_slice_size(f):
    def ret(*args, **kwargs):
        slice_size = 1
        while slice_size <= 1_000_000:
            f(*args, **kwargs, slice_size=slice_size)
            slice_size *= 10
    return ret

def tune_k(f):
    def ret(*args, slice_size, **kwargs):
        k = 1
        while k <= slice_size:
            f(*args, **kwargs, k=k, slice_size=slice_size)
            k *= 10
    return ret

def topk_with_sort(tensor, k, dim=-1, largest=True):
    values, indices = tensor.sort(dim=dim, descending=largest)
    return values.narrow(dim, 0, k), indices.narrow(dim, 0, k)

def run50sync(f):
    for _ in range(50):
        f()
    torch.cuda.synchronize()

def warmup():
    N = 1000000
    for i in range(1, N // 10000):
        torch.randn(i, device='cuda')

def benchmark_one(slice, slice_size, k, dtype):
    input_ = torch.empty((slice, slice_size), dtype=dtype, device=""cuda"").random_()
    torch.cuda.synchronize()
    time = timeit.timeit(lambda: run50sync(lambda: torch.topk(input_, k, dim=1)), number=1)
    torch.cuda.synchronize()
    time_sort = timeit.timeit(lambda: run50sync(lambda: topk_with_sort(input_, k, dim=1)), number=1)
    method = ""orig"" if time < time_sort else ""sort""
    speedup = time / time_sort
    print(f""(dtype={dtype}, slice={slice}, slice_size={slice_size}, k={k}) -> (method={method}, speedup={speedup})"")

if __name__ == ""__main__"":
    warmup()
    tune_dtype(tune_slice(tune_slice_size(tune_k(benchmark_one))))()

```
Benchmark result see next comment.",pytorch
68783,d4l3k,pr,2021-11-22T23:58:08Z,torch/monitor: add C++ events and handlers,"Differential Revision: D32606547

",pytorch
68794,z-a-f,pr,2021-11-23T01:56:35Z,[sparsity] Fix for the failing pruner test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #68794

The pruner `test_constructor` fails because of a typo in the regular expression matching for the error that the pruner throws. See for details: https://github.com/pytorch/pytorch/runs/4293615334?check_suite_focus=true
This fixes it.

Test Plan:

Separate test is not needed -- single letter change.
Previous test: `python test/test_ao_sparsity.py -- TestBasePruner

Differential Revision: [D32609589](https://our.internmc.facebook.com/intern/diff/D32609589)",pytorch
68804,jjsjann123,pr,2021-11-23T09:29:17Z,fixing removeProfilingNodes duplicated functions (#1282),"Unfortunately there're two versions of removeProfilingNodes function and one of them is not cleaning up profile_ivalue nodes properly. This leads to a dangling profile_ivalue node, which ended up being profiled multiple times and could give us false assert failures.
",pytorch
69007,jjsjann123,pr,2021-11-29T17:41:39Z,re-enable layer_norm in autodiff,"Turn on layer_norm in autodiff

#67732 should have fixed the previously issue exposed by enabling layer_norm in autodiff.",pytorch
69090,jjsjann123,pr,2021-11-30T17:35:42Z,enable autocast for Lazy CUDA device,"This enables the autocast for lazy device similar to what XLA does

Note that this isn't a safe approach, as we default to AutocastCUDA here, but Lazy fallback could be either a CPU or CUDA.",pytorch
69210,jjsjann123,pr,2021-12-01T12:05:29Z,fixing layer_norm cuda bug,"Fixes #69208 
",pytorch
69255,robieta,pr,2021-12-01T21:56:53Z,[Profiler] Pull helper methods into dedicated file. (And start `torch/csrc/profiler` folder.,"Summary: One thing that I've found as I optimize profier is that there's a lot of intermingled code, where the kineto profiler relies on the legacy (autograd) profiler for generic operations. This made optimization hard because I had to manage too many complex dependencies. (Exaserbated by the USE_KINETO #ifdef's sprinkled around.) This PR is the first of several to restructure the profiler(s) so the later optimizations go in easier.

Test Plan: Unit tests

Reviewed By: aaronenyeshi

Differential Revision: D32671972

",pytorch
69369,robieta,pr,2021-12-03T17:02:55Z,Add OpaqueTag and lazyTagKind to IValue.,"Summary:
In profiler we currently use `tagKind` when capturing input shapes and dtypes. This is horrible for two reasons:
1) We're turning a uint32_t into a std::string on the hot path
2) It forces all other dtypes to be strings as well, so we wind up stringifying Tensor dtypes as well.

IValue very clearly doesn't want to just make Tag public as that would limit future extensibility. This change adds an OpaqueTag type which simply wraps `IValue::Tag` and allows the stringification to occur later.

Test Plan: N/A. The old API is preserved and this is purely for debugging.

Differential Revision: D32835288

",pytorch
69415,z-a-f,pr,2021-12-04T08:28:42Z,[AO] Clear the contents of the torch/ao/__init__.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #66779
* #66778
* #66777
* __->__ #69415

Adding the imports inside the torch/ao/__init__.py has a high chance of causing circular dependencies, especially if sparsity and quantization use each other's resources.
To avoid the dependency issues, we can just keep the __init__ empty.

Notes:
- This means that the user will have to explicitly import the `torch.ao.quantization` or `torch.ao.sparsity` instead of `from torch import ao; ao.quantization.???`.
- The issue of circular dependencies that are caused by the imports with binding submodules is [fixed in Python 3.7](https://docs.python.org/3/whatsnew/3.7.html#other-language-changes), which means this solution will become obsolete at the [3.6's EoL](https://www.python.org/dev/peps/pep-0494/#and-beyond-schedule), which comes [12/23/2022](https://devguide.python.org/#status-of-python-branches).

Future options to resolve the circular dependencies (subject to discussion):
1. Use interfaces for binding submodules. For example, have a torch/ao/_nn with all the source code, and an interface torch/ao/nn with only the __init__.py file. The __init__ files inside the torch/ao/_nn will be empty
2. Completely isolate the common code into a separate submodule, s.a. torch/ao/common. The other submodules will not be referencing each other.

Differential Revision: [D32860168](https://our.internmc.facebook.com/intern/diff/D32860168)",pytorch
69421,robieta,pr,2021-12-05T00:15:18Z,[Profiler] Clean up profiler includes.,"Summary: I've hit a lot of build issues in D32671972, and I've come to realize that a lot of it boils down to header hygene. `function.h` includes `profiler.h` *solely* to transitively include `record_function.h` which winds up leaking the profiler symbols. Moreover several files are relying on transitive includes to get access to `getTime`. As long as I have to touch all the places that use `getTime`, I may as well also move them to the new namespace.

Test Plan: Unit tests and CI.

Differential Revision: D32865907



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69428,jjsjann123,pr,2021-12-05T10:55:05Z,Nvfuser code bump 12 5,"Things added in this PR that requires review:
1. cuLaunchCooperativeKernel driver API added
aten/src/ATen/cuda/detail/LazyNVRTC.cpp
aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h

nvfuser code update:
1. perf turning on codegen scheduler that improves performance.
2. permutation support has been extended beyond contiguous/channels-last. (The improvements could be observed on PW benchmark)

Things reverted from local changes:
1. aten::gelu with approximation
2. local changes that is upstreamed in PR #68804 ",pytorch
69436,z-a-f,pr,2021-12-05T19:56:25Z,Add sparsity to the fx2trt conversion.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69574
* #69573
* __->__ #69436

This only for the post-training sparsity. Because the sparse quantized CUDA kernels are available in Ampere architectures and up, this benchmark is to be run on A100+ machines to see the benefits.

Test Plan:

```
python torch/fx/experimental/fx2trt/example/sparse_quantized_resnet_test.py
```

Differential Revision: [D32872091](https://our.internmc.facebook.com/intern/diff/D32872091)",pytorch
69459,robieta,pr,2021-12-06T17:33:38Z,[Profiler] Factor common logic into `torch/csrc/profiler/api.h`,"Summary:
This change breaks the dependency between the kineto and legacy profiler; instead of `profiler_kineto.h` including `profiler_legacy.h`, they both include `profiler/api.h`. As part of this refactor, I injected some intermediate classes to keep legacy behavior from leaking into the kineto profiler:

1) ProfilerThreadLocalState has become ProfilerThreadLocalStateBase which just handles config and callback handle. Legacy and Kineto profilers inherit this and implement their own very disjoint set of logic.

2) CUDAStubs is a pure virtual class to make the interface more readable, and the ""always fail"" behavior has been moved to a `DefaultCUDAStubs` class in `api.cpp`.

Test Plan: Ran the overhead ubenchmark.

Differential Revision: D32678163



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69514,gmagogsfm,pr,2021-12-07T06:53:10Z,Add Union type to TorchScript Language Ref,,pytorch
69552,zasdfgbnm,pr,2021-12-07T19:32:00Z,Make ProcessGroupNCCL load torch_ucc.so when TORCH_UCC_LIBRARY_PATH is set,"This is the very first step for the UCC-NCCL integration. This PR lets `ProcessGroupNCCL` load the `torch_ucc.so` if the user specifies an environmental variable `TORCH_UCC_LIBRARY_PATH`. If this environment variable is not specified by the user, then there will be no visible change.

In the future, we may want to make PyTorch smart enough to automatically detect the `torch_ucc.so` in the user's system, but before doing that, I believe we should first make sure that `ProcessGroupUCC` is very well tested.

Note that in this PR, `ProcessGroupNCCL` just loads the library but will not use it. I am trying to make PRs small, so the usage of `torch_ucc.so` will be submitted in later PRs.

This PR requires the change in https://github.com/facebookresearch/torch_ucc/pull/56, otherwise `torch_ucc.so` can not be successfully loaded. But his PR can be landed separately without waiting for https://github.com/facebookresearch/torch_ucc/pull/56 because, in PyTorch's unit tests, UCC is never used or tested.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69564,zasdfgbnm,pr,2021-12-07T21:48:32Z,Create UCC ProcessGroup when ucc_lib available,"This PR is based on https://github.com/pytorch/pytorch/pull/69552, please review that PR first. This PR requires https://github.com/facebookresearch/torch_ucc/pull/57, but this PR can be landed separately because in PyTorch's unit tests, UCC is never used or tested.


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69567,d4l3k,pr,2021-12-07T22:14:24Z,torch/monitor: add pybind,"Summary:
This exposes torch.monitor events and stats via pybind11 to the underlying C++ implementation.

* The registration interface is a tad different since it takes a lambda function in Python where as in C++ it's a full class.
* This has a small amount of changes to the counter interfaces since there's no way to create an initializer list at runtime so they now also take a vector.
* Only double based stats are provided in Python since it's intended more for high level stats where float imprecision shouldn't be an issue. This can be changed down the line if need arises.

```
events = []

def handler(event):
    events.append(event)

handle = register_event_handler(handler)

log_event(Event(type=""torch.monitor.TestEvent"", timestamp=datetime.now(), metadata={""foo"": 1.0}))
```

Test Plan: buck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor

Differential Revision: D32924141

",pytorch
69573,z-a-f,pr,2021-12-07T22:52:56Z,[fx][quant] Add `intrinsic.quantized.LinearReLU` to the fx2trt converters,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #69574
* __->__ #69573
* #69436

The fx2trt handles the fused operators by replacing them with the TRT layer followed by the TRT activation.
This adds the quantized LinearReLU conversion routine to the fx2trt.
This is required because the `torch.ao.quantization.convert_fx` replaces the Linear + ReLU with a fused LinearReLU operator that fx2trt should be aware of.

Differential Revision: [D32933041](https://our.internmc.facebook.com/intern/diff/D32933041)",pytorch
69574,z-a-f,pr,2021-12-07T22:52:59Z,Add sparse MLP test to the fx2trt,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69574
* #69573
* #69436

THe issue with the sparse resnet tests is that the linears are [replaced with convolutional layers in TRT](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fusion-types).
In order to check the fully connected layers, we need to create a sequence of linear layers that make an MLP

Test Plan:

`python ./torch/fx/experimental/fx2trt/example/sparse_quantized_mlp_test.py`

Differential Revision: [D32933042](https://our.internmc.facebook.com/intern/diff/D32933042)",pytorch
69578,zasdfgbnm,pr,2021-12-08T00:04:12Z,Fix errors in common_utils.py,"This fixes the following error:
```python
Traceback (most recent call last):
  File ""/home/gaoxiang/pytorch-ucc2/test/distributed/test_distributed_spawn.py"", line 40, in <module>
    run_tests()
  File ""/home/gaoxiang/.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py"", line 618, in run_tests
    ['--import-slow-tests'] if IMPORT_SLOW_TESTS else List[str]([]))
  File ""/usr/lib/python3.9/typing.py"", line 680, in __call__
    raise TypeError(f""Type {self._name} cannot be instantiated; ""
TypeError: Type List cannot be instantiated; use list() instead
Traceback (most recent call last):
  File ""/home/gaoxiang/pytorch-ucc2/test/run_test.py"", line 1058, in <module>
    main()
  File ""/home/gaoxiang/pytorch-ucc2/test/run_test.py"", line 1036, in main
    raise RuntimeError(err_message)
RuntimeError: distributed/test_distributed_spawn failed!
```
",pytorch
69587,zasdfgbnm,pr,2021-12-08T01:34:19Z,"Use UCC for send, and recv for ProcessGroupNCCL on CPU","This PR is based on [#69552](https://github.com/pytorch/pytorch/pull/69564), please review that PR first.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69651,zasdfgbnm,pr,2021-12-09T00:11:49Z,"[WIP] Install UCX, UCC and torch_ucc in PyTorch CI","Background: https://github.com/pytorch/pytorch/issues/70654

In order to run distributed tests with UCC through the NCCL PG on PyTorch CI, installing ucx, ucc, and torch_ucc are required.

This PR does not enable any tests. It just builds out the infrastructure needed in CI.",pytorch
69796,gmagogsfm,pr,2021-12-10T23:01:36Z,"Back out ""[pytorch][PR] Add ability for a mobile::Module to save as flatbuffer""","Differential Revision: D33032671

",pytorch
69798,robieta,pr,2021-12-11T00:34:03Z,[Profiler] Add glue layer to reduce the use of `#ifdef USE_KINETO` in the profiler code.,"Summary:
One of the major sources of complexity in `profiler_kineto.cpp` is that kineto may or may not be available. The code (including the types) follows two related but often distict codepaths, and large sections may or may not be `#ifdef`'d out.

Optimizing such code which preserving correctness is quite difficult; at one point I realized that I had broken the non-Kineto case, because moving work into the finalize step runs astray of a very large `#ifdef` around the finalize logic.

In order to make optimization more tractable, I gathered all of the calls to Kineto APIs and isolated them in the `kineto_shim.h/.cpp` files: the header allows callers to pretend as though Kineto is always available (mostly), and the cpp file hides most of the horrible `#ifdef`s so they don't pollute the main profiler code.

Test Plan: Unit tests.

Differential Revision: D32690568



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69923,d4l3k,pr,2021-12-14T20:04:07Z,"Back out ""Revert D32606547: torch/monitor: add C++ events and handlers""","Summary:
Original commit changeset: fbaf2cc06ad4

Original Phabricator Diff: D32606547 (https://github.com/pytorch/pytorch/commit/e61fc1c03b64e61ca4f5bbe278db7ee2cf35e8ff)

This is the same thing as the original diff but just using a normal std::mutex instead of std::shared_timed_mutex which is not available on OSX 10.11. The performance difference should be negligible and easy to change down the line if it does become a bottleneck.

Old failing build: https://github.com/pytorch/pytorch/runs/4495465412?check_suite_focus=true

Old pull request: https://github.com/pytorch/pytorch/pull/68783

Test Plan:
buck test //caffe2/test/cpp/monitor:monitor

will add ciflow tags to ensure mac builds are fine

Differential Revision: D33102715

",pytorch
69952,d4l3k,pr,2021-12-15T01:06:20Z,torchx/monitor: simplify event naming,"Summary: This cleans up the naming for events. type is now name, message is gone, and metadata is renamed data.

Test Plan: https://www.internalfb.com/intern/testinfra/testconsole/testrun/7318349461027367/

Reviewed By: kiukchung

Differential Revision: D32969391

",pytorch
69953,zasdfgbnm,pr,2021-12-15T01:17:23Z,Add support for deleteKey for FileStore,"torch_ucc uses `deleteKey`, and trying to run PyTorch tests with torch_ucc leads to failure about `deleteKey not implemented for FileStore`.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
69985,zasdfgbnm,pr,2021-12-15T18:42:01Z,Fix build on latest main branch of thrust,"Our internal CI that builds PyTorch with the latest main branch of thrust fails with
```
#22 466.9 /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMAGMA_V2 -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_MPI -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -Iaten/src -I../aten/src -I. -I../ -I../cmake/../third_party/benchmark/include -I../cmake/../third_party/cudnn_frontend/include -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -Iinclude -I../torch/csrc/distributed -I../aten/src/TH -I../aten/src/THC -I../aten/src/ATen/cuda -Icaffe2/aten/src -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -Inccl/include -I../c10/cuda/../.. -I../c10/.. -I../third_party/tensorpipe -Ithird_party/tensorpipe -I../third_party/tensorpipe/third_party/libnop/include -I../torch/csrc/api -I../torch/csrc/api/include -isystem=third_party/gloo -isystem=../cmake/../third_party/gloo -isystem=../cmake/../third_party/googletest/googlemock/include -isystem=../cmake/../third_party/googletest/googletest/include -isystem=../third_party/protobuf/src -isystem=/opt/conda/include -isystem=../third_party/gemmlowp -isystem=../third_party/neon2sse -isystem=../third_party/XNNPACK/include -isystem=../third_party -isystem=../cmake/../third_party/eigen -isystem=/opt/conda/include/python3.8 -isystem=/opt/conda/lib/python3.8/site-packages/numpy/core/include -isystem=../cmake/../third_party/pybind11/include -isystem=/opt/hpcx/ompi/include/openmpi -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent -isystem=/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include -isystem=/opt/hpcx/ompi/include -isystem=/usr/local/cuda/include -isystem=../third_party/ideep/mkl-dnn/third_party/oneDNN/include -isystem=../third_party/ideep/include -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20236 -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -Xcompiler=-fPIC -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD -Xcompiler=-Wall,-Wextra,-Wno-unused-parameter,-Wno-unused-variable,-Wno-unused-function,-Wno-unused-result,-Wno-unused-local-typedefs,-Wno-missing-field-initializers,-Wno-write-strings,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-sign-compare,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-error=deprecated-declarations,-Wno-missing-braces,-Wno-maybe-uninitialized -DTORCH_CUDA_BUILD_MAIN_LIB -Xcompiler -pthread -std=c++14 -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu.o.d -x cu -c ../aten/src/ATen/native/cuda/LegacyThrustHelpers.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu.o
#22 466.9 ../aten/src/ATen/native/cuda/LegacyThrustHelpers.cu(53): error: namespace ""thrust"" has no member ""make_constant_iterator""
#22 466.9 
#22 466.9 1 error detected in the compilation of ""../aten/src/ATen/native/cuda/LegacyThrustHelpers.cu"".
```
The failure is because this file uses `thrust::make_counting_iterator`, but didn't include the file where this function is defined.

cc: @xwang233 ",pytorch
70000,jjsjann123,pr,2021-12-15T21:49:16Z,Allow single node fusion for nvfuser,"Setting `PYTORCH_NVFUSER_ONE_OP_FUSION=1` will take all nodes nvFuser support, instead of waiting for fusion opportunity.",pytorch
70050,robieta,pr,2021-12-16T16:18:47Z,Prevent divide-by-zero errors in Timer,"Fixes https://github.com/pytorch/pytorch/issues/66503
",pytorch
70127,jjsjann123,pr,2021-12-17T20:01:22Z,fixing conv2d decomposition and tests,"Current implementation has a bug where decomposed `add_optional` from `conv2d` is placed before the producer node, this causes linter error on graph.

Cherry-picked from https://github.com/csarofeen/pytorch/pull/1333
Fixing issue posted in https://github.com/csarofeen/pytorch/issues/1325",pytorch
70327,robieta,pr,2021-12-22T21:15:45Z,[Profiler] Address issues from profiler bifurcation.,"Summary:
After D32678163 (https://github.com/pytorch/pytorch/commit/7ea86dfdb162758c9fbbf6807ab1dd778591c062), test_rpc_profiler began failing. This was surprising, because it should have been a no-op refactor. However, one change is that a Kineto profiler is no longer also an autograd profiler; the RPC framework was assuming a legacy profiler but when a kineto profiler was active things still kind of worked due to that implementation detail. (But crashed after the class split.)

This diff tidys up a couple of things:
1) Move `getProfilerConfig` into `api.cpp`, since it is no longer correct to static_cast a `KinetoThreadLocalState` to a `ProfilerLegacyThreadLocalState`. (And really the class we want is `ProfilerThreadLocalStateBase` anyway.)

2) Add a mechanism for callers to check if the active profiler is a legacy or kineto profiler. (So callers like RPC can adjust or provide a nice error message.)

3) Fix the RPC test to create a legacy profiler.

Test Plan: `caffe2/torch/fb/training_toolkit/backend/tests:test_rpc_profiler` now passes, and before the fix to `test_rpc_profiler.py`, I verified that the test failed with the error message added to `utils.cpp` rather than just crashing.

Reviewed By: suphoff

Differential Revision: D33283314



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
70905,z-a-f,pr,2022-01-06T10:36:12Z,[quant] Add QuantizedLSTM class,"Summary:
The nn.LSTM is quantized through the custom module mechanism, which uses the nn.quantizable.LSTM for both observed and quantized paths. This is potentially a source of confusion. This creates a `quantized.LSTM` class, which completely takes the quantized path. Note that after this, the old usage will throw an error.

New way of using it:

```
>>> custom_module_config = {
...     'float_to_observed_custom_module_class': {
...         nn.LSTM: nn.quantizable.LSTM,
...     },
...     'observed_to_quantized_custom_module_class': {
...         nn.quantizable.LSTM: nn.quantized.LSTM,
...     }
... }
>>> tq.prepare(model, prepare_custom_module_class=custom_module_config)
>>> tq.convert(model, convert_custom_module_class=custom_module_config)
```

Test Plan: `python test/test_quantization.py`

Differential Revision: D33451338

",pytorch
70907,z-a-f,pr,2022-01-06T11:01:09Z,[quant] Add QuantizedMHA class,"Summary:
The nn.MultiheadAttention is quantized through the custom module mechanism, which uses the nn.quantizable.MultiheadAttention for both observed and quantized paths. This is potentially a source of confusion. This creates a quantized.MultiheadAttention class, which completely takes the quantized path. Note that after this, the old usage will throw an error.
New way of using it:

```
>>> custom_module_config = {
...     'float_to_observed_custom_module_class': {
...         nn.MultiheadAttention: nn.quantizable.MultiheadAttention,
...     },
...     'observed_to_quantized_custom_module_class': {
...         nn.quantizable.MultiheadAttention: nn.quantized.MultiheadAttention,
...     }
... }
>>> tq.prepare(model, prepare_custom_module_class=custom_module_config)
>>> tq.convert(model, convert_custom_module_class=custom_module_config)
```

Test Plan: `python test/test_quantization.py`

Differential Revision: D33452179

",pytorch
70908,z-a-f,pr,2022-01-06T11:02:15Z,[quant] Quantizable documentation,"Summary: Minor documentation entry for the quantizable LSTM and MHA classes.

Test Plan: raichu_docs

Differential Revision: D33452284

",pytorch
70911,z-a-f,pr,2022-01-06T11:54:16Z,[quant] Create default custom modules for LSTM and MHA,"Summary: Currently we expect the users to provide custom modules for LSTM and MHA. However, as we almost always ask the users to use those modules in the custom context, it is better to make this behavior default.

Test Plan: `python test/test_quantization.py`

Differential Revision: D33452890

",pytorch
70943,zasdfgbnm,pr,2022-01-06T19:23:31Z,Remove sync in embedding,"This together with https://github.com/pytorch/pytorch/pull/66580 and https://github.com/pytorch/pytorch/pull/68376 will remove all syncs in embedding.

This PR includes https://github.com/pytorch/pytorch/pull/68376, please review after merging https://github.com/pytorch/pytorch/pull/68376

This PR introduces perf regressions and increases memory usage: 
- `exclusive_sum` is now computing the entire `numel` elements instead of `num_of_segments` elements, and the trailing `numel - num_of_segments` results will be discarded.
- Some memory allocation now needs `numel` spaces instead of `num_of_segments` or `num_of_partial_segments`.

These are the prices we must pay in order to get a sync-free implementation.

I haven't done any benchmark yet. I will do it later.",pytorch
71017,zasdfgbnm,pr,2022-01-07T20:14:52Z,"Install UCX, UCC and torch_ucc in PyTorch CI","Background: #70654

In order to run distributed tests with UCC through the NCCL PG on PyTorch CI, installing ucx, ucc, and torch_ucc are required.

This PR does not enable any tests. It just builds out the infrastructure needed in CI.",pytorch
71120,robieta,pr,2022-01-10T18:42:49Z,Make Kineto + distributed a warning rather than an error,"Summary: D33283314 (https://github.com/pytorch/pytorch/commit/681e78bacec69c3ac6653483da2236d0e0416c6e) is causing jobs to fail when profiled, which is not ideal.

Differential Revision: D33437773



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang",pytorch
71135,robieta,pr,2022-01-10T23:50:15Z,[Profiler] Split observer implementations based on ProfilerState,"Summary:
The NVTX profiler is quite different from the other Kineto cases, so it's worth it to peel it off early so that later logic can assume either KINETO or KINETO_GPU_FALLBACK. This is more important since we're going to change the Kineto internals. (You can see the python tracer was unnecessarily coupled to NVTX just because the control logic was intermingled.)

There's also no reason to put the legacy observer state in the header rather than the cpp file now that the kineto profiler doesn't need it, so we should shield it from prying eyes.

The recent headaches with TLS downcasting and RPC integration (D32678163 (https://github.com/pytorch/pytorch/commit/7ea86dfdb162758c9fbbf6807ab1dd778591c062), D33283314 (https://github.com/pytorch/pytorch/commit/681e78bacec69c3ac6653483da2236d0e0416c6e), D33437773) have made crystal clear that we need a lot more safety in the profiler, particularly as we shift things around.

Test Plan: Unit tests. This is no longer a performance PR.

Reviewed By: aaronenyeshi

Differential Revision: D32710829

",pytorch
71417,robieta,pr,2022-01-18T17:35:10Z,[Profiler] Fix memory profile type from recent refactor,"Summary: I accidentally changed CPU_INSTANT_EVENT to CPU_OP, which broke TensorBoard.

Test Plan: Make memory profiling unit test check this case.

Differential Revision: D33637286

",pytorch
71481,d4l3k,pr,2022-01-19T17:50:44Z,monitor: add docstrings to pybind interface,"This adds argument names and docstrings so the docs are a lot more understandable.

Test plan:

docs/tests CI should suffice

![Screenshot 2022-01-19 at 16-35-10 torch monitor — PyTorch master documentation](https://user-images.githubusercontent.com/909104/150240882-e69cfa17-e2be-4569-8ced-71979a89b369.png)


",pytorch
71503,jeffdaily,pr,2022-01-19T22:54:53Z,[DO NOT MERGE] test.sh remove temporary NUM_TEST_SHARDS for Jenkins,"Now that ROCm GHA migration is complete, remove the Jenkins work-around in test.sh.

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
71538,robieta,pr,2022-01-20T05:32:38Z,[Profiler] Optimize `reportMemoryUsage`,"Summary: `reportMemoryUsage` is kind of awful. It does a bunch of string writes and such that makes it VERY expensive. Just moving that work off the hot path reduces the overhead for `profile_memory` from ~6.5 us to ~1.2 us. (85% reduction in the kineto contribution to profiling overhead.)

Test Plan: Ran ubenchmark with `--op empty --stressTestKineto --kinetoProfileMemory`

Reviewed By: swolchok

Differential Revision: D32730167

",pytorch
71539,robieta,pr,2022-01-20T05:33:55Z,[Profiler] Defer KinetoEvent and GenericTraceActivity creation to post processing.,"Summary:
This is the first of the optimizing changes. One of the issues with kineto sometimes being unavailable is we cannot use it as a storage mechanism. KinetoEvent currently fills this role, however KinetoEvent is VERY expensive. A second issue is that because we currently write to two objects, we hold the state lock for the duration of both event creations which is not ideal.

This applies the following optimizations:
1) Intermediate data is stored in a deque in KinetoThreadLocalState, which saves a data->KinetoObserverContext->KinetoEvent double copy. The new KinetoObserverContext just holds a pointer to the element in the deque.
2) OpEventData is much lighter weight (though still far from ideal)

Test Plan:
Script: P470970719
Result: P470970794
For the base case (no special flags), 40% reduction in the `profiler_kineto` portion of the overhead.

Differential Revision: D32691800

",pytorch
71577,zasdfgbnm,pr,2022-01-20T20:11:48Z,"Add complex support for Jiterator, port sinc to Jiterator","I copy-pasted part of std c++ from LLVM, make it a string, and modify it to use it to implement complex support for Jiterator",pytorch
71581,d4l3k,pr,2022-01-20T21:11:27Z,torch/monitor: make tests more robust on windows,"Summary: 

Fixes #71553

Test Plan:
add ciflow/windows to CI

  buck test //caffe2/test:monitor -- --stress-runs 100 test_interval_sec

I don't have a windows machine so need to rely on CI to test

Differential Revision: D33691540

",pytorch
71658,d4l3k,pr,2022-01-21T22:40:33Z,torch/monitor: TensorboardEventHandler,"Summary: This adds the beginnings of a TensorboardEventHandler which will log stats to Tensorboard.

Test Plan: buck test //caffe2/test:monitor

Differential Revision: D33719954

",pytorch
71665,jjsjann123,pr,2022-01-22T00:31:43Z,fixing stride order for expanded tensor,"The default initialization of stride order were not correct. This ended up with an expanded tensor showing wrong stride, since stride 0 is ignored by TensorIterator stride computation logic [Computing output strides].

Quick fix with cpp tests as well.

Note that things still look strange when we expand from a rank 1 size 1 tensor, as that gives us inconsistent strides.
```
In [7]: x = torch.rand([1])

In [8]: x.expand(1, 1, 4, 4).stride()
Out[8]: (0, 0, 0, 0)

In [9]: x.expand(4, 4, 1, 1).stride()
Out[9]: (0, 0, 1, 1)

In [10]: x.expand(4, 1, 4, 1).stride()
Out[10]: (0, 0, 0, 1)
```

Meanwhile, scalar tensor seems to work fine.
```
In [2]: x = torch.tensor(1.0)

In [3]: x.expand(4, 1, 1, 4).stride()
Out[3]: (0, 0, 0, 0)

In [4]: x.expand(4, 1, 4, 1).stride()
Out[4]: (0, 0, 0, 0)

In [5]: x.expand(4, 4, 1, 1).stride()
Out[5]: (0, 0, 0, 0)

In [6]: x.expand(1, 1, 4, 4).stride()
Out[6]: (0, 0, 0, 0)
```",pytorch
71862,jeffdaily,pr,2022-01-26T16:24:34Z,add ZeroTensor specialization to div.Tensor,"Completes a task from #69687.
",pytorch
71881,jeffdaily,pr,2022-01-26T21:52:52Z,rocblas alt impl during backward pass only,"In preparation of adopting future rocblas library options, it is necessary to track when the backward pass of training is executing.  The scope-based helper class `BackwardPassGuard` is provided to toggle state.",pytorch
71892,d4l3k,pr,2022-01-26T23:16:31Z,torch/monitor: support tensors logged as part of events,"Summary: This adds support for logging tensors as part of events. This greatly increases the potential type of events that could be logged (such as images, histograms, etc)

Test Plan: buck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor

Differential Revision: D33802539

",pytorch
71950,d4l3k,pr,2022-01-27T23:06:17Z,torch/monitor: update pyi definitions,"Summary: This updates the .pyi definitions to match the pybind interfaces.

Test Plan:
```
pyre
```

CI

Differential Revision: D33830311

",pytorch
72009,d4l3k,pr,2022-01-28T18:30:08Z,torch/monitor: merge Interval and FixedCount stats,"Summary: This simplifies the Stats interface by merging IntervalStat and FixedCountStat into a single Stat w/ a specific window size duration and an optional max samples per window. This allows for the original intention of having comparably sized windows (for statistical purposes) while also having a consistent output bandwidth.

Test Plan:
```
buck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor
```

Differential Revision: D33822956

",pytorch
72127,jjsjann123,pr,2022-02-01T18:45:40Z,Nvfuser code bump 2_1_2022,"Things changed in this PR that requires review:
1. aten/src/ATen/core/interned_strings.h
2. torch/csrc/jit/ir/alias_analysis.h : exposing createValue to allow efficient mutation
3. torch/csrc/jit/runtime/symbolic_shape_registry.cpp : added gelu/tanh/erf in registry
4. torch/jit/_script.py : throws scripting model sees autocast as decorator since it's not supported  

nvfuser code update:
1. codegen improvements and performance tuning
2. integration bug fixes for shape expression logic
3. kernel segmentation update to address perf regression from horizontal fusion
4. scalar cpu tensor promotion to support inter-device operation between cpu scalar tensor and cuda tensor

Things reverted from local changes:
aten::gelu with approximation (tracked in PR: https://github.com/pytorch/pytorch/pull/61439)",pytorch
72140,d4l3k,pr,2022-02-01T21:07:16Z,torchelastic: log events to torch.monitor,"Summary:
This adds support so torchelastic events are also logged via `torch.monitor`. Once this is in a stable spot we'll remove the original torch elastic event interface and handlers.

This ignores events where the destination is `console` as well as `RdzvEvents` since per aivanou those aren't actually used.

This also switches the events lib_test to use `define_tests` instead of `define_mp_tests` since there's no need to run these test serially.

Test Plan:
```
buck test //caffe2/test/distributed/elastic/events:lib_test
```

Differential Revision: D33925442

",pytorch
72142,jeffdaily,pr,2022-02-01T21:21:51Z,enable GHA workflow defaults for ROCm,Also updates the ROCm per-job health check.,pytorch
72208,jjsjann123,pr,2022-02-02T22:02:50Z,[DO NOT REVIEW] Ci test,"Fixes #ISSUE_NUMBER
",pytorch
72649,ppwwyyxx,pr,2022-02-10T07:47:35Z,Stop writing logs to root logger,"Fixes https://github.com/pytorch/pytorch/issues/72648
",pytorch
72686,jeffdaily,pr,2022-02-10T23:48:02Z,add rocm ciflow/slow workflow,Enables additional tests that historically have been missed for ROCm CI.,pytorch
72867,gmagogsfm,pr,2022-02-15T18:34:37Z,[Hack] Allow true retracing of a scripted module,,pytorch
73409,robieta,pr,2022-02-25T01:00:45Z,[Profiler] Specialized AppendOnlyQueue,"Summary: We can do better than `vector` or `deque`, and it's sufficiently important to the hot path to justify a custom container. (This is part of the larger queue refactor, but this is a standalone drop-in replacement so we don't need to wait.)

Test Plan: It's a pretty simple container type, so I just added a few cpp tests for emplace and read back. I also ran the overhead benchmark (replicates=9) with both `--stressTestKineto` (0.99 -> 0.94 us) and `--stressTestKineto --kinetoProfileMemory` (1.36 -> 1.27 us).

Differential Revision: D34231072

",pytorch
73462,robieta,pr,2022-02-25T23:51:55Z,Cleanup C10::Scalar stringification,"Summary: Because `operator<<` for Scalar is in `namespace at`, overload resolution fails very outside of `at`. By moving to `c10`, `namespace c10` will be included in the candidates due to the Scalar arg. I also didn't a good reason for the definition to be inline, and we're missing a `toString` overload so I added one.

Test Plan: Template deduction during compile should be sufficient.

Differential Revision: D34483818

",pytorch
73627,jjsjann123,pr,2022-03-02T04:08:54Z,Nvfuser code bump 030122,"Things changed in this PR that requires review:

test/forward_backward_compatibility/check_forward_backward_compatibility.py

Our previous function overload extension names were wrong and has been updated in this PR, hence the compatibility list updated.

nvfuser code updates with bug fixes towards failures we encountered in OpInfoTests as well as failures reported by AOTAutograd team.",pytorch
73673,zasdfgbnm,pr,2022-03-02T18:26:32Z,do not merge,"Fixes #ISSUE_NUMBER
",pytorch
73855,robieta,pr,2022-03-07T17:06:32Z,[Profiler] Prefer TSC to wall clock when available,"Summary: Calling the clock is one of the most expensive parts of profiling. We can reduce the profiling overhead by using `rdtsc` instead. The tradeoff is that we have to measure and convert. (shift and scale)

Test Plan: I added a cpp unit test with *very* aggressive anti-flake measures. I also ran the overhead benchmark (9 replicates) with `--stressTestKineto` (0.94 -> 0.89 us) and `--stressTestKineto --kinetoProfileMemory` (1.27 -> 1.17 us)

Differential Revision: D34231071

",pytorch
73952,gmagogsfm,pr,2022-03-09T02:34:15Z,Support tensor.__getitem__() in TorchScript compilation,,pytorch
74129,jeffdaily,pr,2022-03-11T22:55:52Z,[ROCm] revert cat operator performance work-around,"revert d5ca53c9554fd63d1fd69e58416dbf17a7952af9 (#46097).  The changes only affect ROCm.  Reverts a work-around for a compiler performance issue that is no longer needed.

`python -m pt.cat_test --tag_filter all --device cuda`

```
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : all

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1,1,1)_N2_dim0_cuda
# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda
OLD Forward Execution Time (us) : 48.833
NEW Forward Execution Time (us) : 8.318

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(512,512,2)_N2_dim1_cuda
# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda
OLD Forward Execution Time (us) : 54.508
NEW Forward Execution Time (us) : 23.824

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(128,1024,2)_N2_dim1_cuda
# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda
OLD Forward Execution Time (us) : 52.117
NEW Forward Execution Time (us) : 14.942

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda
# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda
OLD Forward Execution Time (us) : 98.790
NEW Forward Execution Time (us) : 74.334

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda
# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda
OLD Forward Execution Time (us) : 102.063
NEW Forward Execution Time (us) : 76.008

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda
# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda
OLD Forward Execution Time (us) : 167.786
NEW Forward Execution Time (us) : 123.679

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f1b1dec7b00>,111,65]_N5_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f1b1dec7b00>, 111, 65], N: 5, dim: 0, device: cuda
OLD Forward Execution Time (us) : 98.320
NEW Forward Execution Time (us) : 67.436

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[96,<function<lambda>at0x7f1b1dec7a70>,64]_N5_dim1_cuda
# Input: sizes: [96, <function <lambda> at 0x7f1b1dec7a70>, 64], N: 5, dim: 1, device: cuda
OLD Forward Execution Time (us) : 91.484
NEW Forward Execution Time (us) : 59.230

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[128,64,<function<lambda>at0x7f18db09d290>]_N5_dim2_cuda
# Input: sizes: [128, 64, <function <lambda> at 0x7f18db09d290>], N: 5, dim: 2, device: cuda
OLD Forward Execution Time (us) : 109.569
NEW Forward Execution Time (us) : 76.557

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f18db09d560>,32,64]_N50_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f18db09d560>, 32, 64], N: 50, dim: 0, device: cuda
OLD Forward Execution Time (us) : 106.603
NEW Forward Execution Time (us) : 87.635

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[32,<function<lambda>at0x7f18db09d5f0>,64]_N50_dim1_cuda
# Input: sizes: [32, <function <lambda> at 0x7f18db09d5f0>, 64], N: 50, dim: 1, device: cuda
OLD Forward Execution Time (us) : 106.693
NEW Forward Execution Time (us) : 88.902

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[33,65,<function<lambda>at0x7f18db09d680>]_N50_dim2_cuda
# Input: sizes: [33, 65, <function <lambda> at 0x7f18db09d680>], N: 50, dim: 2, device: cuda
OLD Forward Execution Time (us) : 110.881
NEW Forward Execution Time (us) : 94.361

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda
# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda
OLD Forward Execution Time (us) : 122.925
NEW Forward Execution Time (us) : 123.046

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda
# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda
OLD Forward Execution Time (us) : 272.442
NEW Forward Execution Time (us) : 271.932

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda
# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda
OLD Forward Execution Time (us) : 457.329
NEW Forward Execution Time (us) : 456.767

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f18db09d710>]_N100_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f18db09d710>], N: 100, dim: 0, device: cuda
OLD Forward Execution Time (us) : 117.688
NEW Forward Execution Time (us) : 87.133

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f18db09d7a0>]_N1000_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f18db09d7a0>], N: 1000, dim: 0, device: cuda
OLD Forward Execution Time (us) : 873.764
NEW Forward Execution Time (us) : 865.075

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f18db09d830>]_N2000_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f18db09d830>], N: 2000, dim: 0, device: cuda
OLD Forward Execution Time (us) : 1746.831
NEW Forward Execution Time (us) : 1730.252

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[<function<lambda>at0x7f18db09d8c0>]_N3000_dim0_cuda
# Input: sizes: [<function <lambda> at 0x7f18db09d8c0>], N: 3000, dim: 0, device: cuda
OLD Forward Execution Time (us) : 2619.303
NEW Forward Execution Time (us) : 2598.717

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(1,160),(1,14)]_N-1_dim1_cuda
# Input: sizes: [(1, 160), (1, 14)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 52.063
NEW Forward Execution Time (us) : 7.904

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(1,20,40),(1,4,40),(1,5,40)]_N-1_dim1_cuda
# Input: sizes: [(1, 20, 40), (1, 4, 40), (1, 5, 40)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 52.275
NEW Forward Execution Time (us) : 8.118

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(1,580),(1,174)]_N-1_dim1_cuda
# Input: sizes: [(1, 580), (1, 174)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 51.896
NEW Forward Execution Time (us) : 7.938

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(20,160),(20,14)]_N-1_dim1_cuda
# Input: sizes: [(20, 160), (20, 14)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 51.745
NEW Forward Execution Time (us) : 7.922

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(20,20,40),(20,4,40),(20,5,40)]_N-1_dim1_cuda
# Input: sizes: [(20, 20, 40), (20, 4, 40), (20, 5, 40)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 52.575
NEW Forward Execution Time (us) : 13.299

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes[(20,580),(20,174)]_N-1_dim1_cuda
# Input: sizes: [(20, 580), (20, 174)], N: -1, dim: 1, device: cuda
OLD Forward Execution Time (us) : 52.090
NEW Forward Execution Time (us) : 8.015
```",pytorch
74151,robieta,pr,2022-03-13T18:45:42Z,[Profiler] Switch to thread local subqueues to reduce lock contention.,"Summary: The first of several changes to move to an optimized recording data structure to back profiler. This PR keeps the existing monolithic `OpEventData` struct, but splits storage into thread local subqueues so we don't have to lock to insert.

Test Plan: Unit tests and benchmarks. The single threaded benchmark is unchanged, and the multithreaded stress test dropped from ~21 us to ~6us.

Reviewed By: chaekit

Differential Revision: D34720171

",pytorch
74228,gmagogsfm,pr,2022-03-15T05:41:46Z,Improve error message of loading saved TS module out of support window,Explicitly state that users should upgrade PyTorch to mitigate issues of loading TS module that's outside of support window,pytorch
74239,jjsjann123,pr,2022-03-15T14:09:40Z,fixing assert condition,fixing assert for `_jit_set_fusion_strategy`,pytorch
74267,zasdfgbnm,pr,2022-03-15T23:45:33Z,Parallelize `gatherTopK` on multiple blocks,"This PR adds `mbtopk::gatherTopK` which uses the same number of blocks as `radixFindKthValues` to gather top k values in parallel. With this new `gatherTopK` kernel, the sort path is no longer needed because `best(mb, sb)` now is always better than the sort path.

## Algorithm
During each pass of `radixFindKthValues`, this kernel will output per slice `desired` and per block digit counters. After every pass, I use kernel `computeBlockwiseWithinKCounts` to compute the number of elements that `>kthValue`(if largest) or `<kthValue`(if !largest)  for each block from the per slice `desired` and per block digit counters. After the last pass, I use ~kernel `computeBlockwiseKthCounts`~ (~edit: fancy iterator~ edit 2: fancy iterator is too large in binary size, so I decided to use `computeBlockwiseKthCounts` in the end) to compute the number of elements that `==kthValue` for each block from the per slice `desired` and per block digit counters. Then I use cub's scan-by-key algorithm to compute the indices in output where each block should write its output to. Then I used the `mbtopk::gatherTopK` to write top k elements to these indices.

## Benchmark

Using script from @yueyericardo: https://github.com/yueyericardo/misc/blob/master/share/topk-script/benchmark.py

I get the following result on RTX3090:

**New mb vs old mb speedup:**
![plot_new](https://user-images.githubusercontent.com/1032377/158536975-aff9d82c-a392-4bb3-986a-1536496183a0.png)

**New dispatched vs old dispatched speedup:**
![plot_new-dispatched](https://user-images.githubusercontent.com/1032377/158702851-eb78f926-b418-4fc8-bab3-98aa6e0fbb4a.png)





Raw data in: https://docs.google.com/spreadsheets/d/e/2PACX-1vQxECU_qP1G-skQ8DmJDo-hx0OYPiJ01EkJMSZQdzfWj-QxhScQCkj9Z3KKBc7svEA6DC03UNvD1ial/pubhtml
This spreadsheet shows that the sort path is no longer needed: `best(mb, sb)` now is always better than the sort path.

cc: @yueyericardo @ngimel @ptrblck ",pytorch
74275,zasdfgbnm,pr,2022-03-16T01:15:25Z,Fixes build of CUDAHooks.cpp,"Fixes https://github.com/pytorch/pytorch/issues/73688

On my system, `MAGMA_INCLUDE_DIR` is `/usr/include`, and doing
```cmake
set_source_files_properties(CUDAHooks.cpp PROPERTIES INCLUDE_DIRECTORIES  ""/usr/include"")
```
causes compilation error
```
[5715/6576] Building CXX object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/cuda/detail/CUDAHooks.cpp.o
FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/cuda/detail/CUDAHooks.cpp.o 
/usr/bin/c++ -DAT_PER_OPERATOR_HEADERS -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_MPI -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -isystem /usr/include -I/home/gaoxiang/pytorch-master/build/aten/src -I/home/gaoxiang/pytorch-master/aten/src -I/home/gaoxiang/pytorch-master/build -I/home/gaoxiang/pytorch-master -I/home/gaoxiang/pytorch-master/cmake/../third_party/benchmark/include -I/home/gaoxiang/pytorch-master/cmake/../third_party/cudnn_frontend/include -I/home/gaoxiang/pytorch-master/third_party/onnx -I/home/gaoxiang/pytorch-master/build/third_party/onnx -I/home/gaoxiang/pytorch-master/third_party/foxi -I/home/gaoxiang/pytorch-master/build/third_party/foxi -I/home/gaoxiang/pytorch-master/build/include -I/home/gaoxiang/pytorch-master/torch/csrc/distributed -I/home/gaoxiang/pytorch-master/aten/src/THC -I/home/gaoxiang/pytorch-master/aten/src/ATen/cuda -I/home/gaoxiang/pytorch-master/build/caffe2/aten/src -I/home/gaoxiang/pytorch-master/aten/../third_party/catch/single_include -I/home/gaoxiang/pytorch-master/aten/src/ATen/.. -I/home/gaoxiang/pytorch-master/c10/cuda/../.. -I/home/gaoxiang/pytorch-master/c10/.. -I/home/gaoxiang/pytorch-master/third_party/tensorpipe -I/home/gaoxiang/pytorch-master/build/third_party/tensorpipe -I/home/gaoxiang/pytorch-master/third_party/tensorpipe/third_party/libnop/include -I/home/gaoxiang/pytorch-master/torch/csrc/api -I/home/gaoxiang/pytorch-master/torch/csrc/api/include -isystem /home/gaoxiang/pytorch-master/build/third_party/gloo -isystem /home/gaoxiang/pytorch-master/cmake/../third_party/gloo -isystem /home/gaoxiang/pytorch-master/cmake/../third_party/googletest/googlemock/include -isystem /home/gaoxiang/pytorch-master/cmake/../third_party/googletest/googletest/include -isystem /home/gaoxiang/pytorch-master/third_party/protobuf/src -isystem /opt/intel/mkl/include -isystem /home/gaoxiang/pytorch-master/third_party/gemmlowp -isystem /home/gaoxiang/pytorch-master/third_party/neon2sse -isystem /home/gaoxiang/pytorch-master/third_party/XNNPACK/include -isystem /home/gaoxiang/pytorch-master/cmake/../third_party/eigen -isystem /opt/cuda/include -isystem /home/gaoxiang/pytorch-master/third_party/ideep/mkl-dnn/third_party/oneDNN/include -isystem /home/gaoxiang/pytorch-master/third_party/ideep/include -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -DHAVE_AVX512_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -g -fno-omit-frame-pointer -O0 -fPIC -DCAFFE2_USE_GLOO -DTH_HAVE_THREAD -Wall -Wextra -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-type-limits -Wno-array-bounds -Wno-sign-compare -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-missing-braces -Wno-maybe-uninitialized -fvisibility=hidden -DTORCH_CUDA_BUILD_MAIN_LIB -std=gnu++14 -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/cuda/detail/CUDAHooks.cpp.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/cuda/detail/CUDAHooks.cpp.o.d -o caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/cuda/detail/CUDAHooks.cpp.o -c /home/gaoxiang/pytorch-master/aten/src/ATen/cuda/detail/CUDAHooks.cpp
In file included from /usr/include/c++/11.2.0/ext/string_conversions.h:41,
                 from /usr/include/c++/11.2.0/bits/basic_string.h:6607,
                 from /usr/include/c++/11.2.0/string:55,
                 from /usr/include/c++/11.2.0/bits/locale_classes.h:40,
                 from /usr/include/c++/11.2.0/bits/ios_base.h:41,
                 from /usr/include/c++/11.2.0/ios:42,
                 from /usr/include/c++/11.2.0/istream:38,
                 from /usr/include/c++/11.2.0/sstream:38,
                 from /home/gaoxiang/pytorch-master/c10/macros/Macros.h:245,
                 from /home/gaoxiang/pytorch-master/c10/core/DeviceType.h:8,
                 from /home/gaoxiang/pytorch-master/c10/core/Device.h:3,
                 from /home/gaoxiang/pytorch-master/c10/core/Allocator.h:6,
                 from /home/gaoxiang/pytorch-master/aten/src/ATen/detail/CUDAHooksInterface.h:3,
                 from /home/gaoxiang/pytorch-master/aten/src/ATen/cuda/detail/CUDAHooks.h:1,
                 from /home/gaoxiang/pytorch-master/aten/src/ATen/cuda/detail/CUDAHooks.cpp:1:
/usr/include/c++/11.2.0/cstdlib:75:15: fatal error: stdlib.h: No such file or directory
   75 | #include_next <stdlib.h>
      |               ^~~~~~~~~~
compilation terminated.
```
This PR fixes the issue.",pytorch
74339,jjsjann123,pr,2022-03-16T21:34:47Z,supporting complex with requires_grad in autodiff,"Fixes #65480

autodiff should propagate requires_grad for complex tensors as well as float tensors.",pytorch
74359,jjsjann123,pr,2022-03-17T01:32:28Z,disable contiguity on cross dimensional overlapped tensor,"Unmarked contiguity on stride properties when we have dimensions potentially covering overlapping memory.
This check could be done more accurately, per dimension instead of a global flag per tensor. I'm just keeping it simple here, as the existing code gives us correctness and that's what's important.",pytorch
74383,jeffdaily,pr,2022-03-17T18:53:52Z,[ROCm] enable HIP IPC,Enables code paths that use hipIpc* functions.  Also enables test_multiprocessing.py.,pytorch
74403,jjsjann123,pr,2022-03-18T01:53:15Z,[DO NOT REVIEW] draft showing code diff,,pytorch
74417,jeffdaily,pr,2022-03-18T16:54:21Z,[ROCm] enable foreach fastpath,"Reverts #46216 now that rocm is fixed.

Benchmark to verify: 

```python
import torch
import time
import torch.optim as optim
from torch.autograd import Variable
from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR
import torch.nn as nn
import time
import torchvision
import torch.utils.benchmark as benchmark_utils

device = ""cuda""
model = torchvision.models.resnet.resnet101(pretrained=True).to(device)
targets = torch.randint(0, 1000, (100, 100), device=device)
criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.1) # <----------------------- optimizer.
                                                          # would compare optim.SGD vs optim._multi_tensor.SGD
optimizer_mta = optim._multi_tensor.SGD(model.parameters(), lr=1e-3, momentum=0.1)
running_loss = 0.0
target = torch.empty(128, dtype=torch.long, device=device).random_(5)

optimizer.zero_grad()
inputs = torch.rand(128, 3, 100, 100, device=device , requires_grad=True)
outputs = model(inputs)
loss = criterion(outputs, target)
loss.backward()
optimizer.step()
running_loss += loss.item()

def main():
    timer = benchmark_utils.Timer(
        stmt=""torch.cuda.synchronize();optimizer.step()"",
        globals=globals(),
        label=""str(optimizer)"",
    )

    timer_mta = benchmark_utils.Timer(
        stmt=""torch.cuda.synchronize(); optimizer_mta.step()"",
        globals=globals(),
        label=""str(optimizer_mta)"",
    )
    for _ in range(1):
        for i in range(1):
           print(f""Run: {i}\n{'-' * 40}"")
           print(f""autorange:\n{timer.blocked_autorange()}\n\n"")

        for i in range(1):
            print(f""Run: {i}\n{'-' * 40}"")
            print(f""autorange:\n{timer_mta.blocked_autorange()}\n\n"")

if __name__ == ""__main__"":
    main()
```

Before revert:
```
Run: 0
----------------------------------------
autorange:
<torch.utils.benchmark.utils.common.Measurement object at 0x7f253e67c910>
str(optimizer)
  7.33 ms
  1 measurement, 100 runs , 1 thread


Run: 0
----------------------------------------
autorange:
<torch.utils.benchmark.utils.common.Measurement object at 0x7f253e67c510>
str(optimizer_mta)
  5.76 ms
  1 measurement, 100 runs , 1 thread
```

After revert:
```
Run: 0
----------------------------------------
autorange:
<torch.utils.benchmark.utils.common.Measurement object at 0x7fa2aa15e8d0>
str(optimizer)
  7.35 ms
  1 measurement, 100 runs , 1 thread


Run: 0
----------------------------------------
autorange:
<torch.utils.benchmark.utils.common.Measurement object at 0x7fa2aa15e4d0>
str(optimizer_mta)
  3.53 ms
  1 measurement, 100 runs , 1 thread
```",pytorch
74484,robieta,pr,2022-03-21T19:02:03Z,[Profiler] Pay for what you use (v2),"Summary:
In my first attempt at this in December I stamped out specializations using variadic templates. However I'm able to get comparable performance using simple conditionals since the branch is very predictable and AppendOnlyList::emplace_back is low enough overhead that multiple calls don't cause an issue.

This is also a chance to do some BE: rather than force ops and backend events to use the same fields (which in practice means setting a bunch of default values when reporting backend events), I just split them and use a variant.

Test Plan: The single threaded benchmark (with no extra options set) improved considerably from ~0.88 us to ~0.62 us. The stress test benchmark improved modestly from ~6.1 us to ~5.8 us. So the bottleneck for multi-threading is somewhere else, but doing less wasted work is still able to move the needle a little bit.

Reviewed By: swolchok

Differential Revision: D34779994

",pytorch
74520,jjsjann123,pr,2022-03-22T03:44:41Z,nvfuser parser skip api,"added python API to disable nvfuser on certain opkind.

```
          ""_jit_set_nvfuser_skip_node_kind"",
          [](const std::string& op_name, bool flip = true) {
            return fuser::cuda::skipNode(op_name, flip);
          })
```

Args:
    `op_name`: Symbol of op;
    `flip`: flag indicating whether to flip the given op in the skip list.
Returns:
    a bool flag indicating if `op_name` was already in the skip list.

The python example that disables the fusion of `aten::add` afterwards.
`torch._C._jit_set_nvfuser_skip_node_kind(""aten::add"", True)  # returns False, as no op is in skip list by default`",pytorch
74641,d4l3k,pr,2022-03-23T21:32:41Z,c10d: retry dns lookup failures,"Summary:
This makes dns hostname lookup failures retryable since in some environments such as Kubernetes they're not guaranteed to be resolvable until the job starts. Retrying this eliminates the race condition.

Fixes https://github.com/pytorch/pytorch/issues/73682

Test Plan:
Added a unit test

```
buck test //caffe2/test/distributed:test_store
```

Reviewed By: aivanou

Differential Revision: D35092284

",pytorch
74780,jjsjann123,pr,2022-03-25T21:39:45Z,Requires grad guard,Adding CudaFusionGuard to guard on device/requires_grad of profiled tensor type.,pytorch
74888,robieta,pr,2022-03-29T03:45:23Z,[Profiler] Limit calls to `recordThreadInfo`,"Summary: So far as I can tell, `recordThreadInfo` only needs to be called once per thread. Once we have thread local subqueues we can easily manage this by simply calling it in the subqueue constructor.

Test Plan: The effect on single threaded overhead is pretty minimal, but it improves stress test overhead from ~6.1 us to ~1.4us  since we're no contending over the lock in Kineto.

Reviewed By: chaekit

Differential Revision: D34811694

",pytorch
74959,jeffdaily,pr,2022-03-30T15:32:14Z,[ROCm] upgrade CI distributed test to ROCm 5.0,"This change was missed by earlier CI upgrade #73105.
",pytorch
74992,jeffdaily,pr,2022-03-30T22:51:35Z,[ROCm] libtorch nightly now correctly uses rocm runners,"Fixes nightly rocm libtorch builds.
",pytorch
75016,jjsjann123,pr,2022-03-31T13:35:37Z,Nvfuser guard patch,"Fixes issue where CudaFusionGuard would return false on backward graph because `requires_grad` flag doesn't match.

This is due to the fact that autodiff uses GradMode switch to turn on/off requires_grad, which is not taken into consideration by nvfuser guard. We verified the implementation under `TensorType::matchTensor`.

- [x] Add python test to verify no fallback is observed",pytorch
75036,robieta,pr,2022-03-31T20:32:24Z,[RecordFunction][Trivial] Reorder `record_function.h`,"Summary:
Move `RecordFunctionCallback` and `ThreadLocalRecordFunctionCallbacksEntry` up in the header.

This is part of a large scale refactor of RecordFunction. Right now the RecordFunction type isn't aware of the callback types, and instead uses `CallbackHandle`s to store which functions to run. Future changes will directly store pointers in RecordFunction, but to do that we have to shuffle some code around.

Test Plan: Unit tests are sufficient.

Reviewed By: chaekit

Differential Revision: D35276156

",pytorch
75094,jjsjann123,pr,2022-04-01T08:31:15Z,fixing the failing issue,"Fixes #75002 

This is not a clean solution yet, I'm pushing this one up so we can discuss what to do next. I did some quick change so we are at least casting optional tensor in autocast pass.

- [ ] revert the function schema to properly reflect alias on outputs.
- [ ] python tests?
- [ ] clean up implementation
",pytorch
75175,jjsjann123,pr,2022-04-04T08:29:51Z,register autocast overload,"Fixes #75002 

Remaining issue with this approach:
1. We can't overload the function between `Tensor` and `Optional[Tensor]` input. Since TorchScript IR node overload `Node::matches` takes functional schema and could match `Tensor` to `Optional[Tensor]` (i.e. it can only distinguish the two in one direction). Current WAR is to rename the optional function something different.
2. Supporting `Optional[Tensor]` output in autodiff turns out to be also very tricky. I'm not totally sure what happened, I think there's something with later optimization that ended up with a invalid graph.

This is to replace #75094, which attempts to add overload autocast that takes/return optional tensor as input in aten. This turns out to be tricky and there are few places that doesn't support optional tensor returned by aten ops.
",pytorch
75235,jjsjann123,pr,2022-04-04T22:55:25Z,disabling view,"Disabling view to avoid codegen errors as we resolve them internally.
This is currently done via simply stop the non-alias transformation for view op in fusion pass.",pytorch
75340,jjsjann123,pr,2022-04-06T18:09:15Z,updating nvfuser tests,"Re-enabled the failing test `test_category_rule` since I don't have the repro;
removed `test_linear_1d_weight_mismatch_bias_dtype` since the old behavior is not supported in aten;
disabled `test_int_tensor_input` for pre-volta device since we have reduction `amax` in the test.",pytorch
75539,jjsjann123,pr,2022-04-08T21:46:27Z,disabling reshape,"Fixes #75282 

Temporarily disables reshape to avoid codegen failure.",pytorch
75558,jjsjann123,pr,2022-04-09T08:09:59Z,patching clamp for one sided clamp,"Fixes #75088 

The solution is just to avoid putting random value for non-specified clamp as pointed out in https://github.com/pytorch/pytorch/issues/75088#issuecomment-1093410036
",pytorch
75632,jeffdaily,pr,2022-04-11T22:55:10Z,[ROCm] enable fsdp tests,,pytorch
75646,jjsjann123,pr,2022-04-12T02:56:45Z,baby steps on patching inf/nan behavior & aten::amin support in nvfuser,"Fixes #75622 

1. Instead of getting max/min_value for reduction init value, we go with (-)infinity instead so we can properly preserve inf inputs;
2. Adding inf/(-)inf/nan for float value.
3. Adding aten::amin in nvfuser (@kevinstephano @rdspring1 for review)",pytorch
75743,zasdfgbnm,pr,2022-04-13T17:45:36Z,Kill dead code in ScanUtils.cuh,,pytorch
75754,robieta,pr,2022-04-13T19:30:30Z,[Profiler] Expose `profilerType` in Python,"Summary: It's currently possible for C++ callers to check if there is an active profiler. This adds Python API parity. For now we just use `torch._C._autograd` namespace, as this is mostly for first party frameworks like RPC. (We can always move to public API if there is demand.)

Test Plan: Added unit test

Differential Revision: D35602425

",pytorch
75807,robieta,pr,2022-04-14T17:53:04Z,[RecordFunction] More effecient machinery to determine which callbacks to run.,"Summary:
There is a tension in RecordFunction between two use cases:
1) In the normal eager path we don't run any callbacks, so we need to bail out of the profiling path as soon as possible to minimize eager overhead.
2) When profiling we want to determine which callbacks to run as efficiently as possible to minimize instrumentation overhead.

The confounding factor in all of this is sampling callbacks because they change which callbacks will run on each call, even in steady state operation. This has traditionally been handled with a two stage procedure: first we flip a coin to determine if a sampled callback *might* run. If false (which it usually is), do nothing. This solves (1). If true, check to see if we need to build the full callback set or if it was a false positive. This procedure has two negative effects:
* It forces us to rebuild the set of callbacks to run on every step when profiling
* It leaks the sampling abstraction, requiring other parts of the code to bump certain values and forces RecordFunction to lazily initialize.

This change introduces a multi-level cache which can (in the common case) quickly determine which callbacks *will* run, rather than if callbacks *might* run. This means that rather than call `shouldRunRecordFunction`, we can simply get the callbacks for an invocation and check if they are empty. (And completely removes the pre-sampling heuristic.) Another major benefit of the new cache structure is that it allows thread-safe registration and unregistration of global callbacks.

It's worth briefly discussing how this maintains eager performance. In the standard eager case (only sampling callbacks registered) the cache first checks that the global callbacks haven't changed (atomic read), decrements a counter to see if a sampling callback fired, and then returns the active callbacks which is simply a SmallVector of pointer pairs and a couple POD values (scope, needs inputs/outputs/ids). The biggest cost according to perf is the SmallVector logic; we could consider adopting a hard limit on active callbacks; more than half a dozen callbacks *running* in a single step would be quite a lot. But the total cost relative to `PYTORCH_DISABLE_PER_OP_PROFILING` is only ~10ns, so debatable if it's worth it to switch to `std::array`.

The primary change is in `record_function.cpp`, which has a more detailed description of the new cache structure. `record_function.h` has some minor changes to align with the new calling convention and the remaining files are simply changes to the call sites.

Future work:
  * RecordFunction no longer needs to be lazily initialized.
  * We can deprecate the disable/reenable APIs, since we can not safely add and remove global callbacks.

Test Plan:
I tested eager mode performance using the overhead benchmark and found that the non-profiled path was unaffected. However the no-op observer dropped from 0.41us to 0.37us (0.25us if no observers are active) which is about 1/3rd reduction in the cost of the callback selection machinery.

I also added several C++ unit tests, as the core RecordFunction machinery (especially sampling) was largely untested.

Differential Revision: D35276158

",pytorch
75810,jeffdaily,pr,2022-04-14T18:08:51Z,[ROCm] enable composite compliance backward tests,"Follow up to #74646.  Do not skip the entire TestCompositeCompliance test_backward for ROCm, only skip the the two unexpected successes.  ",pytorch
75897,jjsjann123,pr,2022-04-15T17:46:57Z,disable fusion of linear but leave it in registratoin for profiling,"Fixes #ISSUE_NUMBER
",pytorch
76016,robieta,pr,2022-04-19T04:32:03Z,[RecordFunction] Don't lazily construct the guts of RecordFunction.,"Summary:
When we were pre-sampling this was a pretty important optimizaton. However now when we make a record function we can be sure that it will be called.

For the RECORD_FUNCTION macros I preserved the old behavior by making a `c10::optional<RecordFunction>` since we can't force callers to have separate paths the way Dispatcher does.

Maybe it makes sense to have a guard that handles the optional logic? If we can move enough out of the internals (e.g. replace `std::string`s with `char*`s) we might not even need the optional to get good perf.

Test Plan: The no-op observer overhead benchmark got a bit better, but even with lots of replicates it's hard to tell if that's just noise. This is primarily a change to simplify the semantics of RecordFunction.

Reviewed By: chaekit

Differential Revision: D35276157

",pytorch
76017,robieta,pr,2022-04-19T04:32:52Z,[RecordFunction] Store a c10::variant of name and schema rather then both.,"Summary: RecordFunction can be created either with a `c10::OperatorHandle` (from the dispatcher) or a string (everywhere else). We store a bunch of fields in RecordFunction to handle both paths, and it also complicates the logic since the input and output size can either be the size of `inputs_` and `outputs_` OR taken from the schema. (And that significantly complicates later changes) Because the dispatcher is the only place where we call the schema based method, we can just bind a reference and pass a `reference_wrapper`. (This is different from the other proposal that RecordFunction holds a schema pointer; in this case the caller just guarantees the schema for the lifetime of the guard.)

Test Plan: Ran the overhead benchmark. It helps quite a bit (0.37us -> 0.33us), presumably because there's just a lot less state (and assigns / dtors) in the guard.

Reviewed By: chaekit

Differential Revision: D35651041

",pytorch
76119,jeffdaily,pr,2022-04-20T17:28:05Z,[ROCm] unskip test_fmod_remainder_by_zero_integral,"The test is updated to check for ROCm-specific undefined behavior for
integral fmod division by zero.

Fixes #48130.
",pytorch
76142,zasdfgbnm,pr,2022-04-20T21:14:38Z,ProcessGroupNCCL check tensor is_contiguous->is_non_overlapping_and_dense,"I don't see any reason why we are asserting contiguous here. In my opinion, `is_non_overlapping_and_dense` makes more sense because this allows tasks like NHWC training.

Currently `check_gpu_tensors_same_device` and `check_gpu_tensors_different_devices` are both checking `is_non_overlapping_and_dense` instead of `is_contiguous`, so I think it makes more sense to modify `check_gpu_single_tensor` to have the same behavior

https://github.com/pytorch/pytorch/blob/f6c275f55ddfd22f8c0558efad3068b0e91f1560/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L1290-L1292
https://github.com/pytorch/pytorch/blob/f6c275f55ddfd22f8c0558efad3068b0e91f1560/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L1260-L1262",pytorch
76144,jjsjann123,pr,2022-04-20T21:50:03Z,add comment for lerp cuda implementation,Add a quick comment so people wouldn't run into the same discussion again.,pytorch
76211,d4l3k,pr,2022-04-22T00:56:05Z,deploy: add dummy metadata for builtin packages,"This adds dummy metadata for frozen builtin packages when using `torch::deploy`. This is a bit hacky but unblocks allows Huggingface transformers library to be used within `torch::deploy` which depends on `importlib.metadata.version` to detect whether torch is installed or not.

https://github.com/huggingface/transformers/blob/main/src/transformers/utils/import_utils.py#L49

Test plan:

Added `importlib.metadata.version(""torch"")` unit test",pytorch
76222,jjsjann123,pr,2022-04-22T07:03:22Z,[DO NOT REVIEW] adversarial clang-format breakage ci test,"Fixes #ISSUE_NUMBER
",pytorch
76226,jjsjann123,pr,2022-04-22T08:04:38Z,patching 11.1 ptxas issue,"Fixes #75708 

`--ptxas-options` only passes its immediate argument to ptxas. So we should have put that in front of every ptxas argument.

It's actually strange how this worked in CUDA TK 11.6. I'm following up with nvrtc team on this internally, meanwhile we should merge this PR to avoid register failures in generated kernels.",pytorch
76291,kulinseth,pr,2022-04-24T23:44:06Z,"Add ""mps"" device to PyTorch framework.","Remove the ""mlc"" device for Mac platforms.

This commit will be followed up with:

* adding MPS runtime components
* PyTorch ops for MPS device

Fixes #ISSUE_NUMBER
",pytorch
76405,jeffdaily,pr,2022-04-26T21:56:12Z,[ROCm] persist test results even on failure,Fixes #75169.  Default shell for workflow steps will fail fast.  Split ROCm test execution and test result copy into two steps so that test results are always persisted.,pytorch
76415,jeffdaily,pr,2022-04-26T22:57:45Z,[ROCm][GHA] keep docker images for at most 1 day,Fixes #76413.  ROCm runners take longer to pull docker images than other runners.  Try to cache images for at most 1 day to improve workflow throughput.,pytorch
76459,jjsjann123,pr,2022-04-27T18:48:55Z,Add a matching lerp implementation to eager mode. (#1612),"Fixes part of #76046

Add a matching lerp to eager mode.

Co-authored-by: jjsjann123 <alex.jann2012@gmail.com>
Co-authored-by: jjsjann123 <jiej@nvidia.com>",pytorch
76499,jeffdaily,pr,2022-04-27T22:50:41Z,"[ROCm] default tests use 1 GPU, distributed tests use 2 GPUs","Fixes #76497.
",pytorch
76563,jjsjann123,pr,2022-04-28T20:53:58Z,Permutation extended,"Extended permutation support in integration (See more details on https://github.com/csarofeen/pytorch/issues/1601). This update allows us to better support permutation propagation on tensors, specifically for binary ops with inputs of different ranks. Our goal is to avoid permuting tensors unless absolutely necessary. We try to preserve the permutation propagation rule in aten, with some known limitation at the time.

The idea in this implementation is the same as with our existing code, which is to permute input/output tensors outside of codegen: For a simplified binary op scenario: `output = binaryOp(input0, input1)`

1. In a simple case where `input0` and `input1` come with the same rank & permutation order, our output would preserve the same permutation;
2. For cases where `input0` and `input1` come with different ranks but with **compatible** permutation, the tensor with the higher rank dictates the permutation of the output;
3. For cases where `input0` and `input1` come with different ranks but with **in-compatible** permutation, this is where permutation propagation fails and the output tensor will be contiguous.

By **compatible** permutation, it means that we can permute the higher rank tensor to contiguous format, and then apply a second permutation to the tensor with lower rank to match their axes. This check is implemented in `MemoryFormat::broadcastToRank(int lower_rank)`.

Some concrete example (note that we comply with eager propagation on cases 1-3, but diverge in behavior for cases 4, 5):
1. different rank & same permutation
```
    t0 = torch.randn(b, h, w, c).cuda().permute([0, 3, 1, 2])  # stride (hwc, 1, wc, c)
    t1 = torch.randn(h, w, c).cuda().permute([2, 0, 1])  # stride (1, wc, c)
    out = scripted_add(t0, t1)  # stride (hwc, 1, wc, c) preserving memory format of t0
```
2. different rank & compatible permutation
```
    t0 = torch.randn(b, h, w, c).cuda().permute([0, 3, 1, 2])  # stride (hwc, 1, wc, c)
    t1 = torch.randn(c, h, w).cuda()  # stride (hw, w, 1)
    out = scripted_add(t0, t1)  # stride (hwc, 1, wc, c) preserving memory format of t0
```
3. different rank & compatible permutation with broadcasting
```
    t0 = torch.randn(b, h, w, c).cuda().permute([0, 3, 1, 2])  # stride (hwc, 1, wc, c)
    t1 = torch.randn(c).cuda().unsqueeze(-1).unsqueeze(-1)  # stride (1, 1, 1)
    out = scripted_add(t0, t1)  # stride (hwc, 1, wc, c) preserving memory format of t0
```
4. different rank & in-compatible permutation
```
    t0 = torch.randn(b, h, w, c).cuda().permute([0, 3, 1, 2])  # stride (hwc, 1, wc, c)
    t1 = torch.randn(h, w).cuda()  # stride (w, 1)
    jit_out = scripted_add(t0, t1)  # stride (hwc, 1, wc, c)  # stride (hwc, wc, c, 1)  # nvfuser outputs contiguous tensor
    eager_out = eager_add(t0, t1)  # stride (hwc, 1, wc, c)  # stride (hwc, 1, wc, c)  # TI preserves memory format of LHS operand
```
5. different rank & in-compatible permutation
```
    t0 = torch.randn(c, h, w).cuda()  # stride (hw, w, 1)
    t1 = torch.randn(b, h, w, c).cuda().permute([0, 3, 1, 2])  # stride (hwc, 1, wc, c)
    jit_out = scripted_add(t0, t1)  # stride (hwc, 1, wc, c)  # stride (hwc, 1, wc, c)  # nvfuser preserves memory format of highest rank tensors
    eager_out = eager_add(t0, t1)  # stride (hwc, 1, wc, c)  # stride (hwc, hw, w, 1)  # TensorIterator preserves memory format of LHS operand
```",pytorch
76598,zasdfgbnm,pr,2022-04-29T18:38:50Z,Add atan2 isfinite isinf isnan isneginf isposinf isreal to nvfuser and its frontend,Fixes: https://github.com/csarofeen/pytorch/issues/1632,pytorch
76601,jeffdaily,pr,2022-04-29T19:03:09Z,increase sleep for TestCuda.test_caching_pinned_memory_multi_gpu,"Fixes #68299.  Fixes #70875.

Test is flaky on ROCm because the HIP runtime occasionally copies asynchronously too quickly for the current sleep value of 50ms.  This is not a bug.  Increasing the sleep value to 1s to avoid flakiness.",pytorch
76604,jjsjann123,pr,2022-04-29T21:18:15Z,Nvfuser faster fallback,"Follow up to #76505 

Addressing https://github.com/pytorch/pytorch/pull/76505#discussion_r861260818 to further improve fallback perf during compilation failure. This allows us to reuse fallback instead re-constructing new code every time.",pytorch
76606,zasdfgbnm,pr,2022-04-29T23:40:01Z,Update isfinite for complex to avoid overflow,"Fixes: https://github.com/pytorch/pytorch/issues/66402
`abs` is larger than both real and imag, therefore, it could overflow, that is, both real and imag are finite, but abs are infinite. We should avoid this case. See also https://github.com/pytorch/pytorch/pull/76598/files#r862257429",pytorch
76668,jeffdaily,pr,2022-05-02T17:08:51Z,"[ROCm] tests set PYTORCH_TESTING_DEVICE_ONLY_FOR=""cuda""",This env var works similarly for both cuda and rocm builds.  Consider removing the ROCm-specific change in #55069 which came before the addition of this env var for cuda.,pytorch
76669,jeffdaily,pr,2022-05-02T17:13:50Z,[ROCm] revert sharding dist tests,Increase timeout for ROCm workflow and revert dist test sharding for ROCm.,pytorch
76678,robieta,pr,2022-05-02T18:07:06Z,[Profiler] Remove `addMemoryUsageActivity` from kineto_shim,"Summary: `addMemoryUsageActivity` is effectively the same as `addCPUActivity` except it also has `addMetadata` calls. This change generalizes `addCPUActivity` (We have to define an enum to map to `libkineto::ActivityType`) to accept arbitrary metadata strings.

Test Plan: Unit tests

Differential Revision: D36070203

",pytorch
76710,jeffdaily,pr,2022-05-03T00:29:00Z,[ROCm][GHA] relax docker purge conditions,"Only purge docker cache once the image count is greater than 10.
",pytorch
76725,kulinseth,pr,2022-05-03T07:58:31Z,Add the Runtime components for MPS backend.,"The PR adds the runtime components and few basic operations like copy, as_strided for MPS backend.

Current list of identified TODOs are:

-  https://github.com/pytorch/pytorch/issues/77176
- Unify the logic with CUDACachingAllocator and remove redundant code.
-  https://github.com/pytorch/pytorch/issues/77170
- Look into using C++ smart pointers where possible with ObjC code
- Use empty_strided_generic() to implement the `empty_strided_mps` code
- https://github.com/pytorch/pytorch/issues/77144",pytorch
76729,z-a-f,pr,2022-05-03T09:03:16Z,[quant] Refactor to reduce the chances of cyclic imports,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #76729

One of the reasons the cyclic imports happen when dealing with the quantization package is that there is a
tight coupling between the core torch.nn and torch.nn.quantized subpackages.
This diff starts the refactor to decouple the subpackages using an ""interface"":

Considering a hypothetical scenario, where the following sequence of events happen

1. `import torch` calls `from . import foo` (inside `torch/__init__.py`
1. `import foo` calls `import torch.nn.quantized`
1. `import torch.nn.quantized` imports all the modules
    1. `from modules import *` inside `torch/nn/quantized/__init__.py`
1. One of the modules in the `quantized` calls `import torch`

That scenario has an ""import-time cyclic dependency`.
In practice that dependency might be a ""run-time dependency"", but due to the `from foo import *` mechanics,
the imports of the current package are not resolved until the imports of all subpackages in the stack
are not resolved.

Changes:

1. Move the `torch/nn/quantized/modules` to `torch/nn/quantized/_modules`.
    1. The contents of the `__init__.py` are empty to break any cyclic imports
1. Create an ""interface"" module `torch/nn/quantized/modules`
    1. The contents of the `__init__.py` define the user API of the subpackage
1. Move the `torch/nn/quantized/modules/utils.py` to `torch/nn/quantized/utils.py`
    1. This is done to avoid direct imports from `_modules`
1. Modify the `torch/ao/quantization/fx/_lower_to_native_backend.py` to use `_modules`

**Note:** If one needs to use the utilities in the subpackages, they must import from the `_modules`.
For example, if you are using `nnq.modules.conv._reverse_repeat_padding`, it must be changed to
`nnq._modules.conv._reverse_repeat_padding`.",pytorch
76769,zasdfgbnm,pr,2022-05-03T20:24:54Z,[Reland] Add atan2 isfinite isinf isnan isneginf isposinf isreal to nvfuser and its frontend,"This reverts commit 4bb59441339074515a4257704acca6ebd80826cc.
",pytorch
76842,zasdfgbnm,pr,2022-05-04T20:05:33Z,tracer compare_outputs should compare in cdouble for complex,,pytorch
76849,jeffdaily,pr,2022-05-04T21:37:05Z,[ROCm][GHA] split 4 GPU hosts across two runners,"Examine the runner name.  If it ends with ""-2"" to indicate the second runner on the host, the docker run arguments will select the last two GPUs.  Otherwise, select the first two GPUs on the host.",pytorch
76930,robieta,pr,2022-05-06T00:39:23Z,[Profiler][Trivial] Format profiler_python.cpp,"Summary: There are some unfortunate style issues, like four space indents and various other minor issues. There is a pretty big overhaul coming to the python tracer, so I want to be able to commit them with more style compliant code.

Test Plan: N/A. Lints only.

Reviewed By: aaronenyeshi

Differential Revision: D36070201

",pytorch
76931,robieta,pr,2022-05-06T00:39:46Z,[Profiler] Add EventFieldsVisitor,"Summary:
One source of complexity in profiler_kineto is that we do most things twice: once to set a field in `kineto_events_.back()`, and once for the metadata json. These have historically been chained, with the KinetoEvent used to populate the metadata fields. However this is hard to read and error prone, as we have one giant block of assignments followed by another giant block. It also means that logic about whether a field is present or not is duplicated.

This PR replaces this logic with a visitor that writes both together. E.g.
```
    auto& dtypes = result_.get().inputs_.dtypes_;
    if (!dtypes.empty()) {
      kineto_event_.get().dtypes(dtypes);
      out.emplace_back(""Input type"", dtypesToStr(dtypes));
    }
```

Test Plan: Unit tests.

Reviewed By: aaronenyeshi

Differential Revision: D36070202

",pytorch
76964,jeffdaily,pr,2022-05-06T15:29:22Z,linux focal builds install cmake from conda,We currently rely on conda to install cmake for ubuntu bionic builds.  Extend this logic to ubuntu focal.,pytorch
76985,jeffdaily,pr,2022-05-06T21:06:08Z,[ROCm] rccl performance improvement via env var,The env var HSA_FORCE_FINE_GRAIN_PCIE=1 enables P2P communication in RCCL without intermediate buffers.  This is necessary on hosts with only PCIe and no P2P high-speed interconnect.,pytorch
77001,jjsjann123,pr,2022-05-06T22:30:42Z,Torchvision patch,"Fixes #76791 

Note that this is a hot patch so we get to run upstream tests. I'm doing proper fix in our local repo and will update upstream code once those are merged/reviewed.",pytorch
77017,jjsjann123,pr,2022-05-07T09:12:42Z,disabling squeeze/unsqueeze; disabling BN/BN_BWD for perf concern,"Fixes #76883 (via disabling squeeze/unsqueeze)

Disabling BN fwd/bwd for our perf concern. I need to update our python tests. Awaiting build to finish so I can update tests accordingly.

",pytorch
77064,z-a-f,pr,2022-05-09T09:28:16Z,[quant][ao_migration] Base package in tests,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* #77586
* #77582
* #77581
* __->__ #77064
* #77344
* #77065

Adding a base package as an argument to the testing routines.
That will allow us to test other locations that are being migrated.
For example

```
AOMigrationTestCase._test_package_import('my_mackage', base='quantization')
```

would check if `torch.quantization.my_package` and `torch.ao.quantization.my_package` are the same.

Differential Revision: [D36459452](https://our.internmc.facebook.com/intern/diff/D36459452)",pytorch
77065,z-a-f,pr,2022-05-09T09:28:20Z,[quant] Reordering the imports in the torch/__init__.py,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* #77586
* #77582
* #77581
* #77064
* #77344
* __->__ #77065

Because the AO stuff depends on the torch packages, but very few (if any)
torch packages depend on AO, we are moving the imports lower.
That will reduce the probability of cyclic imports, as by the time the
AO would start importing, the rest of the torch would be already imported.

Differential Revision: [D36459451](https://our.internmc.facebook.com/intern/diff/D36459451)",pytorch
77066,z-a-f,pr,2022-05-09T09:28:23Z,[quant][ao_migration] Migrating nn.quantized to ao.nn.quantized,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #77066
* #77064
* #77344
* #77065

This migrates the `torch.nn.quantized` to `torch.ao.nn.quantized`,
while maintaining the old imports.
The new location is expected to tbe the main place for all the
`nn.quantized` codebase, while the old one is preserved to avoid
breaking changes.

`import torch.nn.quantized`
converts to
`import torch.ao.nn.quantized`

Test Plan:

```
python test/test_quantization.py TestAOMigration TestAOMIgrationNNQuantized
```",pytorch
77087,jeffdaily,pr,2022-05-09T17:23:48Z,[ROCm] update cmake package DIR paths,"Fixes nightly libtorch builds.  As of ROCm 5.1.x, all *.cmake files are under /opt/rocm/lib/cmake/package instead of /opt/rocm/package/lib/cmake.",pytorch
77097,d4l3k,pr,2022-05-09T20:30:50Z,"torch/deploy,package: log usage for InterpreterManager, PackageExporter, PackageImporter","Summary: This adds logs for usage of deploy and package. These can be used to track where it's being used in production so we can support it better.

Test Plan: no functional changes - existing tests

Reviewed By: PaliC

Differential Revision: D36258876

",pytorch
77146,zasdfgbnm,pr,2022-05-10T05:38:44Z,lshift and rshift stop support floating types,"Fixes #74358
",pytorch
77158,zasdfgbnm,pr,2022-05-10T09:32:17Z,[nvFuser] Improving bitwise ops support,"- Some renaming to better match PyTorch API:
  - `lshift` -> `bitwise_left_shift`
  - `rshift` -> `bitwise_right_shift`
  - `andOp` -> `bitwise_and`
  - `orOp` -> `bitwise_or`
  - `xorOp` -> `bitwise_xor`
  - `notOp` -> `bitwise_not`
- Fix type inferences and type checking of these ops
- Add `bitwise_*` to parser and python frontend
- Improve test coverage",pytorch
77270,d4l3k,pr,2022-05-11T17:15:52Z,torch/deadlockdetection: add TORCH_DISABLE_DEADLOCK_DETECTION env for use with torch deploy,"Summary:
Currently there's an #ifdef USE_DEPLOY to disable deadlock detection in torch for torch deploy. We want to be able to link against binary distributions of PyTorch so we need to have a way to disable deadlock detection at runtime.

https://github.com/pytorch/pytorch/blob/55f55a4cf6cc50cde9e8e8369d92847ca85b23da/torch/csrc/autograd/python_variable.cpp#L1017

Test Plan: buck test //caffe2/c10/test:util_base_test

Differential Revision: D36303256

",pytorch
77273,jjsjann123,pr,2022-05-11T17:32:31Z,nvfuser opinfo test fixes masked_var/std,"Enables guard mode in opinfo tests.
Fixes opinfo failures for 
    test_nvfuser_correctness__masked_var_cuda_xxxx
    test_nvfuser_correctness__masked_std_cuda_xxxx

The root cause of the failure is that tracing changes stride properties and causes nvfuser to use wrong kernel and generate wrong results.",pytorch
77296,jjsjann123,pr,2022-05-11T21:50:13Z,exposing more CUDA driver API,"Exposes `cuFuncSetAttribute` & `cuFuncGetAttribute`
Used for runtime compilation by nvfuser",pytorch
77297,jeffdaily,pr,2022-05-11T21:57:16Z,missing header include in SortImpl.cu,"SortImpl.cu needs to include <thrust/execution_policy.h> for
thrust::host.  Depending on the nvidia/thrust or rocThrust version,
transitive inclusion of this header is not guaranteed.",pytorch
77343,kulinseth,pr,2022-05-12T07:24:30Z,Enable PyTorch operations on MPS Backend.,"Add PyTorch operations to MPS backend.

- https://github.com/pytorch/pytorch/issues/77394",pytorch
77344,z-a-f,pr,2022-05-12T08:14:57Z,[quant][refactor] Remove the base class from __all__,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* #77586
* #77582
* #77581
* #77064
* __->__ #77344
* #77065

In general, if we are expecting the users to use the base class,
such as `_ConvNd`, we should rename it to something like
`BaseConv`. However, because this base class is only used inside of the
AO packages, there is no need to expose it to the users.

Test Plan:

```
python test/test_quantization.py
python test/test_module_init.py
```

Differential Revision: [D36459453](https://our.internmc.facebook.com/intern/diff/D36459453)",pytorch
77381,jjsjann123,pr,2022-05-12T21:57:07Z,disabling unique/unique_consecutive for opinfo nvfuser correctness tests,"Fixes #76354

added expected failure for unique/unique_consecutive failures.",pytorch
77402,jjsjann123,pr,2022-05-13T05:05:26Z,updating tolerance override,"Seeing flake test failures locally and verified that's specific to some input random values.
Bumping up tolerance make tests happy.",pytorch
77438,jeffdaily,pr,2022-05-13T18:59:42Z,[ROCm] support benchmark flag for MIOpen,"Fixes #68172.  Generally, this corrects multiple flaky convolution unit test behavior seen on ROCm.

The MIOpen integration has been forcing benchmark=True when calling `torch._C._set_cudnn_benchmark(False)`, typically called by `torch.backends.cudnn.set_flags(enabled=True, benchmark=False)`.  We now add support for MIOpen immediate mode to avoid benchmarking during MIOpen solution selection.",pytorch
77444,d4l3k,pr,2022-05-13T19:42:39Z,torch/distributed: move quantization ops to libtorch instead of libtorch_python,"Summary: This moves the distributed/c10d/quantization ops to libtorch instead of libtorch_python. Since they don't depend on Python this avoids the duplicate registration error from TORCH_LIBRARY when used in torch::deploy.

Test Plan:
Build

https://www.internalfb.com/intern/testinfra/testconsole/testrun/8162774414164820/

Differential Revision: D36384244

",pytorch
77457,robieta,pr,2022-05-13T22:12:57Z,[Profiler] Introduce `torch::profiler::impl::EventType`,"Summary:
Right now the profiler internals are rather ad-hoc and disjoint. As we move towards a unified experience this needs to be addressed. This PR adds an enum specifying the various types of events that can be profiled and specializes the `ExtraFields` struct on the values of the `EventType` enum. This lets us punt more of the heterogeneity onto the type system and allows a caller to simply think in terms of `ExtraFields<EventType::...>`. (No more ""X field is always present but only makes sense for Y"". e.g. inputs)

For now only ops and backend events are transitioned since they are already in a weird union state. Changes planned for subsequent diffs in the stack:
1) Allocations
2) Python tracer events
3) Kineto (e.g. Cupti) events
4) Use unified event type for more post processing

One rather pleasant observation was that this change exposed several minor bugs in the current implementation:
1) We just didn't plumb `end_thread_id_` from `OpEvent` to `Result`. Switching to using ctors rather than setting fields in `getRecords` fixes this.
2) We were calling `fn.threadId()` to get start TID, but that is wasteful because it is already stored in the `ThreadLocalSubqueue`.
So that gives me some confidence that this is a step in the right direction.

Test Plan: Unit tests

Reviewed By: aaronenyeshi

",pytorch
77458,robieta,pr,2022-05-13T22:14:28Z,[Profiler] Move Allocation into EventType.,"Summary: Continuing the trend of unification, this PR removes the special path for allocation tracking. The overall delta is pretty minimal; it's mostly just extending and unifying visitors. This also comes with the added benefit that memory profiling now gets to take advantage of the lock-free machinery.

Test Plan: Unit tests

Reviewed By: aaronenyeshi

Differential Revision: D36189043

",pytorch
77460,jjsjann123,pr,2022-05-13T22:22:00Z,stirde_properties fix,"Fixes part of #6015

profiled permutation order is wrong and nvfuser generates output in wrong memory format. Though this problem doesn't seem to cause any issue with the test (except a graceful fallback path taken by CudaFusionGuard).",pytorch
77462,kulinseth,pr,2022-05-13T22:26:06Z,MPS: fixes,"- Fix the is_available flag for x86 machines
- Fix the tensor creation for older MacOS platforms
- Addmm fixes for transposition
",pytorch
77468,jjsjann123,pr,2022-05-14T00:03:01Z,skip primTorch nvfuser tests on rocm,Fixes https://github.com/pytorch/pytorch/issues/77237,pytorch
77470,robieta,pr,2022-05-14T00:26:41Z,[Profiler][Trivial] Force Result to be a shared_ptr,"Summary: A lot of the graph manipulation in later changes will rely on the ability to hold stable references, both in C++ and Python.

Test Plan: Unit tests.

Reviewed By: aaronenyeshi

Differential Revision: D36302564

",pytorch
77471,jjsjann123,pr,2022-05-14T00:50:13Z,Upstream master bump 0513,"Updating nvfuser code base.

This should fix the indexing issue observed in https://github.com/pytorch/vision/issues/6015.

Running tests locally as well. Will update the description here at a later point

@bypass-github-export-checks",pytorch
77567,robieta,pr,2022-05-16T17:19:11Z,[Profiler] Propagate metadata into `Engine::evaluate_function` event.,"Summary: https://github.com/pytorch/pytorch/pull/63619 added a RECORD_FUNCTION guard to make calls to `Engine::evaluate_function` visible regardless of the underlying op. While useful, this creates a call that looks like a forward call that somewhat complicates stitching forward and backward ops. I don't want to add complexity (and therefore work) on the hot path; instead it's fairly straightforward to stitch things back together in post. This PR simply propagates sequence number and forward tid info up to the `evaluate_function` event.

Test Plan: Updated `test_autograd.py` (And made it more robust overall)

Differential Revision: D36302562

",pytorch
77568,robieta,pr,2022-05-16T17:19:34Z,[Profiler][Trivial] Switch to nanoseconds for Result's internal representation,"Summary: Certain steps in building the call tree rely on sorting, so we want to retain as much precision as possible. `profiler_kineto.cpp` and KinetoEvent still use microseconds.

Test Plan: Unit tests.

Reviewed By: aaronenyeshi

Differential Revision: D36302563

",pytorch
77570,robieta,pr,2022-05-16T17:21:08Z,[Profiler] Build call tree in `collection.cpp`,"Summary: This PR adds tree building to the post processing of profiler. The basic algorithm is to sort the events, maintain a stack and a priority queue of event ends, and push/pop accordingly. The logic for merging Python events is still separate in `profiler_kineto.cpp`. That can be removed when Python events have an `EventType`.

Test Plan: This PR adds basic python bindings and adds a test in `test_profiler.py` for several simple cases. To keep the tests from getting too large and to help easily inspect the results, the tree is processed into a simple text based icicle representation and checked with expecttest.

Differential Revision: D36321105

",pytorch
77571,robieta,pr,2022-05-16T17:23:53Z,[Profiler] Abstract interface for Python tracer,"Summary: The current machinery to connect libtorch to libtorch_python for profiling is... meh. Adequite for separate components that mostly just need to send a trigger, but not really clean. This PR makes an abstract interface class that the python tracer subclasses so the profiler can actually get at the tracer singleton, albeit through a restricted interface. This will help fold Python tracing into the new unified event structure.

Test Plan: Unit tests.

Differential Revision: D36325739

",pytorch
77581,z-a-f,pr,2022-05-16T18:34:51Z,[quant][ao_migration] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* #77586
* #77582
* __->__ #77581
* #77064
* #77344
* #77065

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
  - [X] [Current PR] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
  - [ ] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
  - [ ] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
  - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
  - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
  - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
  - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
  - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
  - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
    - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
    - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- [Documentation](docs/source/quantization-support.rst) @vkuzo
- [Public API test list](test/allowlist_for_publicAPI.json) @peterbell10

Differential Revision: [D36459449](https://our.internmc.facebook.com/intern/diff/D36459449)",pytorch
77582,z-a-f,pr,2022-05-16T18:34:55Z,[quant][ao_migration] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* #77586
* __->__ #77582
* #77581
* #77064
* #77344
* #77065

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] [Current PR] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [ ] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- Documentation @vkuzo
  - docs/source/conf.py
  - docs/source/quantization.rst
- [quantize_fx](torch/ao/quantization/quantize_fx.py) @jerryzh168
- [common test routine](test/quantization/ao_migration/common.py) @HDCharles
- JIT stuff @jamesr66a
  - torch/csrc/jit/passes/hoist_conv_packed_params.cpp
  - torch/csrc/jit/passes/quantization/helper.h
  - torch/csrc/jit/serialization/import_source.cpp

Differential Revision: [D36459455](https://our.internmc.facebook.com/intern/diff/D36459455)",pytorch
77586,z-a-f,pr,2022-05-16T19:37:35Z,[quant][ao_migration] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* #77627
* __->__ #77586
* #77582
* #77581
* #77064
* #77344
* #77065

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] [Current PR] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- [Documentation](docs/source/quantization-support.rst) @vkuzo
- [Public API test list](test/allowlist_for_publicAPI.json) @peterbell10
- [BC test](test/quantization/bc/test_backward_compatibility.py) @vkuzo
- [IR emitter](torch/csrc/jit/frontend/ir_emitter.cpp) @jamesr66a
- [JIT serialization](torch/csrc/jit/serialization/import_source.cpp) @IvanKobzarev @jamesr66a

Differential Revision: [D36459456](https://our.internmc.facebook.com/intern/diff/D36459456)",pytorch
77613,zasdfgbnm,pr,2022-05-16T23:49:49Z,Fix doc about type promotion of lshift and rshift,,pytorch
77621,zasdfgbnm,pr,2022-05-17T01:35:27Z,Bitwise ops improvements,"- Bitwise shift remove floating point support
- Bitwise and, or, xor add (scalar, tensor) overload
- Use `test_ops.py` to test these ops, including error cases",pytorch
77627,z-a-f,pr,2022-05-17T06:18:35Z,[quant][ao_migration] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #77638
* #77630
* __->__ #77627
* #77586
* #77582
* #77581
* #77064
* #77344
* #77065

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

Test Plan:

```
python test/test_quantization.py
```

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] [Current PR] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- None

Differential Revision: [D36459450](https://our.internmc.facebook.com/intern/diff/D36459450)",pytorch
77630,z-a-f,pr,2022-05-17T07:24:27Z,[quant][ao_migration] `torch.nn.quantizable` → `torch.ao.nn.quantizable`.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* __->__ #77630
* #78715
* #78714
* #78713
* #78712

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

Test Plan:

```
python test/test_quantization.py
```

The list of the `nn.quantized` files that are being migrated:

- [X] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [X] [Current PR] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- None

Differential Revision: [D36459454](https://our.internmc.facebook.com/intern/diff/D36459454)",pytorch
77638,z-a-f,pr,2022-05-17T09:28:22Z,[quant][ao_migration] `torch.nn.qat` → `torch.ao.nn.qat`.,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #77638
* #77630
* #77627
* #77586
* #77582
* #77581
* #77064
* #77344
* #77065

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

Test Plan:

```
python test/test_quantization.py
```

The list of the `nn.quantized` files that are being migrated:

- [X] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [X] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [X] [Current PR] `torch.nn.qat` → `torch.ao.nn.qat`
    - [X] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [X] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- None

Differential Revision: [D36459448](https://our.internmc.facebook.com/intern/diff/D36459448)",pytorch
77674,jjsjann123,pr,2022-05-17T19:41:54Z,nvfuser bump 0517,"Updating nvfuser code base, this is to replace #77471 with a few more perf tuning commits.
Starting this PR for CI tests along with benchmark tests. Will put comment on PR later.",pytorch
77691,robieta,pr,2022-05-17T22:34:24Z,[pytorch][PR] [Profiler] Add EventFieldsVisitor,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77699
* #77698
* #77697
* #77696
* #77695
* #77694
* #77693
* #77692
* __->__ #77691

One source of complexity in profiler_kineto is that we do most things twice: once to set a field in `kineto_events_.back()`, and once for the metadata json. These have historically been chained, with the KinetoEvent used to populate the metadata fields. However this is hard to read and error prone, as we have one giant block of assignments followed by another giant block. It also means that logic about whether a field is present or not is duplicated.

This PR replaces this logic with a visitor that writes both together. E.g.
```
    auto& dtypes = result_.get().inputs_.dtypes_;
    if (!dtypes.empty()) {
      kineto_event_.get().dtypes(dtypes);
      out.emplace_back(""Input type"", dtypesToStr(dtypes));
    }
```

Differential Revision: [D36070202](https://our.internmc.facebook.com/intern/diff/D36070202/)",pytorch
77692,robieta,pr,2022-05-17T22:36:48Z,[pytorch][PR] [Profiler][Trivial] Format profiler_python.cpp,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77699
* #77698
* #77697
* #77696
* #77695
* #77694
* #77693
* __->__ #77692
* #77691

There are some unfortunate style issues, like four space indents and various other minor issues. There is a pretty big overhaul coming to the python tracer, so I want to be able to commit them with more style compliant code.

Differential Revision: [D36070201](https://our.internmc.facebook.com/intern/diff/D36070201/)",pytorch
77693,robieta,pr,2022-05-17T22:37:48Z,[Profiler] Introduce `torch::profiler::impl::EventType`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77693

Right now the profiler internals are rather ad-hoc and disjoint. As we move towards a unified experience this needs to be addressed. This PR adds an enum specifying the various types of events that can be profiled and specializes the `ExtraFields` struct on the values of the `EventType` enum. This lets us punt more of the heterogeneity onto the type system and allows a caller to simply think in terms of `ExtraFields<EventType::...>`. (No more ""X field is always present but only makes sense for Y"". e.g. inputs)

For now only ops and backend events are transitioned since they are already in a weird union state. Changes planned for subsequent diffs in the stack:
1) Allocations
2) Python tracer events
3) Kineto (e.g. Cupti) events
4) Use unified event type for more post processing

One rather pleasant observation was that this change exposed several minor bugs in the current implementation:
1) We just didn't plumb `end_thread_id_` from `OpEvent` to `Result`. Switching to using ctors rather than setting fields in `getRecords` fixes this.
2) We were calling `fn.threadId()` to get start TID, but that is wasteful because it is already stored in the `ThreadLocalSubqueue`.
So that gives me some confidence that this is a step in the right direction.

Differential Revision: [D36189044](https://our.internmc.facebook.com/intern/diff/D36189044/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36189044/)!",pytorch
77694,robieta,pr,2022-05-17T22:39:23Z,[Profiler] Move Allocation into EventType.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77697
* #77696
* #77695
* __->__ #77694

Continuing the trend of unification, this PR removes the special path for allocation tracking. The overall delta is pretty minimal; it's mostly just extending and unifying visitors. This also comes with the added benefit that memory profiling now gets to take advantage of the lock-free machinery.

Differential Revision: [D36189043](https://our.internmc.facebook.com/intern/diff/D36189043/)",pytorch
77695,robieta,pr,2022-05-17T22:41:25Z,[Profiler][Trivial] Force Result to be a shared_ptr,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77697
* #77696
* __->__ #77695
* #77694

A lot of the graph manipulation in later changes will rely on the ability to hold stable references, both in C++ and Python.

Differential Revision: [D36302564](https://our.internmc.facebook.com/intern/diff/D36302564/)",pytorch
77696,robieta,pr,2022-05-17T22:42:19Z,[Profiler] Propagate metadata into `Engine::evaluate_function` event.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77697
* __->__ #77696
* #77695
* #77694

https://github.com/pytorch/pytorch/pull/63619 added a RECORD_FUNCTION guard to make calls to `Engine::evaluate_function` visible regardless of the underlying op. While useful, this creates a call that looks like a forward call that somewhat complicates stitching forward and backward ops. I don't want to add complexity (and therefore work) on the hot path; instead it's fairly straightforward to stitch things back together in post. This PR simply propagates sequence number and forward tid info up to the `evaluate_function` event.

Differential Revision: [D36302562](https://our.internmc.facebook.com/intern/diff/D36302562/)",pytorch
77697,robieta,pr,2022-05-17T22:45:03Z,[Profiler][Trivial] Switch to nanoseconds for Result's internal representation,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77697
* #77696
* #77695
* #77694

Certain steps in building the call tree rely on sorting, so we want to retain as much precision as possible. `profiler_kineto.cpp` and KinetoEvent still use microseconds.

Differential Revision: [D36302563](https://our.internmc.facebook.com/intern/diff/D36302563/)",pytorch
77698,robieta,pr,2022-05-17T22:45:09Z,[Profiler] Build call tree in `collection.cpp`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78162
* #77699
* __->__ #77698

This PR adds tree building to the post processing of profiler. The basic algorithm is to sort the events, maintain a stack and a priority queue of event ends, and push/pop accordingly. The logic for merging Python events is still separate in `profiler_kineto.cpp`. That can be removed when Python events have an `EventType`.

Differential Revision: [D36321105](https://our.internmc.facebook.com/intern/diff/D36321105/)",pytorch
77699,robieta,pr,2022-05-17T22:46:23Z,[Profiler] Abstract interface for Python tracer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78162
* __->__ #77699
* #77698

The current machinery to connect libtorch to libtorch_python for profiling is... meh. Adequite for separate components that mostly just need to send a trigger, but not really clean. This PR makes an abstract interface class that the python tracer subclasses so the profiler can actually get at the tracer singleton, albeit through a restricted interface. This will help fold Python tracing into the new unified event structure.

Differential Revision: [D36325739](https://our.internmc.facebook.com/intern/diff/D36325739/)",pytorch
77777,jjsjann123,pr,2022-05-18T19:30:02Z,[nvfuser] prevent spamming warning message,updating TORCH_WARN to TORCH_WARN_ONCE to prevent spamming the log,pytorch
77779,jeffdaily,pr,2022-05-18T19:45:17Z,[ROCm] unify Loops implementations between CUDA/ROCm,"Fixes code divergence introduced in #32383.
",pytorch
77780,jjsjann123,pr,2022-05-18T19:53:19Z,torch.jit doc link for nvfuser readme.md,"adding a quick link to nvfuser README.md in jit doc

Note that for 1.12 release, we probably want to have the link pointed to the doc in the release code base. I don't know if we have a tag for 1.12 release candidate yet, so we might want to update that.",pytorch
77791,kulinseth,pr,2022-05-18T21:08:08Z,"MPS Fixes: copy operations, addmm and baddmm","Fixes for the copy operations and GEMM operations on MPS backend.

Fixes https://github.com/pytorch/pytorch/issues/77819",pytorch
77884,jjsjann123,pr,2022-05-19T20:07:36Z,fixing trivial reduction & broadcast scheduling,"cherry-picked fixes from https://github.com/csarofeen/pytorch/pull/1714
",pytorch
77927,z-a-f,pr,2022-05-20T02:04:22Z,[quant] Adding tests for qconv output shapes,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77927

Because the shape of the output of the conv module depends on several
parameters, it is worth adding tests specifically for it.
This is also a precursor to make sure the `same` and `valid` paddings
are producing correct results.

Test Plan:

```python
python test/test_quantzation.py TestInputOutputShapes
```

Differential Revision: [D36536581](https://our.internmc.facebook.com/intern/diff/D36536581)",pytorch
77928,z-a-f,pr,2022-05-20T02:04:26Z,[quant] Enable 'same' and 'valid' padding in static qconv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78543
* #78386
* #77929
* __->__ #77928
* #77927

Resolves #76304

The string inputs in the quantized convolutions are not implemented, because:
1. Conv packing expects numerical padding as an input
2. The same padding computation is not implemented internally in the quantized conv

To resolve we use a commbination of padding and conv layers:

1. Precompute the padding for the `F.pad` (`_reverse_padding_repeat_twice`)
    1. If `padding == 'same'`: padding[i] = dilation[i] * (kernel[i] - 1)
    2. If `padding == 'valid'`: padding[i] = 0
1. If `padding_mode == 'zeros'` and `padding` is numeric
    1. Prepack the conv with self.padding
    1. Run the qconv in the forward as is
1. If `padding_mode != 'zeros'` or `padding` is a string
    1. In the constructor: Prepack the conv with 0
    1. In the forward:
        1. Run the `F.pad` with `_reversed_padding_repeated_twice`
        1. Run the qconv

The above steps *should not* affect the performance of the current implementaion,
but just add functionality for `'same'` and `'valid'` padding.

Test Plan:

```python
python test/test_quantzation.py TestInputOutputShapes
python test/test_quantzation.py TestStaticQuantizedModule
```

Differential Revision: [D36536582](https://our.internmc.facebook.com/intern/diff/D36536582)",pytorch
77929,z-a-f,pr,2022-05-20T02:04:29Z,[quant] Enable 'same' and 'valid' padding in dynamic qconv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78543
* #78386
* __->__ #77929
* #77928
* #77927

Resolves #76304

The string inputs in the quantized convolutions are not implemented, because:
1. Conv packing expects numerical padding as an input
2. The same padding computation is not implemented internally in the quantized conv

The logic for the padding + convolution run is the same as with the
static qconv:

- [static qconv](torch/nn/quantized/modules/conv.py)

Test Plan:

```python
python test/test_quantzation.py TestInputOutputShapes
python test/test_quantzation.py TestDynamicQuantizedModule
```

Differential Revision: [D36536580](https://our.internmc.facebook.com/intern/diff/D36536580)",pytorch
77934,kulinseth,pr,2022-05-20T02:58:51Z,MPS: Fix some memory leak issues in release pools,"Fixes #ISSUE_NUMBER
",pytorch
77964,kulinseth,pr,2022-05-20T14:56:11Z,MPS: Add back the memory leak fixes.,"Fixes #ISSUE_NUMBER
",pytorch
77982,jeffdaily,pr,2022-05-20T17:29:01Z,[ROCm] enable jiterator,"### Description
Enables jiterator for ROCm builds.  This includes necessary porting when hiprtc and nvrtc behavior differed.  This also ported ROCm versus CUDA differences w.r.t. MAX_DIMS and NUM_THREADS from the non-jiterator code paths into jiterator.

### Testing
CI with ciflow/trunk label to force running ROCm workflows that are currently trunk-only.",pytorch
77996,robieta,pr,2022-05-20T19:34:40Z,[Profiler] Pop `KinetoThreadLocalState` at the start of post processing.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #77997
* __->__ #77996

An issue recently surfaced internally which highlighted the fact that removing KinetoThreadLocalState from the TLS at the end of post processing means that we are profiling memory during post processing. (Which violates a whole bunch of invariants in the system.) This change switches the global profiling ctx to a shared_ptr, introduces a class to manage it (`init`, `get`, and `pop` methods) and moves the `pop` call to the beginning of `disableProfiler`.

Differential Revision: [D36555738](https://our.internmc.facebook.com/intern/diff/D36555738/)",pytorch
77997,robieta,pr,2022-05-20T19:34:45Z,[Profiler] Fix segfault in AppendOnlyList,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #77997
* #77996

`buffer_last_` is supposed to start at buffer_.before_begin(). It is correctly set in the ctor, but incorrectly set in `clear()`. This causes a segfault in `maybe_grow()` (Specifically, `buffer_.emplace_after(buffer_last_)`) for an AppendOnlyList which has been cleared.

Differential Revision: [D36555737](https://our.internmc.facebook.com/intern/diff/D36555737/)",pytorch
78006,kulinseth,pr,2022-05-20T20:51:49Z,MPS: Fix the memory growing issue and BERT_pytorch network crash fix.,"Fixes #77753
",pytorch
78028,d4l3k,pr,2022-05-21T00:02:21Z,torch/distributed: move WorkerInfo registration into libtorch instead of libtorch_python,"Summary:
This moves torch::class_<WorkerInfo> into `rpc_agent.cpp` so it gets registered in libtorch instead of libtorch_python. This is intermediate work to getting torch::deploy to load an unmodified copy of libtorch. Current RPC is incompatible due to duplicate registrations.

```
unknown file: Failure
C++ exception with description ""Exception Caught inside torch::deploy embedded library:
Custom class with name __torch__.torch.classes.dist_rpc.WorkerInfo is already registered. Ensure that registration with torch::class_ is only called once.
Exception raised from registerCustomClass at ../aten/src/ATen/core/custom_class.cpp:61 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x3e (0x7f3bd9adb92e in /home/tristanr/venvs/multipy/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x5c (0x7f3bd9ab7068 in /home/tristanr/venvs/multipy/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #2: torch::registerCustomClass(std::shared_ptr<c10::ClassType>) + 0x110 (0x7f3bc2258980 in /home/tristanr/venvs/multipy/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: torch::detail::class_base::class_base(std::string const&, std::string const&, std::string, std::type_info const&, std::type_info const&) + 0x3b9 (0x7f3bc225a419 in /home/tristanr/venvs/multipy/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #4: [0x7f3ba45cfea1]
frame #5: <unknown function> + 0x1b5334 (0x5652bdab9334 in ./test_deploy)
frame #6: <unknown function> + 0x1b4f3e (0x5652bdab8f3e in ./test_deploy)
frame #7: <unknown function> + 0x1b519b (0x5652bdab919b in ./test_deploy)
frame #8: loadSearchFile(char const*) + 0x23e (0x7f3ba62f37f8 in /tmp/torch_deploy9ATEFg)
frame #9: deploy_set_self + 0x51 (0x7f3ba62f38f9 in /tmp/torch_deploy9ATEFg)
frame #10: torch::deploy::Interpreter::Interpreter(torch::deploy::InterpreterManager*, std::shared_ptr<torch::deploy::Environment>) + 0x274 (0x5652bdaaa790 in ./test_deploy)
frame #11: void __gnu_cxx::new_allocator<torch::deploy::Interpreter>::construct<torch::deploy::Interpreter, torch::deploy::InterpreterManager*, std::shared_ptr<torch::deploy::Environment>&>(torch::deploy::Interpreter*, torch::deploy::InterpreterManager*&&, std::shared_ptr<torch::deploy::Environment>&) + 0x81 (0x5652bdaaf58b in ./test_deploy)
frame #12: void std::allocator_traits<std::allocator<torch::deploy::Interpreter> >::construct<torch::deploy::Interpreter, torch::deploy::InterpreterManager*, std::shared_ptr<torch::deploy::Environment>&>(std::allocator<torch::deploy::Interpreter>&, torch::deploy::Interpreter*, torch::deploy::InterpreterManager*&&, std::shared_ptr<torch::deploy::Environment>&) + 0x4a (0x5652bdaae320 in ./test_deploy)
frame #13: void std::vector<torch::deploy::Interpreter, std::allocator<torch::deploy::Interpreter> >::_M_realloc_insert<torch::deploy::InterpreterManager*, std::shared_ptr<torch::deploy::Environment>&>(__gnu_cxx::__normal_iterator<torch::deploy::Interpreter*, std::vector<torch::deploy::Interpreter, std::allocator<torch::deploy::Interpreter> > >, torch::deploy::InterpreterManager*&&, std::shared_ptr<torch::deploy::Environment>&) + 0xee (0x5652bdaae4a0 in ./test_deploy)
frame #14: void std::vector<torch::deploy::Interpreter, std::allocator<torch::deploy::Interpreter> >::emplace_back<torch::deploy::InterpreterManager*, std::shared_ptr<torch::deploy::Environment>&>(torch::deploy::InterpreterManager*&&, std::shared_ptr<torch::deploy::Environment>&) + 0xb6 (0x5652bdaad258 in ./test_deploy)
frame #15: torch::deploy::InterpreterManager::InterpreterManager(unsigned long, std::shared_ptr<torch::deploy::Environment>) + 0x123 (0x5652bdaa83b1 in ./test_deploy)
frame #16: TorchpyTest_InitTwice_Test::TestBody() + 0x65 (0x5652bda075a9 in ./test_deploy)
frame #17: void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) + 0x65 (0x5652bda944b7 in ./test_deploy)
frame #18: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) + 0x5a (0x5652bda8cfe7 in ./test_deploy)
frame #19: testing::Test::Run() + 0x100 (0x5652bda68622 in ./test_deploy)
frame #20: testing::TestInfo::Run() + 0x10f (0x5652bda68fb3 in ./test_deploy)
frame #21: testing::TestSuite::Run() + 0x121 (0x5652bda6980d in ./test_deploy)
frame #22: testing::internal::UnitTestImpl::RunAllTests() + 0x38e (0x5652bda756e6 in ./test_deploy)
frame #23: bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) + 0x65 (0x5652bda9586b in ./test_deploy)
frame #24: bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) + 0x5a (0x5652bda8e0f7 in ./test_deploy)
frame #25: testing::UnitTest::Run() + 0xc9 (0x5652bda73fd1 in ./test_deploy)
frame #26: RUN_ALL_TESTS() + 0x11 (0x5652bda169fa in ./test_deploy)
frame #27: main + 0x27 (0x5652bda10ce2 in ./test_deploy)
frame #28: <unknown function> + 0x2d310 (0x7f3bc0431310 in /usr/lib/libc.so.6)
frame #29: __libc_start_main + 0x81 (0x7f3bc04313c1 in /usr/lib/libc.so.6)
frame #30: _start + 0x25 (0x5652bda063b5 in ./test_deploy)
```

Test Plan: CI

Differential Revision: D36564258

",pytorch
78037,Flamefire,pr,2022-05-21T07:59:05Z,Pass WITH_BLAS option from environment to CMake,"Allows to choose the BLAS backend with Eigen. Previously this was a CMake option only and the env variable was ignored.

Related to https://github.com/pytorch/pytorch/commit/f1f3c8b0fad9d647454a4d0507a2db4381563c8e

The claimed options BLAS=BLIS WITH_BLAS=blis are misleading: When BLAS=BLIS is set the WITH_BLAS option does not matter at all, it would only matter for BLAS=Eigen hence this issue went undetected so far.

Supersedes #59220",pytorch
78051,jjsjann123,pr,2022-05-21T23:11:53Z,simple c10 implementation for std::call_once,"A long standing bug on std::call_once: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66146
It could hang during re-entry after an exception handling.

Added a c10 implementation yielding a bulky mutex. Not the most efficient thing but at least it shouldn't hang.",pytorch
78052,jjsjann123,pr,2022-05-21T23:35:55Z,Static initializer update,"Code cleaning, call_once on a static initializer shouldn't be needed.",pytorch
78084,robieta,pr,2022-05-23T14:46:07Z,[Profiler] Fix segfault in AppendOnlyList,Cherrypick https://github.com/pytorch/pytorch/pull/77997,pytorch
78160,jjsjann123,pr,2022-05-24T05:14:10Z,adding a quick link to nvfuser README.md in jit doc for 1.12 release,"adding a link to github 1.12 release branch nvfuser README.md in jit doc

Note that this PR is intended to be cherry-picked by 1.12 release, we'll have a follow up PR to update the link once this PR is merged.",pytorch
78162,robieta,pr,2022-05-24T05:57:44Z,Copy rollbear/strong_type to `c10/util`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78164
* #78163
* __->__ #78162

There are a lot of cases in the profiler (and I suspect elsewhere in the codebase) where we just sling around ints and hope for the best. I've hit a bunch of bugs while refactoring the profiler that amount to ""I fat fingered it, but it still compiles because an int is an int"".

Options considered:
 1) BOOST_STRONG_TYPEDEF
    This was my initial plan, but requires tweaking to remove boost deps and prevent runtime overhead. (https://godbolt.org/z/oKs18Y7a8)

 2) https://github.com/foonathan/type_safe
    Seems cool and well regarded, but is also large and would be a pain to pull in.

 3) https://github.com/rollbear/strong_type
    Single header and very easy to configure.

(3) Seems to be the best fit, and I've found it really pleasant to work with when refactoring the python profiler.

Differential Revision: [D36364595](https://our.internmc.facebook.com/intern/diff/D36364595/)",pytorch
78163,robieta,pr,2022-05-24T05:57:49Z,[Profiler] Move python tracing to unified event type (Part 1),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78164
* __->__ #78163
* #78162

The python function tracer is complicated and separate from the other profile types, so I've chosen to break the change into two diff. The first (this one) reworks the cache structure to make it amenable to integration (as well as some other nice tweaks) and the next one actually moves it over.

The old cache scheme worked very hard to pack all the information about an event into a small struct via bit packing, with a couple secondary caches for things like names. Because of the space constraints on that struct (and the fact that it had to represent all call and return types) there were a lot of subtle invariants swirling around that made it hard to offload anything to a different component. The new cache system is more modular and also, as it turns out, a bit faster. (Benchmarks in part 2)

There is a more detailed description of the cache hierarchy in the PR, but the gist is that I use various specializations to handle the different event types (python call, nn module, c function) and lean on the type system to keep everything safe and organized. (One nice thing about using unique IDs is that they also implicitly encode the event type. They implicitly encode everything!) Given that we are going to want to expand the semantics (e.g. torch ops, DataLoader, etc) this will give a nice way to capture richer semantics without significantly increasing the complexity of the profiler.

Differential Revision: [D36379147](https://our.internmc.facebook.com/intern/diff/D36379147/)",pytorch
78164,robieta,pr,2022-05-24T05:57:54Z,[Profiler] Move python tracing to unified event type (Part 2),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78164
* #78163
* #78162

This PR finishes moving over the python tracer to use the unified event type. Things that changed:

1) The hacky after-the-fact splicing of python events in profiler_kineto.cpp is gone and python events now simply fold into the rest. (Yay!!!) This is a major BE win.
2) Added `ExtraFields<EventType::PyCall>` and `ExtraFields<EventType::PyCCall>`
3) The enter events (time + TraceKey) are now handled by RecordQueue for performance.
4) Python tracing now uses TSC for lower overhead.

Simplifications in profiler_python WRT part 1:
1) Rather than ValueCache emitting an intermediate value_t that gets further converted, load methods can now directly emit ExtraFields<...>
2) The complicated replay in profiler_python.cpp is replaced with a much simpler (and safer) pass to just pair start and end times.
3) During post processing we can now use `CallTypeHelper::map` to automatically pull in all events instead of having to loop over each the entries for each type manually. This will make it simpler to add new types of Python event later.

Differential Revision: [D36515869](https://our.internmc.facebook.com/intern/diff/D36515869/)",pytorch
78211,zasdfgbnm,pr,2022-05-24T21:32:34Z,[primTorch] Rename is_finite->isfinite,"`isfinite` sounds like a better name, because PyTorch, C++, numpy all have this name instead of `is_finite`
",pytorch
78220,zasdfgbnm,pr,2022-05-24T22:30:01Z,[primTorch] impl_nvfuser for unary ops - 1,,pytorch
78230,kulinseth,pr,2022-05-25T01:50:39Z,Fix the MPS Heap volatility,Fixes #77829,pytorch
78244,jjsjann123,pr,2022-05-25T06:55:51Z,[nvfuser_upstream_push] nvfuser code base bump 052422,"Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/

A few bigger updates:
1. Initial support of cp.async and cp.async.wait: https://github.com/csarofeen/pytorch/pull/1619
2. Emulate ampere's mma 16816 with Turing's mma 1688, for a unified interface: https://github.com/csarofeen/pytorch/pull/1643
3. Extending the infrastructure to support mma operators on turing and ampere arch: https://github.com/csarofeen/pytorch/pull/1440
 
Commits that's actually in this PR from the csarofeen branch
```
* dd2325294e236c5082c642819a1103bcfe4561a3 (csarofeen/devel) Fusion Segmenter: Unify single kernel and multi-kernel runtime path (#1710)
* b3d1c3f446355a2d276bac8272e7aa8b5bb6b1f0 Fix missing cooperative launch (#1726)
* dc670a226cbe52be46cecef47001f38bf9a09433 Async gmem copy support on sm80+ (#1619)
* 5e6a8dab5a71aefe0548bbfa15d1a93c556d23fe Add turing mma support and test (#1643)
* d6d6b7d3f10dd91dafa4cdbd5e460bbb38173af4 Fix rFactor when there are indirect root domain(s), and refactor (#1723)
* 7093e39150c6d80e0f9f767d56654714a2e8a927 Mma op integration on ampere (#1440)
* fade8da55e60a118c5595378896d34b862b2fcc3 patch python test for bfloat16 (#1724)
* 8fbd0b18743a72ac10478857c3d2351204375685 Fine-grained kernel profiling (#1720)
* 77c1b4fa633f9e631d267923f4537336fa328939 Adding dry run mode to skip arch dependent checks (#1702)
* 151d95b97bebefc94199bb4a53423ede32b55451 More precise concretization analysis (#1719)
* f4d3630ed54d7069dd377a64be1f91013b285b66 Enable complex python tests (#1667)
* 4ceeee509774cc2ce6c834a4dc1e313f71d94503 Minor bugfix in transform_rfactor.cpp (#1715)
* 3675c70faf218e86d2c78dbd3874b175a3b0a203 Separate root domain and rfactor domain in TransformPrinter (#1716)
* f68b830d5def65dadfe29d4edf52fc703369c84a Fix scheduling with polymorphic broadcast (#1714)
* 4ab5ef7ae2cfd8fffad1e1d882ae7c50631211dc updating_ci_machine (#1718)
* 56585c58b1ff338704cafb0cd6be2b3d536bed5a Merge pull request #1711 from csarofeen/upstream_master_bump_0517
* 174d453d3be0c11a5acb0fff3b3f36e19cfdaf81 Allow using nvFuser on CUDA extension (#1701)
* 18bee67495454b9a79625799776e746bd5e81c4c Validate LOOP concrete IDs have complete IterDomains (#1676)
```",pytorch
78281,zasdfgbnm,pr,2022-05-25T17:59:07Z,Allow using nvFuser on CUDA extension,"Cherry-pick of https://github.com/csarofeen/pytorch/pull/1701
",pytorch
78312,kulinseth,pr,2022-05-25T22:47:14Z,MPS: add ranked tensors for addcmul ops instead of constants and update version_check,"Fixes #ISSUE_NUMBER
",pytorch
78340,kulinseth,pr,2022-05-26T05:23:18Z,"Revert ""Revert MPS changes (#78335)""","This reverts commit ffb31014847faccb724ef70924ba936b75144c85.

Add back the changes. Fix the build break.
Fixes #ISSUE_NUMBER
",pytorch
78386,z-a-f,pr,2022-05-26T20:34:01Z,[quant] Removing unnecessary arguments from F.conv,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #78543
* __->__ #78386
* #77929
* #77928
* #77927

There are several arguments in the functional conv that are not
necessary for the correct operation of the functional conv

- dtype: This argument is not used at all. The dtypes are inferred from inputs
- padding_mode: Functional conv is not supposed to support any
                padding_mode other than zeros. This is also reflected in
                torch/nn/functional.py:convXd definitions.

Test Plan:

```python
python test/test_quantization.py
```

Differential Revision: [D36792545](https://our.internmc.facebook.com/intern/diff/D36792545)",pytorch
78408,kulinseth,pr,2022-05-27T01:39:26Z,MPS: Eye op,"This can be used as a reference PR was to add Op in MPS backend.
",pytorch
78410,kulinseth,pr,2022-05-27T03:02:49Z,MPS: Add adaptive max pool2d op,"Adaptive max pool 2d forward and backward with test
",pytorch
78416,jjsjann123,pr,2022-05-27T09:16:23Z,"[primTorch] Adds broadcast_to, column_stack references","1. Added references for the two ops;
2. Inherited original operators' OpInfo tests;

TODO for future PR:
adding primTorch references for `dsplit` and `dstack`. <- Those two should use `atleast_3d` which is in a different packet right now.",pytorch
78417,jjsjann123,pr,2022-05-27T09:24:50Z,fixing typo,primtorch prod is mistakenly using `_sum_doc`,pytorch
78440,kulinseth,pr,2022-05-27T23:28:54Z,MPS: Fixes the as_strided_mps implementation for contiguous view operations,"Fixes https://github.com/pytorch/pytorch/issues/78107; https://github.com/pytorch/pytorch/issues/77750
",pytorch
78496,kulinseth,pr,2022-05-30T18:37:13Z,MPS: Fix crashes in view tensors due to buffer size mismatch,"Fixes #78247, #77886

",pytorch
78543,z-a-f,pr,2022-05-31T17:38:08Z,[quant] Signatures check for F.convXd,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #78543
* #78386
* #77929
* #77928
* #77927

Introducing tests to make sure the signatures of functionals match
between quantized and FP. One of the issues in the past was that the
quantized functionals had arguments that were not included in the
FP counterpart of it. For example, the conv1d used to have `padding_mode`
which is not needed in the nn.functional.conv1d, but was present in the
nn.quantized.functional.conv1d.

Test Plan:

```python
python test/test_quantization.py TestSignatureEquivalence
```

Differential Revision: [D36792547](https://our.internmc.facebook.com/intern/diff/D36792547)",pytorch
78563,robieta,pr,2022-05-31T19:54:44Z,[Profiler] Weaken ordering check during post processing.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78563

The profiler assembles a call hierarchy by replaying recorded events. There is an assert to ensure that the events form a well structured tree; however many of the inputs are from external sources and small differences (e.g. recording time in a lower precision) leads to traces which violate that assumption. For now this is acceptable; the post processing can handle resolving these descrepencies. As a result, I am relaxing the assert to only test event types where we expect the framework to be able to enforce these strong structural requirements.

Differential Revision: [D36787787](https://our.internmc.facebook.com/intern/diff/D36787787/)",pytorch
78570,kulinseth,pr,2022-05-31T20:20:19Z,MPS: add linespace op,"Fixes #ISSUE_NUMBER
",pytorch
78588,zasdfgbnm,pr,2022-05-31T23:36:10Z,Fix perf regression introduced in #70943,"`numel` is a too loose upper bound for `num_of_segments` and `num_of_partial_segments`. It causes perf regressions.

This PR moves to a tighter upper bound.

Benchmark with jupyter notebook:
```python
import torch
num_embeddings = 1024
embedding_dim = 512
e = torch.nn.Embedding(num_embeddings, embedding_dim).cuda()

size = 1*1024*1024
i = torch.arange(size, device='cuda') % num_embeddings
o = e(i)
g = torch.randn_like(o)
torch.cuda.synchronize()
```
```python
%%timeit
o.backward(g, retain_graph=True)
torch.cuda.synchronize()
```

Before #70943: 3.6 ms
After #70943: 6.9 ms
With this PR: 3.55 ms",pytorch
78612,jjsjann123,pr,2022-06-01T08:36:12Z,[primTorch] Adds broadcast_shapes reference,"1. Added references `_refs.broadcast_shapes`
2. Added OpInfo test for `torch.broadcast_shapes` 

A few minor changes:
- `test_python_ref_meta` and `_ref_test_helper` update to avoid non-tensor outputs
- type annotation update for `_resize_meta`",pytorch
78627,zasdfgbnm,pr,2022-06-01T16:48:57Z,remove prims::square,because it is just `x * x`,pytorch
78644,zasdfgbnm,pr,2022-06-01T18:59:42Z,Fix docs for torch.real,"Non-complex types are supported

```python
>>> import torch
>>> z = torch.zeros(5)
>>> torch.real(z.float())
tensor([0., 0., 0., 0., 0.])
>>> torch.real(z.int())
tensor([0, 0, 0, 0, 0], dtype=torch.int32)
```",pytorch
78655,zasdfgbnm,pr,2022-06-01T19:14:09Z,"Add refs::{isposinf, isneginf}",,pytorch
78670,zasdfgbnm,pr,2022-06-01T20:50:20Z,Cleanup impl_nvfuser for unary ops,,pytorch
78690,kulinseth,pr,2022-06-02T02:05:48Z,MPS: Fix issues with view tensors and linspace.,"Fixes: #https://github.com/pytorch/pytorch/issues/78642, https://github.com/pytorch/pytorch/issues/78511",pytorch
78696,jjsjann123,pr,2022-06-02T03:01:45Z,[primTorch] Adds dsplit/dstack reference,Added references _refs.dsplit & _refs.dstack and PythonRefInfo tests,pytorch
78712,z-a-f,pr,2022-06-02T10:32:27Z,[quant][ao_migration] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* #78717
* #78715
* #78714
* #78713
* __->__ #78712

Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
  - [X] [Current PR] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
  - [ ] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
  - [ ] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
  - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
  - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
  - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
  - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
  - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
  - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
    - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
    - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- [Documentation](docs/source/quantization-support.rst) @vkuzo
- [Public API test list](test/allowlist_for_publicAPI.json) @peterbell10

Differential Revision: [D36792967](https://our.internmc.facebook.com/intern/diff/D36792967/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36792967/)!",pytorch
78713,z-a-f,pr,2022-06-02T10:32:33Z,[quant][ao_migration] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* #78717
* #78715
* #78714
* __->__ #78713



Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] [Current PR] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [ ] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- Documentation @vkuzo
  - docs/source/conf.py
  - docs/source/quantization.rst
- [quantize_fx](torch/ao/quantization/quantize_fx.py) @jerryzh168
- [common test routine](test/quantization/ao_migration/common.py) @HDCharles
- JIT stuff @jamesr66a
  - torch/csrc/jit/passes/hoist_conv_packed_params.cpp
  - torch/csrc/jit/passes/quantization/helper.h
  - torch/csrc/jit/serialization/import_source.cpp

Differential Revision: [D38926012](https://our.internmc.facebook.com/intern/diff/D38926012/)

Differential Revision: [D38926012](https://our.internmc.facebook.com/intern/diff/D38926012)",pytorch
78714,z-a-f,pr,2022-06-02T10:32:40Z,[quant][ao_migration] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* #78717
* #78715
* __->__ #78714
* #78713



Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] [Current PR] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [ ] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- [Documentation](docs/source/quantization-support.rst) @vkuzo
- [Public API test list](test/allowlist_for_publicAPI.json) @peterbell10
- [BC test](test/quantization/bc/test_backward_compatibility.py) @vkuzo
- [IR emitter](torch/csrc/jit/frontend/ir_emitter.cpp) @jamesr66a
- [JIT serialization](torch/csrc/jit/serialization/import_source.cpp) @IvanKobzarev @jamesr66a

Differential Revision: [D36860660](https://our.internmc.facebook.com/intern/diff/D36860660/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36860660/)!

Differential Revision: [D36860660](https://our.internmc.facebook.com/intern/diff/D36860660)",pytorch
78715,z-a-f,pr,2022-06-02T10:32:46Z,[quant][ao_migration] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* #78717
* __->__ #78715
* #78714
* #78713



Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [ ] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] [Current PR] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [ ] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- None

Differential Revision: [D36860927](https://our.internmc.facebook.com/intern/diff/D36860927/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36860927/)!

Differential Revision: [D36860927](https://our.internmc.facebook.com/intern/diff/D36860927)",pytorch
78716,z-a-f,pr,2022-06-02T10:32:55Z,[quant][ao_migration] `torch.nn.qat` → `torch.ao.nn.qat`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #78716
* #78717
* #78715
* #78714
* #78713



Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [X] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [X] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [X] [Current PR] `torch.nn.qat` → `torch.ao.nn.qat`
    - [X] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [X] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- None

Differential Revision: [D36861197](https://our.internmc.facebook.com/intern/diff/D36861197/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36861197/)!

Differential Revision: [D36861197](https://our.internmc.facebook.com/intern/diff/D36861197)",pytorch
78717,z-a-f,pr,2022-06-02T10:38:34Z,[quant][ao_migration] `torch.nn.quantizable` → `torch.ao.nn.quantizable`.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #78716
* __->__ #78717
* #78715
* #78714
* #78713



Context: In order to avoid the cluttering of the `torch.nn` namespace
the quantized modules namespace is moved to `torch.ao.nn`.

The list of the `nn.quantized` files that are being migrated:

- [X] `torch.nn.quantized` → `torch.ao.nn.quantized`
    - [X] `torch.nn.quantized.functional` → `torch.ao.nn.quantized.functional`
    - [X] `torch.nn.quantized.modules` → `torch.ao.nn.quantized.modules`
    - [X] `torch.nn.quantized.dynamic` → `torch.ao.nn.quantized.dynamic`
    - [X] `torch.nn.quantized._reference` → `torch.ao.nn.quantized._reference`
- [X] [Current PR] `torch.nn.quantizable` → `torch.ao.nn.quantizable`
- [ ] `torch.nn.qat` → `torch.ao.nn.qat`
    - [ ] `torch.nn.qat.modules` → `torch.ao.nn.qat.modules`
    - [ ] `torch.nn.qat.dynamic` → `torch.ao.nn.qat.dynamic`
- [ ] `torch.nn.intrinsic` → `torch.ao.nn.intrinsic`
    - [ ] `torch.nn.intrinsic.modules` → `torch.ao.nn.intrinsic.modules`
    - [ ] `torch.nn.intrinsic.qat` → `torch.ao.nn.intrinsic.qat`
    - [ ] `torch.nn.intrinsic.quantized` → `torch.ao.nn.intrinsic.quantized`
        - [ ] `torch.nn.intrinsic.quantized.modules` → `torch.ao.nn.intrinsic.quantized.modules`
        - [ ] `torch.nn.intrinsic.quantized.dynamic` → `torch.ao.nn.intrinsic.quantized.dynamic`

Majority of the files are just moved to the new location.
However, specific files need to be double checked:

- `torch/ao/nn/__init__.py` → Changing the imports to lazy.


Differential Revision: [D36861090](https://our.internmc.facebook.com/intern/diff/D36861090/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36861090/)!

Differential Revision: [D36861090](https://our.internmc.facebook.com/intern/diff/D36861090)",pytorch
78824,jeffdaily,pr,2022-06-03T17:03:48Z,parallel_apply should forward current streams to worker threads,"#71033 moved test_data_parallel_module et al under `instantiate_device_type_tests`.  This had the side effect of now running the tests on a non-default stream.  The parallel_apply creates new threads, one per device, but does not forward the thread local current streams from the parent thread.  This defaults the new per-device threads to use the null stream.  The null stream will not sync with the non-default non-blocking streams, resulting in errors when these tests assert tensors are equal.

CC @janeyx99 ",pytorch
78862,gmagogsfm,pr,2022-06-04T00:21:22Z,Add missing QSCheme IValue conversion logic,"Differential Revision: D36913736

",pytorch
78930,kulinseth,pr,2022-06-06T15:12:02Z,MPS: Fixes,"Cast integer to float in UnaryOps
Add tensor dtype in key generation
Enable FP16 scalars and use placeholder for alpha tensor in add/sum ops

Fixes #ISSUE_NUMBER
",pytorch
78959,z-a-f,pr,2022-06-06T19:36:48Z,[ao][sparsity][DRAFT] Delta SParsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #78959

",pytorch
78979,d4l3k,pr,2022-06-06T23:56:22Z,torch/package: add fix for implicit numpy dependency,"Summary:
This adds a workaround so that you can call `.numpy()` on models loaded via torch.package. Torch implicitly imports numpy when you make that call and it can't be tracked by normal dependencies since it's from C++.

https://github.com/pytorch/MultiPy/issues/46

Test Plan: CI

Reviewed By: PaliC

Differential Revision: D36915129

",pytorch
79131,robieta,pr,2022-06-08T16:50:30Z,Reland #78164,"Reland https://github.com/pytorch/pytorch/pull/78164

Replaced `TEST_CUDA` with `torch.has_cuda`. The test in question was not expected to work with cuda; I was under the impression that `TEST_CUDA` would catch all the cases but it seems not.
",pytorch
79147,jjsjann123,pr,2022-06-08T19:35:40Z,[nvfuser_upstream_push] nvfuser code base bump 060822,"Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/

Bug fixes and minor refactor

Squashed commits to WAR github API
Commits that's actually in this PR from the devel branch:

```
4c60e7dff22a494632370e5df55c011007340d06 Add examples infrastructure for using nvFuser in a standalone program (#1725)
02a05d98334ffa580d73ccb28fdb8c577ad296fe Fix issue #1751 (#1753)
8a69aa320bd7629e1709fe5ceb7104d2c88ec84c Refactor NvFuser transpose API to match eager mode behavior (#1746)
ffdf6b7709048170d768217fcd7083fc8387f932 Remove BroadcastWithoutStride. (#1738)
02bab16035e70734450c02124f5cdaa95cf5749d Fix flipping of a boolean flag (#1745)
465d66890c8242e811224359cbdb1c2915490741 cleanup (#1744)
26d354e68720bc7dd2d3b1338ac01b707a230b6a fixing noncontig broadcast (#1742)
856b6b2f9073662dd98ca22ba6c3540e20eb1cdd Add IterDomainBuilder (#1736)
1fd974f912cd4c1e21cbd16e2abb23598d66a02f fixing warning for gcc7 (#1732)
de2740a43a869f8272c2648e091d7b8235097db9 disabling complex in python tests for #1730 (#1733)
fbbbe0a2e7c7a63e0e2719b8bfccb759b714221a fixing MSVC build (#1728)
b5feee5e2b28be688dbddc766f3c0220389c8175 Fix the fused reduction runtime kernel (#1729)
5247682dff5980bb66edf8d3aac25dea2ef2ced5 Re-entrant GroupedGridReduction (#1727)
```

RUN_TORCHBENCH: nvfuser",pytorch
79173,robieta,pr,2022-06-09T01:12:05Z,"Revert ""Revert ""[Profiler] Move python tracing to unified event type (Part 2)""""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #79175
* __->__ #79173

This reverts commit 4305f8e9bda34f18eb7aacab51c63651cfc61802.

replace TEST_CUDA with torch.has_cuda",pytorch
79175,robieta,pr,2022-06-09T01:12:10Z,Move test_profiler tests to tree rather than icicle format,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79175
* #79173

",pytorch
79185,kulinseth,pr,2022-06-09T05:32:40Z,MPS: Binary cast fix by proper type promotion and remove spurious copy warning,"Fixes #78019, #78020
Fixes https://github.com/pytorch/pytorch/pull/79185",pytorch
79188,kulinseth,pr,2022-06-09T06:04:14Z,MPS: add exponential op,"Add exponential distribution

Fixes #ISSUE_NUMBER
",pytorch
79189,kulinseth,pr,2022-06-09T06:09:28Z,MPS: add layer_norm_backward,"Layernorm backward

Fixes #ISSUE_NUMBER
",pytorch
79276,kulinseth,pr,2022-06-10T14:29:45Z,"MPS: cherry-pick ""add layer_norm_backward"" (#79189)","Layernorm backward. 
This is a much needed feature fix for running hf_Bert and many other Transformer networks on MPS backend. 

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79189
Approved by: https://github.com/razarmehr, https://github.com/albanD

Fixes #ISSUE_NUMBER
",pytorch
79301,robieta,pr,2022-06-10T19:25:25Z,move tree tests to the start of test_profiler.py,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #79356
* __->__ #79301

I recently introduced strict tests to the python function tracer, and part of those tests was to check the line number. This check was limited to `test_profiler.py` so as not to unduly burden the rest of the PyTorch team and make them update the test every time something changed, however it's still kind of annoying when making changes to `test_profiler.py`. This change just moves those tests to the start of the file so we can add/modify other tests without having to regenerate the expected python for the tracer.

CC @louisfeng Sorry that you ran into this, and sorry @janeyx99 for the CI instability that this caused.

Test plan: This is a unit test PR",pytorch
79356,robieta,pr,2022-06-11T22:45:50Z,fix Python parent id,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79356
* #79301

Broke this in a recent refactor.

Test plan: Added a unit test to `test_profiler.py`",pytorch
79406,jjsjann123,pr,2022-06-13T07:39:32Z,[nvfuser_upstream_push] Reland: nvfuser code base bump 060822,"Landing reverted PR #79147.

Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/

Bug fixes and minor refactor

Squashed commits to WAR github API
Commits that's actually in this PR from the devel branch:

```
4c60e7dff22a494632370e5df55c011007340d06 Add examples infrastructure for using nvFuser in a standalone program (#1725)
02a05d98334ffa580d73ccb28fdb8c577ad296fe Fix issue #1751 (#1753)
8a69aa320bd7629e1709fe5ceb7104d2c88ec84c Refactor NvFuser transpose API to match eager mode behavior (#1746)
ffdf6b7709048170d768217fcd7083fc8387f932 Remove BroadcastWithoutStride. (#1738)
02bab16035e70734450c02124f5cdaa95cf5749d Fix flipping of a boolean flag (#1745)
465d66890c8242e811224359cbdb1c2915490741 cleanup (#1744)
26d354e68720bc7dd2d3b1338ac01b707a230b6a fixing noncontig broadcast (#1742)
856b6b2f9073662dd98ca22ba6c3540e20eb1cdd Add IterDomainBuilder (#1736)
1fd974f912cd4c1e21cbd16e2abb23598d66a02f fixing warning for gcc7 (#1732)
de2740a43a869f8272c2648e091d7b8235097db9 disabling complex in python tests for #1730 (#1733)
fbbbe0a2e7c7a63e0e2719b8bfccb759b714221a fixing MSVC build (#1728)
b5feee5e2b28be688dbddc766f3c0220389c8175 Fix the fused reduction runtime kernel (#1729)
5247682dff5980bb66edf8d3aac25dea2ef2ced5 Re-entrant GroupedGridReduction (#1727)
```

RUN_TORCHBENCH: nvfuser",pytorch
79505,z-a-f,pr,2022-06-14T06:10:27Z,"Adding codeowners to quantization, sparsity, ns, etc.","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79505

The notifications for the AO-maintained codebase.
This should not be blocking, just PR/test notifications.",pytorch
79532,kulinseth,pr,2022-06-14T16:13:06Z,[MPS] Add test consistency from OpInfo based tests from PR 78504,,pytorch
79541,jeffdaily,pr,2022-06-14T17:29:44Z,do not write sccache stats to json if missing OUR_GITHUB_JOB_ID,Allows use of .jenkins/pytorch/build.sh without assuming OUR_GITHUB_JOB_ID is set.  This is a regression caused by #79366.,pytorch
79577,jjsjann123,pr,2022-06-14T23:27:24Z,fixing checkArithNode,peephole optimization can be more aggressive to remove no-op add/mul for non integer types.,pytorch
79621,robieta,pr,2022-06-15T17:16:25Z,[Profiler] Prepare for change to unique_ptr in Kineto,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79621

Move profiler to use compat bridge methods on libkineto::CpuTraceBuffer. Note that there is a specific order in which these changes must land:
First, the change adding the methods to Kineto must land.
Second, the change updating the pinned version of Kineto in PyTorch must land.
Third, this change must land AND BE COMMITTED TO FBCODE.
Fourth, the change to change kineto to use unique_ptr must land.
And finally, the pinned commit in pytorch/third_party must be updated again.

Only after all of these can the profiler start to rely on kineto using a unique_ptr under the hood.

Differential Revision: [D36679293](https://our.internmc.facebook.com/intern/diff/D36679293/)",pytorch
79623,robieta,pr,2022-06-15T17:28:09Z,[Profiler] Expose extra fields to Python,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79623

Pybind11 has a really awesome feature where you can tell it how to move a type from C++ to Python just by specializing one template and it has out of the box support for variant types. (You do have to make one change to variant to avoid a bunch of chatty compiler warnings.) This will make it easy to both:
A) Write principled type driven analysis in Python similar to `c10::visit`
B) Expose fields that only make sense for certain events without cluttering up the API of the top level events.

For now I haven't added any fields; this PR is just to handle the foundation.

Differential Revision: [D36988611](https://our.internmc.facebook.com/intern/diff/D36988611/)",pytorch
79639,z-a-f,pr,2022-06-15T20:28:26Z,[ao] Moving the sparsity/experimental to sparsity/_experimental,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #80428
* __->__ #79639
* #80111



The experimental code in the sparsity does not have user-facing api, and should reside under the proivate package. This involves `pruner` and `base_sparsifier`.

Differential Revision: [D37185726](https://our.internmc.facebook.com/intern/diff/D37185726/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37185726/)!

Differential Revision: [D37185726](https://our.internmc.facebook.com/intern/diff/D37185726)",pytorch
79643,jjsjann123,pr,2022-06-15T21:35:46Z,fixing wrong example,One liner fix. The old proxy example doesn't attach nodes to the new graph.,pytorch
79676,kulinseth,pr,2022-06-16T04:10:29Z,[MPS]: Add fix for squeezed input axes handling in BCE loss,"Fixes #79527
",pytorch
79677,kulinseth,pr,2022-06-16T04:14:53Z,MPS: TopK raise an error if K>16,"* Error out in TopK when k>16.
* Add a test case too.

Fixes #78915
",pytorch
79682,kulinseth,pr,2022-06-16T04:59:23Z,MPS: Add amax and amin Ops with tests ,"* Add amax and amin with tests

Fixes #ISSUE_NUMBER
",pytorch
79759,jeffdaily,pr,2022-06-17T00:02:37Z,[ROCm] fix for multiple runners and docker commands,"The ROCm GHA workflow was initially based on the assumption that only a single runner exists on the host.  Now that ROCm CI hosts sometimes have two runners splitting the available GPUs, the docker commands to clean up existing containers and stale images cannot assume a single runner instance.  This updates the docker commands to only stop the current container and to safely ignore when a docker prune is already in progress.

CC @jithunnair-amd ",pytorch
79761,robieta,pr,2022-06-17T00:15:15Z,[Profiler][Trivial] Update test_profiler_tree,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #79763
* __->__ #79761

Update targets file and remove redundant block

Differential Revision: [D37228315](https://our.internmc.facebook.com/intern/diff/D37228315/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37228315/)!",pytorch
79762,robieta,pr,2022-06-17T00:15:20Z,[Profiler] Include ActivityType from Kineto,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79762
* #79761
* #79621

We don't want to compile with Kineto on all platforms, but if we're going to have significant integration between profiler and Kineto profiler will need to be able to rely on simple API constructs like the Kineto enums.

Differential Revision: [D37228314](https://our.internmc.facebook.com/intern/diff/D37228314/)",pytorch
79763,robieta,pr,2022-06-17T00:16:43Z,[Profiler] Include ActivityType from Kineto,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79763
* #79761

We don't want to compile with Kineto on all platforms, but if we're going to have significant integration between profiler and Kineto profiler will need to be able to rely on simple API constructs like the Kineto enums.

Differential Revision: [D37228314](https://our.internmc.facebook.com/intern/diff/D37228314/)",pytorch
79811,z-a-f,pr,2022-06-17T21:30:28Z,Fixing AO tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #79811

A test was introduced, that was adding a constructor with positional arguments to the instance of the `TestCase`. The intention was to add a reusable ""runner"" for the relevant testcases. However, this broke the unittest's ability to discover any tests defined.

The failing behavior was not caught because when running the tests using python, the unittest's is able to run the tests correctly, without trying to identify the discovery/listing of all the tests.

The immediate followup is to rewrite the tests in the `data_sparsifier` to implement proper ""runner"" classes / functions.

Differential Revision: [D37253508](https://our.internmc.facebook.com/intern/diff/D37253508/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37253508/)!",pytorch
79821,jjsjann123,pr,2022-06-17T22:46:22Z,[primtorch] add reference for clamp_min/clamp_max,"Added reference implementation for the two ops;
Added opinfo tests for aten clamp_min/clamp_max;
Added opinfo reference test.",pytorch
79824,zasdfgbnm,pr,2022-06-17T23:40:08Z,[nvFuser] Add real and imag to nvfuser and its python frontend,,pytorch
79939,kulinseth,pr,2022-06-21T13:29:43Z,Add scatter support for view operations,"* Add scatter support for view operations; #78074, #78886, #79672
* Update test_slicing_replace_column to properly test different sizes
* Handle in-place changes for binary ops; add new testcase
* Add new view ops testing scatter; add MPSDebugConfig.h config file for debugging purposes
* Merge gatherViewTensor and scatterViewTensor into a generic function
* Add scatter on demand in scatterViewOperation instead of caching it into a generic graph
* Create separate graphs for scatter and gather;
* Create scatter graph at scatter time

Fixes #ISSUE_NUMBER
",pytorch
80059,z-a-f,pr,2022-06-22T18:16:49Z,[ao][sparsity] Vectorized WeightNormSparsifier,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80059

The previous implementation was using loops to compute the sparsity within a block in a mask, as well as across the mask blocks. This implements the vectorized version.

## Vectorization:

A high level overview of the vectorization procedure falls into a two step process:

### Tensor-level masking

A tensor-level masking is a mask generation routine that has a granularity of `sparse_block_shape`. That means that only patches of that shape can be considered sparse/dense. To vectorize:

1. Reshape the data such that one of the dimensions represents the patches of sparse_block_shape.
2. Create a mask of the same shape as the reshaped data
3. Find the smallest `k` elements in the the data, given the dimension of the sparse ""patches"". `k` represents a derived paramter specifying the sparsity level.
4. Apply the 0/1 to the patches in the mask
5. Reshape the mask back to the original dimensions

Note: because the shape of the mask might not be multiple of the sparse_block_shape, we nudge the sshape of the mask, and truncate it afterwards.

## Block-level masking

A block-level masking is a mask generation routine that concerns itself only with sparsity within a patch of shape `sparse_block_shape`. This is useful when block sparsity allows partial block sparsification.

To vectorize:

Overall the block-level masking follows the same routine as the tensor-level algorithm described above. One distinction is that when reshaping the data/mask tensors we aim for creating a dimension that captures the internals of each patch. For example, if a `sparse_block_shape` is `(2, 2)`, we want to reshape the data/mask into `(2, 2, -1)`. That allows us to sort the internal elements on the last axis, and zero-out the ones that obey the sparse logic.

Differential Revision: [D37352494](https://our.internmc.facebook.com/intern/diff/D37352494/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37352494/)!",pytorch
80111,z-a-f,pr,2022-06-23T00:56:13Z,[ao][sparsity] Fixing failing internal pruner tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #79639
* __->__ #80111

After a recent change in the base_sparsifier API, the internal pruner started failing. This adopts the testcases to the change:

1. Changed `module_groups` to `groups`
2. Changed the fusion logic from taking care of the whole fused module to handling the submodules individually.

Differential Revision: [D37364801](https://our.internmc.facebook.com/intern/diff/D37364801/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37364801/)!",pytorch
80148,zasdfgbnm,pr,2022-06-23T17:25:22Z,"Add prims::{real, imag}, refs::{real, imag}, remove prims::is_infinite","This is a subset of https://github.com/pytorch/pytorch/pull/78655. I want to land this separately because the world is changing so fast, and https://github.com/pytorch/pytorch/pull/78655 is having lots of conflicts with other parts of the world.",pytorch
80235,robieta,pr,2022-06-24T18:44:00Z,[Kineto] Add header only target (and supermodules),"Summary:
I want PyTorch to be able to unconditionally use the Kineto enums without having to pull in the entire library.

Node: New targets run afoul of https://www.internalfb.com/intern/wiki/Supermodules/My_diff_has_Supermodule_violations/, so I have to add that as well.

Test Plan: Sandcastle

Differential Revision: D37365998

",pytorch
80327,Flamefire,pr,2022-06-27T09:47:21Z,Choose test affinity based on current affinity,"This avoids test failures in cgroup environments

Fixes https://github.com/pytorch/pytorch/issues/44368

CC @VitalyFedyunin 

new PR after #44369 got closed.",pytorch
80328,Flamefire,pr,2022-06-27T09:50:49Z,Only sync CUDA if the operation is run on GPU,"This fixes test failures when PyTorch is build without CUDA

Fixes https://github.com/pytorch/pytorch/issues/58563

I used the same is_cuda check that is used in test_nn.py

CC @ailzhang after #58564",pytorch
80329,Flamefire,pr,2022-06-27T09:52:21Z,Handle JIT test failure when the GPU is newer than the CUDA compiler,"The test uses the CUDA compute capabilities of the current device to compile an extension. If nvcc is older than the device, it will fail with a message like ""Unsupported gpu architecture 'compute_80'"" resulting in a `RuntimeError: Error building extension 'cudaext_archflags'` ultimately failing the test.

This checks for this case and allows execution to continue

Fixes #51950

CC @malfet after #55904",pytorch
80330,Flamefire,pr,2022-06-27T10:01:57Z,Increase default test timeout for distributed tests,"When running on clusters the startup time for the subprocesses might be much higher which leads to spurious failures.
So increase this to 300s similar to torch/testing/_internal/distributed/distributed_test.py

Also introduces `DISTRIBUTED_TESTS_DEFAULT_TIMEOUT` as suggested by @malfet in #55896",pytorch
80331,Flamefire,pr,2022-06-27T10:06:12Z,Copy Tensor for tests to avoid in-place transform modifying the original tensor,"Fixes #48591

CC @mruberry  after #60256",pytorch
80332,Flamefire,pr,2022-06-27T10:07:45Z,Add dump function for all Vectorized specializations,"This really helps when debugging issues hence introduce a generic function instead of the per-instantiation dump member function so it can be used consistently.

I needed this for my work on the POWER issues and noticed only some classes/specializations of the `vec.dump()` function, hence the added `dump(vec)` free function which avoids the need to add one for each new specialization.

I'm sure this can be helpful to others too and would even suggest to dump the `dump` member functions (Pun intended ;-) )

CC @VitalyFedyunin after #59443",pytorch
80355,jjsjann123,pr,2022-06-27T16:28:41Z,[WIP] Upstream push 0627,"Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/

Code changes includes:

- TransformPropagator refactor: switched to Dijkstra instead of exhaustive enumeration on all possible paths to reduce compilation time on transform propagation;
- Indexing refactor: remove reference tensor creation in all tensor indexing logic (#1690)
- (more) generic grouped grid reduction kernel;
- Minor parser/fuser patches:
  1. zero-dim tensor reduction support
  3. no-op binary removal within fused graph
  4. expand supported in fusion

Squashed commits to WAR github API
Commits that's actually in this PR from the devel branch:

```
a054b3efcf5af58ea518de283f55aaf9fe06ff5f Refactor TransormPropagator to allow specifying a position and propagating to part of the DAG (#1775)
d67e1cda9b802036841a371318014a818a849b0a Indexing refactor stage 1: remove reference tensor creation in all tensor indexing logic (#1690)
1b6529956a1ace220898ad09dde0bf85e49827f7 Issue 1770 (#1774)
35b04276b648c9b55cdb6a67f3889f54e745c3d2 Avoid compilation errors like below: (#1773)
452c77326a340d2a4130b7802f4f319aec60e72a Ignore reductions of zero-dim tensors per PyTorch conventions (#1771)
31d6c56d88afba09ac53b2d5dd3493d625f8cd57 TransformPropagator refactor (#1769)
570c5a84b91a3cf67207331be9650d26a2d37e3d Merge pull request #1767 from csarofeen/upstream_merge_0621
9d6c3d84be86da643df6fd51695543938111f20d merging upstream 61305cd638b6fcd73a0b66b4cde7014fecb9e8ce
0ed815f76b08f285bda855dd500692ff10a8abce New TransformPropagator algorithm (#1763)
6c195200c0a92fb0f38c833431a8940ed07569b9 no-op binary removal (#1764)
ec7fa4187c177186527409dfc5c7b1754d30bc92 Proper propagation of IterType (#1762)
b263562dbc3c865007ad7d7d42a58a20be8d7922 Fix dimensionality check (#1759)
2d6343f6cc1e47b63ef20a50d1446f6480736478 More generic grouped grid reduction kernel (#1740)
64e2b56df2c8b9fd22a362d9cc05974a8607ef3d [nvfuser] prevent spamming warning message (#77777) (#1758)
0c431624ff15b6458b9f9b674a3852373fc426b1 [nvFuser] Improving bitwise ops support (#77158) (#1757)
b93a14777fde3b9b39684b9cf1715651a806b281 Parser expand (#1754)
```

RUN_TORCHBENCH: nvfuser",pytorch
80428,z-a-f,pr,2022-06-28T06:52:57Z,[ao] Fix failing tests,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80428
* #79639
* #80111

Two commits were landed, with one changing the location of the experimental folder, and the other implementing more functionality. That caused some of the tests to import incorrect locations. This fixes the imports from torch.ao.sparsity.experimental

Differential Revision: [D37481163](https://our.internmc.facebook.com/intern/diff/D37481163/)",pytorch
80440,jjsjann123,pr,2022-06-28T10:13:52Z,nvfuser opinfo patch test_nvfuser_extremal_values_native_layer_norm_cuda,"1. patching override tolerance on `test_nvfuser_correctness_nn_functional_gaussian_nll_loss_cuda_bfloat16` for bfloat16 on A100.
2. `native_layer_norm` returns `nan` mean stats on inputs with extremal `inf`, which differs from codegen outputs.",pytorch
80491,kulinseth,pr,2022-06-29T02:10:52Z,[MPS] Move the View ops to a separate file and reduce the number of graphs created,"This is dependent on the PR to go in first: https://github.com/pytorch/pytorch/pull/79939

Remove the data_ptr from the View Graph key which reduces the number of
graphs created significantly.

Don't wait when copying from MPS to MPS tensors

",pytorch
80533,robieta,pr,2022-06-29T16:38:46Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80534,robieta,pr,2022-06-29T16:38:52Z,[Profiler] Eliminate transitive include of profiler implementation headers.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)",pytorch
80535,robieta,pr,2022-06-29T16:41:31Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80536,robieta,pr,2022-06-29T16:41:36Z,[Profiler] Eliminate transitive include of profiler implementation headers.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)",pytorch
80537,robieta,pr,2022-06-29T16:51:24Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80538,robieta,pr,2022-06-29T16:51:29Z,[Profiler] Eliminate transitive include of profiler implementation headers.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)",pytorch
80539,robieta,pr,2022-06-29T17:03:19Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80540,robieta,pr,2022-06-29T17:03:24Z,[Profiler] Eliminate transitive include of profiler implementation headers.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)",pytorch
80562,robieta,pr,2022-06-29T20:31:38Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80563,robieta,pr,2022-06-29T20:36:15Z,[DO_NOT_SUBMIT] Test Kineto update,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #80822
* #80797
* #80796
* #80810
* #80750
* #80564
* __->__ #80563

The rest of this stack relies on https://github.com/pytorch/kineto/commit/135e94c6f371d431d2b1ddb2d6ece2cecc83f030. I can't stack on top of https://github.com/pytorch/pytorch/pull/79925 so I'm making a dummy PR so the rest of the stack can run on OSS CI.

Differential Revision: [D37520165](https://our.internmc.facebook.com/intern/diff/D37520165/)",pytorch
80564,robieta,pr,2022-06-29T20:39:48Z,[Profiler] Eliminate transitive include of profiler implementation headers.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #80750
* __->__ #80564

This PR hides `torch/csrc/profiler/collection.h` and `torch/csrc/profiler/kineto_shim.h` from the public headers by forward declaring the symbols in `profiler_kineto.h` and hiding them in a unique_ptr.

I want to directly use Kineto symbols (specifically, the enums) in profiler; however without this change they leak into the PyTorch public headers and that's no good. Thus, we have to break that dependency.

Differential Revision: [D37365997](https://our.internmc.facebook.com/intern/diff/D37365997/)",pytorch
80567,z-a-f,pr,2022-06-29T21:49:34Z,[ao] Moving the sparsity/experimental to sparsity/_experimental,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80567

The experimental code in the sparsity does not have user-facing api, and should reside under the proivate package. This involves `pruner` and `base_sparsifier`.

Two commits were landed, with one changing the location of the experimental folder, and the other implementing more functionality. That caused some of the tests to import incorrect locations. This fixes the imports from torch.ao.sparsity.experimental

Differential Revision: [D37533981](https://our.internmc.facebook.com/intern/diff/D37533981/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37533981/)!

@diff-train-skip-merge",pytorch
80749,robieta,pr,2022-07-01T00:36:13Z,[Profiler] Include ActivityType from Kineto,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

We don't want to compile with Kineto on all platforms, but if we're going to have significant integration between profiler and Kineto profiler will need to be able to rely on simple API constructs like the Kineto enums.

Differential Revision: [D37228314](https://our.internmc.facebook.com/intern/diff/D37228314/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37228314/)!",pytorch
80750,robieta,pr,2022-07-01T00:39:08Z,[Profiler] Include ActivityType from Kineto,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80750

We don't want to compile with Kineto on all platforms, but if we're going to have significant integration between profiler and Kineto profiler will need to be able to rely on simple API constructs like the Kineto enums.

Differential Revision: [D37228314](https://our.internmc.facebook.com/intern/diff/D37228314/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37228314/)!",pytorch
80759,kulinseth,pr,2022-07-01T05:51:37Z,MPS: Fix handling of 1D tensors in linear backward,"Fixes #https://github.com/pytorch/pytorch/issues/79784
",pytorch
80760,kulinseth,pr,2022-07-01T05:58:43Z,MPS: Add multinomial op,"Add multinomial with replacement

",pytorch
80796,robieta,pr,2022-07-02T20:41:53Z,[Profiler] Move Kineto activity generation into `collection.cpp`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #80797
* __->__ #80796
* #80810

In order to fold kineto events into TorchTidy, the new collection machinery needs to handle moving events to and from kineto. This PR is one step in that transition. In order to limit the scope of changes, we keep metadata generation in `profiler_kineto.cpp` for now.

Differential Revision: [D37406955](https://our.internmc.facebook.com/intern/diff/D37406955/)",pytorch
80797,robieta,pr,2022-07-02T20:41:57Z,[Profiler] Handle events from Kineto in the unified result class.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81319
* #80822
* __->__ #80797
* #81965
* #81895
* #81958

At long last, `torch::profiler::impl::Result` can handle all data for the profiler. The principle change is that `collection.cpp` now extracts events that Kineto collected rather than `profiler_kineto.cpp`, and there is now an `ExtraFields<Kineto>` type added to the Result variant.

Differential Revision: [D37406956](https://our.internmc.facebook.com/intern/diff/D37406956/)",pytorch
80810,robieta,pr,2022-07-03T17:32:24Z,[Profiler] Use parent time for implicitly finished Torch ops,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #80797
* #80796
* __->__ #80810

When we implicitly mark an event as finished (for instance for user annotations which are never closed but whose parent finishes) the end time is never set. This can cause issues if we try to compute durations because the uninitialized value is less than start time and thus it makes no sense to compute a duration. This PR addresses this by allowing Torch ops which have been implicitly finished to use their parent's end time.

This issue was masked by the nanosecond to microsecond conversion: dividing the two times before subtracting keeps the numerics valid. (Even if the quantity itself is nonsensical.) It was only when I replaced
```
e->endTimeNS() / 1000 - e->start_time_ns_ / 1000
```
with
```
(e->endTimeNS() - e->start_time_ns_) / 1000
```
that UBSAN catches the issue.

Differential Revision: [D37591996](https://our.internmc.facebook.com/intern/diff/D37591996/)",pytorch
80813,robieta,pr,2022-07-03T18:45:42Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80813

Differential Revision: [D37592348](https://our.internmc.facebook.com/intern/diff/D37592348/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37592348/)!",pytorch
80815,robieta,pr,2022-07-03T20:47:01Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80815

Differential Revision: [D37592878](https://our.internmc.facebook.com/intern/diff/D37592878/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37592878/)!",pytorch
80816,robieta,pr,2022-07-03T20:58:45Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80816

Differential Revision: [D37592935](https://our.internmc.facebook.com/intern/diff/D37592935/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37592935/)!",pytorch
80817,robieta,pr,2022-07-03T21:02:33Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80817

Differential Revision: [D37592961](https://our.internmc.facebook.com/intern/diff/D37592961/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37592961/)!",pytorch
80818,robieta,pr,2022-07-03T21:07:40Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80818

Differential Revision: [D37592975](https://our.internmc.facebook.com/intern/diff/D37592975/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37592975/)!",pytorch
80819,robieta,pr,2022-07-03T21:16:41Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80819

Differential Revision: [D37593013](https://our.internmc.facebook.com/intern/diff/D37593013/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37593013/)!",pytorch
80820,robieta,pr,2022-07-03T21:26:40Z,[TESTING] [skip ci] ghexport bidirectional smoketest,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #80820

Differential Revision: [D37593052](https://our.internmc.facebook.com/intern/diff/D37593052/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37593052/)!",pytorch
80822,robieta,pr,2022-07-03T22:37:54Z,[Profiler] Clean up visit logic,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #82584
* #81322
* #81321
* #81320
* #81319
* __->__ #80822

It's rather tedious to constantly have to specify `extra_fields_` in visit; especially since it tends to add a line. The `DEFINE_VISITOR` logic was also getting rather unwieldy and hard to read. Positional arguments in macros are quite bug prone. I think the new way is clearer.

Differential Revision: [D37481560](https://our.internmc.facebook.com/intern/diff/D37481560/)",pytorch
80922,jeffdaily,pr,2022-07-05T23:46:47Z,[ROCm] unskip external streams tests,These two tests are passing for ROCm 5.1.1 and 5.2.,pytorch
81122,robieta,pr,2022-07-08T18:08:19Z,"Back out ""Revert D37228314: [Profiler] Include ActivityType from Kineto""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81122

Reland

Differential Revision: [D37720837](https://our.internmc.facebook.com/intern/diff/D37720837/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37720837/)!",pytorch
81125,jeffdaily,pr,2022-07-08T18:19:50Z,[ROCm] unskip test_fx tests,,pytorch
81134,jjsjann123,pr,2022-07-08T20:03:33Z,Nvfuser opt in for decomposition,"Regarding issues reported in #79246, we notice that bias decomposition from conv/linear could actually hurt perf, due to the overhead of compilation. This PR changes it to make decomposition an explicit opt-in from user to avoid these regressions.",pytorch
81149,z-a-f,pr,2022-07-08T23:13:38Z,[ao] Moving the sparsity/experimental to sparsity/_experimental,"The experimental code in the sparsity does not have user-facing api,
and should reside under the proivate package. This involves pruner and
base_sparsifier.
",pytorch
81258,jjsjann123,pr,2022-07-11T18:02:35Z,fixing call_module on subscripting into generator,"named_modules() return a generator, which is not subscriptable and causes node support query to fail",pytorch
81281,jeffdaily,pr,2022-07-11T21:46:45Z,[ROCm] unskip test_jit TestBackendsWithCompiler,,pytorch
81290,kulinseth,pr,2022-07-11T23:24:11Z,[MPS] Handle 1D inputs for NLL,"* Add test for NLL 1d
* Fix forward NLL for 1D case
* Handle NLL backward for 1d
",pytorch
81303,kulinseth,pr,2022-07-12T01:33:37Z,[MPS]: Added op upsample_nearest1d,"Fixes #ISSUE_NUMBER
",pytorch
81312,z-a-f,pr,2022-07-12T07:09:34Z,Modules under migration in the public binding test,"If a module is being migrated, a common practice is to temporarily support
the old location. That might break the assertion that the `__module__`
of a function is pointing to the same location as where it is created.

 ## Example

1. Assume there is `torch/nn/quantized/functional.py`
2. The file is copied to `torch/ao/nn/quantzied/functional.py`
3. The old location is changed to have `from torch.ao.nn.quantized.functional import *`

In such a situation, importing from the old location will have `__module__`
pointing to the new `torch/ao/nn/...` location. This will break the
current test.

 ## What changed

This PR adds the following:

1. Added a key `""being_migrated""` to the `allowlist_for_publicAPI.json`
2. Added a check in the `test_public_bindings.py` to check if the JSON file has the `""being_migrated""` key.

 ## How to add migration entries

1. Add an entry to the `""being_migrated""`
   For the example above, add `""torch.nn.quantized.functional"": ""torch.ao.nn.quantized.functional""`
2. Change any existing keys for the old location
   For example, if there is an existing entry `""torch.nn.quantized.functional"": [...]`
   outside the `""being_migrated""`.
   Change it to `""torch.ao.nn.quantized.functional"": [...]`",pytorch
81314,z-a-f,pr,2022-07-12T07:21:59Z,Modules under migration in the public binding test,"If a module is being migrated, a common practice is to temporarily support
the old location. That might break the assertion that the `__module__`
of a function is pointing to the same location as where it is created.

 ## Example

1. Assume there is `torch/nn/quantized/functional.py`
2. The file is copied to `torch/ao/nn/quantzied/functional.py`
3. The old location is changed to have `from torch.ao.nn.quantized.functional import *`

In such a situation, importing from the old location will have `__module__`
pointing to the new `torch/ao/nn/...` location. This will break the
current test.

 ## What changed

This PR adds the following:

1. Added a key `""being_migrated""` to the `allowlist_for_publicAPI.json`
2. Added a check in the `test_public_bindings.py` to check if the JSON file has the `""being_migrated""` key.

 ## How to add migration entries

1. Add an entry to the `""being_migrated""`
   For the example above, add `""torch.nn.quantized.functional"": ""torch.ao.nn.quantized.functional""`
2. Change any existing keys for the old location
   For example, if there is an existing entry `""torch.nn.quantized.functional"": [...]`
   outside the `""being_migrated""`.
   Change it to `""torch.ao.nn.quantized.functional"": [...]`

Fixes #ISSUE_NUMBER
",pytorch
81319,robieta,pr,2022-07-12T14:59:11Z,[Profiler] Make KinetoEvent a view of Result (Part 1: trivial fields),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #82584
* #81322
* #81321
* #81320
* __->__ #81319

Now that all data in the profiler flows through `torch::profiler::impl::Result` it is no longer necessary for `KinetoEvent` to maintain its own storage. Setting the fields on `KinetoEvent` has been a source of great consternation. It's very easy to forget to set a field for a particular case, and very hard to reason about what fields need to be set when.

We have to continue to support `KinetoEvent` because existing profiler machinery relies on it and (I think) it is part of the public API. That said, making it a simple view into a `Result` makes the semantics of each field much more clear.

This PR moves the fields which are simple views into one EventType (mostly TorchOp) and uses a default value elsewhere. Later PRs will tackle the thornier aspects.

Differential Revision: [D37481559](https://our.internmc.facebook.com/intern/diff/D37481559/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37481559/)!",pytorch
81320,robieta,pr,2022-07-12T14:59:16Z,"[Profiler] Make KinetoEvent a view of Result (Part 2, python and stacks)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #82584
* #81322
* #81321
* __->__ #81320
* #81319

The semantics of python in KinetoEvent are sort of crazy because values can come either from metadata captured by JIT or from the python tracer. Things aren't helped by the fact that we encode the python hierarchy in the chrome trace for tensorboard, so we have to do a tree traversal with skips based on types.

By simply constructing the Python stack in KinetoEvent's ctor we're able to skip a lot of the complexity in `EventFieldsVisitor` and just lean on `KinetoEvent.stack()`.

Differential Revision: [D37481561](https://our.internmc.facebook.com/intern/diff/D37481561/)",pytorch
81321,robieta,pr,2022-07-12T14:59:22Z,"[Profiler] Make KinetoEvent a view of Result (Part 3, forwarded from `result_`)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #82584
* #81322
* __->__ #81321
* #81320
* #81319

`Result` already handles some common fields. (time, name, etc.) For those we can simply forward the `KinetoEvent` call to the underlying `result_`. (Modulo minor transforms.)

Differential Revision: [D37481558](https://our.internmc.facebook.com/intern/diff/D37481558/)",pytorch
81322,robieta,pr,2022-07-12T14:59:33Z,"[Profiler] Make KinetoEvent a view of Result (Part 4 (final), stragglers)","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #82584
* __->__ #81322
* #81321
* #81320
* #81319

This PR just moves all the KinetoEvent methods which didn't fit the previous categories. Now that we no longer need to set Kineto event fields in `EventFieldsVisitor` we can remove the reference wrapper and rename the visitor to `AddKinetoMetadata` since that's all it does now.

Differential Revision: [D37490053](https://our.internmc.facebook.com/intern/diff/D37490053/)",pytorch
81338,kulinseth,pr,2022-07-12T17:49:06Z,[MPS] Reduce the number of command_buf created and improve performance,The PR improves performance and reduces the CPU overhead by reducing the number of command buffers created. It uses commit and continue feature in MPS.,pytorch
81418,jjsjann123,pr,2022-07-13T19:11:10Z,Cudnn conv cache key patch,"Fixes #81106 

Patches on cudnn algo cache to consider the right memory_format used in descriptors, instead of blindly copy the memory_format on inputs.
Note that to be on the safe side, we could actually cache on all tensor strides instead. But given how we short-cut and align memory_format from pytorch tensor to cudnn descriptor, it suffice to have a single field in the cache.
",pytorch
81450,robieta,pr,2022-07-14T01:55:27Z,"Back out ""Revert D37720837: Back out ""Revert D37228314: [Profiler] Include ActivityType from Kineto""""","Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81450

Differential Revision: [D37842341](https://our.internmc.facebook.com/intern/diff/D37842341/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37842341/)!",pytorch
81451,robieta,pr,2022-07-14T02:03:52Z,[Trivial] Delete C10_DECLARE_TLS_class_static and C10_DEFINE_TLS_class_static,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81453
* #81452
* __->__ #81451

AFAICT only C10_DEFINE_TLS_static is actually used anywhere.

Differential Revision: [D37842248](https://our.internmc.facebook.com/intern/diff/D37842248/)",pytorch
81452,robieta,pr,2022-07-14T02:03:56Z,Make C10_DEFINE_TLS_static use a template functor rather than a lambda. (For non C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE cases),"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81453
* __->__ #81452
* #81451

There is a non-trivial cost to storing an accessor lambda as a pointer and calling it at each access: https://godbolt.org/z/s8zf19TsM By contrast, if we pass a trivial struct then the compiler can inline and optimize everything away.

It seems that roughly 4% of the time to make an empty Tensor comes from this overhead when checking whether to profile memory in the allocator. That said, it's hard to get a precise estimate (even with multiple runs and 50+ replicates). But the benchmark does confirm that the extra instructions actually matter.

Differential Revision: [D37842249](https://our.internmc.facebook.com/intern/diff/D37842249/)",pytorch
81453,robieta,pr,2022-07-14T02:04:01Z,Optimize ThreadLocalDebugInfo lookup,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81453

The way we do ThreadLocalDebugInfo lookup is to walk along a chain of shared_ptrs. This isn't particularly efficient since we expect reads to overwhelming dominate the API usage. (Push an observer, run millions of ops, pop the observer.)

This PR introduces a very simple lookup table so that `ThreadLocalDebugInfo::get` becomes trivial. The impact on the other methods is minimal; the simply do the chain walk that `get` was doing. This saves another ~4% on `empty`.

The one case where this may increase overhead is thread migration. (e.g. jit fork) I'm not particularly concerned since rebuilding the lookup table is cheap. (In general `debug_info` should not be deep.) And if we find that that's not the case, we can simply migrate the lookup table along with the TLS.

Differential Revision: [D37842246](https://our.internmc.facebook.com/intern/diff/D37842246/)",pytorch
81461,jjsjann123,pr,2022-07-14T09:18:36Z,Bump nvfuser executor lru cache max size,"default 128 cache size has been causing no cache hit on some benchmark results with more than 128 partition. Bumping up to a more reasonable cache size.
Note that the simple LRU_CACHE doesn't give us any reuse of repetitive pattern, but that shouldn't be of much issue in our next iteration of nvfuser python API.

script for running benchmarks vvv
https://github.com/SherlockNoMad/NvFuserSample
",pytorch
81480,kulinseth,pr,2022-07-14T18:26:39Z,[MPS] Handle Boolean inputs in the cat op,"Fixes https://github.com/pytorch/pytorch/issues/80850
",pytorch
81519,kulinseth,pr,2022-07-14T23:41:32Z,[MPS] Handle 1D bias for addmm,Fixes https://github.com/pytorch/pytorch/issues/80288,pytorch
81546,jjsjann123,pr,2022-07-15T06:02:40Z,Upstream push 0714,nvfuser code bump~ CI smoke test,pytorch
81583,zasdfgbnm,pr,2022-07-15T23:52:20Z,UCC PG build in CI,"- Modifies the current cmake build definitions to use `find_package` to find UCX and UCC installed in the system
- Install UCX and UCC in CUDA dockers
- Build PyTorch with `USE_UCC=1` in pipelines
- Currently, we are not running unit tests with the UCC PG. Those tests will be added in future PRs.",pytorch
81672,d4l3k,pr,2022-07-19T00:34:45Z,libtorch: exclude from libomnibus to support multipy usage from pybind,"Summary: When libtorch is bundled into libomnibus all of the symbols are marked as unexported which causes issues when deploy/multipy tries to link in a subinterpreter at runtime. This excludes `libtorch` and `ATen-core` from libomnibus so the symbols remain exported and available.

Test Plan:
stacked diff

```
buck2 test @//mode/opt -c python.package_style=inplace //multipy/runtime:test_deploy_from_python
```

Differential Revision: D37946374

",pytorch
81730,kulinseth,pr,2022-07-19T21:21:47Z,[MPS] Get the correct size of the view tensor when copying from cpu to mps ,"Fixes: https://github.com/pytorch/pytorch/issues/81567, https://github.com/pytorch/pytorch/issues/80844
* Get the correct size of the view tensor when copying from cpu to mps

* Use 'computeStorageNbytesContiguous' to get the size just when src is a view

* Add asserts and tests to check for storage_offset 

* Add testcase for https://github.com/pytorch/pytorch/issues/80844

* Replace assert_allclose with assertEqual

* Replace TORCH_CHECK with TORCH_INTERNAL_ASSERT",pytorch
81735,kulinseth,pr,2022-07-19T22:12:14Z,[MPS] Fixes for MPS testConsistency,"Fixes #ISSUE_NUMBER
",pytorch
81791,zasdfgbnm,pr,2022-07-20T18:15:57Z,New TORCH_UCC_BLOCKING_WAIT env variable,"Cherry-pick of https://github.com/facebookresearch/torch_ucc/pull/95.

I recommend waiting until https://github.com/pytorch/pytorch/pull/81583 is merged first, so the CI is checking if this PR compiles correctly.

Marking this as a draft for now, will change to ""ready for review"" once https://github.com/pytorch/pytorch/pull/81583 merged.",pytorch
81792,jjsjann123,pr,2022-07-20T18:16:31Z,Type inference patch (#1848),"Fixes type inference issue where update is written to a temporary optional holder, other than the updating the graph.

Fixes CI failure in #81725 
",pytorch
81820,jjsjann123,pr,2022-07-20T22:02:39Z,disable partition for aten ops without torch._refs decomposition,"With current partitioning logic, we would lower nodes that has no reference implementation into nvfuser partition, which can't be handled by `strictly_nvfuser`. (reference of executors #81043).

Note that once we added the short-cut of list of ops with `impl_nvfuser` outside of torch._refs. we would also need to put them in the partitioner allow list. (or alternatively, add them in aten2prim_decomp (could use a better name)).",pytorch
81861,jjsjann123,pr,2022-07-21T10:11:29Z,[NVFuser] Upstream push 0714,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #81861

Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/

Code changes includes:

- codegen improvements:
  1. Indexing refactor -> Remove reference tensor in predicate indexing logic
  2. MMA Rfactor support for cross-warp and cross-CTA split on K dimension
  3. Grouping grid allreduces across iterations
  4. Swizzle op formulation for non-affine swizzles
  5. Use scheduler_utils to cache inputs and outputs in schedulePointwise
- scheduler refactor
  1. New compute at interface
- transformation propagation refactor on MaxInfoSpanningTree
  1. Added sibling path that is required to generate consistent replay for some cases where `MaxInfoSpanningTree` is used with a selector.
  2. Optimization to skip Transform propagator
  3. SpanningTreePrinter for debugging
- parser update
  1. Fixes `div`
  2. Added `_to_copy`
  3. Broadcast in dim with expand to support expanding to concrete size
  4. Dropout prob extremal patch
- executor patch on caching strides for output allocation

Squashed commits to WAR github API
Commits that's actually in this PR from the devel branch:

```
3b87896706fc98aa4d5b5c811af034cc4dddfbab Fix allocation of work buffers and `fused_reduction::ParallelReduce` with unswitch (#1818)
4cae1227f666b68d275144afd6e4be1fa7aa0786 schedulePointwise cleanup: - computeAt + InlinePropagator (#1815)
3df97426adfb5ecc6fe2c12c43d56d59670e5020 Use scheduler_utils to cache inputs and outputs in schedulePointwise (#1811)
03180aa8facde51237dffa29f6632ffa870cf923 improve broadcast resolution (#1792)
bee6c69979d8c34d6d6ef7514f8886cf1416d64f bug fix (#1819)
4413c8f43a5a64dd0a6ddb0763523bbc7314f4b5 Support PYTORCH_NVFUSER_DUMP=transform_propagator (#1812)
de6b7ca5ce755061ae0d26e006c4403653627ab5 Fix negative position in InlinePropagator (#1813)
10a996cb4dce5d514f09fd0d49ffcd3b88869a28 Remove redundant check in schedulePointwise (#1810)
acd5ed4df825d4c25999e8c9041e0f8ca1a3448f Swizzle op formulation for non-affine swizzles (#1441)
3ed8330f881f429fe2df0e5af9000b91355a96da Kernel args patch to show zero_init buffer (#1809)
037a75a42048f1d8a9c30efb466f1ffbfd2894ad Dropout prob extremal patch (#1804)
282c42902bff07f759cddbbe619249cf5e7c5281 spam nvrtc options (#1783)
3ba6a5fe0a47044179cd36b5b62e628c75180da5 Broadcast in dim with expand (#1794)
fd4be1236ddfeb31ca0659e1b0df36546424c979 remove dead indexing code (#1806)
fa4e6a4739a9daaa0e4111fb4730704d79c91010 Check siblings in getMaxPosAll (#1805)
025c840c76d89b0d032b65a78a375719cab78d46 Grouping grid allreduces across iterations (#1755)
37c579e64f8145fc292273cdebb6519edeb9cf76 Temporarily disable test requring large shared memory. (#1802)
5f375d074524ab65cb78282eff7abe5846cc4203 More cleanup on InlinePropagator (#1800)
8d384da0cfb50a7c5082e91585c12f4c3a775e6c Indexing refactor stage 2 : Remove reference tensor in predicate indexing logic (#1784)
f008140e26335584a143f71c2cb9e91fd61ec530 MMA Rfactor support for cross-warp and cross-CTA split on K dimension (#1554)
76b3cca5cc9a18a56db8107d2f6c8e94851bb85c Add parsing support for `_to_copy` to handle AMP casts. (#1756)
ef04f6c4c0ee043979ac7aad4e5be6f22faeb547 Coding style cleanups (#1798)
38c7f3cf69ea58cc9480b0621506bbfd90a7c9d3 InlinePropagator please don't replay (#1797)
3f2c263ade35017be2d99fe8e4ec97fd0f14f754 validateDomain in TransformPropagator (#1796)
c07708520d99ef815ce15ec367bf7e98797d602b Use TransformPropagatorWithCheck in many tests (#1795)
d0d0908aee2e2b7615c28d04ee80a54b01a02bcd Some further cleanup for the new computeAt interface (#1793)
45f5203b5744cd3512d83263b3fb07c99795a271 Fix TransformReplay::getMatchedLeafPosWithoutReplay* (#1791)
28cbaf931870086cf59807dd60ce412d6dfad0fd New compute at interface (#1743)
635ebfc79bc016eea94d4cbde2c12324171b908b Add SpanningTreePrinter (#1786)
59f3c3223c48ea89549fe7d323f17cbecbebede0 Output allocate patch (#1790)
fe93bf5a6485696ffb36751606a84080349967b5 Transform propagator skip replay when possible (#1782)
ebf23a50f3adf3d28e824c3b3b4ed6ea6f9cf483 Fix isIntegralType error msg (#1789)
0c82ecf04d12b9fe5428af6824a7a978cf5e0ddd Disable register reuse across serial broadcast ops (#1787)
33a824d8d9ace7790a4a58d497e525a7a059579d Adding sibling path for MaxInfoSpanningTree (#1776)
86f46aad83cbb2aa06943419a7335d71a8798f2a Fix div(Val, TensorView) (#1778)
d3de227ade763bdac9e9df15ba8671be78565ee9 Fix FusionMaxRootDomainInfoSpanningTreePrintTwice_CUDA (#1781)
ecc7a87cdaaed66672d08bf819ad58d2980384cb Extend mma dimension and layout checking to support strided batched matmul and tensor contractions (#1761)
```

RUN_TORCHBENCH: nvfuser

Differential Revision: [D38043938](https://our.internmc.facebook.com/intern/diff/D38043938)",pytorch
81895,robieta,pr,2022-07-21T16:47:44Z,[Profiler] Improve tree testing,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81319
* #80822
* #80797
* #81965
* __->__ #81895
* #81958

In the course of trying to get https://github.com/pytorch/pytorch/pull/80797 out the door, I've wound up making a number of minor tweaks to `test_profiler_tree` which collectively have made it much easier to debug. It's also revealed what seems to be a correctness issue with how profiler assigns lineage on certain platforms. So I've decided to pull those testing improvements into a standalone PR.

Differential Revision: [D38038122](https://our.internmc.facebook.com/intern/diff/D38038122/)",pytorch
81951,kulinseth,pr,2022-07-21T23:42:03Z,[MPS]  Perf fixes.,"Fixes https://github.com/pytorch/pytorch/issues/81610
* Use fillBuffer() for zero_mps()
Fix minor bug in add_sub_template() with value=0.0
Change default value of use_scalar_value to false in getTensorsStringKey()

* Fallback to fill_scalar_mps() if buffer isn't contiguous.

* Fix high memory consumption in view ops

* Change commitAndWait to Commit in View Ops",pytorch
81958,robieta,pr,2022-07-22T00:38:04Z,[Profiler][Python tracer] Add ephemeral inputs to the value cache.,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81319
* #80822
* #80797
* #81965
* #81895
* __->__ #81958

There are a couple of bugs in the python tracer related to how we cache values. The first is that `ValueCache::store<CallType::PyModuleCall>` wrongly assumes that it will only be called from the profiling callback and calls `PyEval_GetFrame`, effectively violating the encapsulation of the cache by accessing global state. Secondly, we use `arg` to cache bound C functions. This turns out not to be correct, and collisions are resulting in incorrect traces.

In both cases, we can solve the problem by introducing a concept of ephemeral data which is used to materialize a cached value, but is not part of the cache key. (And the author is responsible for making sure that is done correctly.)

Differential Revision: [D38062921](https://our.internmc.facebook.com/intern/diff/D38062921/)",pytorch
81965,robieta,pr,2022-07-22T05:47:49Z,[Profiler] Add `tag` property to `Result`,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #81319
* #80822
* #80797
* __->__ #81965
* #81895
* #81958

With a bit of template deduction we can make the variant tell us what type it is, and then we don't have to rely on (and maintain) a bunch of `isinstance` checks in Python.

Differential Revision: [D38066829](https://our.internmc.facebook.com/intern/diff/D38066829/)",pytorch
82002,jeffdaily,pr,2022-07-22T17:56:17Z,[ROCm] Enable MIOpen fused convolution relu,"Adds MIOpen fused convolution relu for fp32 and contiguous memory format.  Adds fallbacks for conv + z + bias + relu, fp16, and channels last until MIOpen adds these features.

cc @sunway513 @jithunnair-amd @ROCmSupport @KyleCZH",pytorch
82022,z-a-f,pr,2022-07-22T21:04:19Z,being_migrated modules use the original allowlist,"The `allowlist_for_publicAPI.json` allows specifying the modules that
are being migrated. However, the exceptions in that file are only
applied to the original entry. This introduces a change to the
`test_correct_module_names` to extend the `allow_dict` with the modules
that are being migrated.

 ## Example Scenario

Assume there is an ""allow list"" for some module `torch.foo`:

```json
{
    ""torch.foo"": [
        ""Any"",
        ""Optional"",
    ]
}
```

Assume that the module is also being migrated to `torch.bar`, with
a `*` import in the original location (s.a. `from torch.bar import *`)

```json
{
    ""being_migrated"": {
        ""torch.foo"": ""torch.bar""
    },
    ""torch.foo"": [
        ""Any"",
        ""Optional"",
    ],
    ""torch.bar"": [
        ""Any"",
        ""Optional"",
    ],
}
```

In that case, both `torch.foo` and `torch.bar` must have the same list
of exceptions. One way to do it, is to enforce the developers to add
new ""allow list"" to the JSON file for the migrations. As an alternative
this PR just creates a duplicate entry to support exceptions in both
`torch.foo` and `torch.bar`.

With this PR, we don't need to modify anything beyond the `being_migrated` list:

```json
{
    ""being_migrated"": {
        ""torch.foo"": ""torch.bar""
    },
    ""torch.foo"": [
        ""Any"",
        ""Optional"",
    ],
}
```

",pytorch
82183,kulinseth,pr,2022-07-25T23:48:33Z,[MPS] Handle int inputs of matmul ops by returning error for unsupported data types,"This is in-continuation of fixes for TestConsistency for MPS backend. 

* Add error messages for unsupported matmul ops

* Add error handling for int inputs for linear op

### Description
<!-- What did you change and why was it needed? -->

### Issue
<!-- Link to Issue ticket or RFP -->

### Testing
<!-- How did you test your change? -->
",pytorch
82187,jeffdaily,pr,2022-07-26T00:06:22Z,move ROCmBackwardPassGuard from autograd engine.cpp to function.h,"This moves the ROCmBackwardPassGuard back to its previous, verified location.",pytorch
82258,jeffdaily,pr,2022-07-26T21:06:22Z,[ROCm] parse rocm version during hipify,"### Description
The hipify mappings file now parses the rocm version header file and can use this information to conditionalize the mappings.  This is necessary while rocm packaging matures.

### Issue
#80849 updated the hipify mappings, but it wasn't backward compatible with ROCm versions prior to 5.2.

### Testing
Verified by building rocm pytorch using both the rocm 5.1 and 5.2 dockerfiles.
",pytorch
82353,jeffdaily,pr,2022-07-27T19:22:38Z,[ROCm] update nightly builds to rocm5.2,"### Description
Updates ROCm arches for nightly binaries to 5.1.1 and 5.2, dropping 5.0.

Corresponding builder PR:
https://github.com/pytorch/builder/pull/1033

### Testing
PR CI with extra label to enable binary builds.",pytorch
82366,kulinseth,pr,2022-07-27T21:01:57Z,[MPS] Add MPS implementation for constant_pad_nd() (#75),"MPS has a native implementation of the constant pad nd. Adding that instead of going through the view ops helps improve performance in several benchmarks in torchbench.
",pytorch
82498,jeffdaily,pr,2022-07-29T21:23:01Z,[ROCm] enable nvfuser,"### Description
The nvfuser is enabled for ROCm.

### Testing
CI label ciflow/trunk covers the newly enabled ROCm functionality as well as any CUDA regressions caused by these changes.
",pytorch
82505,kulinseth,pr,2022-07-29T22:21:13Z,[MPS] Remove checks that incur unnecessary syncs on GPU with tensor.item(),"

### Description
<!-- What did you change and why was it needed? -->

### Issue
<!-- Link to Issue ticket or RFP -->

### Testing
<!-- How did you test your change? -->
",pytorch
82507,kulinseth,pr,2022-07-29T22:26:05Z,[MPS] Register index.Tensor_out,"* Add more tests from test_indexing into test_mps
* Cache the indexing library on the MPSDevice",pytorch
659,ppwwyyxx,pr,2015-12-31T21:39:05Z,Fix typo,"Fix typo
",tensorflow
662,ppwwyyxx,pr,2016-01-01T01:20:57Z,optimize slice_input_producer,"When shuffle=False, input_slice_producer can be implemented directly with FIFOQueue.
The original implementation using range and array indexing is slower.

I could pass the unit test `input_test.py`. But please correct me if I could use the name and `set_shape` in a better way (i'm not totally sure).

I simply run it on my old laptop on CPU and I saw `input_slice_producer + batch` operations (which is a no-op) introduces no overhead, while with the original implementation I see obvious overhead.
",tensorflow
951,ppwwyyxx,pr,2016-02-01T16:46:12Z,fix gradient of tf.floor,"Fix #897 .
",tensorflow
1527,ppwwyyxx,pr,2016-03-16T14:47:52Z,Typo?,,tensorflow
2179,ppwwyyxx,pr,2016-04-30T20:42:28Z,fix typo,,tensorflow
3242,ppwwyyxx,pr,2016-07-08T23:47:56Z,fix nightly download links in readme,"I grabbed these links from [build history](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/).  Fix #2939 
",tensorflow
5315,z-a-f,pr,2016-11-01T00:53:42Z,Add synthetic datasets,"This is only one possible synthetic dataset. Following methods added + sample code:

```python
import tensorflow as tf

# Create synthetic data:
## Generic make_synthetic wrapper routine
circles = tf.contrib.learn.datasets.make_synthetic(name='circles', n_samples=1000, noise=0.1)
```

Files modified/added:
- `/.gitignore`: **/tools/git/gen is not necessary to commit (for dev only)
- `/tensorflow/contrib/learn/python/learn/datasets/__init__.py`: Added the support for `make_synthetic` and `split_data`
- `/tensorflow/contrib/learn/python/learn/datasets/base.py`: Added new method `split_into_train_and_test`
- `/tensorflow/contrib/learn/python/learn/datasets/synthetic.py`: Created a file to hold all synthetic generators",tensorflow
5418,z-a-f,pr,2016-11-05T22:01:51Z,Pylint `disable` fix,"I have added `# pylint: enable=wildcard-import` wherever I found an unclosed `disable`, but only for the `wildcard-import`. There are many more for other parameter, including the custom definitions (`g-...` parameters).",tensorflow
5855,zuoxingdong,pr,2016-11-25T10:13:42Z,[Tutorial->MNIST]`write_version=tf.train.SaverDef.V2` is already the default setting i…,…n latest TF version,tensorflow
5856,zuoxingdong,pr,2016-11-25T10:47:12Z,"[Anaconda Installation Tutorial]As in issue #5832, pip installed TF s…","…hould be uninstalled if one want to use TF with Anaconda environment, because Anaconda will search system site-packages with higher priority, this might result in the latest upgrade of TF within Anaconda environment inaccessible.",tensorflow
5875,zuoxingdong,pr,2016-11-26T20:42:13Z,[TF-Slim]The latest version has `write_version=saver_pb2.SaverDef.V2` as default set…,…ting. And V2 is more efficient than V1.,tensorflow
5903,zuoxingdong,pr,2016-11-28T15:04:59Z,typo `Conv` to `conv2d`,,tensorflow
5906,zuoxingdong,pr,2016-11-28T15:32:46Z,No file exists called preprocess.py,"It has been quite some time without this file existing, if there is actually a plan to work on this functionality in  Slim, then feel free to close this issue. ",tensorflow
5957,zuoxingdong,pr,2016-11-29T23:00:16Z,[TF-Slim README] A few typos and latest APIs,,tensorflow
5976,zuoxingdong,pr,2016-11-30T11:53:24Z,More similar typo to #5973,,tensorflow
6014,zuoxingdong,pr,2016-12-01T16:13:42Z,"[loss_op]Denote the returned value is already mean, it happens quite a few tim…",…es that we often add tf.reduce_mean after that since tf.nn.sparse... returns loss without averaging it.,tensorflow
6090,zuoxingdong,pr,2016-12-05T17:36:36Z,"Fix Python 3 dict object to list, described in issue #5488","It is also compatible with Python 2, converting a list to a list is a list.",tensorflow
6127,ppwwyyxx,pr,2016-12-06T20:48:41Z,Clarify about 'SAME' padding in docs,Fix #5203. Also use `//` instead of `/` to be consistent with Python2/3.,tensorflow
6173,ppwwyyxx,pr,2016-12-08T00:20:50Z,Fix bug in docs of summary.merge,"From the code, the default collection of `summary.merge` should be `[]`.",tensorflow
6359,zuoxingdong,pr,2016-12-16T10:37:01Z,Additional change to pull request #6014,Additional change to pull request #6014 ,tensorflow
6433,z-a-f,pr,2016-12-21T05:52:20Z,Add synthetic datasets fix,"This fixes #5314 and #5315  that was reverted by #6288 by re-introducing a fix by @comicchang in  #6283 . Fix #6283 was not merged as it was submitted after #6288 reverted it.  

The error was that `import tensorflow as tf` is not allowed (sorry didn't know that).
I think I checked everything, lgtm

@ilblackdragon @caisq @martinwicke 

",tensorflow
6556,z-a-f,pr,2016-12-29T07:29:41Z,[minor] Silenced the 'pushd',"It doesn't fix the #6555, it just silences the `pushd` during the `./configure` step.",tensorflow
6872,z-a-f,pr,2017-01-16T06:11:15Z,"Added possible responses for uname on Windows, fixes #6871","According to https://en.wikipedia.org/wiki/Uname#Examples, other responses
are possible on Windows, added mingw, cygwin, and uwin",tensorflow
7037,ppwwyyxx,pr,2017-01-24T17:04:35Z,Replace tf.mul by tf.multiply in docs.,,tensorflow
7086,ppwwyyxx,pr,2017-01-26T03:48:21Z,make weighted_cross_entropy consistent with sigmoid_cross_entropy.,"Some relevant discussions in #6700.
Since Python doesn't allow positional arguments to come after keyword arguments, this PR will enforce `pos_weight` to be passed as named arguments as well.
But maybe it's still better to throw error than to fail silently.",tensorflow
7273,ppwwyyxx,pr,2017-02-05T15:13:02Z,fix docs formatting of tf.while_loop,"The [docs](https://www.tensorflow.org/api_docs/python/control_flow_ops/control_flow_operations) of tf.while_loop were wrongly formatted:
![0205-23 11 37](https://cloud.githubusercontent.com/assets/1381301/22627166/24fd63aa-eb83-11e6-8170-a535a2a8a882.png).

By looking at similar docs in `tf.cond`, this seems to be the fix.",tensorflow
7442,zasdfgbnm,pr,2017-02-11T21:02:40Z,Add a bit more descriptions on adding an op?,"Hi,

I'm trying to add my custom op and feeling that the document doesn't give enough information on how it works. I struggled a lot and finally I'm able to figure things out by reading the code of some header files.

Can anyone take a look if the changes to the document in this PR will make it better for new people?",tensorflow
7603,ppwwyyxx,pr,2017-02-17T03:33:19Z,fix typo,,tensorflow
8980,ppwwyyxx,pr,2017-04-05T14:08:56Z,Update adding_an_op.md,The issue (#1569) is related to gcc 5 and also above.,tensorflow
9054,zasdfgbnm,pr,2017-04-07T20:17:02Z,remove unused include,,tensorflow
9063,zasdfgbnm,pr,2017-04-08T06:20:13Z,Ops and kernels for reduce_slice_ops,"This is a PR for https://github.com/tensorflow/tensorflow/issues/7662

This PR implements `reduce_slice_sum`, `reduce_slice_prod`, `reduce_slice_max`, `reduce_slice_min`.

There are also tests for these new ops.

No gradients are implemented now, but I will start working on that after this PR is merged.",tensorflow
9452,zasdfgbnm,pr,2017-04-26T04:36:40Z,add support for flat both inner and outer dims,"Hi,

I'm trying to flat both the inner and outer dims and only keeps the middle ones but I didn't find any existing method in tensorflow that support this.  So I implement one. Hope this is helpful and can be merged into tensorflow.

A brief changelog:

1. add new method `flat_inner_outer_dims`
2. change private method `ComputeFlatInnerDims` and `ComputeFlatOuterDims` to static member
3. add tests for `flat_inner_outer_dims`",tensorflow
9491,zasdfgbnm,pr,2017-04-27T16:10:15Z,make configure smarter in detecting cuda,,tensorflow
9492,zasdfgbnm,pr,2017-04-27T16:41:25Z,make configure smarter in detecting cuda,,tensorflow
10032,zasdfgbnm,pr,2017-05-19T13:27:23Z,"add Cuda{2D,3D}LaunchConfig that maximizes occupancy",Add `Cuda2DLaunchConfig` and `Cuda3DLaunchConfig` that uses `cudaOccupancyMaxPotentialBlockSize` to calculate the best kernel launch parameters.,tensorflow
10068,zasdfgbnm,pr,2017-05-21T02:21:10Z,"InferenceContext::UnknownShapeOfRank support unknown rank, check rank>=0",,tensorflow
10070,zasdfgbnm,pr,2017-05-21T04:18:27Z,clear compiler warnings in tensor_format.h,,tensorflow
10235,zasdfgbnm,pr,2017-05-27T00:47:07Z,fix the return value of Tensor::flat_inner_outer_dims,The const version of `flat_inner_outer_dims` should return `ConstTensor` instead of `Tensor`,tensorflow
10280,ppwwyyxx,pr,2017-05-29T23:41:34Z,Compare base_dtype instead of dtype in piecewise_constant,Compare base_dtype instead of dtype in piecewise_constant. Fix #10086 ,tensorflow
10995,zasdfgbnm,pr,2017-06-22T20:04:00Z,Use native compute capabilities as default,"Instead of using 3.5 and 5.2, would it be better to use compute capabilities of native GPUs?",tensorflow
10999,zasdfgbnm,pr,2017-06-23T01:02:02Z,pass -O options when generating dependency,"In the current implementation, when generating the dependency in [L126](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl#L216), the optimization option is not passed to nvcc, which makes nvcc to generate a lot of warnings that looks like:

```
_FORTIFY_SOURCE requires compiling with optimization (-O)
```
This PR clear these warnings by passing optimization option when generating dependency.

The following issues are fixed by this PR: 
https://github.com/tensorflow/tensorflow/issues/9149
https://github.com/tensorflow/tensorflow/issues/2153",tensorflow
11393,zasdfgbnm,pr,2017-07-09T13:50:30Z,fix broken link in adding_an_op.md,,tensorflow
11394,zasdfgbnm,pr,2017-07-09T15:13:02Z,"fix broken links, add links check to sanity",,tensorflow
12069,ppwwyyxx,pr,2017-08-07T06:18:46Z,Fix typo,`values` is not defined.,tensorflow
12527,ppwwyyxx,pr,2017-08-23T15:15:27Z,Optimize non_max_suppression by iterating backwards,"In typical scenarios, high-overlapping boxes are likely to have similar scores.
Therefore if there are any high-overlapping boxes, it's faster to find them starting from boxes that have similar scores.

This simple heuristic improves performance of the op on several actual workload by 10%~20%. The workloads are like ~10k boxes produced by a region proposal network.",tensorflow
12580,ppwwyyxx,pr,2017-08-25T03:25:21Z,Add kernels for FusedBatchNormGrad when is_training=False,#10857 ,tensorflow
13368,d4l3k,pr,2017-09-28T20:24:25Z,tensorflow/go: add in LDFLAGS to support android,This is required since the libtensorflow_inference.so generated by contrib/android links against these libraries. Go requires these to be specified when compiling against it.,tensorflow
13836,ppwwyyxx,pr,2017-10-19T19:08:14Z,"Add ""PlainSessionCreator""","Not sure if this is desirable, but I found this improves the usability of the `SessionCreator` interface. When I use a factory to create session, it's helpful that it can at least create session in the simplest way.
An example usage would be to write a ""train"" function that takes a SessionCreator and a bunch of other things.",tensorflow
15031,ppwwyyxx,pr,2017-12-01T08:52:25Z,Fix error message of WhereOpCPU,,tensorflow
15264,ppwwyyxx,pr,2017-12-11T08:20:04Z,Support empty input tensor for some ops (fix #14657),"Cudnn kernels doesn't work for empty input tensors.
This PR adds support for empty input tensor for FusedBatchNorm,FusedBatchNormGrad,Conv2DBackpropFilter, and cudnn pooling. (fix #14657)",tensorflow
15287,ppwwyyxx,pr,2017-12-12T01:23:28Z,Use base_dtype for self._dtype in tf.layers,"This avoids mismatch dtype (ref vs no_ref) when using variables as inputs to a layer.
See #15262",tensorflow
16817,ppwwyyxx,pr,2018-02-07T01:54:23Z,Python3 support of docs generation,"1. Replace codegen by astor.
2. Use tuple._asdict() to replace tuple.__dict__.

Fix #9437",tensorflow
17261,zasdfgbnm,pr,2018-02-25T19:10:43Z,"add _div_metric, sensitivity, specificity","### This PR propose the following changes:
#### 1. Add two new metrics: `tf.metrics.sensitivity` and `tf.metrics.specificity`.
`tf.metrics.sensitivity` is identical to `tf.metrics.recall`. These two metrics accept `labels` and `predictions` as parameters, cast `predictions` to boolean, and then do the computation according to formula:
sensitivity = true_positive / (true_positive + false_negative)
specificity = true_negative / (true_negative + false_positive)

#### 2. Add a helper function `_div_metric`
This helper function can be used to compute all metrics that is defined as the quotient of two metrics. This helper function simplifies the computation of a couple of metrics.

### Further comments:
1. Would it be beneficial if we rename `_div_metric` to `div_metric` and make it public?
2. I don't see any unit test for `tf.metrics`, do I miss something?

",tensorflow
17517,jjsjann123,pr,2018-03-07T19:47:03Z,add error message when importing contrib.tensorrt without libnvinfer,,tensorflow
17772,jjsjann123,pr,2018-03-16T16:16:24Z,[update TensorRT converter],"  fixed FusedBatchNorm to support broadcast;
  remove fp16 conversion for type int const
  add Snapshot in conversion (treated as identity)",tensorflow
17780,jjsjann123,pr,2018-03-16T20:07:09Z,[update TensorRT converter],"  fixed FusedBatchNorm to support broadcast;
  remove fp16 conversion for type int const
  add Snapshot in conversion (treated as identity)",tensorflow
17857,jjsjann123,pr,2018-03-20T10:55:13Z,improve fp16 tftrt prediction,  delay fp32 to fp16 conversion to reduce accumulated rounding error,tensorflow
18135,jjsjann123,pr,2018-03-30T20:50:51Z,updated installation instructions for Tensowflow-TensorRT integration,@aaroey @cliffwoolley @gunan ,tensorflow
18433,jjsjann123,pr,2018-04-11T22:04:50Z,[tftrt update],"  Added support for TRT plugin during conversion
  - converter & shape inference are now aware of plugin factory.
  - PluginTensorRT
    - wrapper for nvinfer1::IPlugin
    - base class for custom plugins
    - provides serialization/deserialization of plugin op type & input tensor dimensions
  - PluginFactoryTensorRT
    - wrapper for nvinfer1::PluginFactory
    - singleton with plugin registration, allow user to register plugin op type (string), plugin construction function & plugin deserialization function.
    - owns constructed & deserialized plugins.
  - Unit tests for plugin registration

  * compatible with TRT 3.0.4 plugin API.
  * future plugin API changes willl be updated.",tensorflow
19135,jjsjann123,pr,2018-05-08T03:42:35Z,[tftrt update],"  code cleaning, removed some boilerplate code",tensorflow
19915,d4l3k,pr,2018-06-11T20:07:48Z,tensorflow/go: add operation Input methods + tests,The go version of tensorflow only has operation output methods. This adds in the needed code to return operation inputs and jump from output to input and vice versa. This is required to actually be able to traverse the graph.,tensorflow
19953,d4l3k,pr,2018-06-12T19:45:44Z,tensorflow/go: operation attribute getters,"This adds the ability to fetch operation attributes. This is needed for any sort of graph rewriting ability.

The tests on this are pretty minimal since I wasn't sure the best way to test all of the attributes. Hunting down operations that match each param seems like a huge pain and the C api style of creating test ops isn't currently possible from Go. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc#L1703-L1753

Not sure if it'd be possible to add in a small .h and .cc file containing just those lines while running the Go tests. Thoughts?",tensorflow
19987,ppwwyyxx,pr,2018-06-13T13:44:16Z,Cast size() to int to avoid implementation-defined conversion,"`selected.size()` is unsigned and originally 0.
The conversion from `(size_t)0 - 1` to int is implementation defined: 

From https://en.cppreference.com/w/cpp/language/implicit_conversion:

> If the destination type is signed, the value does not change if the source integer can be represented in the destination type. Otherwise the result is implementation-defined. (Note that this is different from signed integer arithmetic overflow, which is undefined).

This fixes #19578 ",tensorflow
20103,d4l3k,pr,2018-06-18T19:53:02Z,tensorflow/go: add tests for zero length arrays passed to C,"This fixes some nil panics on zero length arrays passed to C in Consumers and Attr introduced in #19953  and #19915 as well as tests for them.

It also preserves the property that an unknown attribute name throws an error instead of just returning the empty value in all cases.

CC @asimshankar ",tensorflow
20159,d4l3k,pr,2018-06-20T20:23:38Z,Cast: support casting to and from quantized types,"This adds support for casting to and from quantized data types using the Cast operator. It pretty much just changes the types to the non quantized version before calling cast and then changing them back since the quantized types are just a semantic difference and not an underlying one.

Issue #20150.

Tested with
```
bazel test //tensorflow/core/kernels:cast_op_test -c dbg --test_output=errors
```",tensorflow
20197,ppwwyyxx,pr,2018-06-21T18:30:48Z,Untrunctaed normal distribution in VarianceScalingInitializer,"Fixes #19996 

RELNOTES: VarianceScalingInitializer now takes ""truncated_normal"" or ""untruncated_normal"" as values for its distribution argument. Using ""normal"" is equivalent to ""truncated_normal"" (the current behavior), but its use is deprecated. 

@martinwicke ",tensorflow
20350,jjsjann123,pr,2018-06-27T16:46:37Z,TRT 4.0 update,"  updated feature support for TRT 4.0 layers
  disabled broken shape inference (added TODO)
  removed unused code
  code is compatible with TRT 3.0.4",tensorflow
20360,ppwwyyxx,pr,2018-06-27T20:41:37Z,Fix gradient of nccl_ops,"In Python3, `op.get_attr` returns bytes, not str, which causes many failures.",tensorflow
20774,jjsjann123,pr,2018-07-13T08:13:48Z,[tftrt update],"  Added python tests for converter functions
  Added BUILD for python tests",tensorflow
20862,jjsjann123,pr,2018-07-17T00:19:57Z,[tftrt],"  Added alignment in trt_allocator
  Since TensorFlow gpu_bfc_allocator does not abide to the alignment requested.",tensorflow
21075,jjsjann123,pr,2018-07-24T00:25:24Z,[tftrt],"  trt 4 update input check on tensor dimensions.
  relaxing tensor dimension for trt 4 to support non-4-dimensional inputs
  added unit test (currently disabled due to failed INT8 conversion)",tensorflow
21131,jjsjann123,pr,2018-07-25T19:10:19Z,[tftrt_tests],unit test added for memory alignment test in trt.,tensorflow
21385,ppwwyyxx,pr,2018-08-04T18:39:38Z,Support empty inputs in some maxpool kernels. (#21338),"This only partially remedies #21338. A more serious issue is that such bugs cannot be detected by the unit test framework and the cudaError will leak into other ops.

Similar issues probably exist in many other ops as well.",tensorflow
21565,ppwwyyxx,pr,2018-08-13T05:39:05Z,Manually call right-operand version of binary ops to preserve error message,"#12454

```python
import tensorflow as tf
x = tf.get_variable('asdfds', shape=[10], dtype=tf.int32)
x*0.5
```
used to print:
```
TypeError: unsupported operand type(s) for *: 'Variable' and 'float' 
```

It now prints:
```
TypeError: Expected int32, got 0.5 of type 'float' instead.
```",tensorflow
23752,jeffdaily,pr,2018-11-14T22:01:38Z,Fix #22455.,RdmaAdapter is allocated outside of RdmaMgr so that ibv_device is available to the static visitor registration.,tensorflow
23857,jeffdaily,pr,2018-11-19T16:57:44Z,Add RCCL package and ops.,"These contributed RCCL ops are based on the NCCL ops. Due to the intentional similarities of the RCCL API to the NCCL1 API, in most cases, these can be used as a drop-in replacement for NCCL when compiling with ROCm.",tensorflow
24037,ppwwyyxx,pr,2018-11-29T05:56:48Z,Fix deprecated use of `sparse_to_dense`.,"Calling `sparse_to_dense` gives a deprecation warning that asks users to use `sparse.to_dense`:
https://github.com/tensorflow/tensorflow/blob/71f40f044450736cd6acd29e92ffbfc0e571ee14/tensorflow/python/ops/sparse_ops.py#L952-L955

However, `sparse.to_dense` calls `sparse_to_dense`, which again produces the deprecation warning.",tensorflow
24708,ppwwyyxx,pr,2019-01-04T20:18:50Z,improve docs of depthwise conv,"State that the documentation is for ""NHWC"" format.
Otherwise, the documentation can be confusing: it confuses one user at https://github.com/tensorpack/tensorpack/issues/1029#issuecomment-451552581.",tensorflow
25727,ppwwyyxx,pr,2019-02-13T19:51:25Z,Update clip_ops.py,"Interestingly, the deprecation warning uses a deprecated function (`tf.to_float`).
This changes it to `tf.cast`.",tensorflow
25818,ppwwyyxx,pr,2019-02-17T23:15:15Z,Add support for cudnn's group convolution.,"This PR enables group convolution in cudnn, a feature that's highly desired for many years (#3332, https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320465264, #11662, #10482).

With this PR, now it's allowed to call `tf.nn.conv2d(inputs, filters)`, where the depth of `inputs` is not necessarily equal to `filters.shape[2]`, but be a multiple of `filters.shape[2]`.

The core of this PR is only two lines of code (https://github.com/tensorflow/tensorflow/issues/3332#issuecomment-464308902) which removes the shape check. Then I added some extra checks and tests.

This benchmark script:
```python
import tensorflow as tf
import time
import os

N = 64
C = 256
G = 32
H, W = 64, 64
print(""N, C, H, W:"", [N, C, H, W])


def benchmark_all(use_loop, format):
    shape4d = [N, C, H, W] if format == 'NCHW' else [N, H, W, C]

    tf.reset_default_graph()
    input = tf.get_variable('input', shape=shape4d, dtype=tf.float32)
    filter = tf.get_variable('filter', shape=[3, 3, C // G, C], dtype=tf.float32)

    if use_loop:
        inputs = tf.split(input, G, axis=1 if format == 'NCHW' else 3)
        filters = tf.split(filter, G, axis=3)
        output = tf.concat(
            [tf.nn.conv2d(i, f,
                strides=[1,1,1,1],
                padding='SAME',
                data_format=format) for i, f in zip(inputs, filters)], axis=1 if format == 'NCHW' else 3)
    else:
        output = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', data_format=format)


    forward_op = output.op
    cost = tf.reduce_sum(output)
    backward_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)

    def benchmark(op, nr_iter=200, nr_warmup=10):
        for k in range(nr_warmup):
            op.run()
        start = time.perf_counter()
        for k in range(nr_iter):
            op.run()
        end = time.perf_counter()
        itr_per_sec = nr_iter * 1. / (end - start)
        return itr_per_sec

    sess = tf.Session()
    with sess.as_default():
        sess.run(tf.global_variables_initializer())

        spd_forward = benchmark(forward_op)
        print(""Loop={}, Format={}, Forward: {} itr/s"".format(use_loop, format, spd_forward))
        spd_backward = benchmark(backward_op)
        print(""Loop={}, Format={}, Backward: {} itr/s"".format(use_loop, format, spd_backward))


formats = ['NHWC', 'NCHW']
for format in formats:
    for use_loop in [True, False]:
        benchmark_all(use_loop, format)
```
Executed on V100, cuda10, cudnn 7.4.2, it prints:
```
N, C, H, W: [64, 256, 64, 64]
Loop=True, Format=NHWC, Forward: 65.49446747235214 itr/s
Loop=True, Format=NHWC, Backward: 32.26484275606916 itr/s
Loop=False, Format=NHWC, Forward: 117.40288830454352 itr/s
Loop=False, Format=NHWC, Backward: 50.051492362319074 itr/s
Loop=True, Format=NCHW, Forward: 98.8428390274372 itr/s
Loop=True, Format=NCHW, Backward: 35.672312085388455 itr/s
Loop=False, Format=NCHW, Forward: 152.24726060851506 itr/s
Loop=False, Format=NCHW, Backward: 56.21414524041962 itr/s
```
which shows around 50~80% speed up over a naive loop-based implementation.",tensorflow
26428,ppwwyyxx,pr,2019-03-07T07:12:17Z,Fix the wrong container name in AddResourceInput,"When `container == """"`, this function puts the resource in container `default_container()`, but sets the handle to the wrong container `""""`.

This PR fixes the behavior.

// Originally found by Wendy Huang. fix #26454",tensorflow
27013,ppwwyyxx,pr,2019-03-22T03:10:16Z,DEBUG_LD -> LD_DEBUG,"DEBUG_LD has no effect. I assume it is meant to be LD_DEBUG?

http://www.bnikolic.co.uk/blog/linux-ld-debug.html",tensorflow
27524,ppwwyyxx,pr,2019-04-04T23:57:53Z,Clean-up unused functors,"`ShuffleAndReverse`, `InflatePadAndShuffle` and `TransformDepth` were included in the very first public commit of tensorflow.
They are used at the beginning, but as far as I can see they are not used anywhere in the codebase for a long time.
Is there any reason keeping them around?",tensorflow
27849,ppwwyyxx,pr,2019-04-15T04:10:13Z,Update framework.py,Fix #27847,tensorflow
28809,ppwwyyxx,pr,2019-05-17T20:20:02Z,Update nvptx_compiler.cc,,tensorflow
29228,jeffdaily,pr,2019-05-31T19:24:40Z,[ROCm] Add ROCm support for most cwise ops,"This minor mod adds ROCm support for most of the cwise ops.

Background info
These ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.

We have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags
And also PyPI packages: https://pypi.org/project/tensorflow-rocm/

For a sample ROCm test run you can refer to:
http://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console

```
//tensorflow/python/kernel_tests:cwise_ops_binary_test                   PASSED in 270.4s
//tensorflow/python/kernel_tests:cwise_ops_binary_test_gpu               PASSED in 274.3s
//tensorflow/python/kernel_tests:cwise_ops_test                          PASSED in 24.2s
//tensorflow/python/kernel_tests:cwise_ops_test_gpu                      PASSED in 23.7s
//tensorflow/python/kernel_tests:cwise_ops_unary_test                    PASSED in 12.0s
//tensorflow/python/kernel_tests:cwise_ops_unary_test_gpu                PASSED in 12.4s
````",tensorflow
29372,jeffdaily,pr,2019-06-03T21:17:00Z,[ROCm] Add ROCm support for remaining cwise ops and tests,"cwise_ops.h -- packet access is always false for ROCm.
Some ROCm ops do not yet support complex64 / complex128 types.",tensorflow
30373,jeffdaily,pr,2019-07-03T20:57:16Z,add -lm to mlir-tblgen linkopts,"Fixes the following build error:

```
Couldn't build file external/local_config_mlir/mlir-tblgen: Linking of rule '@local_config_mlir//:mlir-tblgen' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
...
/usr/bin/ld: bazel-out/k8-opt/bin/external/llvm/libsupport.a(APInt.o): undefined reference to symbol 'round@@GLIBC_2.2.5'
//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
```",tensorflow
30694,robieta,pr,2019-07-15T00:30:49Z,Fail when np.array and len are called on Tensors.,"When np.array is called on a symbolic Tensor the result is a `shape=()` numpy array of objects which is basically never what was intended. Similarly, length is not defined and can lead to rather cryptic error messages. This surfaced through https://github.com/tensorflow/tensorflow/issues/28619; however the fact is that accidentally passing a Tensor rather than and EagerTensor to a package in the NumPy ecosystem can result in very cryptic error messages.

This PR simply makes Tensors fail with clear error messages in such cases. (Similar to the treatment of `__iter__`)",tensorflow
31453,jeffdaily,pr,2019-08-08T17:43:05Z,[ROCm] Adding RCCL support,"This PR adds ROCm RCCL support.

This PR also adds a general performance improvement for NCCL/RCCL integration where the NcclManager records and waits on an Event as each Participant is added, rather than synchronizing with the tensor stream only after all Participants have been added.  Otherwise, most compute kernels are added to the compute stream prior to the NCCL/RCCL sync Event, delaying the start of the collective.

This PR also adds ROCm-specific performance improvements.  Specifically, this adds the nccl stream as a member of the StreamGroup rather than delaying the creation of the NCCL stream until NCCL is first used in the NcclManager.  This allows the compute and nccl stream to be force-initialized as immediate siblings.
",tensorflow
31481,jeffdaily,pr,2019-08-09T15:51:09Z,[ROCm] improve concurrency between compute and nccl streams,"The NcclManager records and waits on an Event as each Participant is added,
rather than synchronizing with the compute stream only after all Participants
have been added. Otherwise, most compute kernels are added to the compute
stream prior to the NCCL sync Event, delaying the start of the collective.",tensorflow
31483,jeffdaily,pr,2019-08-09T16:11:45Z,[ROCm] move nccl stream to member of StreamGroup,"This allows the compute and nccl stream to be force-initialized as immediate
siblings which is necessary for ROCm performance.",tensorflow
31485,jeffdaily,pr,2019-08-09T16:31:00Z,[ROCm] add ROCm RCCL support,,tensorflow
31864,robieta,pr,2019-08-21T21:19:25Z,"r2.0-CherryPick: Refactor the keras TensorLikeDataAdapter (numpy array, EagerTensor, etc) to use tf.shuffle rather than np.shuffle",This cherrypick depends on: https://github.com/tensorflow/tensorflow/pull/31863,tensorflow
31993,ppwwyyxx,pr,2019-08-27T03:17:00Z,Register flops for BatchMatMulV2,fix #22071,tensorflow
32296,jeffdaily,pr,2019-09-06T18:13:21Z,sort NcclManager::Collective participants using device ID,"Occasionally, the nccl_manager_test subtest `NcclManagerTest/0.BasicAllGather` would fail because the `NcclManagerTest/0.BasicSumReduction` would produce a device-to-rank ordering that might not be monotonically increasing.

```
hostname:25909:25993 [0] NCCL INFO Ring 00 :    0   1   2   3
hostname:25909:25994 [1] NCCL INFO Ring 00 : 1[2] -> 2[1] via direct shared memory
hostname:25909:25993 [0] NCCL INFO Ring 00 : 0[0] -> 1[2] via direct shared memory
hostname:25909:25996 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory
hostname:25909:25995 [2] NCCL INFO Ring 00 : 2[1] -> 3[3] via direct shared memory
```

This communicator would be reused by the `BasicAllGather` test.  However, the allgather reduction is expected to be ordered by rank; no other collective result depends on rank order.  Sorting the participants based on the device ID resolves the failed test.",tensorflow
32403,ppwwyyxx,pr,2019-09-11T00:18:21Z,[r1.15Cherrypick]: Cherry-pick #30893 to r1.15,"fix #32401 

@mihaimaruseac ",tensorflow
32444,jeffdaily,pr,2019-09-11T20:25:00Z,[ROCm] fix CSB build,"Co-authored with @deven-amd.  This PR fixes recent failures in the Linux AMD ROCm GPU Nightly CSB build.  This PR does not depend on https://github.com/tensorflow/tensorflow/pull/32296, however that PR is also required before CSB build is successful again.",tensorflow
34202,Flamefire,pr,2019-11-12T14:33:54Z,Remove adding of /usr/bin to compiler paths,"As the underlying Bazel issue https://github.com/bazelbuild/bazel/issues/5634 is resolved, this code can (and should) go now

This allows for better compatibility on RHEL systems with CUDA.

See https://github.com/bazelbuild/bazel/commit/c6ec22f94faaf1320f576d5658a106483b2bf19f#diff-f6852ce579394c610a139a1f38783138L158

This has been in since Bazel 0.20 and should be picked for the next TF release",tensorflow
34218,Flamefire,pr,2019-11-13T08:28:08Z,Add empty linker_bin_path,Fixes regression introduced in #34202,tensorflow
34531,jeffdaily,pr,2019-11-22T22:10:57Z,R1.15 rccl upstream patch,This patch is a backport of current RCCL support in master for the r1.15 branch.,tensorflow
34532,jeffdaily,pr,2019-11-22T22:29:41Z,[ROCm] r1.15 rccl upstream patch,"This patch is a backport of current RCCL support in master for the r1.15 branch.  RCCL support was not complete in the r1.15 branch, and since this is the last V1 release branch, it is important to have this feature here.

Further, without this PR, the r1.15 branch will not build for the latest ROCm release due to missing clang 10-based header files.  See #31849 for the same change to master.",tensorflow
34769,jeffdaily,pr,2019-12-02T20:28:03Z,[ROCm] eigen patch needed for HIP header changes in r1.15 branch,"This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.  Since this is the last V1 release branch, it is important to have this feature here.  This PR depends on #34532.",tensorflow
34770,jeffdaily,pr,2019-12-02T20:49:37Z,[ROCm] eigen patch needed for HIP header changes,This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.,tensorflow
34979,robieta,pr,2019-12-10T00:51:11Z,"[r2.1 Cherrypick] Remove name-based Variable handling in keras Lambda layers, and add detailed exceptions and warnings for unsafe corner cases.",https://github.com/tensorflow/tensorflow/commit/9422eb1139b3163cf65950c6e713f39344ec33e4,tensorflow
35238,jeffdaily,pr,2019-12-19T02:13:59Z,[ROCm][r2.1] eigen patch needed for HIP header changes,"This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.  Same patch as #34770, cherry-picked.",tensorflow
36106,jeffdaily,pr,2020-01-21T16:55:01Z,[ROCm] add ROCm support for XLA RCCL thunk,,tensorflow
36159,Flamefire,pr,2020-01-23T15:00:17Z,Enhance SlurmClusterResolver,"This enhances the SlurmClusterResolver and fixes its bugs, see #36094

The design goal is that the user simply creates an instance of the class when the python script is run by `srun` (the `mpirun` wrapper on SLURM). As all information is available in the environment this is certainly possible.

- Default all arguments to something reasonable
- Use Slurm step env variables to fill in data

This has been tested (kinda) successfully on a SLURM cluster with TF 2.1.0 and MultiWorkerMirroredStrategy. It works with artificial data. I couldn't test it fully with MNIST due to https://github.com/tensorflow/tensorflow/issues/36153

What should be discussed besides the code:
- Usage of `nvidia-smi` to query the number of GPUs. I couldn't use `context.num_gpus()` at this point because this initializes the context including collectives which is not wanted as collectives might be overwritten later when creating a strategy",tensorflow
37719,Flamefire,pr,2020-03-19T16:00:37Z,Add -lrt to linkflags,"Fixes compilation on e.g. older CentOS, see for details #15129

@gunan You said you'd like to review and merge a fix for #15129, so I created one based on a patch used within EasyBuild introduced in https://github.com/easybuilders/easybuild-easyconfigs/pull/6089 but updated to latest TF master and guarded for windows.",tensorflow
37754,Flamefire,pr,2020-03-20T14:34:48Z,Add -lrt to linkflags,"Second attempt in tackling compilation on e.g. older CentOS, see for details #15129 after #37719 failed on OSX

I added code from which I believe it will only add the flag on Linux. Please verify that I got that right, the code is mostly copied and adjusted as I don't really know Bazel.

ccing @gunan @mihaimaruseac",tensorflow
38112,Flamefire,pr,2020-04-01T10:39:49Z,Add MPI cluster resolver and update documentation of SLURM cluster resolver,"@frankchn As discussed I updated the documentation for the Slurm cluster resolver enhanced in #36159 which became outdated.

I also added the MPI cluster resolver mentioned there and included it in the documentation update to avoid conflicts or dependencies between those 2 related changes.

2 design decisions of the MPI cluster resolver I wanted to highlight:
- import of mpi4py only on constructor (only place where it is needed) to allow importing the file even when the package is not installed. Can be useful when conditionally switching on the used resolver and can reduce program startup time.
- removed parameter `tasks_per_node` (actually just didn't add it). As I highlight in the Slurm doc file using it is usually not a good idea as the resolver is able to do that itself and it is very easy to use wrong and hard to use right. I don't see a usecase for that, especially when using MPI",tensorflow
38355,Flamefire,pr,2020-04-08T12:01:08Z,Update SlurmClusterResolver documentation,"Extracted from https://github.com/tensorflow/tensorflow/pull/38112 to only contain the docu changes and slightly enhanced.

The existing docu for the Slurm cluster resolver became outdated after it was enhanced in #36159.

ccing @frankchn ",tensorflow
41889,Flamefire,pr,2020-07-30T07:51:40Z,Fix environment for protobuf compilation,"The invocation environment for protobuf compilation is ignoring environment variables like LD_LIBRARY_PATH which were used when building `protoc`. This leads to failures due to e.g. mismatches in the `libstdc++.so` versions.


This fixes #41857 by using the patch from upstream https://github.com/grpc/grpc/pull/23664",tensorflow
42266,Flamefire,pr,2020-08-12T14:39:02Z,Use nasmlink genrule,"Avoids cyclic dependency as the name and src must not be the same. This uses the same mechanism as done by cython

Fixes #42264 (see that for more details), TLDR is:

Currently the build file sets the name and src to ""nasm"" which is forbidden: https://docs.bazel.build/versions/master/be/shell.html

>    do not give the rule and the file the same name.

This PR fixes that by using an intermediate rule with a different name",tensorflow
42362,Flamefire,pr,2020-08-14T12:38:31Z,Run SWIG in the default environment,"This avoids failures when swig is build in custom envs e.g. with a compiler installed into another location (e.g. /opt)
See also https://github.com/bazelbuild/bazel/issues/4053
Patch by @boegel from https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886

Fixes #41806",tensorflow
42516,Flamefire,pr,2020-08-20T14:49:20Z,"Include jsonCPP headers via #include ""json/json.h""","Don't use #include ""include/json/json.h"" which is unusual and therefore confusing
This allows to remove the header symlinking done for the system lib version

Closes #42303

Disclaimer: I've tested the system build with this which works, not 100% sure about Bazel as I'm not very familiar with it but the `    includes = [""include""],` in the `cc_library` rule should mean this works.",tensorflow
43070,Flamefire,pr,2020-09-09T10:24:54Z,Don't try to link JSON headers to fix JSON in TF_SYSTEM_LIBS,"This is not required and now avoids conflicting settings of INCLUDEPATH when protobuf and JSONCpp is installed into different prefixes

We have used this patch successfully in compiling TF 2.0, 2.1 and 2.2 on our system, so I'd say it works.

CCing @cbalint13 and @perfinion to maybe verify on their builds.",tensorflow
43153,Flamefire,pr,2020-09-11T15:27:28Z,Do not symlink system protobuf headers but only the required .proto files,"Symlinking the system headers has proven to be problematic as newer versions of protobuf add or remove headers which makes having a static array of header files hard to impossible. Turns out the headers don't need to be symlinked at all but only the .proto files used as inputs need to be present.

Example: In #34792 @cbalint13 removed the `port_def.inc` ""header"". But the workspace.bzl requests protobuf 3.8.0 (https://github.com/tensorflow/tensorflow/blob/610a78b98569e2908809645626b4bd6afd2a22d8/tensorflow/workspace.bzl#L423, now 3.9.2: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L628) which has it: https://github.com/protocolbuffers/protobuf/blob/v3.8.0/src/google/protobuf/port_def.inc
This leads to build failures due to this missing (although I think if all would be working well it would just take the header from the system if `$INCLUDEDIR` is in the compilers header search path).

This partially resolves/helps #37861",tensorflow
43156,Flamefire,pr,2020-09-11T16:40:18Z,Revert renaming of tools to exec_tools,"Reverts part of f827c023906e7d30f0e5f2992b111ab34153310a as that causes trouble due to action_env variables not passed through to any dependent build which breaks builds using TF_SYSTEM_LIBS

Fixes #43019",tensorflow
43951,Flamefire,pr,2020-10-12T07:52:59Z,Avoid empty linker_bin_path breaking the build,"If ctx.attr.linker_bin_path is empty (e.g. if should_download_clang is set)
the GPU build would add a lone `-B` to the build which swallows the next
argument leading to broken builds.

Fixes #41856, fixes #42313

Example from https://github.com/tensorflow/tensorflow/issues/41856#issuecomment-665623137:

```
-Wl,-no-as-needed
-B
-o
bazel-out/k8-opt/bin/external/com_google_protobuf/protoc
bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protoc/main.o
...
```

As can be seen the lone `-B` swallows the `-o` making the compiler thing the intended output is an input.",tensorflow
44144,Flamefire,pr,2020-10-19T13:41:45Z,Fix and improve system protobuf,"This fixes an issue that TF build throws

> google/protobuf/compiler/plugin.proto: google/protobuf/descriptor.proto is imported, but @com_google_protobuf//:compiler_plugin_proto doesn't directly depend on a proto_library that 'srcs' it.

The reason is the missing dependency. To fix this and make future updates easier I replaced the current code with the upstream `WELL_KNOWN_PROTO_MAP` and generate the list of files and the `proto_library` invocations out of that, which is basically a copy of the upstream code. This makes it more maintainable and shorter.

Furthermore I also fix #37835 by introducing a new env and Bazel variable `PROTOBUF_INCLUDE_PATH` (named after e.g. `PYTHON_INCLUDE_PATH` but I'm open to suggestions) to allow installing each dependency into a separate prefix folder as done e.g. on HPC clusters.",tensorflow
44145,Flamefire,pr,2020-10-19T13:48:05Z,Add flagsaver to system absl_py,The tpu_test_wrapper_test references this and hence errors when this is not defined,tensorflow
44522,Flamefire,pr,2020-11-02T14:44:51Z,"Revert ""systemlibs: protobuf: Add missing headers""","This reverts commit 62a71ef214d380847a72e2416a8f3e406ef15ab5.

The commit reintroducing the headers should not be required and existing headers were likely an artifact from a previous build. See https://github.com/tensorflow/tensorflow/pull/44489#issuecomment-720081499

@perfinion Can you test this to verify this is working?

Includes the merge conflict fix also present in #44488",tensorflow
44524,Flamefire,pr,2020-11-02T16:55:41Z,Update list of systemlibs and add script to verify,"I noticed that in addition to com_github_googleapis_googleapis being missing as a system lib (see #42361) the new typing_extensions_archive wasn't added to the list of valid options.

This simply removes the com_github_googleapis_googleapis, the alternative is to readd it as the new name com_google_googleapis and readd (and potentially update) the system build file but I can't verify this. Maybe  @perfinion may?

It also adds typing_extensions_archive as a valid option.

I also made a script which points out those issues. I'd like to have that added to CI as this acts like a basic sanity check. Could anyone from the TF team guide me on how to do that?",tensorflow
44549,Flamefire,pr,2020-11-03T11:52:21Z,Add use_default_shell_env = True to all ctx.actions.run_shell rules,"To start subprograms, even simple bash snippets, Bazel uses an
executable `process-wrapper` which is potentially built using a custom
toolchain and hence requires a set up LD_LIBRARY_PATH.
Ommitting the `use_default_shell_env` (defaulting to false) clears the
whole environment and the binary may try to use older system libs such
as /lib64/libstdc++.so causing it to fail in case it is (much) older
than the used libstdc++ from the custom toolchain which is very common
in HPC environments.
Hence I added `use_default_shell_env = True` as already done in e.g.
`_local_genrule_impl`.",tensorflow
44901,Flamefire,pr,2020-11-16T10:20:31Z,Rename exec_tools to tools,"Follow up to #43156
Based on https://github.com/bazelbuild/bazel/issues/12059#issuecomment-725641997 exec_tools might no longer be needed and hence can be replaced by tools.
This fixes various build failures caused by missing environment variables in environments where they are required, e.g. using custom compilers.",tensorflow
45264,Flamefire,pr,2020-11-30T10:41:59Z,[CherryPick:r2.4] Revert renaming of tools to exec_tools,"Reverts part of f827c023906e7d30f0e5f2992b111ab34153310a as that causes trouble due to action_env variables not passed through to any dependent build which breaks builds using TF_SYSTEM_LIBS

Cherry-pick of #43156, similar PR #45124 already merged but this one was missed.",tensorflow
45265,Flamefire,pr,2020-11-30T10:48:36Z,Add missing arch-specific dependencies to tf_to_kernel,"The tf_to_kernel target is missing some dependencies which shows on e.g. POWER by undefined reference errors
Fixes #45104",tensorflow
47921,kulinseth,pr,2021-03-19T16:39:26Z,[Pluggable Device] Add custom device mem allocator for Pluggable device.,"The commit provides a simple alternative to BFCAllocator for Pluggable devices
to do their own device memory management.

@penpornk , @annarev , @jzhoulon ",tensorflow
48782,kulinseth,pr,2021-04-27T16:40:44Z,[Pluggable Device] Fix the MacOS regression for registering Pluggable…,"Earlier[ commit in ](https://github.com/tensorflow/tensorflow/pull/45784/commits/463cad1324fcfbb082ae757484614f77bdc3929d) pluggable [impl ](https://github.com/tensorflow/tensorflow/pull/45784) seems to have caused regression on mac platforms to register pluggable device. When _pywrap_tensorflow .so loads up the plugin it goes through the device initialization fine then it hands over to libtensorflow_framework dylib to query the plugin handle using the Platform name in MultiPlatformManager . And during this part it fails with ""Platform "" not found. Reverting the commit fixes the problem for Mac. The change was added to fix unit tests on MacOS tests, which seem to be working with this.

@penpornk and @jzhoulon ",tensorflow
49110,kulinseth,pr,2021-05-11T16:27:52Z,[Pluggable Device] Adding more DEVICE_DEFAULT kernels.,"The checkin extends the device default kernels to
    - int32 operations
    - certain resource operations
    - TensorArray and TensorList operations",tensorflow
49111,kulinseth,pr,2021-05-11T16:37:01Z,[Pluggable Device] Adding more DEVICE_DEFAULT kernels.,"The checkin extends the device default kernels to
    - int32 operations
    - certain resource operations
    - TensorArray and TensorList operations

@penpornk ",tensorflow
49275,kulinseth,pr,2021-05-18T22:05:25Z,"[Pluggable Device] Use default settings when device ""architecture"" field is not set.","For pluggable devices (with device type GPU) in Grappler optimization pass there is currently code which expects the ""architecture"" field to be set. In pluggable interface currently there is no way to set the architecture field to provide these default values. This bypasses the check and sets some dummy values.  

@penpornk , @reedwm ",tensorflow
49717,kulinseth,pr,2021-05-26T00:10:24Z,[Kernel C API] Implementation of variable ops RFC.,"The implementation for the Variable Ops RFC.
https://github.com/tensorflow/community/blob/master/rfcs/20210504-kernel-extension-variable-ops.md

@penpornk , @reedwm , @saxenasaurabh , @jzhoulon ",tensorflow
50462,kulinseth,pr,2021-06-25T18:56:37Z,Add the newly added kernels_experimental header file to pip package.,"The change adds the kernels_experimental.h header file to `kernels_hdrs` target so that it can appear to pip package.

@penpornk can you please take a look.",tensorflow
51581,kulinseth,pr,2021-08-19T21:03:20Z,Register unique op for Default device.,"Adding unique op for DEVICE_DEFAULT.
@penpornk ",tensorflow
55691,Maratyszcza,pr,2022-04-20T20:34:12Z,r2.9 cherry-pick: Fix crash in TF Lite Java API on Android API <= 19,"Work around crash in dlsym when trying to check for the presence of a XNNPACK
delegate symbols on Android API <= 19 by detecting the respective Android
versions and opting out of XNNPack inference.

PiperOrigin-RevId: 441825578",tensorflow
56480,kulinseth,pr,2022-06-16T14:23:14Z,Add macos_minimum_version to the bazelrc.,"This passes the flag to clang for compilation for macos_arm64 config on MacOS.

@nitins17 @learning-to-play",tensorflow
56591,Flamefire,pr,2022-06-27T10:44:36Z,Fix interleave_test in CGroup restricted CPU envs,"Use `len(os.sched_getaffinity(0))`, which returns the number cores available to the job,
rather than `multiprocessing.cpu_count()`, which returns the total number of cores in the node,
and which causes the interleave_test to fail when e.g. CGroups limit the
number of cores to less than the total core count.

Originally written by @smoors",tensorflow
56606,Flamefire,pr,2022-06-28T11:51:09Z,Link libdl in core/platform/default/stacktrace,"~Required due to use of `dladdr` via the `stacktrace_handler` dependency~

Required due to use of `dladdr` in `stacktrace.h` which is e.g. a dependency of `test_main` via `stacktrace_handler`

Fixes #45013",tensorflow
56633,Flamefire,pr,2022-06-30T09:57:15Z,Enhance NumSchedulableCPUs to allow for nodes with more than 1024 cores,"In systems with large core counts the default size of `cpu_set_t` is not large enough to hold a space for each of them.
Use the `CPU_ALLOC` macro to dynamically create space while doubling the number of possible CPUs (and hence required space) when `sched_getaffinity` fails with `EINVAL`.

Code based on CPythons implementation of `os.sched_getaffinity`
See https://github.com/python/cpython/blob/21cbdae90ffdac047d27d1b83a5442fabcf89f7c/Modules/posixmodule.c#L7197-L7214

Fixes #49833",tensorflow
56634,Flamefire,pr,2022-06-30T10:17:58Z,Fix crash during shutdown in distributed training,"Releasing and reaquiring the GIL during Python shutdown is not possible and leads to force termination.
Remove that.

Fixes #50853",tensorflow
