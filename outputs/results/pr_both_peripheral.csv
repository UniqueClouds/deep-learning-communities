issue_id,author,issue_type,timestamp,title,body,community
255,jekbradbury,pr,2016-11-25T23:01:25Z,Fix batch_first in AutogradRNN,Fixes #253.,pytorch
490,vra,pr,2017-01-19T02:14:19Z,format fix,,pytorch
502,ronrest,pr,2017-01-19T12:01:48Z,Update docstring for ConvTranspose functions,**Transposed convolutions** are often (*but incorrectly*) referred to as **Deconvolutional operations**. Made mention of this in the docstring to make it easier for people to search for this operation in the documentation.,pytorch
503,ronrest,pr,2017-01-19T12:55:25Z,Update batchnorm docstrings,"Add missing full stops, and added blank line for increased clarity on rendered documentation.",pytorch
505,ronrest,pr,2017-01-19T13:43:43Z,Update docstrings for  testing object type,Add docstring for `is_storage()` and `is_tensor()`,pytorch
506,ronrest,pr,2017-01-19T14:15:40Z,Add Random Number Generator Docstrings,Add Random Number Generator Docstrings,pytorch
572,jfsantos,pr,2017-01-24T19:27:57Z,Fix math block of GRUCell in docs,"Added a blank space between the beginning of the `.. math::` block, otherwise it is displayed as a code block.",pytorch
693,jfsantos,pr,2017-02-07T15:01:09Z,Fixed typo and rendering of some equations,,pytorch
989,wangg12,pr,2017-03-13T06:08:06Z,fix doc of conv3d in conv.py,the second dimension should be height.,pytorch
1231,eklitzke,pr,2017-04-11T17:22:11Z,update README to install pyyaml from conda,"Pretty straightforward: PyYAML is in conda, so instead of mixing conda and pip, I think it makes more sense to just install everything with conda.

I was tempted to remove the `requirements.txt` file entirely, since the only other references to it are in `.travis.yml` and `Dockfile`, but I don't know enough about testing Docker to validate that change. That said, I don't see any reason that PyYAML couldn't be installed from conda in Travis/Docker as well.",pytorch
1232,jfsantos,pr,2017-04-11T17:24:25Z,Fixing function signatures: long -> ptrdiff_t,,pytorch
1238,eklitzke,pr,2017-04-12T03:47:46Z,add pyyaml to conda note here as well,Earlier you merged ade105fb7c198546a0a099ddf7ed2c87a9add5cd for me. I inadvertently only updated the OS X instructions; this adds the pyyaml note to the Linux instructions as well.,pytorch
1370,Jiaming-Liu,pr,2017-04-26T22:44:30Z,Lr scheduler,"Providing a unified LR scheduler.

Currently supports:
 - ReduceLROnPlateau (ported from [Keras](https://keras.io/))
 - LambdaLR
 - StepLR
 - MultiStepLR
 - ExponentialLR
 - GroupLambdaLR (Need testing)",pytorch
1373,aam-at,pr,2017-04-27T03:01:59Z,Magma SVD speed improvement,"Use `magma_*gesdd` instead of `magma_*gesvd`. Gesdd uses divide-and-conquer algorithm and is significantly faster than gesvd routine. Speed comparison (10 runs, Titan X):
<table border=""2"" cellspacing=""0"" cellpadding=""2"" rules=""groups"" frame=""hsides"">


<colgroup>
<col  class=""org-right"" />

<col  class=""org-right"" />

<col  class=""org-right"" />
</colgroup>
<tbody>
<tr>
<td class=""org-right"">N</td>
<td class=""org-right"">Old</td>
<td class=""org-right"">New</td>
</tr>


<tr>
<td class=""org-right"">1088</td>
<td class=""org-right"">0.7469</td>
<td class=""org-right"">0.2180</td>
</tr>


<tr>
<td class=""org-right"">2112</td>
<td class=""org-right"">3.4861</td>
<td class=""org-right"">0.9318</td>
</tr>


<tr>
<td class=""org-right"">3136</td>
<td class=""org-right"">8.7378</td>
<td class=""org-right"">2.2060</td>
</tr>


<tr>
<td class=""org-right"">4160</td>
<td class=""org-right"">14.7790</td>
<td class=""org-right"">4.3269</td>
</tr>


<tr>
<td class=""org-right"">5184</td>
<td class=""org-right"">25.9419</td>
<td class=""org-right"">8.6554</td>
</tr>
</tbody>
</table>


",pytorch
1414,Jiaming-Liu,pr,2017-05-01T05:07:47Z,Implement Nadam Optimizer,"This PR implements Nadam algorithm.
It has been proposed in [Incorporating Nesterov Momentum into Adam](https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ). Tested on MNIST.",pytorch
1481,aam-at,pr,2017-05-05T07:11:32Z,Fix bug in magma qr decomposition,"[Documentation](http://icl.cs.utk.edu/magma/forum/viewtopic.php?f=2&t=1015&p=2800&hilit=geqrf_gpu#p2800) for `geqrf_gpu` is incorrect. Tests pass because for smaller matrices `magma` falls back on `lapack` for qr factorization. For large matrices, returned `r` is incorrect. Bug can be reproduced using the following script (`r` will contains matrix with ones on diagonal):
```python
import torch
a = torch.rand(1000, 1000).cuda()
q, r = torch.qr(a)
```",pytorch
1556,stegben,pr,2017-05-15T06:27:41Z,"fix issue #1549, make and operator correct",A quick attempt to fix #1549 ,pytorch
1559,ronrest,pr,2017-05-15T12:39:35Z,minor typo and style changes to _torch_docs.py,,pytorch
1568,stegben,pr,2017-05-16T07:30:09Z,"[Need Discussion] fix #1524, let long_args False for param ""size"" of set_","It passed on my python3.5 w/o CUDA environment.

Just curious, why does `size` need to be a long argument? I thought that the size of a tensor should be fixed.

p.s. The test will be removed if this PR is approved.",pytorch
1675,jfsantos,pr,2017-05-29T15:55:06Z,Fix CUDA_HOME detection,"The way `CUDA_HOME` detection was implemented in `tools/setup_helpers/cuda.py` does not work, as `ctypes.util.find_library(cudart)` returns only the library name, not its path. `os.path.dirname(...)` in this case would return `/usr/local/cuda/lib`, not `/usr/local/cuda`. I am proposing a fix based on using the path to `nvcc` instead of `ctypes.util.find_library`. ",pytorch
1691,jekbradbury,pr,2017-06-01T00:50:34Z,tensor coalescing dance to avoid copy for cudnn rnn parameters,"Intended to solve #914, and currently passing tests.
I haven't yet run memory/performance comparisons, but I wanted to see if @apaszke and others are okay with the approach I've taken. I also coalesce on the gradients during the backward pass, which should save a memcpy but doesn't save any memory unless we can get the tensors that will receive the weight gradients to persist between calls.",pytorch
1862,stegben,pr,2017-06-21T10:35:16Z,fix #1241,,pytorch
1974,lanpa,pr,2017-07-04T15:05:39Z,corrects typo,,pytorch
1975,qbx2,pr,2017-07-04T19:14:22Z,Fix typo in model_zoo docstring,,pytorch
2037,DeNeutoy,pr,2017-07-10T16:53:33Z,support dictionary return types in nn.Module's __call__,"using `__call__` currently throws an error if the return type of `forward` is a `dict`. It would be useful to be able to return complex outputs with names - this PR just hooks the backward hooks onto an arbitrary element of the return dict (I assumed this was ok because previously the hook is just registered to the first element returned from `forward`). Let me know if i've put the test in the wrong place/ you would like it to be different. 

Thanks!",pytorch
2073,lanpa,pr,2017-07-13T06:32:38Z,resolves #1991,,pytorch
2158,stegben,pr,2017-07-20T06:21:03Z,fix typo of error message of cmul in THSTensorMath.h,,pytorch
2280,lanpa,pr,2017-08-03T07:42:20Z,fix doc of lr_scheduler,,pytorch
2417,jekbradbury,pr,2017-08-15T01:35:56Z,Add DistributedDataParallel to docs,DataParallel was included twice.,pytorch
2433,jekbradbury,pr,2017-08-15T20:45:14Z,"add ""Basics"" section to distributed docs",Some multiprocessing/distributed basics that might help new users. Also links there from the DistributedDataParallel docs.,pytorch
2477,stsievert,pr,2017-08-17T18:34:48Z,MAINT: kwargs refactor for distributed/__init__.py,"Modified API for init_process_group and kwargs error messages in {scatter, gather}.",pytorch
2544,Jiaming-Liu,pr,2017-08-26T23:24:31Z,Implement ReduceLROnPlateau with Backtrack,"Responding to https://github.com/pytorch/pytorch/issues/2478:
1. Enhance ReduceLROnPlateau with backtracking & autosaving function.
2. ReduceLROnPlateau.step(...) now return a status code.",pytorch
2562,iamaziz,pr,2017-08-29T15:30:23Z,Fix typo in docstring example,,pytorch
2744,stsievert,pr,2017-09-14T23:26:05Z,"DOC: i{send, recv} message order with MPI backend","Adds a link in the docs to http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54

This comes from a forum post I asked: https://discuss.pytorch.org/t/sending-many-tensors-with-isend-possible-identify-which-tensor-sent/7406",pytorch
2750,stsievert,pr,2017-09-15T17:08:19Z,DOC: adding note about distributed MPI backend,From https://github.com/pytorch/pytorch/issues/241#issuecomment-324945374,pytorch
2883,eklitzke,pr,2017-09-28T03:37:03Z,make torch.save() atomic by default,"This makes `torch.save()` atomic, by using a temporary file and then renaming it. This approach is guaranteed to be atomic on POSIX filesystems. I didn't make an option to use non-atomic saves since the overhead of doing this is very low (esp. compared to the other things that `torch.save()` has to do), so it didn't seem like it was worth the cognitive burden. I added some basic sanity tests.

Short version of why I want this: I have a training job that serializes model state every *N* iterations during training, and I have another process that periodically scans the state directory to look at the state files. Making `torch.save()` atomic ensures that there aren't any race conditions between process *A* saving a file and process *B* loading the file.",pytorch
2894,Jiaming-Liu,pr,2017-09-29T01:12:08Z,import lr_scheduler in optim/__init__.py,Fix https://github.com/pytorch/pytorch/issues/2809,pytorch
3000,Scitator,pr,2017-10-06T06:31:10Z,DistributedDataParallel IndexError bugfix for 2428 issue,"As you ask, https://github.com/pytorch/pytorch/issues/2428",pytorch
3127,blackyang,pr,2017-10-15T20:42:58Z,adaptive pooling supports only specifying size in certain dimension,"Adaptive pooling supports only specifying size in certain dimensions. For example, we can have:
```
x = Variable(torch.rand(1,3,10,10))
m = nn.AdaptiveMaxPool2d((5,None))
y=m(x)
y.size()  # torch.Size([1, 3, 5, 10])
```

This is useful for certain sequence related tasks",pytorch
3180,lanpa,pr,2017-10-19T15:43:13Z,implements operator + for Dataset class,"This PR implements operator + for `Dataset` class with a corresponding test case.
So we can write something like `tranval_dataset = train_dataset+val_dataset`",pytorch
3746,blackyang,pr,2017-11-16T21:55:22Z,change doc for Adaptive Pooling,See #3127 ,pytorch
3752,jekbradbury,pr,2017-11-17T01:21:33Z,Detect aliasing in cuDNN RNN flatten_parameters,"Fixes #3751. 
Currently, `nn.RNN.flatten_parameters` sets up a contiguous weight buffer to enable the no-copy code path for cuDNN RNNs, then uses `set_` to redirect the data pointers within the RNN's parameters to point into that weight buffer. Lastly it caches the list of parameter pointers within the buffer so that `nn.RNN.forward` can detect if the RNN's parameters change and fall back to the slow path. If certain preconditions aren't met, the cached list of parameter pointers is empty, forcing the RNN to always use the slow path. This patch adds an aliasing check to those preconditions.

The added test fails on master and succeeds with this change.",pytorch
3774,jekbradbury,pr,2017-11-18T19:09:37Z,fix cuDNN RNN weight tying test,"Apparently the warnings filter behaves differently on Py2 and Py3 (or maybe CentOS vs Ubuntu, etc.) and while the warnings we check for are in fact thrown on both platforms, they aren't caught on the 2.7 conda build worker. This removes the tests for the warnings and relies solely on the final correctness check. cc @soumith ",pytorch
3776,stefan-it,pr,2017-11-18T21:54:29Z,Fix torch::hash for GCC >=6 and clang,"Hi,

this is a follow-up PR for #3756 and #3767. It fixes a compilation on GCC >= 6 and clang, see #3772.

I tested this patch on following Distributions and compilers - so I hope it won't break something:

| Compiler     | Distribution
| ---------------- | -----------------
| GCC 4.9.4    | Ubuntu 16.04
| GCC 5.4.1    | Ubuntu 16.04
| GCC 6.3.0    | Ubuntu 16.04
| GCC 7.2.0    | Arch Linux
| clang 3.8.0  | Ubuntu 16.04
| clang 5.0.0  | Arch Linux

Fixes #3772.",pytorch
3777,stefan-it,pr,2017-11-18T23:00:58Z,flake8 fix,"Hi,

[this build](https://travis-ci.org/pytorch/pytorch/jobs/304105993) failed due to a `flake8` error:

```bash
./torch/distributions.py:108:1: W293 blank line contains whitespace
```

So this PR removes the whitespace.",pytorch
3951,mrshu,pr,2017-11-30T16:49:10Z,[docs] rnn.py: Note zero defaults for hidden state/cell,"* Add a note on zero defaults for hidden states/cells of
  RNNs/LSTMs/GRUs.

* Should fix the note in #434

Signed-off-by: mr.Shu <mr@shu.io>

--------------------

I am not sure about the wording here, so please feel free to suggest any changes.",pytorch
3962,lanpa,pr,2017-12-01T11:11:42Z,fix math notation in torch.squeeze doc,"Rendered result after fix:

![image](https://user-images.githubusercontent.com/2005323/33480239-62bbb83e-d6cb-11e7-9593-b2f8cb3329a8.png)
",pytorch
4032,lopuhin,pr,2017-12-05T13:41:13Z,Use enabled argument in torch.autograd.profiler.emit_nvtx,"Or else it's always enabled, and fails on CPU when ``enabled=False``",pytorch
4407,lanpa,pr,2017-12-29T12:07:56Z,fix documentation of RNN weight_ih_l[k] shape, fixes #4403 ,pytorch
4430,jusjusjus,pr,2018-01-01T17:57:09Z,[WIP] Added method cuda to PackedSequence.,,pytorch
4436,lanpa,pr,2018-01-02T16:10:21Z,Fix _trace function in torch.onnx,"Found this bug while trying to call it directly, an argument is missing.

This function was introduced in 47ac468. Looks like it's not covered by unit test. ",pytorch
4446,lanpa,pr,2018-01-02T20:16:01Z,Fix _trace function in torch.onnx (clean up of #4436),"Found this bug while trying to call it directly, an argument is missing.

This function was introduced in 47ac468. Looks like it's not covered by unit test.",pytorch
4470,sighingnow,pr,2018-01-04T03:05:43Z,"Improve float precision stability of `linspace` op, fix 4419.",This patch swap the order `*` and `/` operation in `linspace` to improve float precision stability. This patch should fix the issue #4419 .,pytorch
4488,jekbradbury,pr,2018-01-04T23:48:50Z,Fix typo in fusion compiler,"This mismatched paren causes a syntax error in generated code. I'm guessing the parentheses are worth keeping, since there was one in there before, but I don't actually know whether the compiler can produce things like a - (b - c) that would make them required.
  ",pytorch
4521,sighingnow,pr,2018-01-07T11:48:02Z,Fix abs specialization for `uint8_t` type.,Fixes #4520 ,pytorch
4527,sighingnow,pr,2018-01-08T02:25:13Z,"Fix the inconsistency of `polygamma` on Tensor and Variable, for issue #4466","In issue #4466, @fritzo found that the `torch.polygamma` works inconsistently on `Tensor` and `Variable`. When call the module methods on variables, the first two arguments need to be swapped if the `self`  argument comes second.

This patch call `IMPLEMENT_STATELESS_ADDXX` instead of `IMPLEMENT_STATELESS` for the swap action before dispatch, should fix the issue #4466.",pytorch
4561,sighingnow,pr,2018-01-09T16:56:26Z,Define USE_AVX2 macro when avx2 found.,The macro USE_AVX2 is required in avx_mathfun.h. This should fix issue #4531 .,pytorch
4579,sighingnow,pr,2018-01-10T14:37:44Z,"[ONNX] export sum, prod, sqrt improve log_softmax.","More exports:
+ `sum`
+ `prod`
+ `sqrt`

Improve:
+ `log_softmax`

Doc:
+ Fix a URL typo in `docs/source/onnx.rst`",pytorch
4627,sighingnow,pr,2018-01-12T06:46:32Z,Make any and all on ByteTensor behave like sum/prod.,"Make `ByteTensor`'s  methods `any` and `all` can accept `dim` and `keepdim` parameters, behave like `sum/prod`.

I add two macro `TENSOR_IMPLEMENT_ACC` and `TENSOR_IMPLEMENT_ACCALL` to make the code less duplicate.

This feature should fix issue #4313 ",pytorch
4637,sighingnow,pr,2018-01-12T16:14:20Z,More strict shape check on Conv operators.,"Add more strict shape check on Conv operators. The computed input size should never less than kernel size.

This patch should fix issue #4607 ",pytorch
4933,temporaer,pr,2018-01-30T11:13:14Z,fix copy/paste error in debug message in rnn.py,,pytorch
5147,lazypanda1,pr,2018-02-09T00:55:58Z,Added check and test for betas parameter in Adam optimizer,"This PR adds a check to prevent division by zero errors and give users friendlier error messages when using the Adam optimizer.

Currently, if one specifies the beta value of the Adam optimizer as `1.0` for the first parameter, the program fails with the error message, `ZeroDivisionError: float division by zero`. According to the definition of [Adam](https://arxiv.org/abs/1412.6980), beta values should be in the range \[0, 1\).

Also referring [#751](https://github.com/uber/pyro/pull/751) where I encountered and tried to fix this issue in the first place. I believe this would benefit all clients using pytorch as backend.",pytorch
5179,klshrinidhi,pr,2018-02-11T23:49:43Z,Fix compiler error introduced in 3e856137 by PR #5074,"The above mentioned PR introduced new files that currently fail to compile (gcc 4.9.2 / Debian Jessie) when using the below commands:
```
mkdir pytorch && cd pytorch
git clone --recursive https://github.com/pytorch/pytorch .
git submodule update --init
export NCCL_ROOT_DIR='/usr'
export TORCH_CUDA_ARCH_LIST=""3.5 5.2 6.0 6.1+PTX""
export TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""
export CMAKE_PREFIX_PATH=""$CONDA_HOME""
pip install -v .
```
Following is the error:
```
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/generated/aten_dispatch.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/generated/aten_dispatch.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH\
_CUDA -DCUDA_LIB_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/script/compiler.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/script/compiler.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB\
_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/passes/canonicalize.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/passes/canonicalize.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -D\
CUDA_LIB_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/autograd/init.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/autograd/init.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/l\
ocal/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/python_arg_flatten.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/python_arg_flatten.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCU\
DA_LIB_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/autograd/engine.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/autograd/engine.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/u\
sr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/python_ir.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/python_ir.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/l\
ocal/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/autograd/variable.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/autograd/variable.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PAT\
H=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/autograd/input_buffer.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/autograd/input_buffer.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA\
_LIB_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/autograd/python_function.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/autograd/python_function.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA \
-DCUDA_LIB_PATH=/usr/local/cuda/lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/include/ -I/tmp/pip-zICEN5-build -I/tmp/pip-zICEN5-build/torch/csrc -I/tmp/pip-zICEN5-build/torch/lib/pybind11/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include -I/tmp/pip-zICEN5\
-build/torch/lib/tmp_install/include/TH -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THNN -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/ATen -I/opt/conda/lib/python2.7/site-packages/numpy/core/include -I/tmp/pip-zICEN5-build/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/tmp/pip-zICEN5-buil\
d/torch/lib/tmp_install/include/THCUNN -I/usr/include -I/opt/conda/include/python2.7 -c torch/csrc/jit/type.cpp -o build/temp.linux-x86_64-2.7/torch/csrc/jit/type.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -Wno-missing-braces -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/local/cuda/\
lib64 -DWITH_NCCL -DWITH_CUDNN -DWITH_SCALARS
    torch/csrc/jit/script/compiler.cpp:349:41: error: converting to 'const AttributeMap {aka const std::unordered_map<std::basic_string<char>, std::pair<double, std::basic_string<char> > >}' from initializer list would use explicit constructor 'std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::unordered_map(std::unorder\
ed_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type, const hasher&, const key_equal&, const allocator_type&) [with _Key = std::basic_string<char>; _Tp = std::pair<double, std::basic_string<char> >; _Hash = std::hash<std::basic_string<char> >; _Pred = std::equal_to<std::basic_string<char> >; _Alloc = std::allocator<std::pair\
<const std::basic_string<char>, std::pair<double, std::basic_string<char> > > >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type = long unsigned int; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::hasher = std::hash<std::basic_string<char> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::key_equ\
al = std::equal_to<std::basic_string<char> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::allocator_type = std::allocator<std::pair<const std::basic_string<char>, std::pair<double, std::basic_string<char> > > >]'
           const AttributeMap& attributes = {},
                                             ^
    torch/csrc/jit/script/compiler.cpp:350:50: error: converting to 'const ListAttributeMap {aka const std::unordered_map<std::basic_string<char>, std::pair<const std::vector<double>, std::basic_string<char> > >}' from initializer list would use explicit constructor 'std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::uno\
rdered_map(std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type, const hasher&, const key_equal&, const allocator_type&) [with _Key = std::basic_string<char>; _Tp = std::pair<const std::vector<double>, std::basic_string<char> >; _Hash = std::hash<std::basic_string<char> >; _Pred = std::equal_to<std::basic_string<\
char> >; _Alloc = std::allocator<std::pair<const std::basic_string<char>, std::pair<const std::vector<double>, std::basic_string<char> > > >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::size_type = long unsigned int; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::hasher = std::hash<std::basic_string<char> >;\
 std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::key_equal = std::equal_to<std::basic_string<char> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::allocator_type = std::allocator<std::pair<const std::basic_string<char>, std::pair<const std::vector<double>, std::basic_string<char> > > >]'
           const ListAttributeMap& list_attributes = {}) {
                                                      ^
```",pytorch
5297,rkaplan,pr,2018-02-19T10:54:09Z,[WIP] Add nn.ModuleDict (#4048),"Issue #4048

This PR implements an nn.ModuleDict class. Its purpose is similar to nn.ModuleList (ensuring modules in a collection are properly registered) but it exposes a dict interface instead. See issue #4048 for discussion.

This PR is still untested as I have been unable to compile PyTorch from source on my Mac; please do not merge it yet. I am putting it out for feedback now until I find the chance to sit down and fix the compilation issues and test it myself.

This is my first contribution to PyTorch, please let me know if I should be doing anything differently. Cheers.
",pytorch
5358,lazypanda1,pr,2018-02-22T23:28:38Z,Fixed distribution constraints and added some test cases for distributions parameter check,"Hi,

This PR fixes an issue with the constraints: GreaterThan and LessThan. I have also added testcases to check that pytorch raises exceptions for invalid arguments for all distributions. 

Introduced a global flag `_validate_args` in `Distribution` which allows enabling `validate_args` for all distributions rather than enabling it for every distribution individually. Without this, adding this parameter for all distribution might be tedious for the user.

Also, fixed some existing issues with several distributions and tests.

This partially fixes [#5248](https://github.com/pytorch/pytorch/issues/5248) and can go along with [#133](https://github.com/probtorch/pytorch/pull/133)

This has been reviewed in [probtorch#134](https://github.com/probtorch/pytorch/pull/134)",pytorch
5503,theweiho,pr,2018-03-01T20:21:13Z,Add per-element unique op for CPU,"Questions/possible future works:
* How to template-ize to extend support beyond LongTensor?
* How to check if autograd works (and if not, how to add explicit gradient)?
* CUDA support?

Testing command:
DEBUG=1 NO_CUDA=1 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build &&  DEBUG=1 NO_CUDA=1 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py develop && python3 test/test_torch.py

Commands to preview generated documentations:
cd docs
pip install -r requirements.txt
make html",pytorch
5608,Officium,pr,2018-03-07T09:05:13Z,fix issue 5556: arange floating point error,One can use `a / c - b / c` to reduce floating point error of `(a - b) / c`,pytorch
5621,jpuigcerver,pr,2018-03-07T23:51:59Z,Fix compilation with CUDA < 8.0,"I noticed that I could not compile the master with CUDA 7.5 anymore. I know it's not officially supported anymore, but I thought this were easy fixes.

Fixed issues:
- CUDA versions prior to 8.0 do not allow static variables in device code, other than __shared__.
- nccl needed workaround flags for CUDA < 8.0 and GCC >= 4.9
",pytorch
5659,Officium,pr,2018-03-09T09:18:44Z,make dimension checker of `scatter_add_` consistent with `scatter_`,fix issue #5405 ,pytorch
5913,theweiho,pr,2018-03-20T21:29:11Z,Optimize unique sorting by using std::vector+sort instead of std::set,"Revert to original implementation using std::vector and std::sort instead of templatizing std::set since profiling shows std::set taking ~5x as long as vector+sort.

@apaszke @goldsborough @colesbury @ezyang ",pytorch
5952,jekbradbury,pr,2018-03-23T01:01:22Z,mention cmake3 for Trusty,"I'm guessing in general you don't want to mention a bunch of special cases here, but Ubuntu 14.04 is probably the second or third most common platform overall and PyTorch doesn't seem to build with cmake 2 (or give a useful error message). I was only able to get a build to work on our 14.04 workstations after finding [this](https://github.com/pietern/pytorch-dockerfiles/blob/master/common/install_base.sh#L13) in the Jenkins dockerfiles.",pytorch
6000,lazypanda1,pr,2018-03-26T04:22:55Z,Added parameter range checks for all optimizers,"This PR adds parameter range checks to all optimizers to ensure that end-users do not end up providing invalid values to the optimizers and be confused by the output when there is no actual problem with their model.

For example, running the following program produces `NaN`s in the output, due to invalid value of `rho` (>1.0).

```python
import torch
from torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10

x = Variable(torch.randn(N, D_in))
y = Variable(torch.randn(N, D_out), requires_grad=False)

model = torch.nn.Sequential(
    torch.nn.Linear(D_in, H),
    torch.nn.ReLU(),
    torch.nn.Linear(H, D_out),
)
loss_fn = torch.nn.MSELoss(size_average=False)

learning_rate = 1e-4
optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=1.1)
for t in range(2):
    y_pred = model(x)
    loss = loss_fn(y_pred, y)
    print(t, loss.data[0])
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

Output:
```
0 651.8707885742188
1 nan
```

I tried adding constraints for all the parameters that I could infer from the corresponding articles, but I am still missing some. Please feel free to suggest what should be bound for the ones which are missing.

This is similar to the bounds check which I added for [`Adam Optimizer`](https://github.com/pytorch/pytorch/pull/5147)

I can also add tests if needed.",pytorch
6031,Jiaming-Liu,pr,2018-03-27T01:27:57Z,Block set from param_group['params'],"This might cause `list(params)` to output in random order. In this case, in `load_state_dict()`, keys & values of `id_map` would not be matched correctly.",pytorch
6116,lazypanda1,pr,2018-03-29T18:50:20Z,Fixed some tests by using the correct optimizer,Some tests were using the incorrect optimizer. Fixed them.,pytorch
6719,mrshu,pr,2018-04-18T19:33:39Z,rnn: A note on zero defaults for recurrent cells,"* Add a note on zero default for recurrent cells

* Fixes #434

Signed-off-by: mr.Shu <mr@shu.io>

",pytorch
6752,103yiran,pr,2018-04-19T08:41:13Z,Make the variable closer to usage,"`chain` is used for the loop below.

",pytorch
6923,fumihwh,pr,2018-04-25T01:19:03Z,[Caffe2] add default value to ConstantFill doc,"Add default value 0.0f to ConstantFill doc.

cc @houseroad 
",pytorch
6954,fumihwh,pr,2018-04-25T16:33:02Z,[ONNX] export tile op,,pytorch
7063,theweiho,pr,2018-04-28T02:11:07Z,"Open-source extractMetaNetDef & runGlobalInitialization, add new Predictor constructor from db file, and add run_map_outputs","1. Open-source extractMetaNetDef and runGlobalInitialization, for use in
2. new Predictor constructor from db file.
3. Add new run function that returns outputs as TensorMap.
",pytorch
7367,103yiran,pr,2018-05-08T04:10:35Z,Correct the parameter annotation,"Make the annotation keep pace with  the parameter.

",pytorch
7406,103yiran,pr,2018-05-09T02:48:59Z, Delete duplicated Macro ,The content of `USE_SIMPLE_BASE_CTOR_DTOR` is the same as `USE_SIMPLE_CTOR_DTOR`.,pytorch
7473,knightXun,pr,2018-05-10T17:17:20Z,fix some annotation in setup.py/setup_caffe2.py,just modify some annotation in setup.py/setup_caffe2.py,pytorch
7534,domschl,pr,2018-05-13T09:00:24Z,[DOC] return value of LSTM example fixed.,,pytorch
7724,qixiuai,pr,2018-05-21T04:24:14Z,Fix typo in extending.rst,"replace `Linear.apply `  with `linear`

",pytorch
7725,qixiuai,pr,2018-05-21T05:48:29Z,Fix typo in extending.rst,"replace `Linear.apply` with `linear`.

I deleted the branch unexpectedly. Sorry for repeat code review.",pytorch
7876,qixiuai,pr,2018-05-26T05:10:33Z,Fix typo,,pytorch
8116,Ir1d,pr,2018-06-04T16:07:00Z,docs: Add warning to torch.repeat(),closes #7993 ,pytorch
8155,Ir1d,pr,2018-06-05T07:34:25Z,docs: add canonical_url and fix redirect link,closes #7222 ,pytorch
8580,rainwoodman,pr,2018-06-16T01:02:21Z,Mention MPICH_MAX_THREAD_SAFETY=multiple.,"Currently, this is a common step to enable level 3 support on MPICH based systems.

",pytorch
8807,jongwook,pr,2018-06-22T21:43:36Z,Doc: specify batch_first is True by default in RNN,This is a trivial edit - I was going through the docstring of `nn.LSTM` and noticed that it doesn't explicitly say the default value for the `batch_first` argument unlike the other arguments.,pytorch
9240,103yiran,pr,2018-07-08T08:24:26Z,fix the reference link path,,pytorch
9262,103yiran,pr,2018-07-09T09:48:53Z,make the variable declaration closer to usage,,pytorch
9380,103yiran,pr,2018-07-12T08:05:35Z,fix the annotation,,pytorch
9475,hartb,pr,2018-07-16T22:21:40Z,test_cuda: ensure tests use float and adjust HalfTensor tolerances,"test_cuda.py uses routine 'number' to prepare many testscases.
number should return a floating point value for float-type tensor
types, or integer otherwise. But number's test to classify the type
is incorrect, so it always returns the integer value.
(type(t).__name__ is always 'torch.tensortype' so never matches
'Double', 'Float', or 'Half'.)

Update number to use the existing is_floating() helper to make the
check.

The change to number causes a few tests to fail for HalfTensor. Relax
the tolerance for those in line with other HalfTensor testcases. The
failing tests--for addcdiv and fill--were not previously relaxed for
HalfTensor so are held to the over-strict 1e-5 default tolerance.

Finally, update a couple other tests for HalfTensor type to use the
existing is_half() helper.

",pytorch
9852,achalshah20,pr,2018-07-26T00:29:50Z,OpenCV 4.0 Compatibility fix,caffe2 compiles with latest opencv after committed changes. ,pytorch
9868,tomguluson92,pr,2018-07-26T07:52:07Z,revise a little spell mistake in tensor.py,"Hello! I just find a small spell mistake while reading this source code. Just PR it, Thx!

",pytorch
9966,achalshah20,pr,2018-07-28T00:12:13Z,OpenCV 4.0 Compatibility fix,caffe2 compiles with latest opencv 4.0 after committed changes.,pytorch
10098,achalshah20,pr,2018-07-31T22:03:43Z,Stop propagating std flags to downstream gcc/nvcc,"When we directly use -std=c++11, it propagates to the downstream applications. 

Problems:
1. Gcc flags propagating to nvcc.
2. nvcc flags propagating to nvcc. (Which throws an error like redeclaration of std flag)

This PR will fix these propagation issues! 

Similar problem:
https://github.com/FloopCZ/tensorflow_cc/pull/92
https://github.com/CGAL/cgal/issues/2775

Requires: Cmake 3.12",pytorch
10330,jekbradbury,pr,2018-08-07T21:49:13Z,use Ubuntu PPA to get Git 2.9 in Dockerfile,Should fix #3542. My use case is trying to get PyTorch to build with Google Cloud Build using an equivalent of `docker build https://github.com/pytorch/pytorch.git#master:docker/pytorch` (which also runs into this problem locally).,pytorch
10340,tomguluson92,pr,2018-08-08T02:51:42Z,Unified Style of type judgement,"unified the code style as the below `elif ...` sentences

",pytorch
10519,hartb,pr,2018-08-14T21:55:13Z,relax tolerance for two torch.half (float16) tests,"Two tests in the 'nn' test bucket may fail when the torch.half
(float16) data type is used. The assertions used in the tests
intend to allow slight floating point imprecision in the results,
but the tolerances used for the comparisons are too strict for
the half type.

Relax the tolerances so that slight float16 imprecision won't
cause test failures.

The affected tests are:

- test_variable_sequence_cuda
- test_Conv2d_groups_nobias

For more information, see issue:

https://github.com/pytorch/pytorch/issues/7420",pytorch
11328,tomguluson92,pr,2018-09-06T07:54:11Z,ADD child module at ARBITRAY POSITION in current module,"I revised member function add_module(...) of Module. I reinforce it by add child module to arbitary position of the current module.
Samples like this:

I revised member function `add_module(...)` of Module. I reinforce it by add child module to arbitary position of the current module.
Samples like this:
``` python
class Perceptron(nn.Module):

    def __init__(self, in_features, out_features, hidden_features):
        super(Perceptron, self).__init__()

        self.layer1 = nn.Linear(in_features, hidden_features)
        self.layer2 = nn.Linear(hidden_features, 4)
        self.layer3 = nn.Linear(4, out_features)



    def forward(self, x):

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        return x
```
When we have a lot of module like this kind. if we need to add a child module to all of this kind of module.
We can easily specify the child module position via setting the parameter `index` to a number(default is -1, the same
as now PyTorcher's already define.)

It can easily use the following code to do add a specified child module between **model.layer2** and **model.layer3**:
``` python
class Perceptron_ADD(nn.Module):
    def __init__(self):
        super(Perceptron_ADD, self).__init__()
        model = Perceptron(10, 5, 4)

        model.add_module(""arbitarylayer"", nn.Linear(4, 4), index=2)

        self.model = model

    def forward(self, x):
        return self.model(x)

",pytorch
11370,wangg12,pr,2018-09-07T06:14:45Z,update CUDAExtension doc,fix typo,pytorch
12196,103yiran,pr,2018-09-29T11:13:20Z,add filename extension,,pytorch
12239,sclarkson,pr,2018-10-01T22:16:59Z,Allow linking to backwards-compatible cuDNN at runtime,Fixes #12193,pytorch
12258,Ir1d,pr,2018-10-02T11:08:42Z,docs: change links to https,"Hi, I think it might be better to use https instead of http in the README.md.",pytorch
12265,svenstaro,pr,2018-10-02T19:15:10Z,Document CUDAHOSTCXX environment variable,"This variable is already being used so this just serves to document that. I think it's an important variable, too, so it should definitely be documented there somewhere.",pytorch
12440,marcemq,pr,2018-10-07T23:12:16Z,[Docs] Add missing url links to README.md file.,"Signed-off-by: Marcela Morales Quispe <marcela.morales.quispe@gmail.com>

",pytorch
12441,marcemq,pr,2018-10-07T23:21:35Z,[Docs] Format inline code block.,"Signed-off-by: Marcela Morales Quispe <marcela.morales.quispe@gmail.com>

",pytorch
12513,tomguluson92,pr,2018-10-10T02:27:29Z,Delete redundant state of `col2im`,,pytorch
12514,tomguluson92,pr,2018-10-10T02:36:55Z,[ATen/CUDA]Delete redundant statement of `col2im`,"Hi, I found that there was two statement of `col2im` in `im2col.h` and think the former one 
may be redundant.",pytorch
12526,103yiran,pr,2018-10-10T09:59:01Z,Remove duplicate codes,,pytorch
12612,hartb,pr,2018-10-12T18:15:49Z,test_proper_exit: avoid truncation of info message,"test_proper_exit in the dataloader test bucket includes
(as its docstring) a reassuring message about complaints that
may appear during the test. The message is displayed
when the tests are run in verbose mode.

But the docstring includes a line break, and the unittest
framework only prints the first line of the docstring (see
shortDesription()). As a result, the 2nd (more reassuring)
half of the message is not displayed.

Catenate the docstring onto a single line so all is visible.

",pytorch
12636,eklitzke,pr,2018-10-14T21:04:21Z,Rewrite http://pytorch.org -> https://pytorch.org throughout project,"The pytorch.org site redirects all of the http:// requests to the https:// site anyway, so the comments and error messages might as well refer directly to the https:// site. The GitHub project description should also be updated to point to https://pytorch.org",pytorch
12753,103yiran,pr,2018-10-17T05:57:51Z,Remove redundant semicolon,,pytorch
12969,103yiran,pr,2018-10-23T01:53:29Z,Move the location of annotation,,pytorch
12974,103yiran,pr,2018-10-23T02:41:57Z,Rearrange the code for data pointers,,pytorch
13102,lyuwenyu,pr,2018-10-25T03:24:14Z,Keep `ModuleList` consistent with python `list` in `__setitem__` function.,"`ModuleList` class function `__setitem__` has implicit rist
```
In [26]: mlist = nn.ModuleList([nn.ReLU(), nn.Conv2d(10, 10, 3, 1)])

In [27]: mlist
Out[27]: 
ModuleList(
  (0): ReLU()
  (1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))
)

In [28]: mlist[-1] = nn.ReLU()

In [29]: mlist
Out[29]: 
ModuleList(
  (0): ReLU()
  (1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))
  (-1): ReLU()
)

In [30]: mlist[-1]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-30-229d1b6823a0> in <module>()
----> 1 mlist[-1]

~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py in __getitem__(self, idx)
    134             return ModuleList(list(self._modules.values())[idx])
    135         else:
--> 136             return self._modules[self._get_abs_string_index(idx)]
    137 
    138     def __setitem__(self, idx, module):

KeyError: '2'

```

modified as 
```
    def __setitem__(self, idx, module):
        idx = self._get_abs_string_index(idx)
        return setattr(self, str(idx), module)
```
to fix it.

```
In [31]: class NewModuleList(nn.ModuleList):
    ...:     def __setitem__(self, idx, module):
    ...:         idx = self._get_abs_string_index(idx)
    ...:         return setattr(self, str(idx), module)
    ...:     

In [32]: mlist = NewModuleList([nn.ReLU(), nn.Conv2d(10, 10, 2, 1)])

In [33]: mlist[-1] = nn.ReLU()

In [34]: mlist
Out[34]: 
NewModuleList(
  (0): ReLU()
  (1): ReLU()
)
```

",pytorch
13307,ragulpr,pr,2018-10-30T14:46:06Z,Fix add out=None to digamma docstring (Fixes #13225),Fixes #13225 ,pytorch
13339,ragulpr,pr,2018-10-30T21:32:54Z,Cleanup gumbel_softmax,"Fixes #12643, amends to #3341.

- Allow multidimensional input ~~(but apply softmax over `dim=-1`)~~ with `dim` argument
- Cleaner: Less lines of code
- Faster (1.32x speedup vs original, 2x speedup vs using `torch.Distributions`)
- Small fixes in docstring
- Remove some references in docstring. Was the linked (excellent) ipynb the first to do the straight-through trick? Instead, I propose changing to reference to the two papers most known for it.
- Add deprecationwarning for `eps`. It's not needed anymore.
- Initial commit keeps some code alternatives commented to exploit CI

## Controversies:
- As of discussion when `gumbel_softmax` was added (#3341), this was merged into `torch.nn.functional` before all the work with `Distributions` and `Pyro`, and there will probably be multiple other best practices for this in the future. 
I've tested building using the `Distributions`-api, but it was too slow, see below.

I therefore propose not using `Distributions` to keep it fast and simple, but adding a comment in docstring that `gumbel_softmax` may be deprecated in the future. 

### Build on `torch.distributions.RelaxedOneHotCategorical`?
```
dist = torch.distributions.RelaxedOneHotCategorical(temperature=tau, logits=logits, validate_args=False)
y_soft = dist.rsample()
```

Pros:
* Built using tricks like `logsumexp` etc
* Explicitly uses `torch.distributions.utils._finfo` to avoid overflow (old implementation had an `eps` flag)
* Maintained for this exact purpose.

Cons:
* Very slow. Construction of distribution adds overhead see timings below. May be solved in future with speedups of `TransformedDistribution` and `Distribution`.
* Assumes which `dim` to apply softmax over.

### Build on `torch.exponential_()` (as proposed)?
```
    y_soft = logits.new(logits.shape)
    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise
    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise
```
Pros:
* Faster

## Timings:
```
    import time
    start = time.time()
    num_draws = 1000000
    logits = torch.randn(1,3)

    for draw in range(num_draws):
        y_draw = gumbel_softmax(logits, hard=True)
        counts = counts + y_draw
    print(end - start)        

## torch.nn.functional.gumbel_softmax()
>> 12.995795965194702

## Using exponential_() as this commit
>> 7.658372640609741

## Using RelaxedOneHotCategorical
>> 20.3382670879364
````

## TODO
Decide on which path to chose. I'll commit in changes to the unit tests in a while to show that it passes both old tests and new tests. I'll also remove the commented code about `RelaxedOneHotCategorical`",pytorch
13727,dan-zheng,pr,2018-11-08T18:24:06Z,Fix typo in CTC loss doc comments.,`target_lenghts` -> `target_lengths`,pytorch
14034,mrTsjolder,pr,2018-11-15T17:23:50Z,use correct initialisation in linear module,"From [this discussion](https://discuss.pytorch.org/t/whats-the-default-initialization-methods-for-layers/3157/17) it appears that the linear layer should be initialised according to (LeCun et al., 1998). However, LeCun et al. promote a variance of `1/n`, rather than a uniform distribution with bounds `1/n`! Also biases are generally expected to be initialised with zeros.

",pytorch
14302,achalshah20,pr,2018-11-22T00:22:57Z,Cuda version comparison with CUDA_VERSION_STRING,"Cuda headers include cuda version in form of major.minor. But when we do find_package(cuda). CUDA_VERSION variable includes patch number as well which fails following condition.

`
if(NOT ${cuda_version_from_header} STREQUAL ${CUDA_VERSION})
`

**For example:**
I have cuda 10.0 installed. My nvcc output looks like this
`Cuda compilation tools, release 10.0, **V10.0.130**
`

If I compile my application with caffe2. It gives me following error: 

```
CMake Error at /usr/share/cmake/Caffe2/public/cuda.cmake:59 (message):
  FindCUDA says CUDA version is (usually determined by nvcc), but the CUDA
  headers say the version is 10.0.  This often occurs when you set both
  CUDA_HOME and CUDA_NVCC_EXECUTABLE to non-standard locations, without also
  setting PATH to point to the correct nvcc.  Perhaps, try re-running this
  command again with PATH=/usr/local/cuda/bin:$PATH.  See above log messages
  for more diagnostics, and see
  https://github.com/pytorch/pytorch/issues/8092 for more details.
```

**In this case, it got failed because** 
cuda_version_from_header = 10.0
CUDA_VERSION = 10.0.130 (Came from NVCC)


`if(NOT ${cuda_version_from_header} STREQUAL ${CUDA_VERSION})
`

**Fix:**
We should compare header version with **major.minor format** which is given by CUDA_VERSION_STRING",pytorch
14306,achalshah20,pr,2018-11-22T02:12:35Z,CAFFE2_INCLUDE_DIRS points to invalid path,"I know that including CAFFE2_INCLUDE_DIRS in include headers are not necessary for newer cmakes. But I had this in one of my old projects and **cmake gave me error that ""/usr/lib/include"" is invalid path**. 

It seems like ""${_INSTALL_PREFIX}/lib/include"" should be changed to ""${_INSTALL_PREFIX}/include"" as all caffe2 headers are in /include rather than /lib/include/

Please correct me if I am wrong?",pytorch
14718,JoshVarty,pr,2018-12-03T21:00:22Z,[C++ Frontend] Kaiming Initialization,"/cc @goldsborough 

Working on #14582 

The corresponding python implementations are at: [pytorch/torch/nn/init.py](https://github.com/pytorch/pytorch/blob/6302e4001ab54b3ddeca2b608d337fe7077e801c/torch/nn/init.py#L261-L327)

Here is my initial implementation of Kaiming Initialization. I have not been able to figure out how to successfully run tests locally so I haven't added any yet. 

A couple questions: 
- Are the enums defined in the right place? I copied their names from Python, but do you prefer different naming conventions for C++?
- To run tests locally do I use `python setup.py test`? Can I run just a subset of the tests somehow?
- Should I add my tests at [test/cpp/api/misc.cpp](https://github.com/pytorch/pytorch/blob/master/test/cpp/api/misc.cpp#L47-L54)?
 

",pytorch
15039,daniel-s-ingram,pr,2018-12-11T02:37:07Z,Add error type to raise statement,,pytorch
15136,rkaplan,pr,2018-12-12T20:50:51Z,"Remove ""early-release beta"" disclaimer from README","Now that PyTorch 1.0 is out, this should be updated :)",pytorch
15396,epicfaace,pr,2018-12-19T16:08:19Z,Replace getargspec with getfullargspec,Replace `getargspec` with `getfullargspec` to resolve test warnings. Fixes #15344 .,pytorch
15615,hmaarrfk,pr,2018-12-29T16:18:59Z,Add a patch for OSX with SDK<10.12,"Fixes https://github.com/pytorch/pytorch/issues/15614

Build passing on SDK 10.9
https://dev.azure.com/ramonaoptics/feedstock-builds/_build/results?buildId=13",pytorch
15733,BoboTiG,pr,2019-01-04T14:20:36Z,Fix several DeprecationWarning: invalid escape sequence,"Hello,

This is a little patch to fix `DeprecationWarning: invalid escape sequence`.

",pytorch
15746,BoboTiG,pr,2019-01-04T21:29:37Z,Fix several ResourceWarning: unclosed file,"Hello,

This is a patch to fix `ResourceWarning: unclosed file`.",pytorch
16196,lanpa,pr,2019-01-20T16:51:23Z,TensorBoard support within PyTorch,"This PR adds TensorBoard logging support natively within PyTorch. It is based on the tensorboardX  code developed by @lanpa and relies on changes inside the tensorflow/tensorboard repo landing at https://github.com/tensorflow/tensorboard/pull/2065.

With  these changes users can simply `pip install tensorboard; pip install torch` and then log PyTorch data directly to the TensorBoard protobuf format using

```
import torch
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
s1 = torch.rand(1)
writer.add_scalar('data/scalar1', s1[0], 0)
writer.close()
```

Design:
- `EventFileWriter` and `RecordWriter` from tensorboardX now live in tensorflow/tensorboard
- `SummaryWriter` and PyTorch-specific conversion from tensors, nn modules, etc. now live in pytorch/pytorch. We also support Caffe2 blobs and nets.

Action items:
- [x] `from torch.utils.tensorboard import SummaryWriter`
- [x] rename functions
- [x] unittests
- [x] move actual writing function to tensorflow/tensorboard in https://github.com/tensorflow/tensorboard/pull/2065

Review:
- Please review for PyTorch standard formatting, code usage, etc.
- Please verify unittest usage is correct and executing in CI

Any significant changes made here will likely be synced back to github.com/lanpa/tensorboardX/ in the future.

cc @orionr, @ezyang ",pytorch
16201,shahzadlone,pr,2019-01-20T23:36:12Z,Reserve vectors that we know the size in advance for.,"Save reallocation costs, by reserving vectors according to how many elements we expect to put in.

",pytorch
16340,hartb,pr,2019-01-24T22:46:38Z,caffe2: fix PinnedCPUAllocator cudaHostRegister() leak,"In the NUMA case, PinnedCPUAllocator's allocate() would return a
DataPtr constructed by DefaultCPUAllocator, which would reference
the Default... Delete() rather than the Pinned... Delete(). That
meant Pinned... Delete() would never run, so cudaHostUnregister()
would never be called when regions were freed.

See: https://github.com/pytorch/pytorch/issues/16280

This change adds a 'naked_allocate()' method to the Default allocator
that just returns a pointer to the allocated memory rather than
wrapping it in a DataPtr. Pinned allocator uses that then constructs
a DataPtr with reference to its own Delete().

",pytorch
16842,ZhuBaohe,pr,2019-02-07T13:46:27Z,DOC: correct docstring for torch and torch.Tensor package,"This PR is a simple fix for the mistake in the  ""tensor""  and ""torch.Tensor""doc.",pytorch
16995,knightXun,pr,2019-02-12T05:49:20Z,refactor: a bit intricate so I refactor it,this code is a bit intricate so i refactor it ,pytorch
17022,bdaskalov,pr,2019-02-12T19:55:39Z,Make mkldnn Stream object thread_local and enable mkldnn thread-safe,"This PR fixes following issue: https://github.com/pytorch/pytorch/issues/16828

It is a combination of two things:
1) MKLDNN streams are not thread-safe but are currently shared between different threads. This change makes them thread_local
2) By default MKLDNN primitives can share global memory and can't be invoked from multiple threads. This PR enables the MKLDNN_ENABLE_CONCURRENT_EXEC cmake configuration option that makes them thread-safe.

",pytorch
17052,ZhuBaohe,pr,2019-02-13T14:44:16Z,Correct conv and pooling docstrings in nn module,This PR fix conv and pooling docstrings in nn module,pytorch
17197,ZhuBaohe,pr,2019-02-16T07:40:40Z,Correct padding and activations docstrings in nn module,,pytorch
17219,knightXun,pr,2019-02-17T13:55:50Z,refactor THCUNN/IndexLinear.cu,,pytorch
17238,ZhuBaohe,pr,2019-02-19T01:23:39Z,Correct recurrent/linear/dropout/sparse layers docstrings ,,pytorch
17300,ZhuBaohe,pr,2019-02-20T13:41:19Z,Correct loss docstrings,"In the loss doc description, replace the deprecated 'reduct' and 'size_average' parameters with the 'reduction' parameter.",pytorch
17307,igormq,pr,2019-02-20T18:10:23Z,Fixing docstring in CTCLoss,"The argument `zero_infinity` is in the wrong place! :)

",pytorch
17351,ZhuBaohe,pr,2019-02-21T12:52:18Z,Correct docstring of vision/init functions,,pytorch
17653,youkaichao,pr,2019-03-04T11:52:22Z,typo fix,,pytorch
17744,youkaichao,pr,2019-03-07T11:03:51Z,add function to select GPUs,"Currently, we only have the `torch.cuda.device_count` function. It would be better if we can know which devices are available.

Say that there are 8 GPUs on the server, but the GPU 2 & GPU 3 are often in usage cause they are Titan V while others are GTX 1080. If I want to use 4 GPUs, I should manually select 4 GPU IDs every time I run the code, which is painful and not elegant.

If we have a function to tell which devices are available, it would rescue people from manually selecting GPUs and eliminate the need of `--gpu` cmd argument :)",pytorch
17836,lopuhin,pr,2019-03-09T08:30:26Z,Fix PySlice_Unpack not available on PyPy 3.6 yet,"This is one of the fixes needed to support compilation on PyPy 3.6, see https://github.com/pytorch/pytorch/issues/17835",pytorch
17837,lopuhin,pr,2019-03-09T08:30:32Z,PyPy compatibility: let unmodified slots be inherited in the standard way,"This is needed to fix a segfault on PyPy 3.6, see https://bitbucket.org/pypy/pypy/issues/2968/segfault-calling-cpyext_tp_new_tuple and https://github.com/pytorch/pytorch/issues/17835",pytorch
17841,sighingnow,pr,2019-03-09T15:44:15Z,"When openblas exists, ""OpenBLAS_FOUND"" is defined, rather than ""OPENBLAS_FOUND"".","See https://github.com/pytorch/pytorch/blob/master/cmake/Modules/FindOpenBLAS.cmake#L36

This typo lead to cmake fails to detect openblas on ubuntu.",pytorch
18420,ZhuBaohe,pr,2019-03-25T06:08:24Z,Fix loss functions doc,Correct docstring display error on web page caused by my previous PR,pytorch
18422,nihui,pr,2019-03-25T08:43:44Z,Fix caffe2 build with BLAS=OpenBLAS,"g++ complains about failing to find the declaration of cblas_sscal and cblas_dscal BLAS function
let's fix it  :)

fedora 29, gcc 8.3.1, openblas 0.3.5
build with cmake -DBLAS=OpenBLAS ..",pytorch
18568,sublee,pr,2019-03-28T15:57:37Z,Make checkpointing evaluate only the related tasks,"This patch fixes https://github.com/pytorch/pytorch/issues/18566 what I reported.

> When the autograd graph has parallel lanes, and a checkpointing is on one of the lanes, the checkpointing evaluates irrelevant backwards on another lane.

For example, in the below autograd graph, `Checkpoint(Eval2)` evaluates `Eval1` as its recursive call:
```
          +--> Eval1 --> AccumulateGrad1
Sum --> Cat
          +--> Checkpoint(Eval2) --> AccumulateGrad2
```

The reason I found is the order of priority between `Eval1` and `Eval2` which is recomputed by `Checkpoint(Eval2)`. In the autograd engine, the order priority relies on a function's sequence number. The sequence number generator is thread-local. `Eval1` is computed on the main thread but `Eval2` is recomputed on the worker thread. So there's no logical synchronization between them.

## How `Eval1` evaluates during `Checkpoint(Eval2)`

Let's suppose there were 10 autograd functions before this case. The sequence numbers of the functions would be like:
```
Eval1: 10
Checkpoint(Eval2): 11
Cat: 12
Sum: 13

Recomputed Eval2: 0
```

The autograd engine evaluates a task with a function having a higher sequence number first. Thus, the tasks/functions will be evaluated in the following order: (I omitted `GraphRoot` to simplify)

1. Evaluate `Sum: 13`, then schedule `Cat`.
1. Evaluate `Cat: 12`, then schedule `Eval1` and `Checkpoint(Eval2)`.
1. Evaluate `Checkpoint(Eval2): 11` which has a higher priority than `Eval1: 1`, then schedule a new recomputed `Eval2: 0`.
   1. **(recursion) Evaluate `Eval1: 10` which has a higher priority than `Eval2: 0`**
   1. (recursion) Evaluate `Eval2: 0`

`Eval1: 10` should be evaluated AFTER `Checkpoint(Eval2): 11` done. But it is evaluated WITHIN `Checkpoint(Eval2): 11`.

## How this patch fixes it

I propose a stack depth counter for nested backward. Now the autograd engine evaluates a deeply nested task first even it has a lower sequence number. Nested backward would evaluate only the tasks created in the backward.

The priority is modified like: (depth, sequence number)

```
Eval1: (0, 10)
Checkpoint(Eval2): (0, 11)
Cat: (0, 12)
Sum: (0, 13)

Recomputed Eval2: (1, 0)
```

Please review my pull request. I would be happy to discuss this approach.",pytorch
18839,theweiho,pr,2019-04-04T02:55:48Z,"Back out ""[pytorch][PR] Adding pin_memory kwarg to zeros, ones, empty, ... tensor constructors.""","Differential Revision: D14766095
",pytorch
19337,lhendre,pr,2019-04-17T04:25:34Z,"[jit] math module support: isnan, asinh, atanh, cosh, sinh, and tanh"," @driazati and @eellison Please review This PR is for #19026 .  Specifically, isnan, asinh, atanh, cosh, sinh, and tanh",pytorch
19403,qbx2,pr,2019-04-18T10:56:17Z,Fix isinstance() for WeakScriptModuleProxy,"Close #19348

cc: @driazati @ailzhang ",pytorch
19586,deadeyegoodwin,pr,2019-04-22T22:49:43Z,Caffe2 shouldn't fail if CUDA peer access is already enabled,,pytorch
19794,Officium,pr,2019-04-26T04:12:13Z,update Anaconda download link,Now `https://www.continuum.io/` is redirected to `https://www.anaconda.com` and old Anaconda download link `https://www.continuum.io/downloads` is dead. This PR update it to `https://www.anaconda.com/distribution/#download-section`.,pytorch
19959,lanpa,pr,2019-04-30T11:00:02Z,specify data type in the doc,addresses comments in #19915,pytorch
20003,qbx2,pr,2019-05-01T10:25:05Z,[DOC] Fix examples in jit#user-defined-types documentation,,pytorch
20007,lanpa,pr,2019-05-01T12:54:53Z,[tensorboard] smoke test for add_graph,"Do tests with common models from torchvision.

",pytorch
20008,lanpa,pr,2019-05-01T14:12:28Z,[tensorboard] Clarify API and add examples for all methods,"As a part of supporting writing data into TensorBoard readable format, we show more example on how to use the function in addition to the API docs.
",pytorch
20137,nihui,pr,2019-05-05T08:06:50Z,fix build with python-2.7.5,"pytorch failed to build with the following error, complaining about the first regex match
It may be caused by a bug in python 2.7.5
This change proposed is a workaround for building pytorch with python 2.7.5
Since the '*' star notation is greedy in python regex, the new expression shall produce the identical result with the old one.

```
Traceback (most recent call last):
  File ""/data2/nihuini/pytorch/cmake/../aten/src/ATen/gen.py"", line 14, in <module>
    import preprocess_declarations
  File ""/data2/nihuini/pytorch/aten/src/ATen/preprocess_declarations.py"", line 3, in <module>
    from function_wrapper import TYPE_FORMAL_GENERIC
  File ""/data2/nihuini/pytorch/aten/src/ATen/function_wrapper.py"", line 5, in <module>
    from code_template import CodeTemplate
  File ""/data2/nihuini/pytorch/aten/src/ATen/code_template.py"", line 13, in <module>
    class CodeTemplate(object):
  File ""/data2/nihuini/pytorch/aten/src/ATen/code_template.py"", line 23, in CodeTemplate
    subtitution = re.compile(substitution_str, re.MULTILINE)
  File ""/usr/lib64/python2.7/re.py"", line 190, in compile
    return _compile(pattern, flags)
  File ""/usr/lib64/python2.7/re.py"", line 242, in _compile
    raise error, v # invalid expression
sre_constants.error: nothing to repeat
-- 
CMake Error at cmake/Codegen.cmake:162 (message):
  Failed to get generated_cpp list
Call Stack (most recent call first):
  caffe2/CMakeLists.txt:2 (include)


```",pytorch
20223,lanpa,pr,2019-05-07T16:40:48Z,[tensorboard] Have add_video use NamedTemporaryFile directly,"address comment in #16196
https://github.com/pytorch/pytorch/pull/16196/files#r278676986

cc @orionr ",pytorch
20317,lanpa,pr,2019-05-09T13:07:36Z,[tensorboard] add test coverage for make_np,"addresses https://github.com/pytorch/pytorch/pull/16196#discussion_r276381946

cc @orionr ",pytorch
20394,lanpa,pr,2019-05-11T11:25:21Z,[tensorboard] Remove duplicated _optimize_trace,"The duplicated code of `_optimize_trace` in _pytorch_graph.py is used to bypass some optimization step which causes missing scope.

It seems that most of the problematic steps have been fixed recently. Standard models implemented in torchvision are visually inspected before the commit. However, the `+=` in https://github.com/pytorch/vision/blob/50d54a82d1479ffb6dd7469ed05fccdf290a1d84/torchvision/models/resnet.py#L63 will let https://github.com/pytorch/pytorch/blob/f4d9bfaa4dd983288518a310bb900756ee3c6046/torch/onnx/utils.py#L159 produce a bad result. It can be fixed by replacing it with `out += identity`. This also implies that `+=` has non-intuitive behavior.

cc @orionr @ezyang ",pytorch
20437,zenogantner,pr,2019-05-13T16:33:41Z,"fix two typos: ""a the"" => ""the""",,pytorch
20688,lanpa,pr,2019-05-19T17:56:21Z,[tensorboard] fix add_histogram_raw,"This is a porting of the fix from:
https://github.com/lanpa/tensorboardX/issues/421

cc @orionr ",pytorch
20793,tomguluson92,pr,2019-05-22T02:43:45Z,Update Convolution.cpp,"it just strikes to me that the computation process of gI under non-transposed conv is different from the comments above?
I just make it consistent with the code.

",pytorch
20827,thorjohnsen,pr,2019-05-22T22:24:36Z,Cuda persistent softmax,"Adds persistent cuda kernels that speed up SoftMax applied over the fast dimension, i.e. torch.nn.Softmax(dim=-1) and torch.nn.LogSoftmax(dim=-1). When the size is <= 1024, this code is 2-10x faster than the current code, speedup is higher for smaller sizes. This code works for half, float and double tensors with 1024 or fewer elements in the fast dimension. Numerical accuracy is on par with the current code, i.e. relative error is ~1e-8 for float tensors and ~1e-17 for double tensors. Relative error was computed against the CPU code.

The attached image shows kernel time in us for torch.nn.Softmax(dim=-1) applied to a half precision tensor of shape [16384,n], n is plotted along the horizontal axis. Similar uplifts can be seen for the backward pass and for LogSoftmax.

![image](https://user-images.githubusercontent.com/41591019/58212822-b63ebb00-7cb5-11e9-910d-1fc7d8585d58.png)
",pytorch
20908,djsutherland,pr,2019-05-24T14:42:10Z,tweak tqdm to have download speed in kB/MB/etc,This changes the progress bars in `_download_url_to_file` from saying things like `49773343.40it/s` to `47.5MB/s`.,pytorch
20985,sublee,pr,2019-05-27T12:41:26Z,Immutable BatchNorm options,"Related issue: https://github.com/pytorch/pytorch/issues/20967

This patch makes `BatchNorm`'s `affine` and `track_running_stats` options immutable. They should be strictly determined at the initialization.",pytorch
20987,lanpa,pr,2019-05-27T15:30:55Z,[tensorboard] add tests for add_custom_scalars and others,"Originally, the tests for tensorboard writer are smoke tests only. This PR lets CI compare the output with expected results at low level. The randomness of the tensors in the test are also removed.
ps. I found that how protobuf serializes data differs between different python environment. One method to solve this is to write the data and then read it back instantly. (compare the data at a higher level)

For `add_custom_scalars`, the data to be written is a dictionary. and the serialized result might be different (not `ordereddict`). So only smoke test for that.",pytorch
21006,sublee,pr,2019-05-28T05:50:21Z,Deprecate variadic inputs of checkpoint_sequential,"I've reported inconsistency between `checkpoint_sequential` and `nn.Sequential` at https://github.com/pytorch/pytorch/issues/19260. Both should provide the same input signature but they don't. I think the consistency is important and I agree with @apaszke that `nn.Sequential`'s semantics should be kept instead of `checkpoint_sequential`.

I hope `checkpoint_sequential` raises `TypeError` on variadic arguments since PyTorch 1.2.0. But for now, it's okay just to warn as `DeprecationWarning`. I've talked about this approach with @soumith.

Please review this pull request. Any comment will be my pleasure.",pytorch
21192,sublee,pr,2019-05-31T09:55:32Z,"Fix wrong type hints for Tensor.is_cuda, is_leaf","`Tensor.is_cuda` and `is_leaf` is not a predicate function but a `bool` attribute. This patch fixes the type hints in `torch/__init__.pyi` for those attributes.

```diff
- def is_cuda(self) -> bool: ...
+ is_cuda: bool
- def is_leaf(self) -> bool: ...
+ is_leaf: bool
```",pytorch
21339,lanpa,pr,2019-06-04T13:00:01Z,[tensorboard] replace LegacyTracedModule with torchscript used in add_graph,"The new implementation of tracing supports more module. So many error-handling code can be removed by placing the old one (LegacyTracedModule).

cc @orionr",pytorch
21376,Kab1r,pr,2019-06-04T21:34:39Z,Gumbel-Softmax Arxiv Docs Link Fix,"Links separated #20297

",pytorch
22026,stephenroller,pr,2019-06-20T17:13:36Z,Update tensorboard.rst,"**Patch Description**:
Update the docs to reflect one no longer needs to install tensorboard nightly, as Tensorboard 1.14.0 was [released last week](https://github.com/tensorflow/tensorboard/releases/tag/1.14.0).

**Testing**:
Haven't actually tested pytorch with tensorboard 1.14 yet. I'll update this PR once I have.
",pytorch
23134,lanpa,pr,2019-07-20T18:46:46Z,[tensorboard] hyperparameter plugin,"closes #16838

example usage:
```python
writer.add_hparam(hparam_dict= {'lr': 0.1, 'bsize': 12}, metrics= {'accuracy': 0.987, 'loss': 10})

```
cc @orionr ",pytorch
23863,hugovk,pr,2019-08-06T10:55:04Z,Add python_requires to help pip,"`python_requires` helps the installer choose the correct version of this package for the user's running Python.

This is especially necessary when dropping Python 2 (#23795) but is useful now too.",pytorch
23864,hugovk,pr,2019-08-06T11:02:14Z,Docs: Fix contributing guidelines link,"When opening a pull request, GitHub shows you this:

![image](https://user-images.githubusercontent.com/1324225/62534181-30142880-b851-11e9-9b39-32d0ed6ff26c.png)

Or this:

![image](https://user-images.githubusercontent.com/1324225/62534569-24753180-b852-11e9-8242-8905ddda1f6f.png)

However, that links to https://github.com/pytorch/pytorch/blob/master/.github/CONTRIBUTING.md which looks like:

![image](https://user-images.githubusercontent.com/1324225/62534607-3656d480-b852-11e9-8c8c-37f54e8ca774.png)

As the commit message shows, that was a placeholder. There's already a real `CONTRIBUTING.md` document, so move it from the root to the expected location.
",pytorch
23869,hugovk,pr,2019-08-06T13:42:08Z,Docs: Delete placeholder to use top-level file,"Replaces and closes #23864.

When opening a pull request, GitHub shows you this:

![image](https://user-images.githubusercontent.com/1324225/62534181-30142880-b851-11e9-9b39-32d0ed6ff26c.png)

Or this:

![image](https://user-images.githubusercontent.com/1324225/62534569-24753180-b852-11e9-8242-8905ddda1f6f.png)

However, that links to https://github.com/pytorch/pytorch/blob/master/.github/CONTRIBUTING.md which looks like:

![image](https://user-images.githubusercontent.com/1324225/62534607-3656d480-b852-11e9-8c8c-37f54e8ca774.png)

As the commit message shows, that was a placeholder. There's already a real `CONTRIBUTING.md` document, so *delete the placeholder*.
",pytorch
24078,sublee,pr,2019-08-09T06:35:49Z,Documentation for Tensor.record_stream(),"This patch writes documentation for `Tensor.record_stream()`, which is not a documented API currently. I've discussed publishing it with @colesbury in #23729.

The documentation is based on [the introduction at `CUDACachingAllocator.cpp`](https://github.com/pytorch/pytorch/blob/25d1496d581dcc3e2c0e15c38b5f19455800971f/c10/cuda/CUDACachingAllocator.cpp#L47-L50). ~~I didn't explain full details of the life cycle of memory blocks or stream awareness of the allocator for the consistent level of details with other documentations.~~ I explained about the stream awareness in a note block.",pytorch
24143,leomao,pr,2019-08-10T05:15:47Z,OpenCV 4 compatibility fix for caffe2/video,Trying to fix #24073 as in #9966.  Make caffe2 compile with OpenCV 4.,pytorch
24181,Meteorix,pr,2019-08-12T13:52:20Z,Extend nn.Transformer to support BERT (gelu),"To use transformer for BERT, we need `gelu` activation. https://github.com/pytorch/pytorch/issues/24177",pytorch
24819,ZhuBaohe,pr,2019-08-18T13:45:02Z,Modify the sublayer connection in Transformer  module,"This PR modifies the sublayer connection in Transformer  module.

1.  According to the  implementation of tensor2tensor and OpenNMT,   normalizing the embedding to the first TransformerEncoderLayer/TransformerDecoderLayer.  It can improve the result of  loss training.

Reference:
(a) [OpenNMT discuss](https://github.com/OpenNMT/OpenNMT-py/issues/770#issuecomment-398299135)
(b) [OpenNMT code](https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py#L48)

2. Fix the bug that the last TransformerEncoderLayer/TransformerDecoderLayer is subject to repeated LayerNorm.
For example , the last TransformerEncoderLayer runs code
`src = self.norm2(src)`  [class TransformerEncoderLayer -> forward()]
followed by
`output = self.norm(output)`  [class TransformerEncoder -> forward()]
So  two consecutive  LayerNorms apply to the last TransformerEncoderLayer.",pytorch
24888,ZhuBaohe,pr,2019-08-20T03:42:55Z,Resolve the atten weight making NaN in MultiheadAttention forward func,This PR resolves [#24816](https://github.com/pytorch/pytorch/issues/24816#issue-481976681).,pytorch
25599,lanpa,pr,2019-09-03T19:14:51Z,[tensorboard] Fix empty graph problem,"This fixes the empty graph problem since pytorch 1.2

To prevent such things happen, we have to make the test harder.

There 3 levels of verification.
lv 1. make sure that the graph is saved to some event file.  <--currently here
lv 2. make sure the file can be read by tensorboard.
lv 3. make sure the graph in tensorboard is human-friendly.

I think (3) must be involved by a human. 
(2) is possible, but it will be useless if we want to use lv 3 directly.

cc @orionr ",pytorch
25827,rajanksin,pr,2019-09-08T03:31:11Z,Fix for Conv shape check prints overflowed ints,Fix for issue #19947,pytorch
25985,sublee,pr,2019-09-11T08:02:49Z,Fully deprecate variadic inputs of checkpoint_sequential,"To support variadic inputs of `checkpoint_sequential` was deprecated at #21006. This case should be warned with `DeprecationWarning` for PyTorch 1.2, but it should be simply failed with `TypeError` since PyTorch 1.3. This patch removes the `DeprecationWarning` for PyTorch 1.2.",pytorch
26607,sublee,pr,2019-09-21T18:33:29Z,In-place after view in no_grad (issue #26546),"Issue: #26546

In `torch.no_grad()` mode, an in-place operation on a view tensor (`t()`, `view()`, `squeeze()`, etc.) attaches grad_fn `AsStridedBackward` to the tensor.

For example, `y` in the below code should not have any `grad_fn` because it has passed through `torch.no_grad()`:

```python
x = torch.ones(1, requires_grad=True)
with torch.no_grad():
    y = x.t()
    y.add_(0)
```

But it has `grad_fn`:

```python
>>> y
tensor([1.], grad_fn=<AsStridedBackward>)
```

This behavior may cause a bug difficult to understand when we compose InstanceNorm (it makes a view tensor internally) and in-place ReLU in a checkpoint:

```python
x = torch.ones(3, 2, 1, requires_grad=True)
model = nn.Sequential(
    nn.InstanceNorm1d(2),
    nn.ReLU(inplace=True),
)
y = checkpoint(model, x)
y.norm().backward()

# x.grad should be tensor(...). But it is still None because it didn't receive any gradients.
assert x.grad is not None  # FAILS!

# y.grad_fn should be <CheckpointFunctionBackward>. But it is <AsStridedBackward> actually.
assert y.grad_fn.__class__ is CheckpointFunction._backward_cls  # FAILS!
```

To access `Tensor.grad_fn` in Python triggers `Variable::grad_fn()` in [autograd/variable.cpp](https://github.com/pytorch/pytorch/blob/v1.2.0/torch/csrc/autograd/variable.cpp#L135-L162). This method always returns `<AsStridedBackward>` if the tensor is a view with in-place mutation regardless of the grad mode it has passed through. So I've tried to make the tensor as not a view if the view was created in `no_grad` mode.

To suppress `DifferentiableViewMeta` in `no_grad` mode seems to fix those problems. I just made `as_view()` check `GradMode::is_enabled()` before calling `make_view_variable()`.

```diff
  inline Tensor as_view(const Tensor & base, Tensor tensor, bool is_differentiable = true) {
    auto base_var = Variable(base);
    if (base_var.is_view()) {
      base_var = base_var.base();
    }

+   if (!GradMode::is_enabled()) {
+     is_differentiable = false;
+   }

    return make_variable_view(std::move(base_var), std::move(tensor), is_differentiable);
  }
```

Please review this solution and the following code.",pytorch
26639,lanpa,pr,2019-09-23T02:00:41Z,pytorch 1.3 graph,"This patch fixes the problem that the rendered graph is not meaningful.

Since pytorch 1.2, the representation of the traced graph has changed alot, which includes:

1. Many operators were added.
2. The parameter node is moved to graph.nodes(), rather than graph.inputs()
3. The graph is been traced in detail. Take convolution for example, the traced result looks like
aten::_convolution(%input.5, %weight.2, %7, %66, %69, %72, %73, %76, %77, %78, %79, %80)
Besides input, weight, and bias, other nodes are parameters such as stride, kernel size, etc. This information are encoded as ""attributes"" before.

cc @orionr @sanekmelnikov ",pytorch
27371,sublee,pr,2019-10-04T16:39:41Z,record_stream() for shifted view tensors,"Issue: https://github.com/pytorch/pytorch/issues/27366

The address of a view tensor might be shifted from the head of the storage.

```python
>>> x = torch.rand(10, 10, device=0, requires_grad=True)
>>> y = x[2:]
>>> hex(x.data_ptr())
'0x7f1b15c00000'
>>> hex(y.data_ptr())
'0x7f1b15c00050'
```

Currently, `Tensor.record_stream()` silently ignores shifted view tensors, because `CUDACachingAllocator` cannot find the block from the shifted address.

```c++
void recordStream(void* ptr, cuda::CUDAStream stream)
{
  if (ptr) {
    std::lock_guard<std::recursive_mutex> lock(mutex);
    Block* block = find_allocated_block(ptr);
    if (block) {
      ...
    }
    // 'block' is nullptr if 'ptr' is shifted.
  }
}
```

So we cannot protect shifted view tensor which is used to compute or copy in an arbitrary stream against unexpected reallocation. Once we call `record_stream()` on a tensor, our intention is to protect the storage behind the tensor against reallocation until all works in the stream finish. This rule should be consistent regardless of the type of tensors including the view.

We can retrieve the head of the address from any types of tensors by `tensor.storage().data_ptr()`. Hence, I've thought it's better to pass to `recordStream()` rather than `tensor.data_ptr()` for consistent behavior.",pytorch
27446,nuka137,pr,2019-10-06T14:25:24Z,C++ API: torch::nn::Softmax,"Add torch::nn::Softmax module support for the C++ API

Related Issue: #25883 

Reviewer: @yf225 ",pytorch
27459,nuka137,pr,2019-10-07T05:32:19Z,C++ API: torch::nn::Softmin,"Add torch::nn::Softmin module and functional support for the C++ API.

Related Issue: #25883 

Reviewer: @yf225 ",pytorch
27462,nuka137,pr,2019-10-07T13:55:55Z,C++ API: torch::nn::LogSoftmax,"Add torch::nn::LogSoftmax module and functional support for the C++ API.

Related Issue: #25883 

Reviewer: @yf225 ",pytorch
27509,nuka137,pr,2019-10-07T22:53:01Z,C++ API: torch::nn::Softmax2d,"Add torch::nn::Softmax2d module support for the C++ API.
Softmax2d only supports module in Python API, so this PR adds only module support as well.

This PR is WIP because it uses the function in #27446 .
After #27446 is merged, I will remove WIP.

Related Issue: #25883 

Reviewer: @yf225 ",pytorch
27800,nuka137,pr,2019-10-12T06:13:07Z,C++ API: torch::nn::LPPool1d,"Add torch::nn::LPPool1d module and functional support for the C++ API.

Related Issue: #25883

Reviewer: @yf225",pytorch
28176,nuka137,pr,2019-10-16T23:23:38Z,C++ API: torch::nn::BatchNorm1d,"Add torch::nn::BatchNorm1d function/module support for the C++ API.
torch::nn::BatchNorm{2,3}d will be added after this PR is merged.

Related Issue: #25883 

Reviewer: @yf225 

I would like to discuss about below items.

* Necessity of `num_batches_tracked` in `BatchNormImplBase`
  * `num_batches_tracked` is needed to calculate `momentum` when we do not feed `momentum` argument in Python API. But in C++ API, `momentum` argument has a default value.
  * `num_batches_tracked` is only used for counting up `BatchNorm1d::foward()` call. I think it is no necessary for user anymore.
* The design of `BatchNorm{1,2,3}dOptions`
  * We have already `BatchNormOptions` used for deprecated `BatchNorm` module. However, it is hard to use it for `BatchNorm{1,2,3}dOptions` because of the arguments disagreement of each modules.
  * In this PR, I introduce `BatchNormOptionsv2` template class for the `BatchNorm{1,2,3}dOptions`. But I'm not sure this design is good or not.",pytorch
28492,nuka137,pr,2019-10-23T04:26:30Z,C++ API: torch::nn::LPPool2d,"Add torch::nn::LPPool2d module and functional support for the C++ API.

Related Issue: #25883 #27800 

Reviewer: @yf225",pytorch
28651,titaneric,pr,2019-10-25T13:42:32Z,Remove the redundant calculation of derivative of power function,"Hi, I notice that the pytorch faced the the issue as HIPS/autograd#541 .
I try to solve it, hope it can help.",pytorch
28936,nuka137,pr,2019-10-30T23:33:28Z,"C++ API: torch::nn::BatchNorm{2,3}d","Add torch::nn::BatchNorm{2,3}d module and functional support for the C++ API.

Related Issue: #25883 #28176

Reviewer: @yf225",pytorch
29005,HanGuo97,pr,2019-10-31T22:30:32Z,fixed replicate typo in torch/nn/parallel/__init__.pyi,Fix for #29004 ,pytorch
29539,shinh,pr,2019-11-11T04:54:48Z,Improve CHECK_OP macro,"- Show values in question like glog.
- Handle expressions with logical operators properly by adding
  parentheses around expressions.
- Allow outputting nullptr (some build failed without this)
",pytorch
29650,hartb,pr,2019-11-12T14:44:31Z,fix possible pandas import error during tensorboard tests,"TensorBoard tests using SummaryWriter() may fail with a pandas import
complaint if TensorFlow packages are installed in the same python
environment as PyTorch:

Traceback (most recent call last):
  File ""test_tensorboard.py"", line 212, in test_writer
    with self.createSummaryWriter() as writer:
  File ""test_tensorboard.py"", line 64, in createSummaryWriter
    return SummaryWriter(temp_dir)
...
  File ""[...]/site-packages/pandas/core/arrays/categorical.py"", line 52, in <module>
    import pandas.core.algorithms as algorithms
AttributeError: module 'pandas' has no attribute 'core'

The exact failure may depend on the pandas version. We've also seen:

  File ""[...]/site-packages/pandas/core/arrays/categorical.py"", line 9, in <module>
    import pandas.compat as compat
AttributeError: module 'pandas' has no attribute 'compat'

The module import chain leading to the failure is tensorboard imports
tensorflow imports tensorflow_estimator imports pandas. pandas includes
a submodule named 'bottleneck', whose name collides with the PyTorch
'test/bottleneck/' subdirectory.

So IF tensorboard, tensorflow, tensorflow_estimator, and pandas are
installed in the python environment AND IF testing is run from within
PyTorch's 'test/' directory (or maybe just with 'test/' in PYTHONPATH,
etc.), then TensorBoard tests using SummaryWriter() will fail.

Rename the 'bottleneck/' directory slightly to avoid the name collision.",pytorch
29721,nuka137,pr,2019-11-13T10:36:54Z,"C++ API: torch::nn::ConvTranspose{1,2,3}d","Add torch::nn::ConvTranspose{1,2,3}d module and functional support for the C++ API.

Related Issue: #25883

Reviewer: @yf225",pytorch
29890,take-cheeze,pr,2019-11-15T06:50:38Z,Set TORCH_CXX_FLAGS in minimal example,"To avoid ABI issue

EDIT: After this PR, the example CMakeLists.txt will always use the `-D_GLIBCXX_USE_CXX11_ABI` value set in `share/cmake/Torch/TorchConfig.cmake`, regardless of the `-D_GLIBCXX_USE_CXX11_ABI` value passed to the `cmake` command by the user.",pytorch
29991,songyouwei,pr,2019-11-18T10:06:23Z,Add missing error messages for container modules,"Container `Module`s, including `ModuleList`, `ParameterList` and `ParameterDict`, should not be called like a regular `Module`.
This PR add error messages for these special modules.",pytorch
30232,mrTsjolder,pr,2019-11-21T14:20:26Z,[WIP] Refactor optimisers,"I refactored some of the optimisers to make them more readable by
 * extracting a `reset_state` function cf. `Module.reset_parameters` that initialises the state of the optimiser,
 * reformulating the optimisation step so that most algorithms can be written as `theta -= lr * update` - this could probably even be extended to include weight decay etc., 
 *  extracting the code to compute the update for a single parameter with dense gradients in a function called `get_update`,
 * extracting the code to compute the update for a single parameter with sparse gradients in a function called `get_sparse_update`,
 * providing a base class for the decoupled weight decay optimisers.

I wanted to test this locally before creating a pull request, but I did not manage to build pytorch from source or run the tests in any way, so I figured I could have the CI figure it out for me. I tried to keep the original code as much as possible, but I might have fiddled with some things here and there.

I am open to any suggestions and comments to how exactly the interface should look. Probably, also the docs will need some reworking at some point.

Fixes #29814, fixes #32580",pytorch
30814,shinh,pr,2019-12-05T13:56:56Z,Fix a broken link in contribution_guide.rst,,pytorch
30815,shinh,pr,2019-12-05T13:58:22Z,Fix broken links in governance.rst,,pytorch
30854,take-cheeze,pr,2019-12-06T02:55:31Z,Fix CPU min_values/max_values initial value bug,Without specifying initial value in `binary_kernel_reduce_vec` it would be `0` which cause unexpected result in min_values/max_values.,pytorch
30941,lanpa,pr,2019-12-08T09:01:27Z,[tensorboard] Add strings to image boxes,"Addresses https://github.com/pytorch/pytorch/issues/27300

sample usage:
```python
import torch
from torch.utils.tensorboard import SummaryWriter
with SummaryWriter() as w:
     w.add_image_with_boxes('imagebox_label', torch.ones(3, 240, 240) * 0.5,
             torch.Tensor([[10, 10, 100, 100], [101, 101, 200, 200]]),
             global_step=0, labels=['label1', 'label2'])
```
![image](https://user-images.githubusercontent.com/2005323/70387144-53580b80-19dc-11ea-91a1-9275de13ca79.png)


cc @sanekmelnikov @orionr ",pytorch
31130,stephenroller,pr,2019-12-11T19:26:23Z,Add types for the remaining optimizers.,"**Patch Description**
Round out the rest of the optimizer types in torch.optim by creating the stubs for the rest of them.

**Testing**:
I ran mypy looking for just errors in that optim folder. There's no *new* mypy errors created.
```
$ mypy torch/optim | grep optim
$ git checkout master; mypy torch/optim | wc -l
968
$ git checkout typeoptims; mypy torch/optim | wc -l
968
```",pytorch
31288,timgates42,pr,2019-12-14T11:18:33Z,Fix simple typo: whos -> whose,"Closes #31287

",pytorch
31301,lanpa,pr,2019-12-15T11:47:16Z,[tensorboard] Fix function input parameter for add_hparams,"closes #30943 

both parameters in add_hparams are mandatory. 

cc @sanekmelnikov @orionr ",pytorch
31544,lanpa,pr,2019-12-21T11:33:54Z,[tensorboard] Let hparam render values correctly,"The root cause of incorrect rendering is that numbers are treated as a string if the data type is not specified. Therefore the data is sort based on the first digit.

closes #29906 
 cc @orionr @sanekmelnikov ",pytorch
31755,nihui,pr,2020-01-01T02:28:14Z,try to find cudnn header in /usr/include/cuda,"With fedora negativo17 repo, the cudnn headers are installed in /usr/include/cuda directory, along side with other cuda libraries.
",pytorch
32084,lanpa,pr,2020-01-11T12:08:45Z,[TensorBoard] update TensorBoard warning message,fixes #30863,pytorch
32290,hugovk,pr,2020-01-16T14:59:29Z,Pin Pillow to latest and use a torchvision that works with it,"Follow on from https://github.com/pytorch/pytorch/pull/31777, as suggested in https://github.com/pytorch/pytorch/pull/31777#issuecomment-575166543.

Pillow 7.0.0 removed `PILLOW_VERSION` and `__version__` should be used instead.

torchvision 0.5.0 switched from using `PILLOW_VERSION` to `__version__`.

",pytorch
32294,luxe,pr,2020-01-16T16:27:04Z,"""batchSize"" was set but never used","fixes a compiler warning:  
```
torch/aten/src/ATen/native/cuda/MaxUnpooling.cu.cc(402):  
warning: variable ""batchSize"" was set but never used
```

",pytorch
32389,hugovk,pr,2020-01-18T13:33:22Z,"Fix version comparisons for Python 3.6, 3.10 and 4","There's some code which uses `six.PY3`, similar to:

```python
if six.PY3:
    print(""Python 3+ code"")
else:
    print ""Python 2 code""
```

Where:

```python
PY3 = sys.version_info[0] == 3
```

When run on Python 4, this will run the Python 2 code! Instead, use `six.PY2` and avoid `six.PY3`.

---

Similarly, there's some `sys.version_info[0] == 3` checks, better done as `sys.version_info[0] >= 3`.

---

Also, it's better to avoid comparing the `sys.version` string, as it makes assumptions that each version component is exactly one character long, which will break in Python 3.10:

```pycon
>>> sys.version
'3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53) \n[Clang 6.0 (clang-600.0.57)]'
>>> sys.version < ""3.3""
False
>>> fake_v3_10 = '3.10.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53) \n[Clang 6.0 (clang-600.0.57)]'
>>> fake_v3_10 < ""3.3""
True
```

---


Finally, I think the intention here is to skip when the Python version is < 3.6:

```python
@unittest.skipIf(sys.version_info[0] < 3 and sys.version_info[1] < 6, ""dict not ordered"")	
```

However, it will really skip for Python 0.0-0.5, 1.0-1.5 and 2.0-2.5. It's best to compare to the `sys.version_info` tuple and not `sys.version_info[1]`:

```python
    @unittest.skipIf(sys.version_info < (3, 6), ""dict not ordered"")
```

---

Found using https://github.com/asottile/flake8-2020:
```console
$ pip install -U flake8-2020
$ flake8 --select YTT
```
",pytorch
32578,hovhannesgithub,pr,2020-01-24T14:28:21Z,Update Docs for building PyTorch for Android.,,pytorch
32673,henryhjung,pr,2020-01-27T23:25:34Z,Fix spelling errors,,pytorch
33614,shinh,pr,2020-02-21T06:37:31Z,[ONNX] Fix translation for -1 in Expand op,"PyTorch's expand uses -1 to tell the dimension should not change while ONNX's Expand uses 1.

This fixes #32926.

",pytorch
33924,thorjohnsen,pr,2020-02-28T01:52:51Z,Avoid unnecessary flattening in all-gather and reduce-scatter,"This PR addresses an important special case, which comes up frequently as we are working towards model parallel training and a few other things. torch.distributed.all_gather takes two arguments, a list of output tensors and a single input tensor. The all_gather op collects inputs from all the ranks into the list of output tensors. Internally, the rank inputs are collected into a flattened tensor and then copied from there to the output tensors. Often, the list of output tensors are simply views into an already flattened tensor, in which case the un-flattening is unnecessary and we can do an in-place all-gather op instead. This saves memory and improves performance. This is particularly important when the all-gather is done within a single node, because all the reductions are done over high-speed nvlinks and the extra D2D copies really hurt performance. To demonstrate the difference this PR makes, I ran the test script shown below on a DGX1 with 8 x 32GB V100 cards, using NVIDIA's 19.11 devel image. Without this PR, it took 13.96 seconds (average of 5 runs) to complete 100 all-gathers of a flattened tensor with 1.07 billion floats (1024^3). With this PR, the same 100 all-gathers took 3.01 seconds, a 4.64x improvement.

The situation for reduce-scatter is fundamentally the same, but reversed. Instead of doing D2D copies after the op to un-flatten the output tensors, reduce-scatter has to do D2D copies before the op to flatten the input tensors. The performance overhead is the same as for all-gather. This PR fixes both all-gather and reduce-scatter.

This PR should have no side-effects. Everything should work exactly like before, but when the special case is detected, you will see a large bump in throughput.

Thanks to @alpha0422 for contributing the original code. 

Command line:
`python -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 test_all_gather.py`

test_all_gather.py
```
import time
import torch

def main(args):
    a = torch.randn([args.group_size*args.size]).float().cuda()
    a_vec = [a[i*args.size:(i+1)*args.size] for i in range(args.group_size)]
    if torch.distributed.get_rank() == 0:
        print(""We used up %d mb of GPU memory."" % (torch.cuda.memory_allocated()/(1024*1024)))
        print(""Doing %d all-gathers of %d x %d floats"" % (args.niter, args.group_size, args.size))
    torch.distributed.barrier()
    before = time.time()
    for iter in range(args.niter):
        torch.distributed.all_gather(a_vec,a_vec[torch.distributed.get_rank()])
    torch.cuda.synchronize()
    torch.distributed.barrier()
    after = time.time()
    print(""rank %d :: a.norm() = %e, run_time was %.2f seconds"" % (torch.distributed.get_rank(), a.norm(), after-before))

if __name__ == '__main__':
    torch.distributed.init_process_group(""nccl"")
    torch.cuda.set_device(torch.distributed.get_rank() % torch.cuda.device_count())
    torch.manual_seed(888)
    class Args:
        size = 1024*1024*128
        niter = 100
        group_size = torch.distributed.get_world_size()
    main(Args())
```",pytorch
34051,ankeshanand,pr,2020-03-02T03:11:48Z,Remove warning about building from source to use the NCCL backend,"I think this warning isn't true anymore, and the NCCL backend works without PyTorch needing to be built from source.",pytorch
35201,take-cheeze,pr,2020-03-23T04:59:19Z,Match case of package name to suppress warning,"https://github.com/Kitware/CMake/commit/ee4673c1ae1e4a1aa4687412717567c2ffbb501b

`find_package(Torch)` is used most of the time: https://pytorch.org/cppdocs/installing.html

",pytorch
35876,sclarkson,pr,2020-04-02T12:27:24Z,Fix Python JIT test failures when built with BUILD_TEST=0,"Some Python tests requiring the C++ JIT tests weren't appropriately flagged.

",pytorch
36416,VivekPanyam,pr,2020-04-10T23:05:03Z,Fix a segfault in DeviceThreadHandlePool and PoolWindow,"This PR fixes a bug related to object destruction order across threads. The bug can cause segfaults during shutdown of processes that use libtorch.

See https://github.com/pytorch/pytorch/issues/36408 for more detail",pytorch
36430,nuka137,pr,2020-04-11T12:35:07Z,[CPU] torch.gather for complex dtypes,This PR resolves #36340 .,pytorch
36438,mathemage,pr,2020-04-11T18:20:59Z,Update contribution_guide.rst,"Fix formatting: change ""Frequently Asked Questions"" into an RST header, which is clickable and one can get a URL of the FAQ section",pytorch
36495,lanpa,pr,2020-04-13T17:21:41Z,[TensorBoard] fix #36471,"cc @orionr @sanekmelnikov 

Confirm that the function was removed already.",pytorch
36496,lanpa,pr,2020-04-13T17:22:17Z,[TensorBoard] fix #34954,cc @orionr @sanekmelnikov ,pytorch
36497,lanpa,pr,2020-04-13T17:23:28Z,[TensorBoard] fix #33140,"cc @orionr @sanekmelnikov 

The fix was ported from https://github.com/lanpa/tensorboardX/commit/9d267066a6d67991ad8263c3a4f4a8c570367c53",pytorch
37365,tlemo,pr,2020-04-27T21:26:40Z,Simplify a few test cases,"Replace custom exception checks with ASSERT_THROW macros.

",pytorch
37504,lanpa,pr,2020-04-29T14:21:31Z,[TensorBoard] Fixes missing doc for add_graph,Fixes #37415 ,pytorch
37654,take-cheeze,pr,2020-05-01T07:38:32Z,ONNX: Use correct opset version in symbolic_caffe2 operators,"Due to change to Pad-11 exporting model with symbolic_caffe2 operators cause incorrect output for AveragePool-11.
To avoid it symbolic_caffe2 should use fallback operators with correct opset version.

cc @BowenBao @neginraoof",pytorch
37739,frgfm,pr,2020-05-03T19:32:23Z,docs: Fixed docstring indentation for documentation,"Hello there,

I was going through the default initialization of some layers, and ended up on the `torch.nn.init` documentation. As shown below, there was a slight issue with the docstrings of both `kaiming_normal_` and `kaiming_uniform_` that yielded a wrong list of function parameters:

![doc_issue](https://user-images.githubusercontent.com/26927750/80923512-88e30400-8d84-11ea-8708-36ed3a0f7749.png)

This PR fixes the indentation in the corresponding docstrings.

Any feedback is welcome!",pytorch
38084,frgfm,pr,2020-05-07T23:55:31Z,Fixed buffer update in BatchNorm when track_running_stats is set to False,"This PR aims at tackling #37823 by:
- ensuring that buffers will be used for normalization computation but won't be updated, when buffers are not None, and `track_running_stats=False`
- adding a corresponding unittest to ensure expected behaviour

Any feedback is welcome!

_Note: we might want to update the docstrings of  `BatchNorm*d`, feel free to share any suggestion!_
",pytorch
38547,Rishit-dagli,pr,2020-05-15T10:34:44Z,Added a Resource section to README,"Added the following entries in the newly made resources section in README:

* [PyTorch.org](https://pytorch.org/)
* [PyTorch Tutorials](https://pytorch.org/tutorials/)
* [PyTorch Examples](https://github.com/pytorch/examples)
* [PyTorch Models](https://pytorch.org/hub/)
* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)
* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)
* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)
* [PyTorch Twitter](https://twitter.com/PyTorch)
* [PyTorch Blog](https://pytorch.org/blog/)
* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)
",pytorch
38692,take-cheeze,pr,2020-05-19T00:42:39Z,ONNX: Fix `scalar_type_to_pytorch_type` for torch.bool,Thank you for the bug report https://github.com/pytorch/pytorch/issues/32280 !,pytorch
38708,take-cheeze,pr,2020-05-19T05:18:58Z,onnx: Use Reciprocal operator,"For torch.reciprocal export



cc @BowenBao @neginraoof",pytorch
39949,mrTsjolder,pr,2020-06-12T17:38:03Z,test for multi-processing support of optimisers,"This is an answer to [this request](https://github.com/pytorch/pytorch/pull/32903#issuecomment-583491882) from @vincentqb to add tests for detecting an issue that occurs when the state of an optimiser is not correctly updated when it is shared among processes (using shared memory).

The idea of the test is to make sure that the state of the optimiser progresses in the same way, independent of whether multi-processing is used or not. The following code snippet should bring this idea across:
```python
def update(optimizer, x):
    optimizer.zero_grad()
    x.sum().backward()
    optimizer.step()

p = torch.zeros(10).requires_grad_(True).share_memory_() 
optim_multi = torch.optim.Adagrad([p]) 
optim_multi.share_memory() 
pool = torch.multiprocessing.Pool(1) 
pool.apply(update, (optim_multi, p))
print(optim_multi.state[p])

p = torch.zeros(10).requires_grad_(True)
optim_ref = torch.optim.Adagrad([p])
update(optim_ref, p)
print(optim_ref.state[p])
```
If the printed states (`dict`s) are the same, then the state was shared correctly. In its current state, Adagrad does seem to correctly share its `'sum'` state, but does does not update its `'step'` state correctly when using multi-processing.",pytorch
39954,mrTsjolder,pr,2020-06-12T18:44:43Z,state initialisation of optimisers in separate method,"This is a continuation of a cluttered and stale pull request from some time ago, tackling point 2 from [this comment](https://github.com/pytorch/pytorch/pull/30232#issuecomment-579465182). This should also resolve  issue #37410 .

The initial commit attempts to keep initialisation where it currently is. However, I think it would be beneficial to unify the state initialisation across all (commonly used) optimisers. @vincentqb Is this still an ongoing discussion?

The API is very simple for now: `opt.reset_state()` resets all state of an optimiser. However, while creating the pull request, I realised that it might be useful to reset the state of only one group or even of single parameters. I am not sure what the API should look like in this case. I suppose something like `def reset_state(par: Tensor = None) -> Union[dict, None]` could work (reset all parameters when `None`, otherwise return the new state for the given parameter). In this case, the group parameters must also be passed on somehow. Not sure how a group-reset should look like.",pytorch
40229,tlemo,pr,2020-06-18T17:51:34Z,Fix for issue #85,,pytorch
40963,yaox12,pr,2020-07-03T09:51:27Z,fix autodoc for torch.distributed.launch,"The doc for `torch.distributed.launch` is missing since v1.2.0 (see issue #36386) because PR #22501 added some imports at the first line.
https://github.com/pytorch/pytorch/blob/542ac74987141c81b6326b9e7d5c6a1d00fc3701/torch/distributed/launch.py#L1-L5
I move it below the docstring to make the autodoc in Sphinx work normally.",pytorch
40984,mathemage,pr,2020-07-04T19:48:00Z,Fix link to PyTorch organization (from Governance),"PR fixes #40666

",pytorch
42064,acxz,pr,2020-07-25T16:40:32Z,Find hip properly,"Fixes #41886 
",pytorch
42072,acxz,pr,2020-07-26T00:29:21Z,find rccl properly,"Fixes #{issue number}
",pytorch
42775,mrTsjolder,pr,2020-08-08T06:40:30Z,Refactor step function in optimiser,"This is a continuation of a cluttered and stale pull request from some time ago, tackling point 3 from [this comment](https://github.com/pytorch/pytorch/pull/30232#issuecomment-579465182).

The idea of this pull request is to create `get_update` and `get_sparse_update` methods that compute the update for dense and sparse gradients, respectively. This allows for a generic function that takes care of the loss closure and parameter groups, and should make it easier to find the code that implements the actual optimisation algorithm. 

@vincentqb Major difference with #30232 is that the state is passed on (instead of retrieved within the functions) as argument to `get_update`. This allows people to compute the update without changing the actual optimiser state.",pytorch
44199,hrw,pr,2020-09-04T15:42:28Z,handle missing NEON vst1_*_x2 intrinsics (#44198),"CentOS 8 on AArch64 has vld1_* intrinsics but lacks vst1q_f32_x2 one.

This patch checks for it and handle it separately to vld1_* ones.

Fixes #44198 
",pytorch
44399,tlemo,pr,2020-09-09T16:53:17Z,Adding .build_profile to .gitignore,"For CPU profiling (and troubleshooting release builds in general), one would normally get best results from a RelWithDebInfo build. 

This tiny PR just adds another ""known"" build subdirectory to .gitignore : .build_profile 
",pytorch
44644,hrw,pr,2020-09-14T17:18:44Z,Fix centos8 gcc,"Fixes #44198 properly this time
",pytorch
44723,tlemo,pr,2020-09-15T17:38:43Z,Kernel IR: Splitting CUDA codegen from IrPrinter,"One of the main goals of having a dedicated kernel IR was separation of concerns: simpler and smaller components which do one thing instead of monolithic implementations.

This PR is a significant step in that direction: the CUDA code generation is now separate from the IrPrinter.


",pytorch
45144,raziel,pr,2020-09-22T17:44:37Z,Moves prim ops from C10 back to JIT,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45144 Moves prim ops from C10 back to JIT**

Moves prim ops from C10 back to JIT.

These were originally moved to C10 from JIT in D19237648

Differential Revision: [D23697598](https://our.internmc.facebook.com/intern/diff/D23697598/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D23697598/)!",pytorch
45187,shinh,pr,2020-09-23T03:45:12Z,[ONNX] Correct a minor typo in warning,"The warning for batch_norm was mentioning dropout.
",pytorch
45550,acxz,pr,2020-09-30T02:21:15Z,[Build] [CMake] [ROCm] find hsa-runtime64 properly,"Properly Fixes #44384
similar in vein to #42064",pytorch
45773,raziel,pr,2020-10-02T21:04:13Z,Stricter backward compatibility check,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#45773 Stricter backward compatibility check**

Changes the function schema's backward compatibility check to be stricter to comply with C++ API backwards compatibility capabilities.

See #45784 for description

Differential Revision: [D24089751](https://our.internmc.facebook.com/intern/diff/D24089751/)",pytorch
46017,ajliu,pr,2020-10-08T04:21:15Z,Add benchmark for per channel tensor quantization,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* #46018 quantize_tensor_per_channel ARM implementation
* **#46017 Add benchmark for per channel tensor quantization**

Summary: Currently on mobile only per tensor quantization is optimized for mobile using ARM intrinsics. This benchmark is added to help gauge performance improvement on mobile after performing the same optimizations for per channel quantization.

Test Plan:
Build for ARM Neon
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=""armeabi-v7a with NEON"" ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
```
Build for ARM64
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
```
Then run the benchmark binary over adb shell. Note that by android cpu is not frequency locked by default and can lead to noisy benchmark results, but this can be changed by running the following for every cpu.
```
adb shell ""echo userspace > /sys/devices/system/cpu/${cpu}/cpufreq/scaling_governor""
adb shell ""echo '2000000' > /sys/devices/system/cpu/${cpu}/cpufreq/scaling_setspeed""
adb push build_android/bin/quantize_per_channel /data/local/tmp/
adb shell ""/data/local/tmp/quantize_per_channel""
```

Reviewers: kimishpatel

Subscribers:

Tasks: T76832258

Tags:

Differential Revision: [D24286488](https://our.internmc.facebook.com/intern/diff/D24286488)",pytorch
46018,ajliu,pr,2020-10-08T04:21:22Z,quantize_tensor_per_channel ARM implementation,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#46018 quantize_tensor_per_channel ARM implementation**
* #46017 Add benchmark for per channel tensor quantization

## Summary:
Currently on mobile devices quantize_tensor has a vectorized implementation using ARM intrinsics; however quantize_tensor_per_channel does not.

## Test Plan:
Build for ARM Neon
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=""armeabi-v7a with NEON"" ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
```
Build for ARM64
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
```
Run the test binary over adb shell with the commands below.
```
adb push build_android/bin/quantized_test /data/local/tmp
adb shell ""/data/local/tmp/quantized_test""
```
Run the benchmark binary over adb shell with the commands below. 
```
adb push build_android/bin/quantize_per_channel /data/local/tmp/
adb shell ""/data/local/tmp/quantize_per_channel""
```
Note that by android cpu is not frequency locked by default and can lead to noisy benchmark results, but this can be changed by running the following for every cpu.
```
adb shell ""echo userspace > /sys/devices/system/cpu/${cpu}/cpufreq/scaling_governor""
adb shell ""echo '2000000' > /sys/devices/system/cpu/${cpu}/cpufreq/scaling_setspeed""
```

Resulting benchmarks are located [here](https://gist.github.com/AJLiu/d1711bb6a5e93b3338eca2c14c8aec9f)
Google spreadsheet comparing results [here](https://docs.google.com/spreadsheets/d/1Ky-rEu2CqOqex2a84b67hB1VLAlfEDgAN2ZXe8IlGF8/edit?usp=sharing)

**Overall results:**
- aarch64
    - 2x slowdown on 2d tensor benchmarks
    - 4x speed up on 4d contiguous tensor benchmarks
    - 4x speed up on 4d channels last tensor benchmarks
- neon
    - 2x speed up on 2d tensor benchmarks
    - 20x speed up on 4d contiguous tensor benchmarks
    - 20x speed up on 4d contiguous tensor benchmarks

I suspect that this is an overall performance boost, however it adds a small amount of overhead that becomes very noticeable when quantizing small tensors.

Reviewers: kimishpatel

Subscribers:

Tasks: T76832258

Tags:

Differential Revision: [D24286528](https://our.internmc.facebook.com/intern/diff/D24286528)",pytorch
46579,hugovk,pr,2020-10-20T08:34:10Z,Remove redundant code for unsupported Python versions,"Remove code for Python 3.5 and lower.

There's more that can be removed/modernised, but sticking mainly to redundant version checks here, to keep the diff/PR smaller.",pytorch
46828,frgfm,pr,2020-10-25T23:19:35Z,fix: Fixed typing of bool in _ConvNd,"Hello there :wave: 

I do believe there is some typo in the typing of the `bool` argument of `_ConvNd`constructor.
The typing of the attribute is correct, but the constructor argument, while being named the same way, is not the value that will be assigned to `self.bias`. 

This PR simply corrects that.

Any feedback is welcome!
",pytorch
47468,ajliu,pr,2020-11-05T22:30:32Z,Fix rounding error flakiness in quantized_test,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#47468 Fix rounding error flakiness in quantized_test**

**Summary:** QuantizePerChannel4d and QuantizePerChannel4dChannelsLast have issues with flakiness on both ARM and x86 builds.

The flakiness stems from two sources:
1. The rounding strategy used by quantization for half values is to round the number to the nearest even integer (e.g. `4.5->4`, `5.5 -> 6`, `6.5->6`; however the above tests are incorrect by expecting the values to be rounded away from zero.

2. On ARM devices, `quantize_val_arm` calculates `zero_point + round(val / scale)` which behaves differently from `quantize_val`, which calculates `zero_point + round(val * (1.0f/scale))`. This small distinction leaves enough room for the floating point arithmetic errors to change rounding behavior (e.g. `3 / .24 = 12.5` whereas `3 * (1.0f / .24) = 12.500001`).

**Test Plan:**
For local builds:
```
python setup.py develop
./build/bin/quantized_test --gtest_filter='TestQTensor.QuantizePerChannel4d*' --gtest_repeat=10000 | grep FAILURE
```

For ARM Neon:
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=""armeabi-v7a with NEON"" ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
adb push ./build/bin/quantized_test /data/local/tmp
adb shell ""/data/local/tmp/quantized_test --gtest_filter='TestQTensor.QuantizePerChannel4d*' --gtest_repeat=1000 | grep FAILURE""
```

For ARM64:
```
BUILD_MOBILE_BENCHMARK=1 BUILD_MOBILE_TEST=1 ANDROID_DEBUG_SYMBOLS=1 BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh  -DANDROID_CCACHE=$(which ccache) -DBUILD_BINARY=ON
adb push ./build/bin/quantized_test /data/local/tmp
adb shell ""/data/local/tmp/quantized_test --gtest_filter='TestQTensor.QuantizePerChannel4d*' --gtest_repeat=1000 | grep FAILURE""
```

**Reviewers:**

**Subscribers:**

**Tasks:** T79019469

**Tags:**

Differential Revision: [D24769889](https://our.internmc.facebook.com/intern/diff/D24769889)",pytorch
47708,tlemo,pr,2020-11-10T23:15:13Z,Replace kernel resource strings with real .cu source files,"The new setup (as implemented in this PR) has 3 key parts:

1. Build system changes to copy the files from under `.../jit/codegen/cuda/runtime` to the build output (and also installing them during a full setup)
2. At runtime, detect the location of the library binary (ex. `torch_cuda.so` or `torch_cuda.dll`). See `torchLibPath()` for details.
3. Pass `--include-path=` to NVRTC to point to the right location

After this, we can simply `#include <nvfuser_runtime/foo.cu>` from the generated code.
",pytorch
47770,tlemo,pr,2020-11-11T18:42:01Z,Replace kernel resource strings with real .cu source files,"The new setup (as implemented in this PR) has 3 key parts:

1. Build system changes to copy the files from under `.../jit/codegen/cuda/runtime` to the build output location (and also installing them during a full setup)
2. At runtime, detect the location of the library binary (ex. `torch_cuda.so` or `torch_cuda.dll`). See `torchLibPath()` for details.
3. Pass `--include-path=` to NVRTC to point to the right location (relative to the library location from step 2)

After this, we can simply `#include <nvfuser_runtime/foo.cu>` from the generated code.
",pytorch
48283,tlemo,pr,2020-11-20T00:17:43Z,Replace kernel resource strings with real .cu source files,"Convert the NVFUSER's runtime CUDA sources (under `.../jit/codegen/cuda/runtime`) to string literals, then include the headers with the generated literals.
",pytorch
48834,sbrodehl,pr,2020-12-04T11:20:18Z,[tensorboard][writer] Add missing 'dataformats' argument to 'add_image' docs.,"The [torch.utils.tensorboard.SummaryWriter.add_image](https://pytorch.org/docs/stable/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.add_image) is missing the argument `dataformats` in the docs.

This PR adds the missing argument to the docs (analogous to `add_images` docs).
",pytorch
49281,hmaarrfk,pr,2020-12-12T10:02:28Z,Import inttypes,"We needed to import inttypes to get this to compile at conda-forge.

https://github.com/conda-forge/pytorch-cpu-feedstock/pull/21

I hope this helps.",pytorch
49399,chenrui333,pr,2020-12-15T07:38:33Z,Bump bazel to v3.7.1,,pytorch
50000,yqtianust,pr,2021-01-02T11:08:57Z,Fix grammar typo in readme.md,"missing `

",pytorch
50019,NathanHowell,pr,2021-01-03T18:17:36Z,Add support for converting sparse bool tensors to dense,"Fixes #49977 
",pytorch
50033,yqtianust,pr,2021-01-04T06:40:43Z,Fixing error in Readme.md.,"Fix incorrect command in readme.
Fix incorrect url in readme.
Add url for dockerfile.

",pytorch
50168,tlemo,pr,2021-01-06T22:58:03Z,Fixing a few iterator mismatches,"This PR is fixing a few instances where we're trying to compare iterators based on different containers:

https://github.com/pytorch/pytorch/blob/dde5b6e177ec24d34651ffd8df04b4ebdf264e6e/torch/csrc/jit/mobile/import.cpp#L444-L446

`result.metadata()` returns the container by value so the comparison is broken.",pytorch
50353,KumaTea,pr,2021-01-10T17:38:19Z,Fix caffe2 import tools.codegen,"Using `insert` instead of `append` to add torch root directory to `sys.path`, to fix `ModuleNotFoundError: No module named 'tools.codegen'`, as mentioned in #47553
",pytorch
50452,tlemo,pr,2021-01-13T01:30:25Z,Dump torch::jit::AliasDb objects as Graphviz files,"This PR adds a simple debugging helper which exports the AliasDb state as a [GraphViz](http://www.graphviz.org/) graph definition. The generated files can be viewed with any Graphviz viewer (including online based, for example http://viz-js.com)

Usage:

1. Call `AliasDb::dumpToGraphvizFile()` from a debugger. Using gdb for example:
`call aliasDb_->dumpToGraphvizFile(""alias.dot"")`

2. Add explicit calls to `AliasDb::dumpToGraphvizFile()`, which returns `true` if it succeeds.

An example output file is attached: [example.zip](https://github.com/pytorch/pytorch/files/5805840/example.zip)
",pytorch
51067,H1Gdev,pr,2021-01-26T00:19:59Z,Build pytorch_android using Gradle wrapper.,"[Here](https://docs.gradle.org/current/userguide/gradle_wrapper.html), there is the following description.
`The recommended way to execute any Gradle build is with the help of the Gradle Wrapper`

I took a little time to prepare Gradle for `pytorch_android` build. (version etc.)

I think using Gradle wrapper will make `pytorch_android` build more seamless.

Gradle wrapper version: 4.10.3

https://github.com/pytorch/pytorch/blob/250c71121b8ac2ef1899a2414c939d4d45fc2be4/.circleci/scripts/build_android_gradle.sh#L13",pytorch
51757,raziel,pr,2021-02-05T01:09:54Z,Enables backend preprocessing to take place outside of the backend interface,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#51757 Enables backend preprocessing to take place outside of the backend interface**

Enables backend preprocessing to take place outside of the backend interface.

What's new:
* A new definition for backend preprocessing is defined (i.e. BackendPreprocessFunction).
* Registration of the backend's PyTorchBackendInterface interface implementation is augmented to take the BackendPreprocessFunction.
* A new registry is created to handle the BackendPreprocessFunction functions, using the backend's name as key.
* When a BackendPreprocessFunction is used, the  PyTorchBackendInterface's ""preprocess"" method is not added to the LoweredModule. Instead, the BackendPreprocessFunction is called and its output used to set the LoweredModule's __processed_module.


Why?:
These changes are needed to avoid forcing ""preprocessing"" part of the LoweredModule, and in the future of PyTorchBackendInterface.
This is important for Mobile use cases where ""preprocess"" can take the bulk of the compilation process, and thus contain code dependencies that we do not want to bring (or cannot bring) to the Mobile binary.

What didn't change:
* Everything is backwards compatible:
** The existing ""preprocess"" method in PyTorchBackendInterface.
** When backend registration is done without the BackendPreprocessFunction, as before, things work the same way: ""preprocess"" is added to LoweredModule, and invoked through the module's instance of the backend interface.

Longer term, the plan is to refactor existing users to move to the new backend registration.

Differential Revision: [D26261042](https://our.internmc.facebook.com/intern/diff/D26261042/)",pytorch
52015,H1Gdev,pr,2021-02-10T01:06:02Z,Update android/README.md to use Gradle wrapper.,"By PR #51067 has been merged, in build of `pytorch_android`, installation of `Gradle` is no longer required.

Remove `Gradle` related described from README.md.
(and update how to check aar files, too.)",pytorch
52258,raziel,pr,2021-02-14T01:27:53Z,Removes deprecated preprocess method from the backend interface,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#52258 Removes deprecated preprocess method from the backend interface**

Removes deprecated preprocess method from the backend interface.

Preprocessing logic should be now registered along with the backend interface (i.e. PyTorchBackendInterface) via the BackendPreprocessFunction.

Also refactored internal dependencies.

Differential Revision: [D26443479](https://our.internmc.facebook.com/intern/diff/D26443479/)",pytorch
52924,danielgordon10,pr,2021-02-26T17:15:37Z,Fix type hints of the callable arguments for DataLoader,"Fixes https://github.com/pytorch/pytorch/issues/52806

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/pytorch/pytorch/52924)
<!-- Reviewable:end -->
",pytorch
53068,raziel,pr,2021-03-01T23:57:31Z,Adds a bool is_available() method to the backend contract,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#53068 Adds a bool is_available() method to the backend contract**

Adds a ```bool is_available()``` method to the backend contract: it returns ```true``` if ```compile()``` and ```execute()``` can be called; ```false``` otherwise.

It is used to implement the following changes in the ```LoweredModule```:
* ```compile()``` in ```__setstate__``` will run if ```is_available()```, else ```__setstate__``` throws an exception (“Backend not available.”).
* ```compile()``` at ```LoweredModule``` creation will run if ```is_available()```, else a WARNING will be thrown.
* ```execute()``` will only be executed if ```is_available()``` returns true; else throws an exception (“Backend not available.”).

The goal of these changes is to ensure we have a well defined behaviour for the different combinations of backend availability on-host and on-target.

More specifically, backends may have different capabilities to compile and/or execute the Module, depending whether this happens on-host (i.e. where the program is being written) or on-target (where the program is being executed).

First of all, we know that ""preprocess"" always takes place, and that only happens on-host at creation time. So, we can assume that any compilation is needed/possible on-host then all of it could be pushed here.

Overall, we want to ensure the following:

**On host**

| compile | execute | Outcome |
| -- | -- | -- |
| No | No | On module creation, LoweredModule is generated, with a warning  (since compilation and execution can still take place on-target). On module load, throws an exception (since execution is not possible). |
| No | Yes | This configuration should not be possible. This assumes the full compiler is not available, even if some work was done in preprocess the program cannot be finalized for execution. |
| Yes | No | In this case, the expectation would be for is_available() to return false, and compilation logic to move into preprocess. |
| Yes | Yes | All good. This is the only case that is_available() should return true. |

**On target**

| compile | execute | Outcome |
| -- | -- | -- |
| No | No | Loading the LoweredModule throws an exception. Since execution is not possible. |
| No | Yes | Basically this is another instance of Yes/Yes: compilation per se may not be possible on device, which means compile() can be called without issue but it is a no-op, and thus is_available should return true. Consequently, loading the LoweredModule: Succeeds, if the preprocessed module is ready for execution. Fails with exception otherwise. |
| Yes | No | This configuration should not be possible. Just putting here for completeness. |
| Yes | Yes | All good. This, along with No/Yes case (because compilation is assumed to have happened on-host, so it's just another instance of Yes/Yes), are the cases where is_available() should return true. |


**Refactoring existing code**
This change also updates other backends (Glow) code, to implement the is_available() method to have the same behaviour as before this change (i.e. always available).

This should not cause backward incompatibilities with already saved models since we're adding a new method to the PyTorchBackendInterface.
Models saved with the old interface that didn't have is_available() will still find the other 2 methods in the bound object (i.e. compile and execute), and the saved LoweredModule logic will be the old one.

**Future**
We plan to use is_available() to implement support for fallback to the PyTorch interpreter.

Differential Revision: [D26615833](https://our.internmc.facebook.com/intern/diff/D26615833/)",pytorch
53106,nihui,pr,2021-03-02T11:43:13Z,Update torch.nn.quantizable.MultiHeadAttention docstring,"Apply the same fix as PR https://github.com/pytorch/pytorch/pull/49950
",pytorch
53694,mrTsjolder,pr,2021-03-10T07:48:21Z,fix SELU gain,"Fixes #24991 

This issue has been wrongly tackled by PR ##50664 in the sense that the [original paper](https://proceedings.neurips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html) explicitly requires the variance of initial weights to be `1/fan_in`.
",pytorch
53968,eltociear,pr,2021-03-13T14:32:24Z,Fix typo in torchvision_models.py,"accross -> across
",pytorch
53978,eltociear,pr,2021-03-14T06:18:03Z,[RPC] Fix typo in rref_context.cpp,"untill -> until
",pytorch
54057,mrTsjolder,pr,2021-03-16T09:24:02Z,Add doc warnings for default SELU gain,"Fixes #24991 and provides the alternative solution suggested in #53694. Also related to #54055

Attempt to make people aware of the difference between paper and implementation of SELU gain.
",pytorch
54130,take-cheeze,pr,2021-03-17T05:48:51Z,[ONNX] Name onnx value name to satisfy C identifier,"Ref:

[Names Within a Graph](https://github.com/onnx/onnx/blob/master/docs/IR.md#names-within-a-graph)
> All names MUST adhere to C identifier syntax rules.

https://github.com/pfnet/pytorch-pfn-extras/pull/102
",pytorch
54867,take-cheeze,pr,2021-03-29T06:36:58Z,Check count_include_pad in pool shape check,"> pad should be smaller than or equal to half of kernel size

check shouldn't happen when it's false",pytorch
54872,leslie-fang-intel,pr,2021-03-29T13:21:35Z,Fix overflow issue in quantized instance_norm/layer_norm/group_norm,"Fixes https://github.com/pytorch/pytorch/issues/54837
`hsum_sq` has the overflow issue when the input image size is large such as (H,W,D) as (224,224,160). `hsum_sq` is used in the quantized instance_norm/layer_norm/group_norm.
",pytorch
55216,mrTsjolder,pr,2021-04-02T06:26:01Z,introduce gain arg for kaiming initialisation,"Fixes #54030

Introduces `gain` argument for `kaiming` initialisation and deprecates `a` and `nonlinearity`.
",pytorch
55602,shinh,pr,2021-04-08T08:23:12Z,[ONNX] Fix exporting torch.norm without axes,"When you export `torch.norm(x)`, an excception is raised:

```
==> Context: Bad node spec: input: ""1"" output: ""2"" name: ""ReduceSum_1"" op_type: ""ReduceSum"" attribute { name: ""axes"" type: INTS } attribute { name: ""keepdims"" i: 1 type: INT }
```

As it internally calls `frobenius_norm` with `dim=[]`, we
should skip setting `axes_i` when an empty list is passed.

Fixes #{issue number}
",pytorch
56269,yorkie,pr,2021-04-16T15:24:22Z,fix the readme link,"Fixes #{issue number}
",pytorch
56368,eltociear,pr,2021-04-19T09:56:16Z,Fix typo in gradcheck.py,"betwen -> between
",pytorch
56763,kbrose,pr,2021-04-23T04:34:43Z,Fix derivative of sinc at x=0,"Attempting to fix #56760

The derivative of `sinc(x)` at `x=0` should be special cased to 0.",pytorch
56972,leslie-fang-intel,pr,2021-04-27T02:04:10Z,refactor autocast to be extensible for devices,"Refer to this POC: https://github.com/pytorch/pytorch/pull/56644 and the discussion of RFC: https://github.com/pytorch/pytorch/issues/55374, here is the PR to refactor the autocast mechanism of CUDA to make it extensible for devices: such as CPU. 
",pytorch
56986,kbrose,pr,2021-04-27T03:49:28Z,Fix for derivative of sinc(x) when x is positive but very very small,"Problem arises for sinc'(x) where x != 0, but x ** 2 == 0, which happens for some very small floats.

I realized that my solution from #56763 was incomplete when I did a quick implementation using `torch.autograd.Function` and still got a `NaN` from my derivative.",pytorch
57097,leslie-fang-intel,pr,2021-04-28T05:53:07Z,test ghstack,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57097 test ghstack**

",pytorch
57103,leslie-fang-intel,pr,2021-04-28T06:29:45Z,testgh,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57103 testgh**

",pytorch
57104,leslie-fang-intel,pr,2021-04-28T06:30:54Z,refactor autocast to be extensible for devices,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57104 refactor autocast to be extensible for devices**

Differential Revision: [D28094173](https://our.internmc.facebook.com/intern/diff/D28094173)",pytorch
57384,leslie-fang-intel,pr,2021-04-30T20:34:22Z,enable torch.cpu.amp.autocast,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57384 enable torch.cpu.amp.autocast**
* #57104 refactor autocast to be extensible for devices

",pytorch
57386,leslie-fang-intel,pr,2021-04-30T20:49:32Z,enable torch.cpu.amp.autocast,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#57386 enable torch.cpu.amp.autocast**

Here is the PR for what's discussed in the RFC https://github.com/pytorch/pytorch/issues/55374 to enable the autocast for CPU device. Currently, this PR only enable BF16 as the lower precision datatype.

Changes:
1.  Enable new API `torch.cpu.amp.autocast` for autocast on CPU device: include the python API, C++ API, new Dispatchkey etc.
2.  Consolidate the implementation for each cast policy sharing between CPU and GPU devices. 
3.  Add the operation lists to corresponding cast policy for cpu autocast.

Differential Revision: [D28572219](https://our.internmc.facebook.com/intern/diff/D28572219)",pytorch
57550,shamanDevel,pr,2021-05-04T16:06:49Z,TensorIterator: documentation on the order of creation,"Adds documentation to TensorIterator and TensorIteratorConfig that outputs need to be added first before inputs.

Fixes #57343
",pytorch
57722,H1Gdev,pr,2021-05-06T11:40:21Z,[NNAPI]Add flatten in serializer.,"Fixes #50533

- Error

```
Exception: Unsupported node kind ('aten::flatten') in node %input : Tensor = aten::flatten(%x, %26, %13) # /venv/lib/python3.6/site-packages/torchvision/models/mobilenetv2.py:195:0
```
",pytorch
58527,H1Gdev,pr,2021-05-19T02:26:16Z,[Android]Removed dependency with AppCompat.,"I build using [Bazel](https://bazel.build/).

When I use `pytorch_android` in latest Android app, I get the following error due to dependencies:

```
$ bazel build //app/src/main:app 
WARNING: API level 30 specified by android_ndk_repository 'androidndk' is not available. Using latest known API level 29
INFO: Analyzed target //app/src/main:app (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/H1Gdev/android-bazel-app/app/src/main/BUILD.bazel:3:15: Merging manifest for //app/src/main:app failed: (Exit 1): ResourceProcessorBusyBox failed: error executing command bazel-out/k8-opt-exec-2B5CBBC6/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox --tool MERGE_MANIFEST -- --manifest ... (remaining 11 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox ResourceProcessorBusyBox failed: error executing command bazel-out/k8-opt-exec-2B5CBBC6/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox --tool MERGE_MANIFEST -- --manifest ... (remaining 11 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
Error: /home/H1Gdev/.cache/bazel/_bazel_H1Gdev/29e18157a4334967491de4cc9a879dc0/sandbox/linux-sandbox/914/execroot/__main__/app/src/main/AndroidManifest.xml:19:18-86 Error:
	Attribute application@appComponentFactory value=(androidx.core.app.CoreComponentFactory) from [@maven//:androidx_core_core] AndroidManifest.xml:19:18-86
	is also present at [@maven//:com_android_support_support_compat] AndroidManifest.xml:19:18-91 value=(android.support.v4.app.CoreComponentFactory).
	Suggestion: add 'tools:replace=""android:appComponentFactory""' to <application> element at AndroidManifest.xml:5:5-19:19 to override.
May 19, 2021 10:45:03 AM com.google.devtools.build.android.ManifestMergerAction main
SEVERE: Error during merging manifests
com.google.devtools.build.android.AndroidManifestProcessor$ManifestProcessingException: Manifest merger failed : Attribute application@appComponentFactory value=(androidx.core.app.CoreComponentFactory) from [@maven//:androidx_core_core] AndroidManifest.xml:19:18-86
	is also present at [@maven//:com_android_support_support_compat] AndroidManifest.xml:19:18-91 value=(android.support.v4.app.CoreComponentFactory).
	Suggestion: add 'tools:replace=""android:appComponentFactory""' to <application> element at AndroidManifest.xml:5:5-19:19 to override.
	at com.google.devtools.build.android.AndroidManifestProcessor.mergeManifest(AndroidManifestProcessor.java:186)
	at com.google.devtools.build.android.ManifestMergerAction.main(ManifestMergerAction.java:217)
	at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$5.call(ResourceProcessorBusyBox.java:93)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:233)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:177)

Warning: 
See http://g.co/androidstudio/manifest-merger for more information about the manifest merger.
Target //app/src/main:app failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.221s, Critical Path: 1.79s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```

This is due to conflict between `AndroidX` and `Support Library` on which `pytorch_android_torch` depends.
(In the case of `Gradle`, it is avoided by `android.useAndroidX`.)

I created [Android application](https://github.com/H1Gdev/android-bazel-app) for comparison.

At first, I updated `AppCompat` from `Support Library` to `AndroidX`, but `pytorch_android` and `pytorch_android_torchvision` didn't seem to need any dependencies, so I removed dependencies.",pytorch
58728,H1Gdev,pr,2021-05-21T04:31:00Z,Fix typo.,"Fix typo in docs and comments.
",pytorch
58799,H1Gdev,pr,2021-05-22T03:50:10Z,[Android]Removed dependency with v7 AppCompat.,"Ref. #58527
",pytorch
59195,leslie-fang-intel,pr,2021-05-31T06:05:23Z,Vectorize the softmax calculation when not along the last dim,"Currently, if we do softmax which are not along the last dim, the calculation will fall to a [scalar version](https://github.com/pytorch/pytorch/blob/d417a094f398f1c4efd7f818b14b8471a597fbcc/aten/src/ATen/native/SoftMax.cpp#L14-L64).  And we find actually we have the chance to vectorize the calculation along the inner_size dim.

Changes we made:

- Use vectorized softmax_kernel instead of host_softmax when not along the last dim.

Performance data on 28 cores' Intel 8280 CPU when the Input size is [32, 81, 15130] and do softmax along the second dim(81).

- FP32 Baseline: 24.67 ms
- FP32 optimized: 9.2 ms",pytorch
59476,sbrodehl,pr,2021-06-04T20:18:15Z,"Make `LR scheduler` stub complete, including `OneCycleLR` and class attributes.","This PR completes the stub file for lr scheduler and includes a previously missing scheduler, namely `OneCycleLR, and adds additional class attributes and methods for all lr scheduler.
",pytorch
59532,take-cheeze,pr,2021-06-07T03:32:50Z,Fix error message formatting in _make_grads,"- TORCH_CHECK doesn't handle printf style format and it will output like: `got %ld tensors and %ld gradients21`
- `got 2 tensors and 1 gradients` should be the expected message for this",pytorch
59703,wbaek,pr,2021-06-09T14:26:30Z,typofix (torch.zero to torch.zeros) in docstring,,pytorch
59716,willfrey,pr,2021-06-09T16:49:20Z,Explicitly import functional into the torch.nn namespace,"The `torch.nn.functional` module is not currently imported into the `torch.nn` namespace. This causes Pylance, at a minimum, to complain about it not being a known member.

This should not add any overhead because `torch.nn.functional` is being imported implicitly in these two common scenarios, at least.

```py3
import torch

assert ""functional"" in dir(torch.nn)
```

```py3
from torch import nn

assert ""functional"" in dir(nn)
```

This change adds an explicit import.
",pytorch
60046,kbrose,pr,2021-06-15T22:23:04Z,Add verbose parameter to lr_scheduler type hints,"Without this, you get a type error when using `lr_scheduler.<some scheduler>(..., verbose=True)`",pytorch
60371,leslie-fang-intel,pr,2021-06-21T15:10:17Z,optimize non lastdim softmax bf16,"Here is the PR to enable the softmax calculation with data type of `bfloat16` when not along the last dim.
* Use bf16 specialization for forward calculation to reduce the bf16/fp32 cast in vec template.
* Release the bf16 limitation for backward calculation.
",pytorch
60513,take-cheeze,pr,2021-06-23T05:12:28Z,Expose onnx_shape_inference option in torch.onnx.export,Some model export fails when it's `True` so I need this for a workaround,pytorch
60729,take-cheeze,pr,2021-06-25T08:05:53Z,Fix missing spaces in error of constant_pad_nd,,pytorch
62528,leslie-fang-intel,pr,2021-07-31T03:48:20Z,Enable jit trace in autocast,"In this PR, we want to exclude these 2 cases in the `Autocast` weight cache usages:

- Using `torch.jit.trace` under the `Autocast` 
As report in https://github.com/pytorch/pytorch/issues/50231 and several other discussions, using `torch.jit.trace` under the `Autocast`, the trace process would hit Autocast's weight cache and fails. So we should disable weight cache under the trace process.
- Using `Autocast` with `Grad mode`

  - Usually we are using `Grad mode` for training. Since in the training phase, the weight will change in every step. So we doesn't need to cache the weight.
  - For the recommended `Autocast` training case in the [doc](https://pytorch.org/docs/stable/amp.html), `Autocast` will clear the cache every step leaving the context. We should disable it to save the clear operations.
    ```
    model = Net().cuda()
    optimizer = optim.SGD(model.parameters(), ...)
    
    for input, target in data:
        optimizer.zero_grad()
        with autocast():
            output = model(input)
            loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    ```
 

",pytorch
62691,hanton,pr,2021-08-03T23:07:11Z,[iOS] Add podspec for libTorch-lite nightly build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62691

Differential Revision: [D30090760](https://our.internmc.facebook.com/intern/diff/D30090760)",pytorch
62855,hanton,pr,2021-08-05T22:34:13Z,[iOS] enable Metal in the nightly build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62855

Differential Revision: [D30174151](https://our.internmc.facebook.com/intern/diff/D30174151)",pytorch
62984,hanton,pr,2021-08-09T18:54:04Z,[DO NOT MERGE][Test] enable Metal in the iOS nightly build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #62984

",pytorch
63045,kbrose,pr,2021-08-10T20:44:24Z,Add verbose parameter to lr_scheduler type hints,"Re-open of https://github.com/pytorch/pytorch/pull/60046 which stalled out, gained merge conflicts, and I had since deleted my fork of pytorch.

In the current version of pytorch, you get a type error when using `lr_scheduler.<some scheduler>(..., verbose=True)` even though this is allowed by the code. This MR results in no type error, as expected.",pytorch
63055,hanton,pr,2021-08-11T01:17:52Z,[DO NOT MERGE][Test] update iOS nightly build podspec file to CocoaPods,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63055

",pytorch
63132,leslie-fang-intel,pr,2021-08-12T05:37:24Z,add subtract of max and testcase,"As discussed here https://github.com/pytorch/pytorch/pull/62897, in the path of BF16/non-last-dim Softmax, we miss the subtractions of max value which will cause the overflow in the `exp()` calculation when the value of input tensor is large, such as `1000.0`.
To avoid this issue, we add the subtractions of max value and the corresponding test cases in this PR.

Note w/o subtractions of max value(accidental reverts or changes), we will get the underlying error message of the test case
```
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.05 and atol=0.05, found 103984 element(s) (out of 126720) whose difference(s) exceeded the margin of error (including 103984 nan comparisons). The greatest difference was nan (0.0 vs. nan), which occurred at index (0, 0, 0, 1).
```
",pytorch
63237,hanton,pr,2021-08-13T17:14:30Z,"[iOS] Add podspec for libTorch-lite nightly build""","Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63237

",pytorch
63239,hanton,pr,2021-08-13T17:31:01Z,[iOS] Add `LibTorch-Lite-Nightly` pod,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63239

Differential Revision: [D30315690](https://our.internmc.facebook.com/intern/diff/D30315690)",pytorch
63473,hmaarrfk,pr,2021-08-18T11:58:34Z,Add const keyword in jit/python/init.cpp,"The missing `const` keywords are appearing as errors in compilation for pypy builds on conda-forge
https://github.com/conda-forge/pytorch-cpu-feedstock/pull/66
",pytorch
63534,leslie-fang-intel,pr,2021-08-18T23:55:43Z,add operation list for AutocastCPU,"In this PR:
* We have changed the default dtype of `AutocastCPU` from `float16` to `bfloat16` as discussed here `https://github.com/pytorch/pytorch/pull/61002`
* We also update the operation list which needs casting to `lower_precision_fp` or `float32`.

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #63552
* __->__ #63534

Differential Revision: [D30644914](https://our.internmc.facebook.com/intern/diff/D30644914)",pytorch
63552,leslie-fang-intel,pr,2021-08-19T07:00:17Z,Allow disabling cache in autocast (automatic mixed precision),"In this PR, we want to exclude these 2 cases in the `Autocast` weight cache usages:

- Using `torch.jit.trace` under the `Autocast` 
As report in https://github.com/pytorch/pytorch/issues/50231 and several other discussions, using `torch.jit.trace` under the `Autocast`, the trace process would hit Autocast's weight cache and fails. So we should disable weight cache under the trace process.
- Using `Autocast` with `Grad mode`

  - Usually we are using `Grad mode` for training. Since in the training phase, the weight will change in every step. So we doesn't need to cache the weight.
  - For the recommended `Autocast` training case in the [doc](https://pytorch.org/docs/stable/amp.html), `Autocast` will clear the cache every step leaving the context. We should disable it to save the clear operations.
    ```
    model = Net().cuda()
    optimizer = optim.SGD(model.parameters(), ...)
    
    for input, target in data:
        optimizer.zero_grad()
        with autocast():
            output = model(input)
            loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    ```

Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63552

Differential Revision: [D30644913](https://our.internmc.facebook.com/intern/diff/D30644913)",pytorch
63718,hanton,pr,2021-08-21T00:10:19Z,[OSS] Enable Metal in PyTorch MacOS nightly builds,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63718

Differential Revision: [D30593167](https://our.internmc.facebook.com/intern/diff/D30593167)",pytorch
63814,hanton,pr,2021-08-23T22:39:26Z,[Test] run on all branch,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #63814
* #63718

",pytorch
64455,chenrui333,pr,2021-09-02T22:19:12Z,build: bump bazel to 4.2.1,,pytorch
67028,hanton,pr,2021-10-21T16:34:38Z,[DO NOT MERGE] iOS 1.10 lite binary push,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67028

",pytorch
67033,hanton,pr,2021-10-21T18:25:56Z,[DO NOT MERGE] iOS 1.10 full jit binary push,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67033
* #67028

",pytorch
67058,hanton,pr,2021-10-21T23:11:45Z,[iOS] Bump up iOS CocoaPods version to 1.10.0,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67058

Differential Revision: [D31846445](https://our.internmc.facebook.com/intern/diff/D31846445)",pytorch
68435,frgfm,pr,2021-11-16T16:45:09Z,docs: Added Union to supported types in documentation,"This PR simply updates the documentation following up on https://github.com/pytorch/pytorch/pull/64234, by adding `Union` as a supported type.

Any feedback is welcome!

cc @ansley @albanD @gmagogsfm ",pytorch
68567,leslie-fang-intel,pr,2021-11-18T02:47:04Z,add autocast cpu doc,"As discussed in https://github.com/pytorch/pytorch/issues/55374#issuecomment-968333614, here we update the cpu autocast operation list in autocast API document.
",pytorch
69341,hanton,pr,2021-12-03T01:58:35Z,[iOS] Add LibTorch nightly build,"Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #69341

Differential Revision: [D32901836](https://our.internmc.facebook.com/intern/diff/D32901836)",pytorch
70494,JimEverest,pr,2021-12-29T15:18:07Z,fix typos in torch/csrc/deploy/README.md,"Fixes typo in torch/csrc/deploy/README.md
",pytorch
70571,ganler,pr,2022-01-03T04:50:03Z,fix: onnx PReLU unidirectional broadcasting,"Fixes https://github.com/pytorch/pytorch/issues/70570
",pytorch
70584,ganler,pr,2022-01-03T18:13:40Z,fix: clip-onnx spec 9 & 11,"clip-v6 and clip-v9 only support float(16/32/64) as inputs such that prior implementation will make exported `clamp(int_min, int_max)` fail in ONNXRuntime. 

![image](https://user-images.githubusercontent.com/38074777/147964382-23d8e173-8de4-4b26-be19-c165ee419b4a.png)

To fix it, use the ""cast-clip-cast"" pattern as a workaround.

cc: @peterbell10 @BowenBao ",pytorch
71193,code-review-doctor,pr,2022-01-12T00:12:59Z,Remove unwanted comma,"Fixes #70611
",pytorch
72401,ganler,pr,2022-02-06T22:33:00Z,[ONNX] Make Non-Float Op Exportation Compatible to Avoid Invalid ONNX Models,"There are a few ONNX operators do not support non-float (e.g., integer) inputs at early versions. For example, Clip supports non-float types until [opset 12](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#type-constraints-280), that said older versions like [opset 6](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#type-constraints-107) cannot deal with integer types. 

I initially find such a bug in Clip (https://github.com/pytorch/pytorch/pull/70584), but later found more:
1. Clip < 12;
2. Min/Max < 12;
3. ReLU < 14;
4. Pad < 11;

In PyTorch, if we export Max-11 with integer inputs, actually the exportation will succeed; however, fail when imported by other frameworks like ONNXRuntime.

```python
import torch

class Net(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: torch.Tensor):
        return torch.max(x, x + 1)

net = Net()
onnx_model = 'test.onnx'

torch.onnx.export(net, (torch.zeros((3, 3), dtype=torch.int32),),
                  onnx_model, verbose=True, opset_version=11)
```

This is an unexpected behavior as we want to ensure that every model exported by PyTorch is valid (https://github.com/pytorch/pytorch/pull/70584#issuecomment-1020636579). Theoretically, we can simply forbid such cases (e.g., `Clip<int>` < 12, `ReLU<int>` < 14). But actually we can enhance the compatibility and flexibility of PyTorch by simply casting inputs of those operators into float tensors, which allows the float operator functions, and then casting it back to original types. 

This PR implements the second approach to achieve better compatibility in PyTorch.

@garymm  @thiagocrepaldi 


",pytorch
73178,acxz,pr,2022-02-21T14:46:25Z,update eigen submodule to latest release (3.4.0) with rocm fixes,"Fixes #73177
",pytorch
74763,AnthonyBarbier,pr,2022-03-25T18:05:14Z,Add new keys for Graphcore IPU (DispatchKey / Backend / DeviceType),We need a key to register our out of tree backend: https://github.com/graphcore/poptorch,pytorch
74823,leslie-fang-intel,pr,2022-03-28T08:20:25Z,<WIP> Test oneDNN user mode scratchpad,"This PR is still WIP and for test user mode scratchpad.
",pytorch
76712,cosine0,pr,2022-05-03T01:09:19Z,Adjust the stubs for PyCharm autocompletion of the Tensor methods.,"With current stub information, PyCharm (maybe other tools too) is not able to offer a correct method auto-completion of `Tensor` object or type inference involving `Tensor`s in a method chain. It is the same for sub-classes such as `FloatTensor`.   
For a workaround, I added `class Tensor` in the stub file and moved sub-class declarations. This allows much richer type inference results from PyCharm.",pytorch
77042,leslie-fang-intel,pr,2022-05-08T05:44:37Z,New generated conv_bn folding should use same weight and bias dtype as original conv module,"When doing the conv_bn folding in `torch.jit.freeze`, the new calculated `weight` and `bias` for the new conv will be promoted to high precision such as `float32` even the original `weight` and `bias` for the conv is low precision such as `bfloat16`. 
In this PR, we will record the original dtype for the conv's `weight` and `bias` and convert the data type back after `conv_bn` folding.
",pytorch
77244,leslie-fang-intel,pr,2022-05-11T12:04:00Z,Update amp document with CPU Training/Inference Examples,"This PR mainly updates the document with CPU Training/Inference Examples.
",pytorch
77953,Sadeedpv,pr,2022-05-20T08:03:04Z,Capitalized first letters in the contents of readme table,"Fixes #ISSUE_NUMBER

Changes have been made to the table in the  readme docs
![Screenshot (98)](https://user-images.githubusercontent.com/96517901/169482656-e36f66aa-61fa-4e40-8f2d-5a833552a921.png)

",pytorch
78241,leslie-fang-intel,pr,2022-05-25T06:29:49Z,Conv BN folding data type issue when conv has no bias,"PR https://github.com/pytorch/pytorch/pull/77042 has fixed the new folding conv-bn data type issue but missing the case when original conv has no bias input.
In this PR:

- Fix the new folding conv-bn's bias data type issue, when conv has no bias but weight as lower precision datatype, the new generated bias data type should be same as conv's weight.  
- Move the Autocast JIT Trace UT from `test_jit.py` to `test_jit_autocast.py`.
",pytorch
78498,hmaarrfk,pr,2022-05-30T19:18:51Z,Use consistent name for USE_SYSTEM_PYBIND11,"Should be consistent with: https://github.com/pytorch/pytorch/blob/master/CMakeLists.txt#L399

Using grep, it is likely that this is the only location in the repo where these variables are used.

```
~/git/pytorch_build/pytorch [v1.11.0|…2]
15:13 $ grep USE_SYSTEM_BIND11 . -R
./cmake/Dependencies.cmake:if(USE_SYSTEM_BIND11)
~/git/pytorch_build/pytorch [v1.11.0|…2]
15:13 $ grep USE_SYSTEM_PYBIND11 . -R
./CMakeLists.txt:option(USE_SYSTEM_PYBIND11 ""Use system-provided PyBind11."" OFF)
./CMakeLists.txt:  set(USE_SYSTEM_PYBIND11 ON)
```",pytorch
78883,hmaarrfk,pr,2022-06-04T20:22:24Z,Link BLAS privately,"We've some users report that they are getting symbol collisions when linking to blas.

I don't see a need to re-export the blas library symbols.

I figured I would share here for other packagers to be able to benefit too.

xref: https://github.com/conda-forge/pytorch-cpu-feedstock/pull/116
xref: https://github.com/conda-forge/openblas-feedstock/issues/134",pytorch
79110,leslie-fang-intel,pr,2022-06-08T05:41:21Z,[WIP] Enable log_softmax vec forward pass,"WIP to enable log_softmax vec forward pass
",pytorch
79287,AnthonyBarbier,pr,2022-06-10T16:06:12Z,Move IPU tensors to the CPU for printing.,"Same reasons as for the XLA backend: this translates into a bunch of operations like slice, indexing, etc which gets compiled in the graph.

",pytorch
79899,ganler,pr,2022-06-20T20:58:51Z,[ONNX][BugFix] prelu(scalar) should output scalar value in onnx,"Exported `PReLU` (scalar input) returns mismatched output.

```python
import torch


class Net(torch.nn.Module):
    def forward(self, x):
        return torch.nn.PReLU()(x)


net = Net().eval()

x = torch.zeros((), dtype=torch.float32)

with torch.no_grad():
    y_trh = net(x)
    torch.onnx.export(net, x, ""output.onnx"", input_names=['inp'], output_names=[
                      'out'], verbose=True, opset_version=14)

import onnxruntime as ort
sess = ort.InferenceSession(
    ""output.onnx"", providers=['CPUExecutionProvider'])
y_ort = sess.run(['out'], {'inp': x.numpy()})[0]
assert y_ort.shape == y_trh.shape, 'shape mismatch, ORT is `{}` but PyTorch is `{}`'.format(
    y_ort.shape, y_trh.shape)
```

Log:

```
Exported graph: graph(%inp : Float(requires_grad=0, device=cpu)):
  %onnx::PRelu_1 : Float(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0.25}, onnx_name=""Constant_0""]() # /home/ganler/miniconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1221:0
  %onnx::Unsqueeze_2 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=""Constant_1""]() # /home/ganler/miniconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1224:0
  %onnx::PRelu_3 : Float(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=""Unsqueeze_2""](%inp, %onnx::Unsqueeze_2) # /home/ganler/miniconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1224:0
  %out : Float(1, requires_grad=0, device=cpu) = onnx::PRelu[onnx_name=""PRelu_3""](%onnx::PRelu_3, %onnx::PRelu_1) # /home/ganler/miniconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py:1224:0
  return (%out)

Traceback (most recent call last):
  File ""test.py"", line 81, in <module>
    assert y_ort.shape == y_trh.shape, 'shape mismatch, ORT is `{}` but PyTorch is `{}`'.format(
AssertionError: shape mismatch, ORT is `(1,)` but PyTorch is `torch.Size([])`
```

The issue is similar to https://github.com/pytorch/pytorch/pull/78701

I did not add a test because there is one there:

https://github.com/pytorch/pytorch/blob/master/test/onnx/test_pytorch_onnx_onnxruntime.py#L6805

Not sure why it can pass the test.

cc: @justinchuby @BowenBao ",pytorch
80849,acxz,pr,2022-07-04T21:42:29Z,change rocblas.h -> rocblas/rocblas.h,"rocblas.h has been deprecated in favor of rocblas/rocblas.h in ROCm 5.2.0

Fixes #80848 
",pytorch
81435,timgates42,pr,2022-07-13T22:21:39Z,docs: Fix a few typos,"There are small typos in:
- caffe2/python/recurrent.py
- test/distributed/test_c10d_nccl.py
- test/test_fx.py
- torch/csrc/jit/runtime/autodiff.cpp
- torchgen/gen.py

Fixes:
- Should read `propagation` rather than `propogation`.
- Should read `multiplied` rather than `multuplied`.
- Should read `eliminate` rather than `elminate`.
- Should read `dispatcher` rather than `disaptcher`.



Semi-automated pull request generated by
https://github.com/timgates42/meticulous/blob/master/docs/NOTE.md",pytorch
81809,raziel,pr,2022-07-20T20:26:13Z,Fixes wrong link in CONTRIBUTING.md,"The link to the ""Where or how should I add documentation"" had some extra characters.

Fixes #ISSUE_NUMBER
",pytorch
82226,acxz,pr,2022-07-26T16:12:33Z,[ROCm] update nightlies to build rocm 5.2/5.2.1,"### Description
<!-- What did you change and why was it needed? -->

Updated nightlies to build rocm version 5.2 and 5.2.1 instead of the older 5.0 and 5.1.1. Needed because the CI has changed to 5.2 (https://github.com/pytorch/pytorch/pull/81168) but the nightlies are lagging behind.

### Issue
<!-- Link to Issue ticket or RFP -->
See: https://github.com/pytorch/pytorch/pull/81168 and https://github.com/pytorch/pytorch/pull/80849#issuecomment-1195620582

cc: @cpuhrsch , @ZainRizvi , @jithunnair-amd , @janeyx99, @kit1980",pytorch
679,NathanHowell,pr,2016-01-04T18:43:46Z,Fix typo,"Fix typo
",tensorflow
749,jfsantos,pr,2016-01-11T22:13:51Z,Adding Python 3 support to Tensorboard,"This PR implements Python 3 support into Tensorboard in a portable way, by importing packages from six when possible and converting strings into the correct encoding whenever necessary.
",tensorflow
764,jfsantos,pr,2016-01-13T18:19:52Z,Fix Python 3 compilation issues,"This fixes the build error when using Python 3 as the return types for import array operations are int in Python 3 and void in Python 2 (as discussed in #733).
",tensorflow
1523,ronrest,pr,2016-03-16T06:42:38Z,fixes #1522 close file object properly for read_data(),"Fixes the issue raised here:  https://github.com/tensorflow/tensorflow/issues/1522
",tensorflow
2147,iamaziz,pr,2016-04-28T10:03:44Z,Remove unused os import,,tensorflow
2173,stephenroller,pr,2016-04-30T03:48:14Z,Import numpy before a call to sys.setdlopenflags,"Must import numpy before a call to sys.setdlopenflags, or there may be a segfault in certain situations. Fixes #2034.

The situation occurs when users have compiled TensorFlow, but use binary distributions of SciPy. Alternatives to this patch would be documentation stating that users who compile TensorFlow from source should also recompile SciPy if they need it.
",tensorflow
2221,qbx2,pr,2016-05-04T18:37:27Z,API Doc: Moved the usual termination code into a finally block,"Moved the usual termination code into a finally block so that the code can be executed even when SystemExit, KeyboardInterrupt, GeneratorExit raised (These exceptions derive not Exception but BaseException.)
",tensorflow
2302,qbx2,pr,2016-05-10T08:53:47Z,"Fixed Tensorboard minimap not being drawn on Safari (OS X, iOS)","Fixed Tensorboard minimap not being drawn on Safari (OS X, iOS)
Tested on Chrome (OS X), Safari (OS X, iOS)
",tensorflow
2495,maximsch2,pr,2016-05-24T21:56:22Z,Truncate random seed to fit into int during protobuf serialization,"Fixes https://github.com/tensorflow/tensorflow/issues/2460
",tensorflow
2543,ajschumacher,pr,2016-05-27T21:54:00Z,"grammar/typo: ""like variable"" -> ""like a variable""",,tensorflow
2601,wangg12,pr,2016-06-01T02:09:05Z,fix typo in setup for development,,tensorflow
2722,qbx2,pr,2016-06-08T06:25:53Z,Fixed tensorboard not to use any lambda functions for compatibility with Safari,"Fixed tensorboard not to use any lambda functions for compatibility with Safari

Tested on Safari (OS X, iOS), Chrome (Windows, OS X)
",tensorflow
3062,igormq,pr,2016-06-27T20:19:23Z,Better docs of returned tensor in ctc_ops.py,"'Logits' aren't a meaningful word for what the ctc_cost function returns. Browsing the implementation (tensorflow/tensorflow/core/util/ctc/) I saw that the cost function is returning the negative log probabilities of the target labelling, so, this new comment erases any doubt. Thanks!
",tensorflow
3064,igormq,pr,2016-06-27T20:43:03Z,Explanation of blank label in ctc_loss,"The doc string of ctc_loss is lacking of several details. I tried to explain a little better of what is needed to get it working, but english is not my native language, so I'm sure that you can do much better!
",tensorflow
3297,igormq,pr,2016-07-13T17:02:58Z,time_major parameter for ctc_loss op,"Just as dynamic_rnn and to be more regular across the TensorFlow's code I added a flag capable of transpose the input data to be in time_major, required by gen_ctc_ops._ctc_loss. Note that, instead of set time_major=False (as the most of TensorFlow data), it was set False to be compatible with previous ctc_loss function call (but I don't think that this is a good idea).
",tensorflow
3488,vra,pr,2016-07-25T04:14:20Z,minor typo fix,,tensorflow
3713,igormq,pr,2016-08-09T17:13:36Z,Explanation of blank label in ctc_loss,"The doc string of ctc_loss is lacking of several details. I tried to explain a little better of what is needed to get it working, but english is not my native language, so I'm sure that you can do much better!
",tensorflow
3783,NathanHowell,pr,2016-08-12T23:29:10Z,ZlibInputBuffer::Inflate() incorrectly considers Z_STREAM_END an error,"When reading compressed records produced by something other than TFRecordWriter it is possible to produce valid files that are not readable by Tensorflow.

This fix adds a test that creates a deflate file using the builtin zlib module that previously caused TFRecordReader to raise an exception:

`DataLossError: inflate() failed with error 1`

Full stack is below:

```
======================================================================
ERROR: testZLibFlushRecord (__main__.TFRecordWriterZlibTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py"", line 495, in testZLibFlushRecord
    k, v = sess.run([key, value])
  File ""/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 710, in run
    run_metadata_ptr)
  File ""/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 908, in _run
    feed_dict_string, options, run_metadata)
  File ""/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 958, in _do_run
    target_list, options, run_metadata)
  File ""/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 978, in _do_call
    raise type(e)(node_def, op, message)
DataLossError: inflate() failed with error 1
     [[Node: ReaderRead = ReaderRead[_class=[""loc:@fifo_queue"", ""loc:@test_reader""], _device=""/job:localhost/replica:0/task:0/cpu:0""](test_reader, fifo_queue)]]
```
",tensorflow
3817,vra,pr,2016-08-15T14:17:25Z,fix typo,,tensorflow
3827,NathanHowell,pr,2016-08-15T20:34:54Z,Fix `ResourceWarning: unclosed file` warnings in reader_ops_test.py,"Some file handles created in these test cases are were not being closed. This causes warnings in py3 along with leaked handles.

```
./source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:245: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/text_line.0.txt'>
  f = open(fn, ""wb"")
/source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:276: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/text_line.1.txt'>
  self._testOneEpoch(self._CreateFiles(crlf=True))
./source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:273: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/text_line.1.txt'>
  self._testOneEpoch(self._CreateFiles(crlf=False))
./source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:279: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/text_line.1.txt'>
  files = self._CreateFiles()
../source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:183: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/whole_file.0.txt'>
  open(fn, ""wb"").write(c)
/source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:183: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/whole_file.1.txt'>
  open(fn, ""wb"").write(c)
/source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:183: ResourceWarning: unclosed file <_io.BufferedWriter name='/var/folders/sq/vmncyd7506q_ch43llrwr8sn6zfknl/T/reader_ops_test/whole_file.2.txt'>
  open(fn, ""wb"").write(c)
...
```
",tensorflow
3863,NathanHowell,pr,2016-08-16T22:38:48Z,Fix some comment and test case typos,,tensorflow
3993,igormq,pr,2016-08-23T20:09:56Z,time_major parameter for ctc_loss op,"Just as dynamic_rnn and to be more regular across the TensorFlow's code I added a flag capable of transpose the input data to be in time_major, required by gen_ctc_ops._ctc_loss. Note that, instead of set time_major=False (as the most of TensorFlow data), it was set False to be compatible with previous ctc_loss function call (but I don't think that this is a good idea).
",tensorflow
4045,cwhipkey,pr,2016-08-25T19:34:22Z,"Switch 'friend class DimensionOrConstant' to use struct, to match act…","…ual declaration.
",tensorflow
4225,wangg12,pr,2016-09-06T13:58:26Z,fix typo(sttdev-->stddev) in image_retraining/retrain.py,,tensorflow
4294,shinh,pr,2016-09-09T08:00:41Z,Fix a typo in g3doc/how_tos/tool_developers,"Add `()' after f.read so we pass a string rather than a
function.
",tensorflow
4323,vra,pr,2016-09-11T13:24:17Z,add an `is` to make it a sentence,,tensorflow
4345,vra,pr,2016-09-13T01:23:16Z,Add code to close session,"Hi all, I read the tensorflow tutorial and find that we must close session when finishing using it. However, when I review the code in [cifar10 tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image/cifar10), I find that in some files,  there is no `sess.close()` line. I don't know whether it's a function of `tf.app` utilities, so I add the session closing code and make you know about this issue.  Hope this can be helpful.
",tensorflow
4618,jusjusjus,pr,2016-09-28T08:49:28Z,Fix typo in 'normalization'.,"This pull fixes a typo in explanations of examples/tutorials/deepdream/deepdream.ipynb.
",tensorflow
4627,jusjusjus,pr,2016-09-28T14:23:58Z,Resolve urllib dependency for py3.5,"After this pull, <..>/tutorials/estimators/abalone.py works with python 3.5.

It resolves the dependency on urllib.urlretreive, which shows a different syntax under python 3.5.
",tensorflow
4633,jusjusjus,pr,2016-09-28T18:06:06Z,Resolve python 3.5's urllib dependency in abalone-tutorial.,"Replacing urllib with six.moves.urllib enables this tutorial for Python 3.5.
",tensorflow
4868,kbrose,pr,2016-10-10T03:14:42Z,Updates to documentation of two tf.contrib.metrics functions.,"- Added code wrapping markdown for function `tf.contrib.metrics.aggregate_metrics`.
- General editing of `tf.contrib.metrics.confusion_matrix` for clarity.
",tensorflow
5091,temporaer,pr,2016-10-20T10:01:08Z,doc: fix possible copy&paste error in cumprod docs,,tensorflow
5108,temporaer,pr,2016-10-21T07:32:06Z,doc: fix possible copy&paste error in cumprod docs,,tensorflow
5292,hoangmit,pr,2016-10-31T00:52:25Z,reduce_logsumexp fix for reduction_indices.  Fixes issue #5291,"Fix #5291
",tensorflow
6038,ronrest,pr,2016-12-02T09:01:59Z,Fix issue #6036,"As per the issue raised [here](https://github.com/tensorflow/tensorflow/issues/6036), changed the following code to the  [**6_lstm.ipynb**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb) file.

**FROM:**
```python
def read_data(filename):
  f = zipfile.ZipFile(filename)
  for name in f.namelist():
    return tf.compat.as_str(f.read(name))
  f.close()
```

**TO:**

```python
def read_data(filename):
  with zipfile.ZipFile(filename) as f:
    name = f.namelist()[0]
    data = tf.compat.as_str(f.read(name))
  return data
```
",tensorflow
6227,danielgordon10,pr,2016-12-09T22:44:17Z,Fix split_v docs,Fixes some inconsistencies and copy/paste typos in split_v's code.,tensorflow
6321,danielgordon10,pr,2016-12-14T23:03:55Z,Fixed typo in embedding tutorial.,,tensorflow
6664,aam-at,pr,2017-01-05T10:32:54Z,Max pool grad grad,"Adds gpu and cpu kernels for max pool grad. Supports NHWC and NCHW layouts for gpu kernel and only NHWC for cpu kernel. Additionally registers gradient for python `MaxPoolGradGrad` which is just `MaxPoolGrad`. Registers gradient for python `AvgPoolGrad` which is `AvgPool` evaluated on `top_diff`.

Continuation of #6299 ",tensorflow
6859,leomao,pr,2017-01-15T08:27:41Z,The type of `seq_len_max` should be int64,"If the `sequence_length` is not given, `max_seq_len` will be assigned to `time_len`.
By default, the type of `time_len` is `tf.int32`. But we need `tf.int64`.
So we need to cast it to `int64`, too. (Just like line 640)",tensorflow
7101,hartb,pr,2017-01-26T23:08:32Z,Improve cudnn.h search during build (#6850),"Update _find_cuda_define() and _cudnn_version() to use
_find_cudnn_header_dir() to locate the cudnn.h header. The latter
already knows how to look in multiple locations for the header.

This works on POWER Ubuntu when cuDNN is installed via tarball or
deb package and CUDNN_INSTALL_PATH is set to /usr/local/cuda-8.0.

In those cases, cudnn.h is ultimately found at:

$ sudo tar -C /usr/local -xzvf cudnn-8.0-linux-ppc64le-v5.1.tgz

/usr/local/cuda-8.0/targets/ppc64le-linux/include/cudnn.h

-or-

$ sudo dpkg -i libcudnn5*deb

/usr/include/cudnn.h (is soft link to /etc/alternatives/libcudnn
is soft link to /usr/include/powerpc64le-linux-gnu/cudnn_v5.h)",tensorflow
7151,hartb,pr,2017-01-30T21:13:23Z,Improve cudnn.h search during build (#6850),"Update _find_cuda_define() and _cudnn_version() to use
_find_cudnn_header_dir() to locate the cudnn.h header. The latter
already knows how to look in multiple locations for the header.

This works on POWER Ubuntu when cuDNN is installed via tarball or
deb package and CUDNN_INSTALL_PATH is set to /usr/local/cuda-8.0.

In those cases, cudnn.h is ultimately found at:

$ sudo tar -C /usr/local -xzvf cudnn-8.0-linux-ppc64le-v5.1.tgz

/usr/local/cuda-8.0/targets/ppc64le-linux/include/cudnn.h

-or-

$ sudo dpkg -i libcudnn5*deb

/usr/include/cudnn.h (is soft link to /etc/alternatives/libcudnn
is soft link to /usr/include/powerpc64le-linux-gnu/cudnn_v5.h)",tensorflow
7277,rkaplan,pr,2017-02-06T00:56:22Z,Improve docs for incompatible protobuf error,"On Mac OS X, importing tensorflow can result in the error: ""TypeError: `__init__()` got an unexpected keyword argument 'syntax'"". As explained in the doc, this is caused by an incompatible protobuf version. The suggested fix, ""pip install --upgrade protobuf"", doesn't resolve the issue if the user has installed protobuf via Homebrew. This commit adds an example command to fix the error in this case.",tensorflow
7337,cwhipkey,pr,2017-02-07T19:47:49Z,Merge in some performance changes to r1.0,"git cherry-pick f49cca9b7f7ecac8b4f2c031d489e99594068645
git cherry-pick 66b5684133bda0a3050e1573f747d86c645dfd67
git cherry-pick f0a1af4a8bcbe456a0d8a748f9805494cc4ab786
git cherry-pick 2b589dc79b16c2d104a71f809af5505fe9476687
git cherry-pick f49721aaf82b44dc420b5898536b5dce3254975b
git cherry-pick 9b7c47c1d48dfbe69e2ab62aae6146823ba7e664
git cherry-pick 44642329df6bc1627c54f92c5f6850e5882da991
git cherry-pick 5be95cbb389bc112161232c8514155947063ea72
git cherry-pick 27ffb49afc9030a5070a860f4fca491d4f8c29e7
git cherry-pick e9602d275295feaeee900147147957aad2594fb4
git cherry-pick bf67d0a1c52c303f9018c00cbc030dc35438ed2c
git cherry-pick 7725c874499991464d5fd0a4fd57216885726a60
git cherry-pick e2127701a5695f393c42a270ab30814703feb64b",tensorflow
7345,cwhipkey,pr,2017-02-07T23:41:50Z,"Apply changes for non-fused winograd, and contrib/BUILD change for nccl, to r1.0",,tensorflow
7692,ankeshanand,pr,2017-02-20T11:27:15Z,Update doc string to indicate clip_by_value accepts tensors as min and max arguments too,"`tf.clip_by_value`'s current documentation mentions that it accepts only scalars as min and max arguments, but it can accept tensors too. This PR fixes that, and resolves the confusion raised in #7225  ",tensorflow
7901,qbx2,pr,2017-02-26T15:56:55Z,Catch PermissionError to raise RuntimeError,,tensorflow
8070,cwhipkey,pr,2017-03-04T00:57:49Z,Cherry-pick: Add the graphdef version to InferenceContext and to ShapeRefiner::Add…,"…Node.

Use this to allow loading reductions saved with older graphdefs.

Change GraphConstructor to not increase the version when importing, but instead take the min of all versions.
Change: 149152437",tensorflow
8072,wangg12,pr,2017-03-04T04:16:55Z,change learn.metric_spec.MetricSpec to learn.MetricSpec due to API change  in version 1.0,As mentioned in #7569.,tensorflow
8106,Jiaming-Liu,pr,2017-03-05T22:50:03Z,Fix a bug of initializing,"Prevent the exceptional illegal call of __initializer.\_\_init\_\_(shape, dtype, partition_info)__ appears when the param ""initializer"" is an init_ops class instead of its instance.",tensorflow
8580,lyuwenyu,pr,2017-03-21T12:01:27Z,modify annotations of data_format of average_pooling2d in file python/layers/pooling.py ,"Modified wrong annotation of data_format in average_pooling2d in file python/layers/pooling.py

from (batch, height, channels, width) to (batch, height, width, channels) when 'channels_last'",tensorflow
8683,wangg12,pr,2017-03-24T06:46:24Z,Update linalg_ops.py,Just add a space to doc of svd...,tensorflow
8915,csarron,pr,2017-04-03T06:13:48Z,Update lstm_ops.cc,fix o in doc,tensorflow
9070,ajschumacher,pr,2017-04-08T19:26:26Z,fix example typo (too many brackets),"The example was like `json.dumps({{}})`, which fails because `{{}}` is not a valid data structure.

Also, just below this, there is `assert config.master == 'host4:2222'`, which I believe would fail since that value comes back as `'grpc://host4:2222'`, but I'm less sure about recommending this change.",tensorflow
9157,domschl,pr,2017-04-12T06:21:01Z,missing dash in install from source at cxxopt added,"option `--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""` needs two dashes.",tensorflow
9200,eklitzke,pr,2017-04-13T20:47:53Z,Fix build warning about comparing signed/unsigned types,This fixes a harmless (but annoying) build warning about comparing signed/unsigned types.,tensorflow
9244,eklitzke,pr,2017-04-16T04:25:20Z,fix warning about comparing signed/unsigned types in auto_parallel.cc,This fixes a harmless (but annoying) GCC warning about comparing signed/unsigned types in auto_parallel.cc. ,tensorflow
9265,ragulpr,pr,2017-04-17T11:34:16Z,fix style_guide.md my_op-example,"2 Typos causes errors:

-tf.add_to_collections -> tf.add_to_collection
-adding missing indentation (0 -> 2 spaces)

Resulting my_op-chunk tested with tf. version 1.0.1

See:
https://www.tensorflow.org/community/style_guide
For the erronous version.",tensorflow
9451,ajschumacher,pr,2017-04-26T02:30:58Z,"typo: ""serialize"" -> ""serialized""",,tensorflow
9649,ajschumacher,pr,2017-05-04T02:40:48Z,fix `iris_test.csv` download link,,tensorflow
9740,thinxer,pr,2017-05-07T16:04:46Z,Fix tf.meshgrid documentation,"The original documentation has the wrong output regarding the default indexing xy.

    x = [1, 2, 3]
    y = [4, 5, 6]
    X, Y = tf.meshgrid(x, y)
    print(X.eval())
    print(Y.eval())

    [[1 2 3]
     [1 2 3]
     [1 2 3]]
    [[4 4 4]
     [5 5 5]
     [6 6 6]]",tensorflow
9802,stsievert,pr,2017-05-09T20:47:59Z,DOC: formats momentum calculation for web,,tensorflow
10025,thinxer,pr,2017-05-19T05:00:47Z,Fix some doc format for CropAndResize,,tensorflow
10237,thinxer,pr,2017-05-27T03:14:58Z,Add a tip for tf.train.LoggingTensorHook,`INFO` logs are not printed by default unless in IPython. Add a friendly tip for newcomers.,tensorflow
10366,jpuigcerver,pr,2017-06-01T08:45:23Z,Fixes issue #10258,"On CUDA versions previous to 8.0, only __shared__ variables could be declared as static in the device code.",tensorflow
10732,Scitator,pr,2017-06-15T12:47:49Z,Padding queue support for tf.estimator generator input pipeline,"So, as I desrcibe [here](https://github.com/tensorflow/tensorflow/issues/10680), there is no padding possibilities in current estimator's input pipelines.

For now I added simple possibility to you data have dynamic shape over last axis. 
If such solution looks good, then I am looking forward for next contributing steps.",tensorflow
10828,DeNeutoy,pr,2017-06-19T16:19:23Z,add log_softmax c++ gradient,,tensorflow
11319,yorkie,pr,2017-07-06T09:47:37Z,c: fix a possible segmentfault,"We also need allocation for `output_values_` when updating `output_`, or we will get a segment fault on:

```c++
static void TF_Run_Setup(int noutputs, TF_Tensor** c_outputs,
                         TF_Status* status) {
  status->status = Status::OK();
  for (int i = 0; i < noutputs; ++i) {
    c_outputs[i] = nullptr;
  }
}
```
",tensorflow
11580,yorkie,pr,2017-07-18T13:48:54Z,python: remove the TRAINABLE_RESOURCE_VARIABLES graph key,"The graph key `TRAINABLE_RESOURCE_VARIABLES` seems to have been removed, drop the lines in this PR.",tensorflow
11671,willfrey,pr,2017-07-21T18:08:15Z,Update fft2d.cmake,"The cmake fft2d library was installing to '(fft2d_INSTALL)' instead of the appropriate variable set by CMake. I suspect this was unintentional. 

Changing the parentheses to curly brackets fixes this.",tensorflow
11731,junluan,pr,2017-07-25T01:59:08Z,Fix wrong template type name in QuantizedAddUsingEigen,input type is T1 and smaller input type is T2,tensorflow
11878,shamanDevel,pr,2017-07-30T07:28:27Z,SVD-operation on the GPU,"This merge request implements the SVD on the GPU using cuSolver.

We are building a sparse Gaussian Mixture Model using Tensorflow and for that we need the SVD and determinant computation (see another merge request). Since these operations were not implemented on the GPU yet and the communication resulted in a large overhead, I implemented them on the GPU.

The SVD is implemented using cuSolver. 
cuSolver, however, has the downside of only supporting matrices with m>=n. The CPU-Version of SVD supports also matrices with m<n. So this kernel throws an error when matrices with m<n are passed and notifies the user to explicitly switch to the CPU kernel.",tensorflow
11879,shamanDevel,pr,2017-07-30T07:46:10Z,Determinant-operation on the GPU,"This pull request implements the computation of the matrix determinant on the GPU using LU-factorization with cuSolver.

This is the second part of the use-case presented in pull request #11878 .

It uses the LU factorization instead of QR-factorization, because this allows to compute the determinant also for non-symmetric matrices. This is required by the definition of the operation in Tensorflow. Hence, I also had to implement the LU-interface to cuSolver, QR would have already been implemented.

Two further notes: 
First, the precision of this algorithm to compute the determinant is less precise than the Eigen-implementation. Hence I had to relax the allowed error in the unit test. This is especially important for singular or near-singular matrices.
Second, if this pull request and #11878 will both be accepted, they will probably clash in cuda_solvers.h and/or cuda_solvers.cc. I'll resolve the conflicts if it happens to be so.",tensorflow
12435,youkaichao,pr,2017-08-21T02:15:37Z,typo in docs: one ==> once,in tensorflow/docs_src/programmers_guide/datasets.md,tensorflow
12594,csarron,pr,2017-08-25T11:24:27Z,fix: profiler bazel build usage,,tensorflow
14854,JoshVarty,pr,2017-11-24T05:11:51Z,Add batch support for various image_ops,"Working on #8926
I used #7369 as a guide for my work here.

I have added batch support for:

- `flip_left_right`
- `flip_up_down`
- `random_flip_left_right`
- `random_flip_up_down`
- `transpose_image`
- `rot90`

I have corrected existing tests in `image_ops_test.py` and introduced a number of new tests based on existing tests for 3D inputs. 

This is my first contribution to this repository and I have tried to follow the [`contributing`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) guidelines. However, running `pylint` on `image_ops_impl.py` and `image_ops_test.py` revealed a number of pre-existing style violations. I've tried to fix the ones relevant to my work but may have missed some.",tensorflow
14930,mrshu,pr,2017-11-28T07:40:24Z,softmax_cross_entropy: Improve docstring,Improve docstring of `softmax_cross_entropy`.,tensorflow
15268,103yiran,pr,2017-12-11T11:04:02Z,correct the misspell of Quantize,,tensorflow
15308,wbaek,pr,2017-12-12T12:03:42Z,improve compute high rank hessians,fix possible compute high rank hessians,tensorflow
15486,stegben,pr,2017-12-19T17:48:08Z,fix _Pooling1D data format bug,"When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.",tensorflow
15500,stegben,pr,2017-12-20T06:27:33Z,fix pooling1D dimension bug,"When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.

When `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.

RELNOTES: Fixed wrong handling of 1D pooling (pooling was not happening on the correct dimension).",tensorflow
15726,JoshVarty,pr,2017-12-30T06:18:40Z,Batch support for deterministic image ops,"Based on #14854 and working on #8926

Previously I had implemented batch support for a number of image ops. However performance concerns were raised and the changes were reverted.

I have re-implemented the changes for:
 - `flip_left_right`
- `flip_up_down`
- `transpose_image`
- `rot90`

I ran performance tests from https://github.com/tensorflow/tensorflow/pull/15348 with:

`bazel run -c opt //tensorflow/python:image_ops_test -- --benchmarks=FlipImageBenchmark`

| Operation | Before (μs) | After (μs) |
| --- | --- | --- |
| benchmarkFlipLeftRight_299_299_3_/cpu:0_1 | 274.49  | 264.26 |
| benchmarkFlipLeftRight_299_299_3_/cpu:0__all | 292.76 |  266.10 |
| benchmarkFlipLeftRight_299_299_3___all | 273.80 |  265.71 |
| *benchmarkRandomFlipLeftRight_299_299_3_/cpu:0_1 | 242.58  | 241.89 |
| *benchmarkRandomFlipLeftRight_299_299_3_/cpu:0__all | 245.27 | 239.88 |
| *benchmarkRandomFlipLeftRight_299_299_3___all | 252.71 | 241.20 |

*\* There were no changes made to `RandomFlipLeftRight in this PR*

Let me know if you would like me to add more performance tests for the other methods. I don't think there should be any performance impact, but I'm happy to add more if you'd like.",tensorflow
16404,stegben,pr,2018-01-25T13:06:58Z,remove SRU num_units == x.shape[-1] restriction,"Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue

#13094 ",tensorflow
16427,klshrinidhi,pr,2018-01-26T01:43:01Z,Fix build errors in contrib/mpi introduced by commit 6042b5d267f,"The commit https://github.com/tensorflow/tensorflow/commit/6042b5d267f42d004087b44c29525951700579f9#diff-7c00d4a3caee74eedf5bb638bce23e5a 
* Introduced code to `tensorflow/contrib/mpi/mpi_rendezvous_mgr.h` to use the type `RecentRequestIds` without including the header `tensorflow/core/distributed_runtime/recent_request_ids.h`.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:182:3: error: 'RecentRequestIds' does not name a type
   RecentRequestIds recv_tensor_recent_request_ids_;
   ^
```
* Probably a typo in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc : MPIRendezvousMgr::AddRequest()`. The variable `req` was probably meant to be `request` as per the commit message.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/variant.h:29:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/device_base.h:23,
                 from ./tensorflow/core/framework/rendezvous.h:22,
                 from ./tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h:22,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:22,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'void tensorflow::MPIRendezvousMgr::AddRequest(tensorflow::RecvTensorRequest, int)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:155:7: error: 'req' was not declared in this scope
       req.request_id(), ""RecvTensor (MPIRendezvousMgr)"", req));
       ^
```

I compiled with following commands:
```
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" > /etc/apt/sources.list.d/bazel.list
curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg
pip install -v ./tensorflow_pkg/tensorflow-*.whl
```",tensorflow
16786,lazypanda1,pr,2018-02-05T22:54:44Z,Fix wrong error message for beta distribution,"This PR fixes the assertion failure message for Beta distribution's valid sample check, which is currently incorrect. For a sample with value `1.0`, the current error message is: `sample must be no larger than '1'`, wrongly suggesting that sample can be `1.0`. After this PR, the will say, `sample must be less than '1'`.",tensorflow
16881,lazypanda1,pr,2018-02-08T23:48:15Z,Enable validate_args for all distributions based on a global parameter,"The probability distributions in Tensorflow have a `validate_args` argument that is initially set to `False`. This PR adds a feature that initializes the value of `validate_args` to the value of a global flag `validate_args_default`, which can be set from the user program

Rationale: Currently, when the user sets up the model incorrectly e.g. wrong parameter values are provided for the distributions, or data values don't match the support for the distribution, the program silently fails and produces nans in the output (or worse, wrong values). The user has no way to debug this easily because `validate_args` is `False` by default, and manually adding `validate_args=True` while initializing each distribution can be tedious for the user.

Resolving: #16839 ",tensorflow
16966,klshrinidhi,pr,2018-02-13T05:41:18Z,Fix compiler error mentioned in #16960 introduced by commit 1baac78627,"Addressing #16960.
The commit https://github.com/tensorflow/tensorflow/commit/1baac7862739525351d25202800dc04e8ec3868b introduced member functions `MklSubAllocator::AddAllocVisitor` and `MklSubAllocator::AddFreeVisitor` which respectively use `allocator_->AddAllocVisitor` and `allocator_->AddFreeVisitor` but `allocator_` is of type `Allocator *` which doesn't have these member functions. 

I am guessing the intention was to change the type of `allocator_` to `BFCAllocator*` to make this work.",tensorflow
17123,hovhannesgithub,pr,2018-02-19T09:01:44Z,Add broadcasting functionality for Div and Sub ops.,"Hi,
Added broadcasting functionality for Div and Sub ops following the examples of Add and Mul.
Regards,
Hovhannes",tensorflow
17171,ilya-biryukov,pr,2018-02-21T16:37:34Z,Fix compiler error with cuda-clang,"segment_reduction_ops.h requires cuda_kernel_helper.h to be
included in clang because it uses some of the helpers directly in the
header (e.g. CudaAtomicMax). It works with nvcc, because the usage is
in a template context and nvcc checks that function is available only later
at template instantiation.
However, clang does more strict erorr-checking for functions found
during template instantiation and requires them to also be found either by
ADL or at the point of template declaration.",tensorflow
17503,yaox12,pr,2018-03-07T09:18:18Z,set default values of args in print_tensors_in_checkpoint_file,"set default values of args in function `print_tensors_in_checkpoint_file` to avoid backwards compatibility problems.   

detail in issue #17498",tensorflow
17740,fumihwh,pr,2018-03-15T15:12:21Z,implement matrix 2-norm,Implement matrix 2-norm by using tf.svd.,tensorflow
17750,jongwook,pr,2018-03-16T00:38:05Z,using finally in tf_record_iterator(),"Hi, this is to handle the case when [generator.throw()](https://www.python.org/dev/peps/pep-0342/#new-generator-method-throw-type-value-none-traceback-none) or [generator.close()](https://www.python.org/dev/peps/pep-0342/#new-generator-method-close) is called on a tfrecord iterator.

It will raise an exception like `GeneratorExit` in the generator's stack, and using `finally:` will promptly release the resources in such cases, without relying on garbage collection.

Please let me know if I'm missing something.",tensorflow
17988,stegben,pr,2018-03-25T14:29:26Z,make rnn cell build with flexible inputs_shape,In case someone manually call them and provide a list or tuple.,tensorflow
18854,mrTsjolder,pr,2018-04-25T07:03:16Z,Fix variance initialisers,Pull request for resolving issue #18706,tensorflow
18876,stefan-it,pr,2018-04-25T21:50:06Z,Fix link to original LSTM paper,"Hi,

the url to the original LSTM paper in `tensorflow/contrib/lite/nnapi/NeuralNetworksShim.h` is no longer available (404 is returned), so this PR fixes it.",tensorflow
19055,yaox12,pr,2018-05-03T11:09:52Z,Feature: Add reduce_average (weighted reduce_mean),"Add `tf.reduce_average` according to issue #7422

In this issue, some people doubt why this function is needed. I think broadcasting `weights` and corresponding `axis` is complex enough and worth a built-in function.

`np.average` is also contributed by me. Related [Issue](https://github.com/numpy/numpy/issues/10989) and [PR](https://github.com/numpy/numpy/pull/10994).  
I add support for any case where `len(axis) == rank(weights)`,  and it's the same in `tf.reduce_average` to keep compatibility with numpy.

A unit test is attached.",tensorflow
19210,achalshah20,pr,2018-05-10T21:09:19Z,Fix cublas wrap macro for cublasGemmBatchedEx,PR #18436 breaks tensorflow build for cuda 9.1. It uses PERFTOOLS_GPUTOOLS_CUBLAS_WRAP instead of STREAM_EXECUTOR_CUBLAS_WRAP. This PR fixes that issue. ,tensorflow
19574,qixiuai,pr,2018-05-26T16:29:02Z,Fix build error,"Fix build error: 'function' in namespace 'std' does not name a template type.

When build tensorflow from source on ubuntu 18.04 with g++-7.3, the error messages below appears.
Add `<functional>` header solves this error.

```
ERROR: /home/guo/Github/tensorflow/tensorflow/python/BUILD:310:1: C++ compilation of rule '//tensorflow/python:cpp_python_util' failed (Exit 1)
tensorflow/python/util/util.cc:276:16: error: 'function' in namespace 'std' does not name a template type
     const std::function<int(PyObject*)>& is_sequence_helper,
                ^~~~~~~~
tensorflow/python/util/util.cc:276:24: error: expected ',' or '...' before '<' token
     const std::function<int(PyObject*)>& is_sequence_helper,
                        ^
tensorflow/python/util/util.cc: In function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)':
tensorflow/python/util/util.cc:280:16: error: 'is_sequence_helper' was not declared in this scope
   int is_seq = is_sequence_helper(nested);
                ^~~~~~~~~~~~~~~~~~
tensorflow/python/util/util.cc:280:16: note: suggested alternative: 'IsSequenceHelper'
   int is_seq = is_sequence_helper(nested);
                ^~~~~~~~~~~~~~~~~~
                IsSequenceHelper
tensorflow/python/util/util.cc:288:8: error: 'next_values_getter' was not declared in this scope
   if (!next_values_getter(nested, &next_values)) return false;
        ^~~~~~~~~~~~~~~~~~
tensorflow/python/util/util.cc:288:8: note: suggested alternative: 'next_values'
   if (!next_values_getter(nested, &next_values)) return false;
        ^~~~~~~~~~~~~~~~~~
        next_values
tensorflow/python/util/util.cc:295:61: error: 'next_values_getter' was not declared in this scope
         FlattenHelper(item.get(), list, is_sequence_helper, next_values_getter);
                                                             ^~~~~~~~~~~~~~~~~~
tensorflow/python/util/util.cc:295:61: note: suggested alternative: 'next_values'
         FlattenHelper(item.get(), list, is_sequence_helper, next_values_getter);
                                                             ^~~~~~~~~~~~~~~~~~
                                                             next_values
tensorflow/python/util/util.cc:297:10: error: in argument to unary !
     if (!success) {
          ^~~~~~~
tensorflow/python/util/util.cc: In function 'PyObject* tensorflow::swig::Flatten(PyObject*)':
tensorflow/python/util/util.cc:472:66: error: invalid conversion from 'int (*)(PyObject*) {aka int (*)(_object*)}' to 'int' [-fpermissive]
   if (FlattenHelper(nested, list, IsSequenceHelper, GetNextValues)) {
                                                                  ^
tensorflow/python/util/util.cc:472:66: error: too many arguments to function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)'
tensorflow/python/util/util.cc:274:6: note: declared here
 bool FlattenHelper(
      ^~~~~~~~~~~~~
tensorflow/python/util/util.cc: In function 'PyObject* tensorflow::swig::FlattenForData(PyObject*)':
tensorflow/python/util/util.cc:485:41: error: invalid conversion from 'int (*)(PyObject*) {aka int (*)(_object*)}' to 'int' [-fpermissive]
                     GetNextValuesForData)) {
                                         ^
tensorflow/python/util/util.cc:485:41: error: too many arguments to function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)'
tensorflow/python/util/util.cc:274:6: note: declared here
 bool FlattenHelper(
      ^~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```",tensorflow
20328,iamaziz,pr,2018-06-27T04:29:19Z,Add a corrected link to Eager Guide,,tensorflow
21066,djsutherland,pr,2018-07-23T18:38:29Z,add Kernel Inception Distance to tf.contrib.gan.eval,"The KID is a score similar to the FID, but with an unbiased, asymptotically normal estimator. It was introduced by our paper [_Demystifying MMD GANs_](https://arxiv.org/abs/1801.01401); a very similar metric was also recommended by [_An empirical study on evaluation metrics of generative adversarial networks_](https://arxiv.org/abs/1806.07755).

For comparison to the FID, see section 4 (starting page 7) and appendices D/E (starting page 30) of our paper. As noted in the docstrings for the FID here, the FID estimator is biased and you can absolutely only compare estimates based on the same number of samples. But there's no guarantee that you still won't be very misled by the bias even then; in particular, see our Appendix D.2:

> This example thus gives a case where, for the dimension and sample sizes at which we actually apply the FID and for somewhat-realistic distributions, comparing two models based on their FID estimates will not only not reliably give the right ordering – with relatively close true values and high dimensions, this is not too surprising – but, more distressingly, will _reliably give the wrong answer_, with misleadingly small variance. This emphasizes that unbiased estimators, like the natural KID estimator, are important for model comparison.

Compared to our original code ([here](https://github.com/mbinkowski/MMD-GAN/blob/master/gan/compute_scores.py)), this version uses a slightly different estimator. Both are unbiased, but I think this block variant is more intuitive for general usage; its output is also ""more normal"" and gives a very simple estimate of the variance of the estimator (unlike the asymptotic one we used before, which is [not a very pretty expression](https://github.com/mbinkowski/MMD-GAN/blob/master/gan/compute_scores.py#L251)).

The functions that estimate the variance currently return an estimate as long as there are at least two blocks. This could be a little misleading; it might make sense to refuse to estimate the std if there are fewer than, say, 10 blocks, but I don't know if that added code complexity is worth it.

xref: https://github.com/google/compare_gan/pull/7",tensorflow
21151,cosine0,pr,2018-07-26T07:54:23Z,Fix compilation error due to typo,"Fixed compilation error below due to the typo in identifier name 'cient_mu_' -> 'client_mu_'.

ERROR: /home/cos/tensorflow/tensorflow/contrib/gdr/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/gdr:gdr_memory_manager' failed (Exit 1)
tensorflow/contrib/gdr/gdr_memory_manager.cc:175:18: error: use of undeclared identifier 'cient_mu_'; did you mean 'client_mu_'?
      GUARDED_BY(cient_mu_);
                 ^~~~~~~~~
                 client_mu_
./tensorflow/core/platform/default/thread_annotations.h:52:64: note: expanded from macro 'GUARDED_BY'
#define GUARDED_BY(x) THREAD_ANNOTATION_ATTRIBUTE__(guarded_by(x))
                                                               ^
./tensorflow/core/platform/default/thread_annotations.h:42:57: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'
#define THREAD_ANNOTATION_ATTRIBUTE__(x) __attribute__((x))
                                                        ^
tensorflow/contrib/gdr/gdr_memory_manager.cc:173:9: note: 'client_mu_' declared here
  mutex client_mu_;
        ^
tensorflow/contrib/gdr/gdr_memory_manager.cc:631:12: warning: unused variable 'checksum' [-Wunused-variable]
  uint64_t checksum = 0;
           ^
1 warning and 1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 24.192s, Critical Path: 11.63s
INFO: 26 processes: 26 local.
FAILED: Build did NOT complete successfully
",tensorflow
21552,sbrodehl,pr,2018-08-11T20:05:08Z,Adam: Fix latex text (docs).,Escape special character and add text section for comments.,tensorflow
22256,thorjohnsen,pr,2018-09-13T15:34:11Z,Option: Use fused_batch_norm instead of batch_norm for layer normalization,"tf.nn.fused_batch_norm uses cudnn for GPU acceleration, tf.nn.batch_normalization does not. This PR adds the option of using fused_batch_norm instead of batch_normalization. It is an opt-in option because fused_batch_norm only improves runtimes on GPUs and does not support double precision tensors.",tensorflow
22312,tomguluson92,pr,2018-09-17T09:12:38Z,[lite]revised a parameter error,"Hi @MarkDaoust , i found that when firstly use `interpreter `as a parameter pass into `eval_model` function, pass `interpreter_quant` two times to `eval_model` may be make some people confused.",tensorflow
22600,knightXun,pr,2018-09-28T16:43:53Z,"print error information, when the os is not supported",,tensorflow
22768,knightXun,pr,2018-10-05T14:41:03Z,remove some blank lines in README.md,remove the ending blank lines in readme.md,tensorflow
22807,marcemq,pr,2018-10-07T17:54:18Z,[Documentation] Format code block.,Signed-off-by: Marcela Morales Quispe <marcela.morales.quispe@gmail.com>,tensorflow
22808,marcemq,pr,2018-10-07T18:13:11Z,[Documentation] Format example list.,Signed-off-by: Marcela Morales Quispe <marcela.morales.quispe@gmail.com>,tensorflow
22835,knightXun,pr,2018-10-09T11:14:04Z,improve contrib/kafka/python/kernel_tests/kafka_test.sh,"1. add `docker pull` step
2. add some print message",tensorflow
23548,mrTsjolder,pr,2018-11-06T09:04:48Z,update references and unify citation style,"Refer to published work if possible.
Use (author-year) style in text and have links in `References` field of docs.",tensorflow
23675,tomguluson92,pr,2018-11-12T07:52:58Z,FastGFile deprecated warning fixed,fix `FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.` problem.,tensorflow
24039,shahzadlone,pr,2018-11-29T06:22:37Z,"Reserve because we know the size before, helps save reallocation cost.",Reserve vector in advance to save reallocation cost.,tensorflow
24113,tomguluson92,pr,2018-12-03T02:50:26Z,[Proto/decode]word order fixed,,tensorflow
24271,daniel-s-ingram,pr,2018-12-10T21:52:23Z,Add raise statement before NotImplementedError,,tensorflow
24449,epicfaace,pr,2018-12-19T15:01:53Z,Update doc references to use `tfp.distributions`,"Update all references in documentation to use `tfp.distributions` instead of `tf.contrib.distributions`, as `tf.contrib.distributions` is deprecated.",tensorflow
24678,sublee,pr,2019-01-03T08:16:02Z,"Fix quotation typo around ""task""",It just replaces inconsistent single quotation marks with backticks.,tensorflow
24688,BoboTiG,pr,2019-01-03T21:44:17Z,Fix several DeprecationWarning: invlid escape sequence,"Hello,

This little patch fixes several invalid sequence warnings.",tensorflow
24796,mrTsjolder,pr,2019-01-09T09:57:16Z,Unify citations in python docs,I took the time to go through most of the docs to get a more consistent reference style (cf. [the request](https://github.com/tensorflow/tensorflow/pull/23548#issuecomment-441763603) from @martinwicke in pull request #23548),tensorflow
24947,tomguluson92,pr,2019-01-16T03:07:22Z,fix spell mistake,,tensorflow
25079,lhendre,pr,2019-01-21T21:51:10Z,24374 Fix tf.einsum so it computes the trace correctly,"This is in relation to and fixes issue #24374.  tf.einsum is not.  Einsum is not properly calculating Trace.  This fix functions by properly identifying a trace call and utilizing math_ops.trace.  Previously It would receive an error

`Subscript not supported: an axis appears more than once: i`
or something similar.  Before that error, people with older versions were receiving the wrongly calculated value.",tensorflow
25347,shahzadlone,pr,2019-01-31T02:04:02Z,Fix variables that seem to not be modified to consts.,"Based on this when I pointed out this to be const but it was too late, it was merging soon: https://github.com/tensorflow/tensorflow/pull/25269#discussion_r252492749

Please guide me incase I can make this even better.

",tensorflow
25561,HanGuo97,pr,2019-02-06T21:19:50Z,fixed the reuse argument in Dense layer's documentation,"PR for #25393.  `reuse` is no longer a valid argument, and should be replaced with `_reuse` instead. Passing reuse to Dense will cause error.",tensorflow
26475,lhendre,pr,2019-03-08T07:36:34Z,Add skip_empty argument to tf.strings.split Issue #26368,"There were two split functions defined in TensorFlow 1.13:

tf.string_split
string_split_v2
We have consolidated these two ops into tf.strings.split in TF 2.0:

tf.strings.split(
    source,
    sep=None,
    maxsplit=-1
)
and wish to deprecate tf.string_split. However, tf.strings.split is missing the skip_empty functionality included in string_split:

tf.string_split(
    source,
    delimiter=' ',
    skip_empty=True
)
This feature request would be to include skip_empty functionality for tf.strings.split.


The code and unit tests are running.  I am currently running End to end tests and ensuring it adheres to clean standards.",tensorflow
26660,nuka137,pr,2019-03-13T10:23:24Z,GraphOptimizationPass(POST_PARTITIONING) after graph partition,"There is difference about GraphOptimizationPass between DirectSession::CreateGraphs and ProcessFunctionLibraryRuntime::InstantiateMultiDevice.
Because **POST_REWRITE_FOR_EXEC** pass is missing in the ProcessFunctionLibraryRuntime::InstantiateMultiDevice, we can not apply the optimizations.

This PR adds the **POST_REWRITE_FOR_EXEC** pass to ProcessFunctionLibraryRuntime::InstantiateMultiDevice.",tensorflow
26817,nuka137,pr,2019-03-18T04:42:08Z,Fix: typo in gpu_hlo_support_checker.h,"There is a comment typo in gpu_hlo_support_checker.h.  
This patch fixes it.",tensorflow
27636,hanton,pr,2019-04-08T12:42:33Z,Fix `cmake` build error in Android example,"1. Remove `gcc`(https://android.googlesource.com/platform/ndk/+/master/docs/ClangMigration.md)
2. Change `gnustl_static` to `c++_static` (https://developer.android.com/ndk/guides/cpp-support.html)",tensorflow
28209,domschl,pr,2019-04-27T08:36:45Z,find_cuda: fix crash caused by unicode chars in ldconfig output,"`find_cuda_config.py` crashes, if Unicode chars are found in `ldconfig` output:
```
Traceback (most recent call last):
  File ""third_party/gpus/find_cuda_config.py"", line 463, in <module>
    main()
  File ""third_party/gpus/find_cuda_config.py"", line 455, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""third_party/gpus/find_cuda_config.py"", line 418, in find_cuda_config
    _get_default_cuda_paths(cuda_version))
  File ""third_party/gpus/find_cuda_config.py"", line 159, in _get_default_cuda_paths
    ] + _get_ld_config_paths()
  File ""third_party/gpus/find_cuda_config.py"", line 139, in _get_ld_config_paths
    match = pattern.match(line.decode(""ascii""))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 27: ordinal not in range(128)
Asking for detailed CUDA configuration...
```
Localized versions of `ldconfig` versions use Unicode chars, e.g.: `»/etc/ld.so.cache«`, causing the crash.

The fix simply skips lines with Unicode chars.",tensorflow
29416,Kab1r,pr,2019-06-04T21:03:10Z,[TF 2.0 API Docs] Added Returns and Raises sections to tf.image.convert_image_dtype,Added missing documentation sections. #29406,tensorflow
29451,Kab1r,pr,2019-06-05T19:45:01Z,"Added missing documentation sections to make_saveable_from_iterator, description corrected.","Added returns and raises section to make_saveable_from_iterator function #29406

@rthadur 
Pull request is now against master.
Recommitted for easier merging into master.",tensorflow
29514,Kab1r,pr,2019-06-06T19:40:14Z,[TF 2.0 API Docs] Added examples to docs for is_jpeg() and is_png(),Examples were missing for the tf.io functions `is_jpeg()` and `is_png()`,tensorflow
30269,knightXun,pr,2019-07-01T08:45:16Z,Fix two line comments error,A little change to fix two line comments error.No change to the real code.,tensorflow
30572,csarron,pr,2019-07-10T17:40:58Z,register BatchMatMul op flops,"this commit adds support for BatchMatMul flops calculation,  and it should solve #22071",tensorflow
30575,csarron,pr,2019-07-10T18:33:30Z,register BatchMatMul op flops,"this commits addresses issue #22071, and add flops calculation for BatchMatMul op. The only change from the MatMul op is that the intermediate k should be from the second to last dim of the a_shape.",tensorflow
31048,nuka137,pr,2019-07-25T22:53:34Z,Fix: typo in document xla/g3doc/jit.md,,tensorflow
31129,AnthonyBarbier,pr,2019-07-29T13:28:08Z,Introduce the concept of Frontend Attributes.,"Summary:
    Frontend Attributes can be set by the user or the frontend and
    are passed through to the XLA backend as a dictionary of strings where they can
    be used to modify the way the HLO instructions are executed.

XLA Development discussion:
    https://groups.google.com/d/msg/xla-dev/9TM0-1N_JlM/Q2R8o2RgBwAJ

Test Plan:
    bazel test returned:
    INFO: Executed 522 out of 522 tests: 522 tests pass.
    INFO: There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.
    INFO: Build completed successfully, 4027 total actions
    SUCCESS!",tensorflow
31211,dan-zheng,pr,2019-07-31T20:41:29Z,Fix documentation commment for Svd.,`tensor containing of left singular vectors` -> `tensor containing the left singular vectors`.,tensorflow
31512,dan-zheng,pr,2019-08-10T19:31:03Z,Remove executable mode from `tensorflow/c/eager/c_api.h`.,"This fixes a codesigning issue for [Swift for TensorFlow](https://github.com/apple/swift/tree/tensorflow) on macOS:
```console
# After this patch: https://github.com/apple/swift/pull/26595.
$ ./swift/utils/build-toolchain-tensorflow --pkg
...
error: cannot parse the debug map for './/Library/Developer/Toolchains/swift-tensorflow-LOCAL-2019-08-09-a.xctoolchain/usr/Library/Frameworks/LLDB.framework/Versions/A/Resources/Swift/macosx/x86_64/modulemaps/CTensorFlow/c_api_eager.h-e': The file was not recognized as a valid object file
error: cannot parse the debug map for './/Library/Developer/Toolchains/swift-tensorflow-LOCAL-2019-08-09-a.xctoolchain/usr/Library/Frameworks/LLDB.framework/Versions/A/Resources/Swift/macosx/x86_64/modulemaps/CTensorFlow/c_api_eager.h': The file was not recognized as a valid object file
```",tensorflow
31745,take-cheeze,pr,2019-08-19T01:33:29Z,Fix link to XLA documentation,,tensorflow
31777,leslie-fang-intel,pr,2019-08-19T23:45:44Z,[Intel MKL] Fix the issue which fails to create memory descriptor in mkl concat,"Fix the issue: https://github.com/tensorflow/tensorflow/issues/30383 which fails to create mkl_concat's memory descriptor in blocked format
",tensorflow
32126,titaneric,pr,2019-08-31T05:13:02Z,Rewrite some code segment to be more elegant and Pythonic,,tensorflow
32411,timgates42,pr,2019-09-11T04:42:34Z,Fix simple typo: activiations -> activations,,tensorflow
32486,rgomathi,pr,2019-09-13T05:55:06Z,[INTEL MKL] Enabled MIN_FIRST support and primitive caching for MKL-DNN Quantize OP,"This PR will enable
1. Min first mode support in the MKL-DNN quantize Op and
2. Primitive Caching for the same Op

Code changes involve
1. Rewriting the rules in the mkl_layout_pass.cc to allow mkl-dnn quantize function being called for MIN_FIRST mode as well
2. Appropriate code added in mkl_quantize_op.cc to support MIN_FIRST mode and primitive caching
3. Added 2 more tests in mkl_quantize_op_test.cc specific to MIN_FIRST mode.",tensorflow
32591,songyouwei,pr,2019-09-17T13:52:28Z,fix typo in pywrap_tfe_src.cc,,tensorflow
32834,rgomathi,pr,2019-09-26T08:47:31Z,[INTEL MKL] Add Weight caching in MKL Quantized Matmul Op,"This PR adds weight buffer caching that improves the performance in scenarios where the weight buffer format is not same as the MKL-DNN expected format. This is achieved by reordering the weight for the first time and caching the same for re-using it in subsequent iterations. 
",tensorflow
32856,kbrose,pr,2019-09-27T03:01:09Z,"Fix tf.image.*_jpeg_quality docs -- they only accept single image, docs say multiple","* Update `tf.image.adjust_jpeg_quality` and `tf.image.random_jpeg_quality` docs to state that only one image (3D, not 4D) can be passed in.
* Update `tf.image.random_jpeg_quality` to clarify it can also accept a 1-channel input, instead of just 3-channel..

Python code showing the one-image restraints, and allowing of 1-channel input:

```python
>>> import tensorflow as tf
>>> tf.__version__
'2.0.0-rc2'
>>> import numpy as np
>>> tf.image.adjust_jpeg_quality(np.random.random((10, 100, 100, 3)), 75).numpy().shape
<extended traceback>
tensorflow.python.framework.errors_impl.InvalidArgumentError: image must be 3-dimensional[10,100,100,3] [Op:EncodeJpegVariableQuality]
>>> tf.image.random_jpeg_quality(np.random.random((10, 100, 100, 3)), 75, 95).numpy().shape
<extended traceback>
tensorflow.python.framework.errors_impl.InvalidArgumentError: image must be 3-dimensional[10,100,100,3] [Op:EncodeJpegVariableQuality]
>>> tf.image.random_jpeg_quality(np.random.random((100, 100, 1)), 75, 95).numpy().shape
(100, 100, 1)
```",tensorflow
33466,AnthonyBarbier,pr,2019-10-17T14:13:57Z,Combine adjacent concatenate operations,"In the algebraic simplifier: look for adjacent concatenate operations
and fuse them if they are compatible.",tensorflow
33699,luxe,pr,2019-10-24T20:01:55Z,suppress unused result in s3_filesystem (-Wunused-result),"Building tensorflow with additional warnings as errors.  Suppress the warning to build the library.

```
error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
```",tensorflow
33970,AnthonyBarbier,pr,2019-11-04T12:00:58Z,Force psutil version to 5.6.3 for python2,"The latest (5.6.4) is broken on python2:
```
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support
Collecting psutil
  Using cached https://files.pythonhosted.org/packages/47/ea/d3b6d6fd0b4a6c12984df652525f394e68c8678d2b05075219144eb3a1cf/psutil-5.6.4.tar.gz
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... error
    ERROR: Command errored out with exit status 1:
     command: /localdata/anthonyb/workspace/external/tf_python_python2/bin/python2 /localdata/anthonyb/workspace/external/tf_python_python2/local/lib/python2.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpbqQ5kk
         cwd: /tmp/pip-install-Rk4IAs/psutil
    Complete output (2 lines):
    running dist_info
    error: 'egg_base' must be a directory name (got )
```",tensorflow
34212,leslie-fang-intel,pr,2019-11-13T02:29:19Z,[INTEL MKL] parallel implementation of the resize nearest neighbor op,"The origin resize_nearest_neighbor_op is running the resize operation in serial and here is the benchmark test result:
```
Executing tests from //tensorflow/core/kernels:resize_benchmark_test
-----------------------------------------------------------------------------
2019-11-12 11:50:12.127157: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
Running main() from test_main.cc
Benchmark                                        Time(ns) Iterations
--------------------------------------------------------------------
BM_Resize_ResizeNearestNeighbor_cpu_10_499_499   48277810        100     154.7M items/s
BM_Resize_ResizeBilinear_cpu_10_499_499          39131850        100     190.9M items/s
```

Here we modified this operation into parallel implementation and here is the new benchmark test result:
```
Executing tests from //tensorflow/core/kernels:resize_benchmark_test
-----------------------------------------------------------------------------
2019-11-12 11:54:53.781602: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
Running main() from test_main.cc
Benchmark                                        Time(ns) Iterations
--------------------------------------------------------------------
BM_Resize_ResizeNearestNeighbor_cpu_10_499_499    5076820        100     1471.4M items/s
BM_Resize_ResizeBilinear_cpu_10_499_499          38758780        100     192.7M items/s
```

Here we see almost **10x** performance improvement with this PR.",tensorflow
34467,nuka137,pr,2019-11-20T22:14:48Z,Fix: typo in tensorflow/compiler/xla/g3doc/shapes.md,,tensorflow
35091,yxsamliu,pr,2019-12-13T15:41:19Z,[GPU][ROCm] Fix hip-clang build,"This patches fixes a bug which causes build failure for hip-clang.

Basically in third_party/gpus/rocm_configure.bzl a string is checked to determine whether the compiler is hip-clang. Comparison fails due to new line not being stripped. This patch strips new line so that comparison succeeds.",tensorflow
35231,yxsamliu,pr,2019-12-18T20:20:13Z,[r1.15 Cherrypick] Fix hip-clang build,Remove new line before comparing string.,tensorflow
35776,hugovk,pr,2020-01-11T16:14:13Z,Fix version comparison for Python 3.10 and 4,"### Fix for Python 3.10: use `sys.version_info` instead of comparing `sys.version` to string

Don't assume the minor version is a single character: `'3.10' >= '3.4'` is `False`!

---

### Fix for Python 4: replace unsafe `six.PY3` with `six.PY2`

There's some code which uses `six.PY3`:

```python
if six.PY3:
    print(""Python 3+ code"")
else:
    print ""Python 2 code"" 
```

Where:

```python
PY3 = sys.version_info[0] == 3
```

When run on Python 4, this will run the Python 2 code!

Instead, use `six.PY2`.

---

Found using https://github.com/asottile/flake8-2020.

",tensorflow
36230,henryhjung,pr,2020-01-27T04:17:16Z,Fix spelling errors,,tensorflow
36305,AnthonyBarbier,pr,2020-01-29T10:38:52Z,Clear caches on eager context reset.,"Clear the caches of the existing eager context before deleting it as some of these caches are cpp static (TFE_TensorHandleCache) and will therefore be inherited by future contexts with some out of date content which might lead to some segfaults.

For example in these Keras tests some constant tensors generated by the `add_weights` function inside the convolutions are re-used across tests leading to some segfault because the eager context has been reset between tests and the device they point to no longer exist.

[sequential_test.py.txt](https://github.com/tensorflow/tensorflow/files/4127467/sequential_test.py.txt)

PS: The eager context reset for these tests is needed in order to clear the XLA executable cache between tests.",tensorflow
36886,Officium,pr,2020-02-19T09:41:48Z,[Docs] update mathjax for lbeta,"This PR fixes some issues in the document of `tf.math.lbeta`, e.g., missing a `\` before `int`. Besides, the style of characters that appear at both interline and inline formulas should be consistent.  ",tensorflow
37281,Ir1d,pr,2020-03-04T06:06:06Z,"docs: add examples for random_uniform, random_normal and random_binomial in tf.keras.backend",closes #31277,tensorflow
37519,rajanksin,pr,2020-03-11T23:18:31Z,Update func_graph.py,"Fixing error message. removed ""contrib"" usage.",tensorflow
37849,svenstaro,pr,2020-03-24T02:47:24Z,Fix typos in .bazelrc,`arch_native_linux` is actually called `native_arch_linux` down below in the `.bazelrc` where it's actually being consumed.,tensorflow
37850,svenstaro,pr,2020-03-24T02:50:59Z,Lower case c++1z config,The `.bazelrc` actually checks for `c++1z` and not `C++1z` (mind the capital `C`) so this should be accounted for in the docs.,tensorflow
38258,ganler,pr,2020-04-06T05:27:23Z,Add float16 to _TENSOR_CONTENT_TYPES in tf.make_tensor_proto,"## Overview

Recently I was doing model serving using tf serving. And my model(which is also the official model provided by tensorflow, [LINK](https://github.com/tensorflow/models/tree/master/official/r1/resnet#pre-trained-model)) takes `float16` as input data type. And I found great sterilization overhead using input tensor with such data types. And I found that in `tensorflow/src/python/framework/tensor_util.py`, the `make_tensor_proto` didn't store the `float16` data in `tensor_content`, but in `repeated` fashion. Hence, I wonder if we should allow `float16` data type to be included in `_TENSOR_CONTENT_TYPES` of `tf.make_tensor_proto`(which is an API that most users are using for networking transmission) for the performance of such scenarios.

## Performance

Using the default `tf.make_tensor_proto`, sending a [16, 224, 224, 3] tensor costs 280 ms(75% time used in serialization and memory copy).

![fp16_](https://user-images.githubusercontent.com/38074777/78525507-ee85b400-7809-11ea-8655-4aa64aa9917b.png)

After manually set the input data into `tensor_content`, a huge performance boost occurred. It only takes 60 ms and the overhead proportion of serialization is less than 2%.

![after](https://user-images.githubusercontent.com/38074777/78525589-2d1b6e80-780a-11ea-8563-89bc4c8020ac.png)
",tensorflow
38472,knightXun,pr,2020-04-12T14:04:34Z,perfer python3 to compile,`which python` may direct to python2. make `which python3` ahead of `which python` may be better.,tensorflow
38952,lanpa,pr,2020-04-27T17:34:46Z,Fix doc for tf.debugging.enable_check_numerics,This make sample code agrees with the explanation.,tensorflow
39095,Rishit-dagli,pr,2020-05-02T03:56:10Z,Addition in resources section,Added Coursera course Machine Learning with TensorFlow on GCP,tensorflow
39220,Officium,pr,2020-05-06T09:39:44Z,fix ops.Tensor initializer in keras.initializers.get,"This PR allows keras.initializers.get method deal with ops.Tensor initializer correctly, and fixes the following issue.

In partitioned variable cases, like distributed training, `constant_initializer` do not parse partitioned_info in call stage, where an ops.Tensor initializer will lead expected results (see line 746 in tensorflow/tensorflow/python/ops/variable_scope.py for more details). But the following code leads a ValueError.

```python
>>> import tensorflow as tf
>>> x = tf.constant([[1], [2]])
>>> layer = tf.keras.layers.Dense(3, kernel_initializer=tf.constant([[1]]))
>>> y = layer(x)
ValueError: Could not interpret initializer identifier: tf.Tensor([[1]], shape=(1, 1), dtype=int32)
```",tensorflow
39285,leslie-fang-intel,pr,2020-05-07T23:37:36Z,[INTEL MKL] Fix dequantize op regression issue,"To support the target type of bfloat16, the dequantize_op always transfer the int8 data into the fp32 then cast to target type of bf16 or fp32.
If the target type is FP32, we can skip the cast operation in this PR.",tensorflow
40181,nihui,pr,2020-06-05T08:50:19Z,tf.StridedSlice op doc fix,,tensorflow
40511,Meteorix,pr,2020-06-16T15:40:15Z,fix LaunchDepthwiseConvBackpropFilterOp,fix this issue with depthwise conv https://github.com/tensorflow/tensorflow/issues/27285,tensorflow
40828,Rishit-dagli,pr,2020-06-26T12:15:25Z,TF Chat Room,Added the [TensorFlow Chat Room on Stackoverflow](https://chat.stackoverflow.com/rooms/216694/tensorflow) URL in the resources section.,tensorflow
40970,Rishit-dagli,pr,2020-07-01T08:09:02Z,"Added ""Get Started with CNNs"" example","Added a ""[Getting Started with CNN](https://github.com/Rishit-dagli/tensorflow/tree/get-started-with-cnn/tensorflow/examples/get_started/get-started-with-cnn)"" example under the [get_started](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/get_started) section. I have made sure that this example is strictly a getting started example with TensorFlow 2.x .

The example contains-

- What is a CNN?
- Loading the data
  * The Fashion MNIST dataset
- Some preprocessing
- Understanding Filters and Convolutions
- Performing simple feature extraction with filters
  * Identifying Vertical lines
- Understanding Pooling
- Implementing Convolutional layers in TensorFlow
  * Convolutions
  * Pooling
  * Accuracy and loss curves
  * Test accuracy
- Visualizing the Convolutions and Pooling
- Save the model for future use",tensorflow
41199,ZhuBaohe,pr,2020-07-08T13:32:04Z,Fix  categorical_crossentropy docstring,This PR fixes the docstring of tf.keras.backend.categorical_crossentropy method.,tensorflow
42095,acxz,pr,2020-08-06T13:11:54Z,add /hip suffix to find hip path,"Resolves #42081 
Fix courtesy of @Imajie

This PR helps move along the build when building tensorflow from source.",tensorflow
42292,acxz,pr,2020-08-13T00:54:21Z,fix path of hipcc to match rocm packaging,"Resolves #42291 

This PR helps move along the build when building tensorflow from source.",tensorflow
42591,sclarkson,pr,2020-08-23T00:01:18Z,Fix compiling against system protobuf,"When trying to compile with the bazel configuration `build --action_env TF_SYSTEM_LIBS=""com_google_protobuf""`, the following errors occur.

```
ERROR: /home/sclarkson/tensorflow-master/tensorflow/core/data/service/BUILD:31:17: no such target '@com_google_protobuf//:any_proto': target 'any_proto' not declared in package '' defined by /home/sclarkson/.cache/bazel/_bazel_sclarkson/fffe72c6b1157d70fb0a456ab7b675c2/external/com_google_protobuf/BUILD.bazel and referenced by '//tensorflow/core/data/service:dispatcher_proto'
```

This PR adds the missing protobuf library definitions to successfully compile against the system protobuf.",tensorflow
44484,chosungmann,pr,2020-10-31T18:13:22Z,Fix the absl module not found error,"ToT workarounds the absl module not found error by using $CMAKE_MODULE_PATH,
but this is still likely to fail due to the fact that $CMAKE_MODULE_PATH is
a semicolon-separated list of directories specifying a search path for CMake
modules. In fact, if we set more than one path to $CMAKE_MODULE_PATH, the absl
module not found error occurs. This patch solves the problem by give |absl_DIR|
the unique CMake module path in the source tree.",tensorflow
45189,VivekPanyam,pr,2020-11-25T21:18:30Z,Always build libtensorflow in a manylinux2010 compatible way,"Previously, only the CUDA builds of `libtensorflow` for TF 2.x would be built in a manylinux2010 sysroot.

This PR builds the CPU version in a manylinux2010 sysroot as well.

Closes #45092",tensorflow
45595,hrw,pr,2020-12-11T07:26:58Z,README: update Linaro CI links,"We have two CI jobs now:

- Tensorflow stable (1.15.x and 2.x)
- Tensorflow nightly (git master HEAD)

Builds are done on CentOS 8 (Python 3.6) and Debian 'buster' (Python 3.7).",tensorflow
45951,yqtianust,pr,2020-12-24T01:21:06Z,fix the hyperlink in the doc,the text and link are reversed.,tensorflow
45989,eltociear,pr,2020-12-27T13:00:53Z,Fix some message typos,absense -> absence,tensorflow
46651,blackyang,pr,2021-01-25T06:48:53Z,Update load.py,"Simple fix for keras model load for raggedTensor. Otherwise there is `AttributeError` about `raggedTensorSpec` not having `name`

",tensorflow
46894,NathanHowell,pr,2021-02-03T19:47:47Z,Automatically treat dataclasses as pytrees,"This change enables dataclasses to be used in pytrees. The relevant tests are in jax: https://github.com/google/jax/pull/5618.

Fixes google/jax#2371",tensorflow
47645,zenogantner,pr,2021-03-08T14:53:18Z,fix **highlighting** of remarks in docstrings,"This was reported in issue #47642. Two closing ""**"" added.

CONTRIBUTING.md says ""As every PR requires several CPU/GPU hours of CI testing, we discourage submitting PRs to fix one typo, one warning,etc. We recommend fixing the same issue at the file level at least (e.g.: fix all typos in a file, fix all compiler warning in a file, etc.)"", so I checked the rest of the file for typos and did not find any. Added two missing periods in docstrings, though.",tensorflow
47796,zenogantner,pr,2021-03-14T11:36:30Z,my changes => your changes,For consistency with the rest of the file's content.,tensorflow
47873,zenogantner,pr,2021-03-17T22:11:50Z,modernize Python in losses.py and its tests," - Do not explicitly inherit from `object`.
 - Simplify `super()` calls.
 - Remove `future` imports.
 - Remove uses of `six`.
 - Replace `.format()` with f-strings.",tensorflow
49308,hrw,pr,2021-05-19T15:55:41Z,Fix build on AArch64,"Execution platform: @local_execution_config_platform//:platform
external/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp:45:10: fatal error: AArch64GenO0PreLegalizeGICombiner.inc: No such file or directory
   45 | #include ""AArch64GenO0PreLegalizeGICombiner.inc""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

Closes #49136",tensorflow
50548,rainwoodman,pr,2021-06-30T18:42:20Z,Describe the behavior on the false branch.,Per discussion with @wangpengmit.,tensorflow
50565,rainwoodman,pr,2021-07-01T16:33:24Z,Be explicit about the type signature of map_func,"Highlight tf.data.Dataset such that the reader doesn't miss it. The typing was mentioned at the top of the doc but not immediately visible at the params table.

Motivation for this change is that after studying this doc, a user still came up with code calling `interleave` with a function that takes element and returns element.",tensorflow
50913,rainwoodman,pr,2021-07-22T22:39:10Z,Mention intro-to-graphs in tf.function docstring.,Making it more discoverable.,tensorflow
51450,sclarkson,pr,2021-08-12T10:35:10Z,Fix protobuf errors when using system protobuf,"When tensorflow and python protobuf use the same instance of
libprotobuf, pywrap_tensorflow must be imported before anything
else that would import protobuf definitions.

Fixes #50545. See that thread for a Dockerfile with a reproduction
of the error.",tensorflow
51452,sclarkson,pr,2021-08-12T11:55:07Z,Only preload kernels from running TF instance,"Previously, kernels from every installed instance of TensorFlow would be
preloaded, causing potential ABI conflicts.

Fixes #51451",tensorflow
51782,KumaTea,pr,2021-09-01T10:32:08Z,`distutils` is deprecated in Python 3.10 #51776,"The `distutils` is deprecated in Python 3.10.

As of #51776, In `python_configure.bzl`, the deprecation message will be printed prior to the include path, causing error on return.",tensorflow
51830,tlemo,pr,2021-09-03T19:03:20Z,Add support for fp16 GEMM BEF thunks,"The corresponding TFRT PR is https://github.com/tensorflow/runtime/pull/79
",tensorflow
52570,H1Gdev,pr,2021-10-19T06:58:20Z,[NNAPI]Fix error in axis of reduce_mean.,"1. create TensorFlow model.
```python
tf.reduce_mean(x, 0)
```
1. convert to TensorFlow lite model.
1. execute in NNAPI delegate.
1. error occurred.
   `E/tflite: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1465 while setting new operand value from memory for tensor XXX.`",tensorflow
52593,H1Gdev,pr,2021-10-20T13:15:12Z,[NNAPI]call memcpy() with null pointer dereference.,"1. create `initialize.tflite` with [model_personalization](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization) `tflite-transfer-convert`.
1. execute delegate with the following option.
   ```cpp
   tflite::StatefulNnApiDelegate::Options options;
   options.accelerator_name = ""nnapi-reference""; // use CPU
   auto delegate = new tflite::StatefulNnApiDelegate(options);
   ```
1. fatal error has occurred.
   ```txt
   signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
   Cause: null pointer dereference
   ```
1. `nn_input_memory_->get_data_ptr()` returns `nullptr`.
   - [total_input_byte_size](https://github.com/tensorflow/tensorflow/blob/e042c277ab6375bf4060e099dd20a101cc0685e0/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L5700-L5701) == 0",tensorflow
52949,tlemo,pr,2021-11-04T23:37:50Z,Add support for GEMMs with complex element types,,tensorflow
53269,frgfm,pr,2021-12-01T10:50:13Z,docs: Removed incorrect note in ctc_beam_search_decoder,"Hello there :wave: 

As mentioned in #21051, there is an issue with the mention in `tf.nn.ctc_beam_search_decoder` claiming that in a special case, it's the same as the greedy decoder. Both from a pure code & theoretical point of view, I would argue that this claim is wrong.

This PR simply removes the incorrect note to fix the documentation :ok_hand: 

Any feedback is welcome!

cc @mihaimaruseac @tatianashp",tensorflow
53308,hmaarrfk,pr,2021-12-05T16:38:01Z,Remove wheel requirement as it is not needed for the pip package,"https://github.com/simonpercivall/astunparse/pull/65

```
astunparse $ for tag in v1.6.0 v1.6.1 v1.6.2 v1.6.3; do git checkout $tag; echo $tag; grep wheel . -R --exclude-dir=.git; done
Previous HEAD position was d7ba156 Bump version
HEAD is now at df08255 Bump version and add HISTORY entry.
v1.6.0
./setup.cfg:[wheel]
./Makefile:	python setup.py bdist_wheel
./requirements.txt:wheel >= 0.23.0, < 1.0
Previous HEAD position was df08255 Bump version and add HISTORY entry.
HEAD is now at c73b675 Bump version.
v1.6.1
./setup.cfg:[wheel]
./Makefile:	python setup.py bdist_wheel
./requirements.txt:wheel >= 0.23.0, < 1.0
Previous HEAD position was c73b675 Bump version.
HEAD is now at d7ba156 Bump version
v1.6.2
./setup.cfg:[wheel]
./Makefile:	python setup.py bdist_wheel
./requirements.txt:wheel >= 0.23.0, < 1.0
Previous HEAD position was d7ba156 Bump version
HEAD is now at 2acce01 Merge pull request #42 from simonpercivall/merge/python3.8-support
v1.6.3
./setup.cfg:[wheel]
./Makefile:	python setup.py bdist_wheel
./requirements.txt:wheel >= 0.23.0, < 1.0
```",tensorflow
53363,chenrui333,pr,2021-12-09T03:10:39Z,build: bump bazel to 4.2.2,,tensorflow
53418,103yiran,pr,2021-12-14T09:44:01Z,Use the same variable,,tensorflow
53419,103yiran,pr,2021-12-14T09:52:48Z,Use the same variable,,tensorflow
53573,JimEverest,pr,2021-12-29T13:20:36Z,fixed some typos in the security markdown,,tensorflow
53574,mathemage,pr,2021-12-29T14:16:43Z,Update post_training_quantization.md,Fix Markdown formatting for the bullet points of use cases,tensorflow
53637,code-review-doctor,pr,2022-01-04T23:00:06Z,Add missing comma,"As per  https://github.com/tensorflow/tensorflow/issues/53636

Note I found this issue while integration testing some new checks we've written. I'm a GitHub code review bot and part of my integration testing of new checks includes running against 1000 codebases (this one included).

If you're interested: you can [add me to GitHub](https://github.com/marketplace/django-doctor/) so I review PRs automatically and prevent issues like this being merged in the first place :)",tensorflow
54120,sighingnow,pr,2022-01-27T02:17:57Z,Requires a newer version of the libclang package.,"We recently made some mistakes during maintaining the package `libclang` and the version v12.0.0 has been broken for some package managers like poetry (see issue https://github.com/sighingnow/libclang/issues/19).

Older versions of libclang has bugs about installed unintented files to users' `site-packages` directory.

This PR force the requirement of libclang to v13.0.0 to fix above issues.
",tensorflow
54192,mrshu,pr,2022-01-28T23:29:13Z,Fix missing commas in `excluded_ops` list.,Fix missing commas in `excluded_ops` list.,tensorflow
55597,rainwoodman,pr,2022-04-12T23:18:37Z,Mention DTensor in 2.9 release notes,,tensorflow
55726,code-review-doctor,pr,2022-04-24T21:14:46Z,Missing `f` prefix on f-strings fix,Fixes #55723,tensorflow
56354,cliffwoolley,pr,2022-06-03T23:21:28Z,Updated find_cuda_config.py.gz.base64,"Now matches https://github.com/tensorflow/tensorflow/pull/56246

Signed-off-by: Cliff Woolley <jwoolley@nvidia.com>",tensorflow
56392,cliffwoolley,pr,2022-06-07T16:22:59Z,[TF-TRT] Simplify redundant TensorRT version stubs,"Most of the library stubs for various TensorRT versions are actually identical, so there's little point in having a new set of stubs for every single TensorRT release.  Doing so unnecessarily complicates the adoption of newer TRT releases that retain API compatibility with earlier releases, which they generally do.

This change removes the redundant stubs, reducing to a minimal set for the supported version ranges.

Note: it's likely that even TensorRT versions >= 9.0, whenever such a thing might come to exist, could also reuse all or part of these same stubs, but since it's more difficult to predict this across major version boundaries, I retained the previous behavior of allowing only known _major_ releases, while relaxing the previous behavior that unnecessarily limited us to only known _minor_ releases.

This allows successful compilation with TensorRT 8.4.",tensorflow
56627,rainwoodman,pr,2022-06-29T20:35:38Z,Describe `tf.squeeze`'s None shape behavior.,,tensorflow
56870,Sadeedpv,pr,2022-07-23T12:45:25Z,Fix markdown files in security.md,Corrected spelling errors and formatted sentences,tensorflow
